arxiv_id,pdf_path,question,answer,content
1608.02315,D:\Database\arxiv\papers\1608.02315.pdf,"In the context of learning the structure of a Markov network, how does the proposed method address the limitations of existing approaches that rely on decomposing the posterior probability of the structure into independent components?","The proposed method, known as the Blankets Joint Posterior (BJP) scoring function, relaxes the independence assumption between Markov blankets by formulating the posterior probability of the structure as the joint posterior probability of the blankets, taking into account the conditional dependencies between them. This allows for a more accurate representation of the structure and potentially improves the efficiency of learning complex networks.","training data set D, ﬁnd an undirected graph G⋆such that
G⋆= argmax
G∈GPr(G|D), (1)
where Pr(G|D) is the posterior probability of a structure given D, andGis the
familiy of all the possible undirected graphs for the domain size. This c lass of
algorithms has been shown to outperform constraint-based algor ithms in the
quality of the learned structures, with equivalent computational c omplexities.
The method proposed in this paper follows this approach.
Since there are no feasible exact methods for computing the poste rior of
MN structures, diﬀerent approximations have been proposed. An important as-
sumption commonly made by the current state-of-the-art metho ds is to suppose
that the posterior of the structure is decomposable [29, 30, 3, 28 , 19]. It means
that the whole posterior can be computed as a product of the post eriors of the
Markov blankets that compose the structure, which are smaller po steriors that
can be computed independently. In fact, this is a good approximatio n that im-
provesthe eﬃciency of search. The researchline of this workaims a t designinga
better approximation to the posterior, by relaxing such independe nce assump-
tion. For this, the contribution of this work is the Blankets Joint Posterior
(BJP), a scoring function that poses Pr( G|D) as the joint posterior probabil-
ity of the Markov blankets of G. This is achieved by formulating Pr( G|D) in
a novel way that relaxes the independence assumption between th e blankets.
Essentially, the whole posterior is obtained by computing the poster ior of the
blanket of each variable as a conditional distribution that takes into account
information from other blankets in the network. In the experiment s we show
that the proposedapproximationcan improvethe samplecomplexity ofstate-of-
the-art scores when learning networks with complex topologies, th at commonly
appear in real-world problems.
After providing some preliminaries, notations and deﬁnitions in Sectio n 2,
we introduce the BJP scoring function in Section 3. Section 4 presen ts the
experimental results for several study cases. Finally, Section 5 s ummarizes this
work, and poses several possible directions of future work.
4
2. Background
We begin by introducing the notation used for MNs. Then we provide s ome
additional background about these models and the problem of learn ing their
independence structure, and also discuss the state-of-the-ar t of MN structure
learning.
2.1. Markov networks
HaveVas a ﬁnite set of indexes, lowercasesubscripts for denoting partic ular
indexes, e.g., i,j∈V, and uppercase subscripts for subsets of indexes, e.g.,
W⊆V. LetXVbe the set of random variables of a domain, denoting single
variables as single indexes in V, e.g.,Xi,Xj∈XVwheni,j∈V. For a MN
representing a probability distribution P(XV), its two components are denoted
as follows:G, andθ.Gis the structure, an undirected graph G= (V,E) where
the nodesV={0,...,n−1}are the indices of each random variable Xiof the
domain, and E⊆ {V×V}is the edge set of the graph. A node iis a neighbor
ofjwhen the pair ( i,j)∈E. The edges encode direct probabilistic inﬂuence
between the variables. Similarly, the absence of an edge manifests t hat the
dependence could be mediated by some other subset of variables, c orresponding
to conditional independences between these variables.
A variableXiis conditionally independent of another non-adjacent variable
Xjgiven a set of variables XZif Pr(Xi|Xj,XZ) = Pr(Xi|XZ). This is
denoted by ⟨Xi⊥Xj|XZ⟩(or⟨Xi̸⊥Xj|XZ⟩for the dependence assertion). As
proven by [31], the independences encoded by Gallow the decomposition of
the joint distribution into simpler lower-dimensional functions called f actors, or
potential functions. The distribution can be factorized as the pro duct of the
potential functions φc(Vc) over each clique Vc(i.e., each completely connected
sub-graph) of G, that is
P(V) =1
Z∏
c∈cliques(G)φc(Vc), (2)
whereZis a constant that normalizes the product of potentials. Such pote ntial
functions are parameterized by the set of numerical parameters θ.
5
For each variable Xiof a MN, its Markov blanket is composed by the set of
all its neighbor nodes in the graph. Hereon we denote the blanket of a variable
XiasBXi. An important concept that is satisﬁed by MNs is the Local Markov
property, formally described as:
Local Markov property . A variable is conditionally independent of all
its non-neighbor variables given its MB. That is
⟨Xi⊥ {XV\BXi}|BXi⟩. (3)
By using such property, the conditional independences of P(XV) can be read
from the structure G. This is done by considering the concept of separability.
Each pair of non-adjacent variables ( Xi,Xj) is said to be separated by a set
of variables XZ⊆XV\ {Xi,Xj}when every path between XiandXjinG
contains some node in XZ[1].
In machine learning, statistical independence tests are a well-know n tool to
decide whether a conditional independence is supported by the dat a. Examples
of independence tests used in practice are Mutual Information [32 ], Pearson’s
χ2andG2[33], the Bayesian statistical test of independence [34], and the Par -
tial Correlation test for continuous Gaussian data [20]. Such test s require the
construction of a contingency table of counts for each complete c onﬁguration of
the variables involved; as a result, they would have an exponential c ost in the
number of variables [35]. For this reason, the use of the local Mark ov property
has a positive eﬀect for learning independence structures, allowing the use of
smaller tests. Accordingly, the BJP score introduced in this work ta kes advan-
tage of this property by computing a set of conditional probabilities that are
more reliable and less expensive.
2.2. MN structure learning
The MN structure is learned from a training dataset D={D1,...,Dd},
assumed to be a representative sample of the underlying distributio nP(XV).
Commonly, Dhas a tabular format, with a column for each variable of the
6
domainXV, andonerowperdatapoint. Thisworkassumesthateachvariableis
discrete, with a ﬁnite number of possible values, and that no data po int inDhas
missing values. As mentioned in the introduction, this work focuses o n methods
for computing Pr( G|D). For this reason, in this subsection we discuss two
recently proposed scoring functions that approximate it: the Mar ginal Pseudo
Likelihood (MPL) score [19], and the Independence-based score (I B-score) [28].
In MPL, each graph is scored by using an eﬃcient approximation to th e
posterior probability of structures given the data. This score app roximates the
posteriorby considering P(G|D)∝P(D|G)×P(G). Since the datalikelihood
of the graph, P(D|G), is in general extremely hard to evaluate, MPL utilizes
the well-known approximation called the pseudo-likelihood [36]. This sco re was
provedtobeconsistent, thatis, inthelimit ofinﬁnitedatathesolutio nstructure
hasthe maximum score. Forﬁnding the MPL-optimalstructure, tw oalgorithms
were presented: an exact algorithm using pseudo-boolean optimiza tion, and a
fast alternative to the exact method, which uses greedy hill-climbing with near-
optimalperformance. Thisalgorithmlearnstheblanketforeachva riable,locally
optimizing the MPL for each node, independently of the solutions of t he other
nodes. For this, it uses an approximate deterministic hill-climbing proc edure
similartothewell-knownIAMBalgorithm[37]. Finally, aglobalgraphdisco very
method is applied by using a greedy hill-climbing algorithm, searching for the
structure with maximum MPL score, but only restricting the search space to
the conﬂicting edges.
The independence-based score (IB-score) [28] is also based on th e computa-
tion of the posterior, but using the statistics ofa set of conditiona lindependence
tests. In this score the posterior Pr( G|D) is computed by combining the out-
comes of a set of conditional independence assertionsthat comple tely determine
G. Such set was called the closureof the structure, denoted C(G). Thus, when
using IB-score the problem of structure learning is posed as the ma ximization
of the posterior of the closure for each structure. Formally,
G⋆= argmax
GPr(C(G)|D). (4)
7
Applying the chain rule over the posterior of the closure,
Pr(C(G)|D) =∏
ci∈C(G)Pr(ci|c1,...,ci−1,D), (5)
the IB-score approximates such probability by assuming that all th e indepen-
dence assertions ciin the closure C(G) are mutually independent. The resulting
scoring funtion is computed as:
IB-score(G) =∏
ci∈C(G)logPr(ci|D), (6)
whereeach term logPr( ci|D) is computed by using the Bayesianstatistical test
of conditional independence [34, 38]. Together with the IB-score, an eﬃcient
algorithm called IBMAP-HC is presented to learn the structure by us ing a
heuristic local search over the space of possible structures.
3. Blankets Joint Posterior scoring function
We introducenowourmain contribution, the BlanketsJointPosterio r(BJP)
scoring function. Consider some graph Grepresenting the independence struc-
ture of a positive MN. It is a well-known fact that, by exploiting the gr aphical
properties of such models, the independence structure can be de composed as
the unique collection of the blankets of the variables [3, Theorem 4.6 o n p. 121].
Thus, the computation of the posterior probability of Ggiven a dataset Dis
equivalent to the joint posterior of the collection of blankets of G, that is,
Pr(G|D) = Pr(BX0,BX1,...,BXn−1|D). (7)
In contrast with previous works, where the blanket posteriors ar e simply as-
sumed to be independent [19, 28], we applied the chain rule to (7), obt aining
Pr(BX0,...,BXn−1|D) =n−1∏
i=0Pr(
BXi⏐⏐⏐⏐⏐{
BXj}i−1
j=0,D)
. (8)
Inthisway,theposteriorprobabilityofeachblanketcanbedescrib edin termsof
conditionalprobabilities,usingthetrainingdataset Dasevidence, togetherwith
8
the blanket of the other variables. Thus, the joint posterior of all the blankets is
computed taking advantage of how the blankets are mutually relate d, instead of
assuming them to be independent. The correctness of the propos ed method is
discussed in Appendix A. Details about how the BJP scoring function p roceeds
are presented below.
The computation of Pr( BX0,...,BXn−1|D) has to be done progressively,
ﬁrst calculating the posterior of the blanket of a variable, and then , the knowl-
edge obtained so far can be used as evidence to compute the poste rior of the
blanket of other variables. However, this decomposition is not uniqu e, since
each possible ordering for the variables is associated to a particular decomposi-
tion. The basic idea underlying the computation of BJP is to sort the b lankets
by their size (that is, the degree of the nodes in the graph) in ascen ding or-
der. This allows a series of inference steps, in order to avoid the com putation
of expensive and unreliable probabilities, thus improving data eﬃcienc y. This
is due to the fact that as the size of the blanket increases, greate r amounts of
data are required for accurately estimating its posterior probabilit y. By using
the proposed strategy, the blanket posteriors of variables with f ewer neighbors
are computed ﬁrst, and this information is used as evidence when co mputing
the posteriors for variables with bigger blankets. As a result, the in formation
obtained from the more reliable blanket posteriors is used for compu ting less
reliable blankets posteriors.
NowconsideranexampleprobabilitydistributionPr( XV)withfourvariables
X={X0,X1,X2,X3}, represented by a MN whose independence structure G
is given by the graph of Figure 1. One possible way of sorting its nodes by
their degree in ascending order is represented by the vector ( X1,X2,X3,X0),
and according to this ordering the blankets joint posterior is decom posed as
Pr(BX0,BX1,...,BXn−1|D) = Pr(BX1|D)
×Pr(BX2|BX1,D)
×Pr(BX3|BX1,BX2,D)
×Pr(BX0|BX1,BX2,BX3,D).
This example allows us to illustrate the intuition behind BJP, since the sa mple
9
0                               21
3
Figure 1: Example of an undirected graph with 4 nodes and hub t opology
complexity of the blanket posterior for variables X1,X2, andX3is lower than
that ofX0. Moreover, in this example it is clear that the posterior distribution
ofBX0is not independent of the posterior distributions of BX1,BX2andBX3.
Clearly, the posterior of BX0is harder to evaluate than the posterior of the
remaining variables, and then, computing Pr( BX0|BX1,BX2,BX3,D) could be
more informative that only computing Pr( BX0|D) independently of the rest of
blankets.
Given an undirected graph G, denoteψthe ordering vector which contains
the variables sorted by their degree in ascending order. Therefor e, we reformu-
late (8) as
BJP(G) =n−1∏
i=0Pr(
Bψi⏐⏐⏐⏐⏐{
Bψj}i−1
j=0,D)
. (9)
We now proceed to express the posterior of a blanket in terms of pr obabilities
of conditional independence and dependence assertions. The com putation of
Pr(Bψi|{Bψj}i−1
j=0,D) can be derived from the posterior of the independences
and dependences represented by each blanket:
Pr(
Bψi⏐⏐⏐⏐⏐{
Bψj}i−1
j=0,D)
=∏
ψk/∈BψiPr(
⟨ψi⊥ψk|Bψi⟩⏐⏐⏐⏐⏐{
Bψj}i−1
j=0,D)
×
∏
ψk∈BψiPr(
⟨ψi̸⊥ψk|Bψi\{ψk}⟩⏐⏐⏐⏐⏐{
Bψj}i−1
j=0,D)
.
(10)
In this way, the whole score is the product of the posterior probab ility of
each blanket, computed in terms of posterior probabilities condition ed in other
blankets. The particular way of determining the posterior of each b lanket of
(10) is inspired by the Markov blanket closure [28, Deﬁnition 2], which is a set
10
of independence and dependence assertions formally proven to de termine a MN
structure.
The two factors in (10) will be interpreted as follows:
•The ﬁrst product computes the probability of independence betwe enψi
and its non-adjacent variables, conditioned on its blanket, given th e pre-
viously computed blankets and the dataset D. It can be computed as
Pr(
⟨ψi⊥ψk|Bψi⟩⏐⏐⏐⏐⏐{
Bψj}i−1
j=0,D)
=

Pr(⟨ψi⊥ψk|Bψi⟩ |D)
ifi<k,
1 if i>k.(11)
Here,i < kindexes over the variables for which the blanket posterior
probability is not already computed. For the remaining variables the p os-
terior of independence will be simply inferred as 1. With this strategy , the
score simply uses the information in the evidence, since the independ ence
is determined by the blanket of ψk. The rationale behind this inference
is that for cases i > k, the blanket of ψkhas already been computed.
As it will be proved in more detail in Appendix A, Bψkalready contains
information about the independence of ψiandψk. By considering the
local Markov property for the blanket of ψk, and the fact that ψkis not in
the blanket of ψi, the opposite must also be true (as these are undirected
edges).
•The second product in (10) computes the posterior probability of d epen-
dence between ψiand its adjacent variables, conditioned on its remaining
neighbors, given the blankets computed previously and the datase tD. It
11"
2203.01441,D:\Database\arxiv\papers\2203.01441.pdf,What are the limitations of using 3D scene geometry to generate realistic corruptions for evaluating the robustness of computer vision models?,"While 3D scene geometry can generate more realistic corruptions than 2D methods, it is limited by the quality of the 3D data and the accuracy of depth prediction models. Additionally, the set of 3D corruptions is not exhaustive and may not capture all real-world scenarios.","sources with different locations and luminosities.
Video corruptions arise during the processing and stream-
ing of videos. Using the scene 3D, we create a video using
multiple frames from a single image by deﬁning a trajec-
tory, similar to motion blur. Inspired by [80], we generate
average bit rate (ABR) andconstant rate factor (CRF) as
H.265 codec compression artifacts, and bit error to capture
corruptions induced by imperfect video transmission chan-
nel. After applying the corruptions over the video, we pick
a single frame as the ﬁnal corrupted image.
Weather corruptions degrade visibility by obscuring parts
of the scene due to disturbances in the medium. We deﬁne
a single corruption and denote it as fog 3D to differentiate
it from the fog corruption in 2DCC. We use the standard
optical model for fog [19, 62, 70]:
I(x) =R(x)t(x) +A(1−t(x)), (1)
where I(x)is the resulting foggy image at pixel x,R(x)
is the clean image, Ais atmospheric light, and t(x)is the
transmission function describing the amount of light that
reaches the camera. When the medium is homogeneous,
the transmission depends on the distance from the camera,
t(x) = exp ( −βd(x))where d(x)is the scene depth and β
is the attenuation coefﬁcient controlling the fog thickness.
View changes are due to variations in the camera extrin-
sics and focal length. Our framework enables rendering
RGB images conditioned on several changes, such as ﬁeld
of view ,camera roll andcamera pitch , using Blender. This
enables us to analyze the sensitivity of models to various
view changes in a controlled manner. We also generate im-
ages with view jitter that can be used to analyze if models
predictions ﬂicker with slight changes in viewpoint.
Semantics: In addition to view changes, we also render
images by selecting an object in the scene and changing its
occlusion level and scale. In occlusion corruption, we gen-
erate views of an object occluded by other objects. This
is in contrast to random 2D masking of pixels to create an
unnatural occlusion effect that is irrespective of image con-
tent, e.g. as in [13, 48] (See Fig. 1). Occlusion rate can
be controlled to probe model robustness against occlusion
changes. Similarly, in scale corruption, we render views of
an object with varying distances from the camera location.
Note that the corruptions require a mesh with semantic an-
notations, and are generated automatically, similar to [2].
This is in contrast to [3] which requires tedious manual ef-
fort. The objects can be selected by randomly picking a
point in the scene or using the semantic annotations.
Noise corruptions arise from imperfect camera sensors. We
introduce new noise corruptions that do not exist in the pre-
vious 2DCC benchmark. For low-light noise , we decreased
the pixel intensities and added Poisson-Gaussian distributed
noise to reﬂect the low-light imaging setting [21]. ISO noise
also follows a Poisson-Gaussian distribution, with a ﬁxed
photon noise (modeled by a Poisson), and varying elec-
tronic noise (modeled by a Gaussian). We also included
Figure 4. Visualizations of 3DCC with increasing shift intensities .
Top: Increasing the shift intensity results in larger blur, less illumination,
and denser fog. Bottom: The object becomes more occluded or shrinks
in size using calculated viewpoint changes . The blue mask denotes the
amodal visible parts of the fridge/couch, and the red mask is the occluded
part. The leftmost column shows the clean images. Visuals for all corrup-
tions for all shift intensities are shown in the supplementary.
color quantization as another corruption that reduces the bit
depth of the RGB image. Only this subset of our corrup-
tions is not based on 3D information.
3.2. Starter 3D Common Corruptions Dataset
We release the full open source code of our pipeline,
which enables using the implemented corruptions on any
dataset. As a starter dataset, we applied the corruptions on
16k Taskonomy [84] test images. For all the corruptions ex-
cept the ones in view changes andsemantics which change
the scene, we follow the protocol in 2DCC and deﬁne 5 shift
intensities, resulting in approximately 1 million corrupted
images ( 16k×14×5). Directly applying the methods to
generate corruptions results in uncalibrated shift intensities
with respect to 2DCC. Thus, to enable aligned comparison
with 2DCC on a more uniform intensity change, we perform
a calibration step. For the corruptions with a direct coun-
terpart in 2DCC, e.g. motion blur, we set the corruption
level in 3DCC such that for each shift intensity in 2DCC,
the average SSIM [73] values over all images is the same
in both benchmarks. For the corruptions that do not have
a counterpart in 2DCC, we adjust the distortion parameters
to increase shift intensity while staying in a similar SSIM
range as the others. For view changes andsemantics , we
render 32k images with smoothly changing parameters, e.g.
roll angle, using the Replica [65] dataset. Figure 4 shows
example corruptions with different shift intensities.
3.3. Applying 3DCC to standard vision datasets
While we employed datasets with full scene geometry
information such as Taskonomy [84], 3DCC can also be ap-
plied to standard datasets without 3D information. We ex-
emplify this on ImageNet [12] and COCO [39] validation
4
sets by leveraging depth predictions from the MiDaS [55]
model, a state-of-the-art depth estimator. Figure 5 shows
example images with near focus ,far focus , and fog 3D
corruptions. Generated images are physically plausible,
demonstrating that 3DCC can be used for other datasets
by the community to generate a diverse set of image cor-
ruptions. In Sec. 5.2.4, we quantitatively demonstrate the
effectiveness of using predicted depth to generate 3DCC.
4. 3D Data Augmentation
While benchmarking uses corrupted images as test data ,
one can also use them as augmentations of training data
to build invariances towards these corruptions. This is the
case for us since, unlike 2DCC, 3DCC is designed to cap-
ture corruptions that are more likely to appear in the real
world, hence it has a sensible augmentation value as well.
Thus, in addition to benchmarking robustness using 3DCC,
our framework can also be viewed as new data augmen-
tation strategies that take the 3D scene geometry into ac-
count. We augment with the following corruption types in
our experiments: depth of ﬁeld ,camera motion , and light-
ing. The augmentations can be efﬁciently generated on-the-
ﬂy during training using parallel implementations. For ex-
ample, the depth of ﬁeld augmentations take 0.87seconds
(wall clock time) on a single V100 GPU for a batch size
of128 images with 224×224 resolution. For compari-
son, applying 2D defocus blur requires 0.54seconds, on
average. It is also possible to precompute certain selected
parts of the augmentation process, e.g. the illuminations for
lighting augmentations, to increase efﬁciency. We incorpo-
rated these mechanisms in our implementation. We show in
Sec. 5.3 that these augmentations can signiﬁcantly improve
robustness against real-world distortions.
5. Experiments
We perform evaluations to demonstrate that 3DCC
can expose vulnerabilities in models (Sec. 5.2.1) that
are not captured by 2DCC (Sec. 5.2.2). The gener-
ated corruptions are similar to expensive realistic synthetic
ones (Sec. 5.2.3) and are applicable to datasets without 3D
information (Sec. 5.2.4) and for semantic tasks (Sec. 5.2.5).
Finally, the proposed 3D data augmentation improves ro-
bustness qualitatively and quantitatively (Sec. 5.3). Please
see the project page for a live demo and more extensive
qualitative results.
5.1. Preliminaries
Evaluation Tasks: 3DCC can be applied to any dataset,
irrespective of the target task, e.g. dense regression or
low-dimensional classiﬁcation. Here we mainly experi-
ment with surface normals and depth estimation as target
tasks widely employed by the community. We note that the
robustness of models solving such tasks is underexplored
compared to classiﬁcation tasks (See Sec. 5.2.5 for results
Figure 5. 3DCC can be applied to most datasets , even those that do not
come with 3D information. Several query images from the ImageNet [12]
and COCO [39] dataset are shown with near focus ,far focus andfog 3D
corruptions applied. Notice how the objects in the circled regions go from
sharp to blurry depending on the focus region and scene geometry. To get
the depth information needed to create these corruptions, predictions from
MiDaS [55] model is used. This gives a good enough approximation to
generate realistic corruptions (as we will quantify in Sec. 5.2.4).
on panoptic segmentation and object recognition). To eval-
uate robustness, we compute the ℓ1error between predicted
and ground truth images.
Training Details: We train UNet [59] and DPT [54] mod-
els on Taskonomy [84] using learning rate 5×10−4and
weight decay 2×10−6. We optimize the likelihood loss
with Laplacian prior using AMSGrad [56], following [79].
Unless speciﬁed, all the models use the same UNet back-
bone (e.g. Fig. 6). We also experiment with DPT models
trained on Omnidata [17] that mixes a diverse set of train-
ing datasets. Following [17], we train with learning rate
1×10−5, weight decay 2×10−6with angular & ℓ1losses.
Robustness mechanisms evaluated: We evaluate several
popular data augmentation strategies: DeepAugment [26],
style augmentation [24], and adversarial training [36]. We
also include Cross-Domain Ensembles (X-DE) [79] that has
been recently shown to improve robustness to corruptions
by creating diverse ensemble components via input transfor-
mations. We refer to the supplementary for training details.
Finally, we train a model with augmentation with corrup-
tions from 2DCC [27] (2DCC augmentation), and another
model with 3D data augmentation on top of that (2DCC +
3D augmentation).
5.2. 3D Common Corruptions Benchmark
5.2.1 3DCC can expose vulnerabilities
We perform a benchmarking of the existing models against
3DCC to understand their vulnerabilities. However, we note
that our main contribution is not the performed analyses
but the benchmark itself. The state-of-the-art models may
change over time and 3DCC aims to identify the robustness
trends, similar to other benchmarks.
Effect of robustness mechanisms: Figure 6 shows the
average performance of different robustness mechanisms
on 3DCC for surface normals and depth estimation tasks.
5
Figure 6. Existing robustness mechanisms are found to be insufﬁcient
for addressing real-world corruptions approximated by 3DCC. Perfor-
mance of models with different robustness mechanisms under 3DCC for
surface normals (left) and depth (right) estimation tasks are shown. All
models here are UNets and are trained with Taskonomy data. Each bar
shows the ℓ1error averaged over all 3DCC corruptions (lower is better).
The black error bars show the error at the lowest and highest shift inten-
sity. The red line denotes the performance of the baseline model on clean
(uncorrupted) data. This denotes that existing robustness mechanisms, in-
cluding those with diverse augmentations, perform poorly under 3DCC.
These mechanisms improved the performance over the
baseline but are still far from the performance on clean data.
This suggests that 3DCC exposes robustness issues and can
serve as a challenging testbed for models. The 2DCC aug-
mentation model returns slightly lower ℓ1error, indicat-
ing that diverse 2D data augmentation only partially helps
against 3D corruptions.
Effect of dataset and architecture: We provide a detailed
breakdown of performance against 3DCC in Fig. 7. We
ﬁrst observe that baseline UNet and DPT models trained
on Taskonomy have similar performance, especially on
the view change corruptions. By training with larger and
more diverse data with Omnidata, the DPT performance
improves. Similar observations were made on vision trans-
formers for classiﬁcation [5, 16]. This improvement is no-
table with view change corruptions, while for the other cor-
ruptions, there is a decrease in error from 0.069 to 0.061.
This suggests that combining architectural advancements
with diverse and large training data can play an important
role in robustness against 3DCC. Furthermore, when com-
bined with 3D augmentations, they improve robustness to
real-world corruptions (Sec. 5.3).
5.2.2 Redundancy of corruptions in 3DCC and 2DCC
In Fig. 1, a qualitative comparison was made between
3DCC and 2DCC. The former generates more realistic cor-
ruptions while the latter does not take scene 3D into ac-
count and applies uniform modiﬁcations over the image. In
Fig. 8, we aim to quantify the similarity between 3DCC and
2DCC. On the left of Fig. 8, we compute the correlations of
ℓ1errors between clean and corrupted predictions made by
the baseline model for a subset of corruptions (full set is in
supplementary). 3DCC incurs less correlations both intra-
benchmark as well as against 2DCC (Mean correlations are
0.32for 2DCC-2DCC, 0.28for 3DCC-3DCC, and 0.30for
2DCC-3DCC). Similar conclusions are obtained for depth
estimation (in the supplementary). In the right, we provide
the same analysis on the RGB domain by computing the ℓ1
Figure 7. Detailed breakdown of performance on 3DCC. The bench-
mark can expose trends and models’ sensitivity to a wide range of corrup-
tions. We show this by training models on either Taskonomy [84] or Om-
nidata [17] and with either a UNet [59] or DPT [54] architecture. The av-
erage ℓ1error over all shift intensities for each corruption is shown (lower
is better). Top: We observe that Taskonomy models are more susceptible
to changes in ﬁeld of view, camera roll, and pitch compared to Omnidata
trained model, which is consistent with their methods. Bottom: The num-
bers in the legend are the average performance over all the corruptions. We
can see that all the models are sensitive to 3D corruptions, e.g. z-motion
blur andshadow . Overall, training with large diverse data, e.g. Omnidata,
and using DPT is observed to notably improve performance.
2DCC 3DCC
Figure 8. Redundancy among corruptions. We quantiﬁed the pair-
wise similarity of a subset of corruptions from 2DCC and 3DCC by com-
puting their correlations in the ℓ1errors of the surface normals predic-
tions (left) and RGB images (right). 3DCC incurs less correlations both
intra-benchmark as well as against 2DCC. Thus, 3DCC has a diverse set
of corruptions and these corruptions do not have a signiﬁcant overlap with
2DCC. Using depth as target task yields similar conclusions (full afﬁnity
matrices are provided in the supplementary).
error between clean and corrupted images, again suggesting
that 3DCC yields lower correlations.
6
Figure 9. Visual comparisons of 3DCC and expensive After Effects
(AE) generated depth of ﬁeld effect on query images from Hypersim.
3DCC generated corruptions are visually similar to those from AE.
5.2.3 Soundness: 3DCC vs Expensive Synthesis
3DCC aims to expose a model’s vulnerabilities to certain
real-world corruptions. This requires the corruptions gen-
erated by 3DCC to be similar to real corrupted data. As
generating such labeled data is expensive and scarcely avail-
able, as a proxy evaluation, we instead compare the realism
of 3DCC to synthesis made by Adobe After Effects (AE)
which is a commercial product to generate high-quality
photorealistic data and often relies on expensive and man-
ual processes. To achieve this, we use the Hypersim [58]
dataset that comes with high-resolution z-depth labels. We
then generated 200 images that are near- and far-focused us-
ing 3DCC and AE. Figure 9 shows sample generated images
from both approaches that are perceptually similar. Next,
we computed the prediction errors of a baseline normal
model when the input is from 3DCC or AE. The scatter plot
ofℓ1errors are given in Fig. 10 and demonstrates a strong
correlation, 0.80, between the two approaches. For calibra-
tion and control, we also provide the scatter plots for some
corruptions from 2DCC to show the signiﬁcance of cor-
relations. They have signiﬁcantly lower correlations with
AE, indicating the depth of ﬁeld effect created via 3DCC
matches AE generated data reasonably well.
5.2.4 Effectiveness of applying 3DCC to other datasets
We showed qualitatively in Fig. 5 that 3DCC can be ap-
plied to standard vision datasets like ImageNet [12] and
COCO [39] by leveraging predicted depth from a state-of-
the-art model from MiDaS [55]. Here, we quantitatively
show the impact of using predicted depth instead of ground
truth. For this, we use the Replica [65] dataset that comes
with ground truth depth labels. We then generated 1280 cor-
rupted images using ground truth depth and predicted depth
from MiDaS [55] without ﬁne-tuning on Replica . Figure 11
shows the trends on three corruptions from 3DCC gener-
ated using ground truth and predicted depth. The trends are
similar and the correlation of errors is strong ( 0.79). This
suggests that the predicted depth can be effectively used to
apply 3DCC to other datasets, and the performance is ex-
pected to improve with better depth predictions.
Figure 10. Corruptions of 3DCC are similar to expensive realistic
synthetic ones while being cheaper to generate. Scatter plots of ℓ1er-
rors from the baseline model predictions on 3DCC against those created by
Adobe After Effects (AE). The correlation between 3DCC near (far) focus
and those from AE near (far) focus is the strongest (numbers are in the leg-
end of left column). We also added the most similar corruption from 2DCC
(defocus blur) for comparison, yielding weaker correlations (middle). Shot
noise (right) is a control baseline , i.e. a randomly selected corruption, to
calibrate the signiﬁcance of the correlation measure.
0.00 0.05 0.10 0.15 0.20
Pred. Depth - Near Focus (1 error)
0.000.050.100.150.20GT Depth - Near Focus (1 error)
r = 0.90
0.00 0.05 0.10 0.15 0.20 0.25
Pred. Depth - Far Focus (1 error)
0.000.050.100.150.20GT Depth - Far Focus (1 error)
r = 0.69
0.00 0.05 0.10 0.15 0.20
Pred. Depth - Fog 3D (1 error)
0.000.050.100.150.20GT Depth - Fog 3D (1 error)
r = 0.78
Figure 11. Effectiveness of applying 3DCC without ground truth
depth. Three corruptions from 3DCC are generated using depth predic-
tions from MiDaS [55] model on unseen Replica data. Scatter plots show
theℓ1errors from the baseline model when corruptions are generated us-
ing the predicted depth (x-axis) or the ground truth (y-axis). The trends are
similar between two corrupted data results, suggesting the predicted depth
is an effective approximation to generate 3DCC. See the supplementary for
more tests including control baselines.
5.2.5 3DCC evaluations on semantic tasks
The previous benchmarking results were focusing on sur-
face normals and depth estimation tasks. Here we perform
a benchmarking on panoptic segmentation and object recog-
nition tasks as additional illustrative 3DCC evaluations. In
particular for panoptic segmentation, we use semantic cor-
ruptions from Sec. 3.1, and for object classiﬁcation, we
introduce ImageNet-3DCC by applying corruptions from
3DCC to ImageNet validation set, similar to 2DCC [27].
Semantic corruptions: We evaluate the robustness of two
panoptic segmentation models from [17] against occlu-
sion corruption of 3DCC. The models are trained on Om-
nidata [17] and Taskonomy [84] datasets with a Detec-
tron [75] backbone. See the supplementary for details.
Figure 13 quantiﬁes the effect of occlusion on the pre-
dictions of models, i.e. how the models’ intersection over
union (IoU) scores change with increasing occlusion, for
selected objects. This is computed on the test scenes from
7
Figure 12. Qualitative results of learning with 3D data augmentation on random queries from OASIS [9], AE (Sec. 5.2.3), manually collected DSLR
data, and in-the-wild YouTube videos for surface normals. The ground truth is gray when it is not available, e.g. for YouTube. The predictions in the last
two rows are from the O+DPT+2DCC+3D (Ours) model. It is further trained with cross-task consistency (X-TC) constraints [83] (Ours+X-TC). They are
noticeably sharper and more accurate. See the project page and supplementary for more results. A live demo for user uploaded images is also available.
0.25 0.50 0.75 1.00
Occlusion ratio0.00.20.40.60.81.0IoUCouch
Taskonomy
Omnidata
0.25 0.50 0.75 1.00
Occlusion ratioFridge
0.25 0.50 0.75 1.00
Occlusion ratioBicycle
Figure 13. Robustness against occlusion corruption of 3DCC.
The plot shows the intersection over union (IoU) scores of Detec-
tron models [75] for different objects over a range of occlusion ra-
tios. The models are trained on Taskonomy [84] or Omnidata [17]
datasets. The occlusion ratio is deﬁned as the number of occluded
pixels divided by the sum of occluded and visible pixels of the ob-
ject. This is computed over the test scenes of Replica [65]. The
plots expose the occlusion handling capabilities of the models and
show that the Omnidata trained model is generally more robust
than the Taskonomy one. The degradation in model predictions
is class-speciﬁc and becomes more severe with higher occlusion
ratios.
Replica [65]. The Omnidata trained model is generally
more robust than the Taskonomy one, though we see a de-
crease in IoU in both models as occlusion increases. The
trends are class-speciﬁc possibly due to shape of the objects
and their scene context, e.g. fridge predictions remain un-
changed up until 0.50occlusion ratio, while couch predic-
tions degrade more linearly for Omnidata model. This eval-
uation showcases one use of semantic corruptions in 3DCC,
which are notably harder to accomplish using other bench-marks that do not operate based on 3D scans.
ImageNet-3DCC: We compare performances of the robust
ImageNet models [24, 26, 29, 61] from RobustBench [11]
and ImageNet-2DCC [27] (i.e. ImageNet-C) leaderboards
in Fig. 14. Following 2DCC, we compute mean corruption
error (mCE) by dividing the models errors by AlexNet [34]
errors and averaging over corruptions. The performance
of models degrade signiﬁcantly, including those with di-
verse augmentations. Thus, ImageNet-3DCC can serve as
a challenging benchmark for object recognition task. As
expected, while the general trends are similar between the
two benchmarks as 2D and 3D corruptions are not com-
pletely disjoint [45], 3DCC exposes vulnerabilities that are
not captured by 2DCC, which can be informative during
model development. See supplementary for further results.
5.3. 3D data augmentation to improve robustness
We demonstrate the effectiveness of the proposed aug-
mentations qualitatively and quantitatively. We evaluate
UNet and DPT models trained on Taskonomy (T+UNet,
T+DPT) and DPT trained on Omnidata (O+DPT) to see the
effect of training dataset and model architecture. The train-
ing procedure is as described in Sec. 5.1. For the other mod-
els, we initialize from O+DPT model and train with 2DCC
augmentations (O+DPT+2DCC) and 3D augmentations on
top of that (O+DPT+2DCC+3D), i.e. our proposed model.
8
SIN (A)StandardSIN (C) SIN (B)ANT (3x3)AugMixDA
AugMix+DA0.50.60.70.80.91.0 Mean corruption error (mCE)ImageNet-3DCC
ImageNet-2DCCFigure 14. Robustness on ImageNet-3DCC and ImageNet-2DCC. Er-
rors on ImageNet validation images corrupted by 3DCC and 2DCC
are computed for the models in robustness leaderboards [11, 27]. Fol-
lowing [27], we compute the mean corruption error (mCE) relative to
AlexNet [34]. The performance degrades signiﬁcantly against ImageNet-
3DCC, thus it can serve as a challenging benchmark. As expected, the
general trends are similar between the two benchmarks as 2D and 3D cor-
ruptions are not completely disjoint. A similar observation was also made
in [45] even when the corruptions are designed to be dissimilar to 2DCC.
Still, there are notable differences that can be informative during model de-
velopment by exposing trends and vulnerabilities that are not captured by
2DCC, e.g. ANT [61] has better mCE on 2DCC compared to AugMix [29],
while they perform similarly on 3DCC. Likewise, combining DeepAug-
ment [26] with AugMix improved the performance on 2DCC signiﬁcantly
more than 3DCC. See the supplementary for more results.
We also further trained the proposed model using cross-
task consistency (X-TC) constraints from [83], denoted as
(Ours+X-TC) in the results. Lastly, we evaluated a model
trained on OASIS training data from [9] (OASIS).
Qualitative evaluations: We consider i.OASIS validation
images [9], ii.AE corrupted data from Sec. 5.2.3, iii.man-
ually collected DSLR data, and iv.in-the-wild YouTube
videos. Figure 12 shows that predictions made by the pro-
posed models are signiﬁcantly more robust compared to
baselines. We also recommend watching the clips and run-
ning the live demo on the project page.
Quantitative evaluations: In Table 1, we compute errors
made by the models on 2DCC, 3DCC, AE, and OASIS val-
idation set (no ﬁne-tuning). Again, the proposed models
yield lower errors across datasets showing the effectiveness
of augmentations. Note that robustness against corrupted
data is improved without sacriﬁcing performance on in-the-
wild clean data , i.e. OASIS.
6. Conclusion and Limitations
We introduce a framework to test and improve model
robustness against real-world distribution shifts, particu-
larly those centered around 3D. Experiments demonstrate
that the proposed 3D Common Corruptions is a challenging
benchmark that exposes model vulnerabilities under real-
world plausible corruptions. Furthermore, the proposed
data augmentation leads to more robust predictions com-
pared to baselines. We believe this work opens up a promis-BenchmarkModelT+UNet T+DPT OASIS [9] O+DPT O+DPT+2DCC Ours Ours +X-TC [83]
2DCC [27] ( ℓ1error) 8.15 7.47 15.31 6.43 5.78 5.32 5.29
3DCC (ℓ1error) 7.08 6.89 15.11 6.13 5.94 5.42 5.35
AE (Sec. 5.2.3) ( ℓ1error) 12.86 12.39 16.85 7.84 6.50 4.94 5.47
OASIS [9] (angular error) 30.49 32.13 24.63 24.42 23.67 24.65 23.89
Table 1. Effectiveness of 3D augmentations quantiﬁed using
different benchmarks. ℓ1errors are multiplied by 100for read-
ability. The O+DPT+2DCC+3D model is denoted by Ours. We
also trained this model using cross-task consistency (X-TC) con-
straints from [83] (Ours+X-TC). Our models yield lower errors
across the benchmarks. 2DCC and 3DCC are applied on the same
Taskonomy test images. More results are given in supplementary.
Evaluations on OASIS dataset sometimes show a large variance
due to its sparse ground truth.
ing direction in robustness research by showing the useful-
ness of 3D corruptions in benchmarking and training. Be-
low we brieﬂy discuss some of the limitations:
3D quality : 3DCC is upper-bounded by the quality of 3D
data. The current 3DCC is an imperfect but useful approx-
imation of real-world 3D corruptions, as we showed. The
ﬁdelity is expected to improve with higher resolution sen-
sory data and better depth prediction models.
Non-exhaustive set : Our set of 3D corruptions and augmen-
tations are not exhaustive . They instead serve as a starter
set for researchers to experiment with. The framework can
be employed to generate more domain-speciﬁc distribution
shifts with minimal manual effort.
Large-scale evaluation : While we evaluate some recent ro-
bustness approaches in our analyses, our main goal was to
show that 3DCC successfully exposes vulnerabilities. Thus,
performing a comprehensive robustness analysis is beyond
the scope of this work. We encourage researchers to test
their models against our corruptions.
Balancing the benchmark : We did not explicitly balance the
corruption types in our benchmark, e.g. having the same
number of noise and blur distortions. Our work can further
beneﬁt from weighting strategies trying to calibrate average
performance on corruption benchmarks, such as [37].
Use cases of augmentations : While we focus on robust-
ness, investigating their usefulness on other applications,
e.g. self-supervised learning, could be worthwhile.
Evaluation tasks : We experiment with dense regression
tasks. However, 3DCC can be applied to different tasks,
including classiﬁcation and other semantic ones. Investi-
gating failure cases of semantic models against, e.g. on
smoothly changing occlusion rates for several objects, us-
ing our framework could provide useful insights.
Acknowledgement: We thank Zeynep Kar and Abhijeet
Jagdev. This work was partially supported by the ETH4D
and EPFL EssentialTech Centre Humanitarian Action Chal-
lenge Grant.
9
References
[1] Abdelrahman Abdelhamed, Stephen Lin, and Michael S
Brown. A high-quality denoising dataset for smartphone
cameras. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 1692–1700,
2018. 3
[2] Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir R Zamir,
Martin Fischer, Jitendra Malik, and Silvio Savarese. 3d scene
graph: A structure for uniﬁed semantics, 3d space, and cam-
era. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision , pages 5664–5673, 2019. 4
[3] Andrei Barbu, David Mayo, Julian Alverio, William Luo,
Christopher Wang, Danny Gutfreund, Joshua Tenenbaum,
and Boris Katz. Objectnet: A large-scale bias-controlled
dataset for pushing the limits of object recognition models.
2019. 2, 4
[4] Brian A Barsky and Todd J Kosloff. Algorithms for ren-
dering depth of ﬁeld effects in computer graphics. In Pro-
ceedings of the 12th WSEAS international conference on
Computers , volume 2008. World Scientiﬁc and Engineering
Academy and Society (WSEAS), 2008. 3
[5] Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner,
Daliang Li, Thomas Unterthiner, and Andreas Veit. Under-
standing robustness of transformers for image classiﬁcation.
arXiv preprint arXiv:2103.14586 , 2021. 2, 6
[6] Tim Brooks and Jonathan T Barron. Learning to synthesize
motion blur. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 6840–
6848, 2019. 3
[7] Prithvijit Chattopadhyay, Judy Hoffman, Roozbeh Mottaghi,
and Aniruddha Kembhavi. Robustnav: Towards bench-
marking robustness in embodied navigation. arXiv preprint
arXiv:2106.04531 , 2021. 2
[8] Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun.
Learning to see in the dark. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
3291–3300, 2018. 3
[9] Weifeng Chen, Shengyi Qian, David Fan, Noriyuki Ko-
jima, Max Hamilton, and Jia Deng. Oasis: A large-scale
dataset for single image 3d in the wild. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 679–688, 2020. 8, 9
[10] Blender Online Community. Blender - a 3D modelling and
rendering package . Blender Foundation, Stichting Blender
Foundation, Amsterdam, 2018. 3
[11] Francesco Croce, Maksym Andriushchenko, Vikash Se-
hwag, Edoardo Debenedetti, Nicolas Flammarion, Mung
Chiang, Prateek Mittal, and Matthias Hein. Robustbench:
a standardized adversarial robustness benchmark. arXiv
preprint arXiv:2010.09670 , 2020. 3, 8, 9
[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE Conference on Computer Vision and
Pattern Recognition , pages 248–255. Ieee, 2009. 2, 4, 5, 7
[13] Terrance DeVries and Graham W Taylor. Improved regular-
ization of convolutional neural networks with cutout. arXiv
preprint arXiv:1708.04552 , 2017. 1, 4
[14] Josip Djolonga, Jessica Yung, Michael Tschannen, Rob
Romijnders, Lucas Beyer, Alexander Kolesnikov, JoanPuigcerver, Matthias Minderer, Alexander D’Amour, Dan
Moldovan, et al. On robustness and transferability of convo-
lutional neural networks. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 16458–16468, 2021. 2
[15] Samuel Dodge and Lina Karam. A study and comparison
of human and deep learning recognition performance under
visual distortions. In 2017 26th International Conference on
Computer Communication and Networks (ICCCN) , pages 1–
7. IEEE, 2017. 1
[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 6
[17] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir
Zamir. Omnidata: A scalable pipeline for making multi-
task mid-level vision datasets from 3d scans. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 10786–10796, 2021. 3, 5, 6, 7, 8
[18] Michael Elad and Michal Aharon. Image denoising via
sparse and redundant representations over learned dictionar-
ies.IEEE Transactions on Image processing , 15(12):3736–
3745, 2006. 3
[19] Raanan Fattal. Single image dehazing. ACM transactions on
graphics (TOG) , 27(3):1–9, 2008. 3, 4
[20] Rob Fergus, Barun Singh, Aaron Hertzmann, Sam T Roweis,
and William T Freeman. Removing camera shake from a
single photograph. In ACM SIGGRAPH 2006 Papers , pages
787–794. 2006. 3
[21] Alessandro Foi, Mejdi Trimeche, Vladimir Katkovnik, and
Karen Egiazarian. Practical poissonian-gaussian noise mod-
eling and ﬁtting for single-image raw-data. IEEE Transac-
tions on Image Processing , 17(10):1737–1754, 2008. 3, 4
[22] Nic Ford, Justin Gilmer, Nicolas Carlini, and Dogus Cubuk.
Adversarial examples are a natural consequence of test error
in noise. arXiv preprint arXiv:1901.10513 , 2019. 2
[23] Robert Geirhos, J ¨orn-Henrik Jacobsen, Claudio Michaelis,
Richard Zemel, Wieland Brendel, Matthias Bethge, and Fe-
lix A Wichmann. Shortcut learning in deep neural networks.
arXiv preprint arXiv:2004.07780 , 2020. 1
[24] Robert Geirhos, Patricia Rubisch, Claudio Michaelis,
Matthias Bethge, Felix A Wichmann, and Wieland Brendel.
Imagenet-trained cnns are biased towards texture; increasing
shape bias improves accuracy and robustness. arXiv preprint
arXiv:1811.12231 , 2018. 2, 5, 8
[25] Majed El Helou, Ruofan Zhou, Johan Barthas, and Sabine
S¨usstrunk. Vidit: virtual image dataset for illumination trans-
fer.arXiv preprint arXiv:2005.05460 , 2020. 3
[26] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-
vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,
Samyak Parajuli, Mike Guo, et al. The many faces of robust-
ness: A critical analysis of out-of-distribution generalization.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 8340–8349, 2021. 2, 5, 8, 9
[27] Dan Hendrycks and Thomas Dietterich. Benchmarking neu-
ral network robustness to common corruptions and perturba-
tions. arXiv preprint arXiv:1903.12261 , 2019. 1, 2, 5, 7, 8,
9
10
[28] Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using
pre-training can improve model robustness and uncertainty.
InInternational Conference on Machine Learning , pages
2712–2721. PMLR, 2019. 2
[29] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph,
Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A
simple data processing method to improve robustness and
uncertainty. arXiv preprint arXiv:1912.02781 , 2019. 2, 8, 9
[30] Xiaowei Hu, Chi-Wing Fu, Lei Zhu, and Pheng-Ann Heng.
Depth-attentional features for single-image rain removal. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 8022–8031, 2019. 3
[31] Jason Jo and Yoshua Bengio. Measuring the tendency of
cnns to learn surface statistical regularities. arXiv preprint
arXiv:1711.11561 , 2017. 1
[32] Christoph Kamann and Carsten Rother. Benchmarking the
robustness of semantic segmentation models. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 8828–8838, 2020. 2
[33] Sanjay Kariyappa and Moinuddin K Qureshi. Improving
adversarial robustness of ensembles with diversity training.
arXiv preprint arXiv:1901.09981 , 2019. 2
[34] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. Advances in neural information processing systems ,
25:1097–1105, 2012. 8, 9
[35] Deepa Kundur and Dimitrios Hatzinakos. Blind image de-
convolution. IEEE signal processing magazine , 13(3):43–
64, 1996. 3
[36] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Ad-
versarial machine learning at scale. arXiv preprint
arXiv:1611.01236 , 2016. 3, 5
[37] Alfred Laugros, Alice Caplier, and Matthieu Ospici. Using
synthetic corruptions to measure robustness to natural distri-
bution shifts. arXiv preprint arXiv:2107.12052 , 2021. 9
[38] Guillaume Leclerc, Hadi Salman, Andrew Ilyas, Sai Vem-
prala, Logan Engstrom, Vibhav Vineet, Kai Xiao, Pengchuan
Zhang, Shibani Santurkar, Greg Yang, et al. 3db: A frame-
work for debugging computer vision models. arXiv preprint
arXiv:2106.03805 , 2021. 2
[39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
European conference on computer vision , pages 740–755.
Springer, 2014. 4, 5, 7
[40] Raphael Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer,
and Ekin D Cubuk. Improving robustness without sacriﬁcing
accuracy with patch gaussian augmentation. arXiv preprint
arXiv:1906.02611 , 2019. 2
[41] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,
Dimitris Tsipras, and Adrian Vladu. Towards deep learn-
ing models resistant to adversarial attacks. arXiv preprint
arXiv:1706.06083 , 2017. 2, 3
[42] Julien Mairal, Francis Bach, Jean Ponce, Guillermo Sapiro,
and Andrew Zisserman. Non-local sparse models for image
restoration. In 2009 IEEE 12th International Conference on
Computer Vision , pages 2272–2279. IEEE, 2009. 3
[43] Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos,
Evgenia Rusak, Oliver Bringmann, Alexander S Ecker,Matthias Bethge, and Wieland Brendel. Benchmarking ro-
bustness in object detection: Autonomous driving when win-
ter is coming. arXiv preprint arXiv:1907.07484 , 2019. 2
[44] John P Miller, Rohan Taori, Aditi Raghunathan, Shiori
Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair
Carmon, and Ludwig Schmidt. Accuracy on the line: On
the strong correlation between out-of-distribution and in-
distribution generalization. In International Conference on
Machine Learning , pages 7721–7735. PMLR, 2021. 2
[45] Eric Mintun, Alexander Kirillov, and Saining Xie. On inter-
action between augmentations and corruptions in natural cor-
ruption robustness. arXiv preprint arXiv:2102.11273 , 2021.
2, 8, 9
[46] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep
multi-scale convolutional neural network for dynamic scene
deblurring. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 3883–3891,
2017. 3
[47] Seungjun Nah, Sanghyun Son, Suyoung Lee, Radu Timofte,
and Kyoung Mu Lee. Ntire 2021 challenge on image de-
blurring. In CVPR Workshops , pages 149–165, June 2021.
3
[48] Muzammal Naseer, Kanchana Ranasinghe, Salman Khan,
Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan
Yang. Intriguing properties of vision transformers. arXiv
preprint arXiv:2105.10497 , 2021. 2, 4
[49] Simon Niklaus, Long Mai, Jimei Yang, and Feng Liu. 3d
ken burns effect from a single image. ACM Transactions on
Graphics (TOG) , 38(6):1–15, 2019. 3
[50] A Emin Orhan. Robustness properties of facebook’s resnext
wsl models. arXiv preprint arXiv:1907.07640 , 2019. 2
[51] Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu.
Improving adversarial robustness via promoting ensemble
diversity. arXiv preprint arXiv:1901.08846 , 2019. 2
[52] Amandalynne Paullada, Inioluwa Deborah Raji, Emily M
Bender, Emily Denton, and Alex Hanna. Data and its (dis)
contents: A survey of dataset development and use in ma-
chine learning research. arXiv preprint arXiv:2012.05345 ,
2020. 2
[53] Michael Potmesil and Indranil Chakravarty. A lens and aper-
ture camera model for synthetic image generation. ACM
SIGGRAPH Computer Graphics , 15(3):297–305, 1981. 3
[54] Ren ´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 12179–12188, 2021. 5, 6
[55] Ren ´e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. arXiv preprint arXiv:1907.01341 , 2019. 5, 7
[56] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On
the convergence of adam and beyond. arXiv preprint
arXiv:1904.09237 , 2019. 5
[57] Jaesung Rim, Haeyun Lee, Jucheol Won, and Sunghyun Cho.
Real-world blur dataset for learning and benchmarking de-
blurring algorithms. In European Conference on Computer
Vision , pages 184–201. Springer, 2020. 3
[58] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit
Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb,
11"
2405.01708,D:\Database\arxiv\papers\2405.01708.pdf,"While the paper focuses on applying causal inference techniques to travel behavior modeling, how do these methods differ from traditional approaches used in other fields like epidemiology, where time-varying treatments are often considered?","The paper's focus on causal inference in transportation differs from epidemiology by emphasizing the impact of self-selection bias and policy interventions on travel behavior, rather than the effects of time-varying treatments on health outcomes.","2.1 Causality analysis in transportation studies
In the realm of transportation, travel behaviour modelling as a predictive tool typi-
cally relies on observational data to anticipate future conditions of the transport sys-
tem. This approach assumes that both the observed and future systems share iden-
tical distributions, implying that our predictions remain valid as long as the system
remains unchanged. Nevertheless, when it comes to assessing future policy impact,
decision-makers and policymakers are interested in forecasting the system’s behaviour
in a counterfactual world, after implementing some changes. This introduces a causal
dimension, where the focus shifts to answering causal-based questions (Rubin, 2019). In
transportation, the self-selection problem, which is rather a common issue, has been en-
couraging transportation researchers to primarily focus on causality analysis (Mokhtar-
ian & Cao, 2008). This phenomenon arises when individuals self-select into certain
travel behaviour choices due to their preferences, attitudes, personal characteristics, or
other unobserved variables, resulting in a biased estimation of the effect of contributing
factors on travel behaviour outcomes. In the discipline of causal inference, this concept
is known as confounding impact, meaning that some variables may not only influence a
travel behaviour outcome, but also are correlated with policies. In fact, the estimated
relationship between the variables of interest and travel behaviour outcomes may be
confounded by the self-selection bias. Consequently, any differences in travel behaviour
outcomes observed among individuals may be a combination of the self-selection effect
and the impact of variables of interest.
Generally, the utilization of causality analysis can be observed in different fields of
study encompassing econometrics, statistics, and computer science. Drawing inspira-
tion from these disciplines, some approaches are employed in transportation studies
to address self-selection bias and evaluate policy impacts. Although these disciplines
share the common goal of inferring causality, their focus and operational methodologies
vary. Table 1 provides a summary of these approaches, which are widely used in trans-
portation studies. It is worth mentioning that in epidemiology literature, significant
attention has also been devoted to causality inference in the context of time-varying
treatments. However, this focus diverges from the scope of our study.
Sample selection is a method used to address self-selection bias in observational
studies to derive unbiased estimates in econometrics. This technique involves mod-
elling the selection process alongside the outcome equation (Mokhtarian & van Herick,
2016). The concept of this method is built based on the Structural Equation Model
(SEM) that simultaneously estimates the relationship between multiple variables and
their interdependencies, which can include latent and observed variables (Golob, 2003).
The standard sample selection approach consists of two equations 1) a selection equa-
tion and 2) an outcome equation which is specified based on travel behaviour indicators
of interest. The Copula-based joint approach is one type of sample-selection method
widely employed to model the joint distribution of multiple variables affecting decision-
making processes (Parsa et al., 2019; Spissu et al., 2009). The copula approach has
empowered researchers to capture a stochastic correlation among variables without be-
ing restricted by specific probability distributions. Instrumental variable (IV) is another
approach used to address the potential endogeneity issue in econometrics. Endogeneity
generally refers to a situation in which an explanatory variable is correlated with the
error term, leading to biased estimates. In the context of causality analysis, endogeneity
4
Table 1: Focus of different fields of study on causality
Discipline Approach Focus Limitations
Econometrics Structural Equation Model Captures selection process Depends on model specification assumption
and allows for the incorporation of
observed and latent variables
Copula joint approach Captures non-linear stochastic dependencies Requires knowledge of joint distribution
between multiple dependent variables Depends on model specification assumption
Instrumental Variable (IV) Addresses endogeneity issues Requires valid instruments
Statistics Propensity Score Matching Overcome selection bias problem Depends on the specification of the propensity model
Stratification Limited to available observed covariates
Inverse Probability Weighting Depends on the specification of the propensity model
Computer Science DAG-based approach Explicitly represents causal assumptions Requires accurate knowledge of causal structure
regarding observed and unobserved variables
Machine learning-based causal models Captures non-linear and complex Interpretability challenges / relies on hyperparameters setting
relationships in selection process
states a broader concept where certain variables may confound the causal relationship
between a policy and an outcome. In transportation studies, the Instrumental Variable
(IV) method is also used (Vance & Hedel, 2007) to address endogeneity by introducing
an instrument, which is a variable that is correlated with the exogenous explanatory
variable, but is not directly related to the dependent variable. The instrument serves
as a means to isolate the variation in the explanatory variable that is not influenced by
the error term.
In statistics, based on the Rubin Causal Model (RCM), the causal effect is defined
as the comparison between two potential outcomes. However, in observational studies,
where both potential outcomes, related to treated and untreated conditions, are not
directly observed for each individual, statisticians strive to provide comparability in
the observational studies by establishing a balance between the treated and untreated
(control) groups. Prominent approaches in statistics for achieving this balance include
Propensity Score Matching, Inverse Probability Weighting, and Stratification. Notably,
the propensity score method is a traditionally used technique in transportation studies
(Cao & Fan, 2012; Cao et al., 2010). This method matches or pairs individuals with
similar propensity but different policy exposure, leading to two distinct groups. After
providing balance, the difference in outcome between the two groups can be attributed
to the treatment, resulting in the true or causal effect of the variable of interest.
In computer science, Pearl’s Directed Acyclic Graphs (DAGs) approach and the
rules of do-calculus emphasize identification rather than estimation. In observational
studies, this method involves 1) causal identification and 2) converting from causality
quantity to a statistical expression, known as an estimand. Interestingly, in contrast
to the aforementioned methods, this approach provides us with three ladders of be-
havioural modelling: 1) associations analysis, 2) causation analysis by isolating the
effect of confounder variables, and 3) policy effect assessment by forecasting a poten-
tial counterfactual world. In recent years, Brathwaite and Walker, 2018 discussed the
5
fundamental problem of the lack of using DAGs-based approaches in advanced travel
behaviour models and highlighted the essential requirement of applying both causal
inference and machine learning algorithms in behavioural analysis.
Inspired by the DAG-based approach, our objective is to present a comprehensive
framework that addresses both causal effect analysis and the forecasting of counterfac-
tual systems in travel behaviour modelling. Our study enjoys causal structure learning
through causal discovery algorithms, utilizes structural equation models built based on
the causal structure, and addresses model specification using deep learning algorithms.
Notably, we leverage the capabilities of deep learning networks in predictive power while
simultaneously maintaining interpretability. This study tried to initiate a new research
area in causality analysis within transportation, and to the best of our knowledge, this
is the first interdisciplinary effort in the context of travel behavioural modelling utiliz-
ing the advantages of traditional DCMs, deep learning algorithms and causal inference
techniques.
2.2 Causal learning
In this section, a general overview of the main concepts of the causal discovery and
causal inference method is presented. In addition, we introduce some fundamental
definitions and assumptions in the causal-based analysis that can be found in previous
studies and several books focusing on inferring causality (Nogueira et al., 2022; Pearl,
2009b; Peters et al., 2017; Spirtes, 2010). Causal discovery and causal inference methods
are two different concepts in the causal learning discipline. Briefly, causal discovery
methods infer a causal graph or causal knowledge (Shen et al., 2020), while causal
inference techniques estimate the effect of a change in a certain variable, known as an
intervention, on an outcome (Yao et al., 2021).
2.2.1 Causal discovery
The first step of inferring causality is to identify a causal structure of a system. Causal
relationships between variables can be specified based on theories, domain behavioural
assumptions, expert knowledge, and results of previous studies. Causal discovery is
a statistical technique that is able to derive Directed Acyclic Graphs (DAGs), repre-
senting causal (in)dependencies between exogenous variables, endogenous variables and
unobserved variables from observational datasets (Shen et al., 2020). Indeed, causal dis-
covery enables us to identify non-causal associations between variables of interest in the
process of modelling.
Definition 1 (Directed Acyclic Graphs) A Directed Acyclic Graph (DAG), G:=
(x, ζ)is a graphical representation of the causal model structure, where the nodes of
the graph represent random variables (x), including exogenous variables, endogenous
variables and unobserved variables, and the edges ζbetween nodes represent causal
association or direct dependencies between variables. In this graphical model, the edges
are directed in such a way that their direction represents the direction of causality and a
node x1∈xis a parent (Pa)or cause of another node x2∈xcalled child, if x1→x2. In
addition, DAGs are acyclic, meaning that there are no cycles in the graph. Therefore,
each DAG consists of several causal paths or directed edges pointing away from parents
or ancestors (xanc)toward the child or descendants (xdes).
6
To learn the relationships in a DAG G, there are various techniques in causal struc-
tural learning, including constraint-based, score-based, asymmetrical distribution, and
hybrid approaches, differing in theories and assumptions (Nogueira et al., 2022). It is
worth mentioning that in the literature, there is no standard causal discovery method
and researchers usually select algorithms based on their studied problem and assump-
tions. Sufficiency and faithfulness are the most popular assumptions in causal discovery
methods, stating that:
Definition 2 (Sufficiency) There are no unobserved confounders in the causal graph
G, meaning that for all variables, all their causes (Pai)are observed in the data.
Definition 3 (Faithfulness) Given a DAG G, the faithfulness assumption states that
the causal relationships that exist in the underlying causal graph accurately reflect the
observed distribution among variables. This is denoted as:
x1⊥ ⊥Gx2←x1⊥ ⊥Px2 (1)
Faithfulness assumption allows us to infer the causal graph, by assuming all condi-
tional independences in probability distribution Pare represented in G. In the following,
a brief description of four causal discovery approaches is presented.
•Constraint-based approach: This method uses conditional independences encoded
in the data to infer a graphical causal structure. Constraint-based approaches are
based on the faithfulness assumption and try to identify the presence or absence
of certain edges in a DAG using conditional independence tests (Nogueira et al.,
2022; Spirtes et al., 2000). It is of note that faithfulness is a strong assumption
that requires big sample sizes for conditional independence tests. The PC (Peter
and Clark) and Fast Causal Inference (FCI) as constraint-based causal discovery
approaches are widely used in the literature. The main difference between these
two algorithms is that in contrast to FCI, PC makes sufficiency assumptions.
•Score-based approach: This algorithm evaluates the goodness of fit of a given
causal model to the data and tries to optimize the goodness of fit score during
learning a DAG. Score-based algorithms are also known as optimization-based al-
gorithms. The Greedy Equivalence Search (GES) is the most popular Score-based
approach starting with an empty graph and iteratively adding directed edges such
that the model score like the Bayesian Information Criterion is maximized (Chick-
ering, 2002). Fast Greedy Equivalence Search (FGES) is another modification of
GES which is free from faithfulness assumption (Ramsey et al., 2017).
•Asymmetrical Distribution-based approach: This method is structured based on
the idea that causal structure can be learned by analyzing the asymmetry in the
distribution of the variables in the dataset. In other words, asymmetry is consid-
ered a fundamental characteristic of causality, and by examining these asymme-
tries, the method aims to uncover the underlying causal relationships. The Non-
linear Additive Noise Model (NANM) and Linear Non-Gaussian Acyclic Model
(LiNGAM) are classified under the asymmetrical distribution-based algorithm
(Hoyer et al., 2008; Shimizu et al., 2006). Both algorithms make sufficiency as-
sumptions. These methods are also known as functional causal models, where
7
LiNGAM assumes a linear relationship with non-Gaussian noise variable while
NANM is specified based on a non-linear function with additive noise. These
approaches involve building models in both directions and selecting the direction
that satisfies the independence criterion between the cause and the noise.
•Hybrid approach: This approach is a combination of multiple approaches. For
instance, causal Generative Neural Networks (CGNNs) as a hybrid method dis-
cover the causal relationships between multiple variables by taking into account
both conditional independencies and distributional asymmetries (Goudet et al.,
2018).
Broadly, DAGs provide valuable insights into the underlying causal structure of
the system, enabling us to know the determining factors and confounding variables in
different interventions. In order to identify confounding variables and isolate the non-
causal associations or paths in the process of modelling, we need to identify a blocking
set.
Definition 4 (Blocking set) A blocking set zis a set of variables in a DAG that
blocks or eliminates the effect of one variable x1on another variable x2, if either of the
following conditions, shown in Figure 1, is held along their path:
•There is a chain structure, x1→xi→x2orx1←xi←x2andxi∈z.
•There is a fork structure x1←xi→x2andxi∈z.
•There is a collider structure, x1→xi←x2, and neither xinor any of its descen-
dants is in z.
Figure 1: Three basic structures in a Directed Acyclic Graph (DAG)
After deriving the blocking set from a DAG, d-separated variables and conditional
in(dependencies) can be defined as follows:
Definition 5 (d-separated) Two variables are d-separated, if every path between them
is blocked by a set variable known as blocking set z. In other words:
(x1⊥ ⊥x2)|z (2)
This definition guides us to isolate or block any non-causal association and only
keep the causal association between two variables of interest by 1) conditioning on a
non-collider set and 2) not conditioning on the collider set.
8
2.2.2 Structural Causal Model (SCM)
Causal discovery is a powerful method to encode causal assumptions, but we still need
to explore to what extent ‘causes’ influence ‘effects’ and how ‘interventions’ change the
system. Structural Causal Models (SCM) is a type of causal inference framework that
analyzes and estimates the underlying causal structures by specifying the functional
relationships or causal mechanisms between variables (Pearl, 2009b). In general, each
causal mechanism generates xigiven all of its causes or parents. This model structure
describes how each variable in the system interacts with its causes (parents). Math-
ematically, in an SCM, the causal relationship of each endogenous variable with its
causes or parents is specified by an asymmetrical structural equation as follows:
xi:=fi(Pai, βi) +εi (3)
where firepresents structural mechanisms, generating an endogenous variable based
on causes (parents) ( Pai). In addition, εiis an exogenous latent variable representing
unobserved factors or sources of variation in modelling. Estimation of the SCM involves
estimating the parameters of each structural causal mechanism βi. Note that equality
in equations represents symmetric; However, in the causality analysis, we must put
emphasis on the asymmetrical equations.
Moreover, SCMs enable the study of counterfactual scenarios in which various inter-
ventions or policies would have occurred. The counterfactual analysis allows modellers
to understand how the system would behave under different changes, and make rigorous
causal inferences, elaborating on the causal impact of policies before implementation.
The distribution of the counterfactual world, known as intervention distribution, is the
most different assumption of behavioural causal-based models compared to the preva-
lent behavioural frameworks.
Definition 6 (Intervention distribution) An intervention distribution, mathemat-
ically denoting with the do operator P(Y|do(xi)), refers to the probability distribution
of an outcome after externally intervening in a random variable of interest xi, known
as treatment or policy.
The intervention operation can be hard or soft as shown in Figure 2, depending
on whether a variable is set to a specific value xi:=a, or whether a new structural
mechanism is defined xi:=f′(Pai, βi) +εi
Intervention distribution conceptually is entirely different from observational dis-
tribution. In the manipulated system, the dooperator graphically changes the causal
structures in a directed acyclic graph. More precisely, regarding the causal graph, the do
operator means all incoming links on the node variable of interest should be removed or
functionality changed, and accordingly the corresponding causal mechanism should be
changed in the SCM. This process results in an interventional structural causal model
representing the intentional manipulation of a data-generating process. Notably the do
or intervention operation on the variable xionly changes the causal mechanism of xi.
This fact is rooted in the Modularity assumption or the Law of Counterfactuals, stat-
ing that in the structural causal model, causal mechanisms are modular or independent
(Pearl, 2009b).
9
Figure 2: Visualization of Intervention Operations (a): original, (b): hard, (c): soft
3 Methodology
To address the inherent lack of causal inference and counterfactual analysis in advanced
travel behaviour models, we introduce two model structures of our deep CAusal infeR-
ence mOdeling for traveL behavIour aNAlysis (CAROLINA) framework 1) interpretable
deep structural causal model, and associated 2) generative counterfactual model.
3.1 Interpertable deep structural causal model
In this framework, we simultaneously capture the unobserved heterogeneity and infer
causal relationships by integrating the deep neural network structure into the struc-
tural causal model in the context of Discrete Choice Models (DCMs). The structure of
this framework in deep CAusal infeRence mOdel for traveL behavIour aNAlysis, (CAR-
OLINA) , can be found in Figure 3. Prior to modelling causal mechanisms, we need to
discover a Directed Acyclic Graph (DAG), representing causal relationships between
variables. In this study, we employ causal discovery algorithms to identify a DAG from
observational datasets, while incorporating a certain level of domain knowledge. Gener-
ally, causal discovery is a powerful approach that allows us to infer causal relationships
from observational data without prior assumptions or domain knowledge. However, it
is also important to consider the constructive impact of domain-specific background
knowledge in improving the causal discovery process and reducing the risk of spurious
or false discoveries. In terms of DAGs searching, domain knowledge can broadly be cat-
egorized into two levels: 1) non-substantive impacts: the edges among some variables,
such as socio-demographic variables should be prohibited, and 2) temporal impacts:
the presence of edges from variables representing the features in the later time to those
at the earlier time should be restricted (Shen et al., 2020).
In our proposed framework, CAROLINA , the DAG obtained from causal discovery
provides the initial information for modelling causal mechanisms of utility functions in
discrete choice modelling. Generally, the structural causal model is a single model that
consists of a collection of asymmetrical structural equations: xi:=fi(Pai, βi)+εi. In the
DAGs, random variables ( xi) can function either purely as an exogenous variable ( xex),
endogenous variable ( xen) or serve the dual role of both. To construct the structural
causal model, we first define the parents of all random variables.
10
Pa(xi) =(
∅, ifPn
j̸=iI(ζji) = 0
{xj, . . . , x n},if∀xj\ {xi},I(ζji)̸= 0(4)
where, for each xithatPa(xi)̸=∅andPa(xi) ={xex},xiis referred as the root of
SCM.
In the case of DCMs, for each individual n, we start computing the utility function
ofUknforxiwith Kdiscrete alternative based on causal structures.
Ui
k1n=:fi
k1(Pai
k1n, βi
k1) +εi
k1n
Ui
k2n=:fi
k1(Pai
k2n, βi
k2) +εi
k2n
...
Ui
Kn=:fi
K(Pai
Kn, βi
K) +εi
Kn(5)
Then, based on the probability of each alternative, the prediction of xiis obtained
and fed to the utility function of other variables that their Pa(xj) ={xex, xi}. This
computation is subsequently continued until we reach the interest outcome variable or
the variable playing only as a child or descendent ( xdes) of other variables in the causal
structure.
CAROLINA utilizes the structure of ResNet-based models, including ResLogit and
Ordinal-ResLogit, which are primarily interpretable learning-based discrete choice mod-
els for both categorical (Wong & Farooq, 2021) and ordinal (Kamal & Farooq, 2024)
datasets. However, the framework is flexible enough that any other combination of
interpetable models can also be used. To obtain the utility function of alternatives,
first, a deterministic K×1 utility vector is computed, where each element represents
the linear impact of causes or parents ( βkPakn). This vector is then passed through m
residual layers using a K×Kmatrix of residual parameters Wm, resulting in the K×1
utility vector ( Ukn). In this vector, each element represents the effect of Paknon the
kthalternative of xiwhile capturing random heterogeneity. Mathematically, the utility
function of each alternative kfor individual ncan be obtained from Equation 6. In the
following equations, for simplicity, iis eliminated from notations.
Ukn=:βkPakn+gkn+εkn (6)
In this equation, ( gkn) is the heterogeneity component of residual layers which is a
kthelement of vector gnas follows:
gn=−MX
m=1ln
1 + exp( WmVn(m−1))
(7)
The residual connection comes from the previous layer and Vn(m−1)is the output
vector of non-linear utility components for the ( m−1)thresidual layer consisting of K
members:
Vnm=Vnm−1−ln(1 + exp( WmVnm−1)) (8)
11"
2205.03186,D:\Database\arxiv\papers\2205.03186.pdf,"Given the limitations of directly concatenating semantic features from adjacent LiDAR scans due to differing coordinate systems, what alternative approach is proposed to address this challenge and enable the integration of these features for moving object segmentation?","The paper proposes an adjacent scan association module (ASA) to convert semantic features from previous scans into the coordinate system of the current scan, allowing for effective integration of these features in the moving object segmentation process.","Fig. 4. The adjacent scan association module. The arrows den ote the corre-
spondences between adjacent LiDAR range images. From top to bottom are
the current range image, previous range image, and the trans formed range
image.
intuitive idea may be to concatenate them directly. However ,
this simple solution does not work because the semantic
features are represented in different coordinate systems. In
this paper, we propose an adjacent scan association module
to convert the semantic features of adjacent scans into the
same coordinate system and then feed them to the MOS
module. The ASA module consists of three steps: coordi-
nate system conversion, range image generation, and featur e
transformation.
Coordinate System Conversion: Assuming that we have a
sequence of NLiDAR scans, and the corresponding odom-
etry information is known. S0denotes the current LiDAR
scan and Sirepresents one previous scan (0< i <=N).
T0
idenotes the relative transformation between the previous
scanSiand the current scan S0. Given this information, we
can convert a LiDAR point in a previous scan Sito the one
in the current scan S0as follows,

xi
0
yi
0
zi
0
1
=T0
i·
xi
yi
zi
1
(2)
where(xi
0,yi
0,zi
0,1)and(xi,yi,zi,1)denote the coordi-
nates of the same 3D point in the coordinate systems of
scanS0and scan Si, respectively. Considering the semantic
featuresFiare represented in the form of range image, we
only need to transform the LiDAR points lying in range
imageRito the coordinate system of scan S0.
Range Image Generation: After the coordinate system
conversion step, we can get the transformed LiDAR point
clouds. In this step, we use Eq. 1 to project the transformed
scanSi
0onto the range image R0and generate a new range
imageRi
0.
Through this Ri⇒Si
0⇒Ri
0process, we can get a corre-
sponding point (u0,v0)in current range image R0if it exists.
The association between the previous and current scans is
beneﬁcial to the subsequent MOS task. If the relative pose T0
i
and the semantic segmentation results are accurate enough,we can even distinguish the moving points from the static by
comparing the corresponding semantic segmentation result s.
The point is static if the semantic prediction in the current
scan is the same as that in the previous scan, otherwise, it is
moving. However, in practical applications, there exist er rors
in both relative pose and semantic segmentation results. Th at
is why we use the semantic features rather than the semantic
results in this paper.
Feature Transformation: In this step, we use the calcu-
lated point correspondence information to assist the featu re
transformation process. Assuming the association between
the previous range image Riand the current range image
R0is represented by Trwith the size of H×W,(u0,v0)is
the corresponding point of (ui,vi)inR0. The transformation
Tris deﬁned as follows,
Tr(ui,vi) ={
u0+v0·W
0,no correspondences(3)
whereTrstores the index information from range image
Rito range image R0.
Then, we use the reshape andscatter functions to trans-
form the semantic features Fiof the previous range images
Rito the current range image R0according to the association
information in Tr.
C. Moving Object Segmentation
Like the semantic segmentation module, we reuse the
existing single-scan-based semantic segmentation models ,
RangeNet++ [10] and SalsaNext [31], for the MOS mod-
ule in this paper. Both RangeNet++ and SalsaNext follow
the encoder-decoder architectures and can obtain accurate
semantic segmentation results in real-time. After the ASA
module, we can get the semantic features of the current range
image and the transformed features of the previous range
image. As mentioned earlier, the differences between these
two semantic features already contain enough information
about the movement. However, if only the semantic features
are used as inputs, the accuracy of relative poses and seman-
tic segmentation network will limit the MOS performance.
Therefore, we also add the range images of the current scan
to minimize the negative impact. The MOS module combines
the semantic features and range images to differentiate the
moving objects. Compared with the range-image-based MOS
methods, the addition of transformed semantic features can
bring signiﬁcant improvement.
After the MOS process, we can get the segmentation
results in the form of a range image as shown in Fig. 2.
In order to further improve the performance, the results wil l
be back-projected to the point cloud form. Then, a k-Nearest -
Neighbor (kNN) search algorithm is applied to remove the
artifacts caused by spherical projection.
IV. E XPERIMENTS
A. SemanticKITTI MOS Dataset
The SemanticKITTI MOS dataset is a large-scale 3D
LiDAR-based moving object segmentation benchmark for
the MOS task in outdoor driving scenes. It is built upon
KITTI [32] and SemanticKITTI [33] datasets. The MOS
dataset has 22 sequences of LiDAR scans, where sequences
00-07 and 09-10 are used for training, sequence 08 for
validation, and sequences 11-21 are used for testing. Only
two classes, moving and static, are used in this dataset.
The intersection-over-union (IoU) [34] value on the moving
objects is the primary metric used for comparison with other
methods.
B. Experimental Setup
The proposed network is implemented on a server with
64GB RAM, an Intel(R) Xeon(R) E5-2650 CPU, and 2
NVIDIA Geforce RTX2080Ti GPUs under Ubuntu using
PyTorch. The evaluation results on the SemanticKITTI MOS
testing dataset are obtained based on all training data, wit h
an epoch size of 150 and batch size of 8. The initial learning
rate and the learning rate policy are set to be the same as the
original networks (RangeNet++ and SalsaNext). If there is
no special description, it indicates that both the single-s can-
based semantic segmentation module and the MOS module
use the SalsaNext network. The odometry information of
each sequence is estimated with the LiDAR-based SLAM
method SuMa [35].
C. Different Transformation Designs
There are two kinds of data representation conversions in
the network. One is the transformation of LiDAR scans to
range images in the semantic segmentation module, and the
other is the transformation of previous semantic features i nto
the current range image in the ASA module. In this section,
we will analyze the inﬂuences of data type, source data, and
hardware on the process of data representation conversion.
1) Data Type: The coordinates of 3D LiDAR points
and the relative pose matrices are saved as ﬂoating-point
numbers, while the coordinates of range images are integer
point numbers. This means that the range images in the
current coordinate system generated based on previous scan s
are different from the range images generated based on the
current scan. Even if the surrounding scene is static, the
relative poses are accurate and there is no occlusion.
2) Source Data: The raw LiDAR scans and the corre-
sponding range images have a different number of points.
Therefore, the range images converted from previous scans
and previous range images are different.
3) Hardware: Converting 3D LiDAR point clouds into 2D
range images inevitably encounters the many-to-one prob-
lem, which means multiple LiDAR points may correspond
to the same point in the range image. In the semantic segmen-
tation module, we only save the points with minimum range
values and perform the transformation on CPUs. However,
in the ASA module, the feature transformation occurs on
GPUs. Parallel computing cannot ensure that points with
minimum range values are saved, and only random points
among the multiple points are saved. This characteristic le ads
to differences between the range images generated based on
CPUs and GPUs.
Fig. 5. The range images in the current coordinate system. Th ey are
generated with different hardware (CPUs or GPUs) and LiDAR p oint clouds
(current scan, previous scan, or previous range image). (a) and (b) are range
images with current scan and based on CPUs and GPUs, respecti vely. (c)
and (d) are range images with the previous scan and based on CP Us and
GPUs, respectively. (e) and (f) are results with previous ra nge images and
based on CPUs and GPUs, respectively.
4) Analysis: Figure 5 shows the range images generated
with different source data and hardware. Among them, Fig.
5 (a) and (c) are the range images of the current and previous
scans generated based on CPUs, respectively. The differenc es
between the two range images are caused by the data type,
the inaccurate relative pose, and the moving objects. The
occlusion also results in more holes at the edge of the object s.
Figure 5 (c) and (e) are the results using the previous scan
and previous range image on CPUs, respectively. We can
ﬁnd that due to the difference in point number, the range
image generated by the previous range image is more sparse
than the one generated by the previous scan. Figure 5 (a)
and (b) are the range images of the current scan based on
CPUs and GPUs, respectively. The difference between the
two range images is visually negligible, which indicates th at
the inﬂuence of the hardware on the range image generation
process can be ignored. Based on the above analysis and the
qualitative comparison results in Fig. 5, we ﬁnally use CPUs
to convert the LiDAR scans into range images (like Fig. 5
(a)), and use GPUs to transform the features of the previous
range images to the current range image (like Fig. 5 (f)).
D. Evaluations on SemanticKITTI MOS Dataset
In this section, we compare our method with the state-
of-the-art MOS networks. Since there are few LiDAR-based
MOS approaches, we also compare our method with some
methods of semantic segmentation and scene ﬂow.
Table I shows the quantitative comparison results. Sal-
saNext(movable classes) denotes the MOS result of di-
rectly marking all the movable classes as moving. Sal-
saNext(retrained) retrains the SalsaNext network with binary
MOS labels. SceneFlow [36] uses the ﬂow vector to de-
TABLE I
MOS P ERFORMANCE COMPARED WITH THE STATE -OF-THE-ART(TEST)
Algorithms IoU
SalsaNext(movable classes) 4.4%
SceneFlow [36] 4.8%
SqSequence [17] 43.2%
SalsaNext(retrained) 46.6%
KPConv [4] 60.9%
LMNet(N=1) 52.0%
LMNet(N=8 + Semantics) 62.5%
Ours 60.6%
termine the moving objects. SqSequence [17] and KPConv
[4] are multiple-scan-based LiDAR semantic segmentation
networks. LMNet(N=1) [29] uses range images and the
precalculated residual image to differentiate the moving o b-
jects. LMNet(N=8 + Semantics) represents the semantically
enhanced version using 8 residual images. The proposed
semantics-guided MOS network takes the semantic features
of current range image, the transformed features of previou s
range image and the range images of current scan as inputs.
As shown in Tab. I, the performance of our semantics-
guided MOS method is basically the same as KPConv ,
slightly worse than the LMNet with 8 residual images
and semantic information. The multiple-scan-based semant ic
segmentation method KPConv is implemented based on point
clouds, which requires high computational overhead and
cannot achieve real-time performance. Our method only uses
the compact LiDAR range images as inputs and can work
in real-time. The LMNet(N=8 + Semantics) is equivalent
to using 9 consecutive scans. Considerable computational
overhead and time are spent on the generation of 8 residual
images on CPUs. Our method only uses two adjacent LiDAR
scans, and the feature association process (the ASA module)
is implemented on GPUs.
Figure 6 shows the ground truths and MOS results in the
forms of range image and point cloud.
E. Ablation Study
In order to save the computational overhead and time cost,
we only use the training data sampled at equal intervals of
4000 scans in the ablation study section. The epoch size and
batch size are set as 30 and 2, respectively.
1) Effectiveness of Semantic Guidance: Most of the ex-
isting LiDAR-based MOS methods use the raw LiDAR data
and conduct the MOS task directly. The proposed network in-
troduces a semantic segmentation module to assist the MOS
process. In this section, we will analyze the effectiveness
of the semantic guidance from the semantic segmentation
module.
Table II shows the MOS results on the validation dataset.
MOS(RXYZI + Range Residual) only uses the MOS mod-
ule and takes range images and residual image as inputs.
MOS(RXYZI + Range Residual) + Semantics represents
its semantically enhanced version and uses the semantic
segmentation results at the end. MOS(RXYZI + Features)
denotes the proposed method with range images and se-
mantic features as inputs of the MOS module. Table IITABLE II
EFFECTIVENESS OF SEMANTIC GUIDANCE (VALIDATION )
Algorithms Params IoU
MOS(RXYZI + Range Residual) 6711043 38.6%
MOS(RXYZI + Range Residual) + Semantics 13422615 41.4%
MOS(RXYZI + Features) 13423863 60.5%
TABLE III
EFFECTIVENESS OF SEMANTIC GUIDANCE (VALIDATION )
Algorithms Params IoU
LMNet(N=1) 6711043 59.9%
LMNet(N=1 + Semantics) 13422615 61.4%
MOS(RXYZI + Features) 13423863 68.4%
are the MOS results on the validation dataset with all
training data. LMNet(N=1) and LMNet(N=1 + Semantics)
use the precalculated residual image. The latter also uses
the semantic segmentation results. MOS(RXYZI + Features)
represents the proposed network using range images and
semantic features in the MOS module.
It is obvious that semantic information can help improve
the MOS performance. The single-scan-based semantic seg-
mentation task is closely related to the MOS task. Compared
with using the residual images or using semantic segmenta-
tion results at the end, our method with semantic features as
the inputs of the MOS module is more effective. The pre-
trained single-scan-based semantic segmentation model ca n
not only simplify the training process but also improve the
MOS performance.
2) Different MOS Inputs: In this section, we will compare
the MOS performance of different inputs, including raw
LiDAR data (RXYZI), range residual image, and semantic
features.
Table IV shows the comparison results. RXYZI gets the
worst MOS performance due to the lack of temporal informa-
tion. Features(current) andRXYZI + Features(current) also
use the current LiDAR scan only, but obviously, semantic
guidance without temporal information can still improve th e
results. RXYZI + Range Residual and RXYZI + Features
combining the range images with the residual image or
semantic features can increase the MOS accuracy. Compared
with the residual image, the semantic features are more
effective for the MOS task. However, using both residual
image and semantic features at the same time degrades the
performance. This may be caused by the conﬂicts between
residual image and semantic features. A comparison between
Features andRXYZI + Features shows that range images also
contribute to the MOS task. Connecting the current semantic
features with the transformed previous features can obtain
better results than using the feature residuals or directly
concatenating the semantic features without ASA operation .
In summary, the transformed semantic features are more
effective than the residual image and the original semantic
features from previous scans.
3) Inﬂuence of Scan Numbers: In this section, we will
analyze the inﬂuence of the LiDAR scans used on the MOS
Fig. 6. The MOS results in the forms of range image and point cl oud. (b), (d), (f) and (h) are the MOS results predicted by our method. (a), (c), (e) and
(g) are the corresponding ground truths. Red denotes the mov ing objects.
TABLE IV
DIFFERENT MOS I NPUTS (VALIDATION )
Algorithms Params IoU
RXYZI 6711011 26.9%
Features(current) 13423063 45.2%
RXYZI + Features(current) 13423223 51.8%
RXYZI + Range Residual 6711043 38.6%
RXYZI + Features 13423863 60.5%
RXYZI + Range Residual + Features 13423895 58.9%
Features 13423703 56.1%
RXYZI + Feature Residuals 13423223 39.1%
RXYZI + Features(concat) 13423863 48.7%
performance. It is similar to the residual image analysis in
LMNet. Theoretically, more LiDAR scans bring more useful
information and improve the MOS performance.
Table V shows the MOS results with 2 to 8 consecutive
LiDAR scans as inputs. It is obvious that the MOS accuracy
does not increase with the increase of LiDAR scans. This is
different from the residual images in LMNet. There may be
two reasons. First, the semantic segmentation module is not
accurate enough. The more LiDAR scans (semantic features),
the more conﬂicts, which affects the improvement of the
MOS result. Second, the relative poses between scans are
also inaccurate. And this inaccuracy will increase with the
increase of time interval.
In addition, although the transformations from the previou s
range images to the current are performed on GPUs, it
still affects the training time cost. Based on the above
considerations, we only use two adjacent LiDAR scans in
our MOS method.
4) Inﬂuence of Modular Design: The proposed network
can be divided into the single-scan-based semantic segmen-
tation module, the ASA module, and the MOS module. In
order to analyze the beneﬁts of modular design, we use
different network architectures to replace the single-sca n-TABLE V
INFLUENCE OF SCAN NUMBERS (VALIDATION )
Scans Params IoU
2 13423863 60.5%
3 13424503 59.3%
4 13425143 57.0%
5 13425783 59.0%
6 13426423 54.4%
7 13427063 60.0%
8 13427703 58.3%
TABLE VI
INFLUENCE OF MODULAR DESIGN (VALIDATION )
Algorithms Params IoU
SS(RangeNet++) + MOS(RangeNet++) 100761335 29.5%
SS(RangeNet++) + MOS(SalsaNext) 57089655 48.4%
SS(SalsaNext) + MOS(SalsaNext) 13423863 60.5%
SS(SalsaNext) + MOS(RangeNet++) 57095543 35.7%
based semantic segmentation module and the MOS module.
Table VI shows the MOS results with different network
architectures. SS(RangeNet++) andMOS(RangeNet++) rep-
resent the use of RangeNet++ network in the single-scan-
based semantic segmentation module and the MOS module,
respectively. SS(SalsaNext) andMOS(SalsaNext) indicate the
use of SalsaNext network.
The SalsaNext network has better semantic segmentation
performance than the RangeNet++ network. The MOS re-
sults in Tab. VI shows that due to the modular design, the
MOS performance can be improved by simply updating any
trainable module in the network.
V. C ONCLUSIONS
In this paper, we propose a moving object segmenta-
tion network with semantic information as guidance. The
whole network includes three modules: a single-scan-based
semantic segmentation module, an adjacent scan associatio n
(ASA) module, and a multiple-scan-based moving object
segmentation module. The ASA module works as an inter-
mediate to connect the semantic segmentation module and
the MOS module. After the ASA process, we can obtain
the correspondences and the difference information betwee n
the semantic features, which are beneﬁcial to the MOS task.
The experiment on the SemanticKITTI MOS dataset shows
the effectiveness of the network. The modular design also
facilitates the upgrading of a single module and further
improves the MOS accuracy.
REFERENCES
[1] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep lea rning
on point sets for 3d classiﬁcation and segmentation,” in CVPR . IEEE
Computer Society, 2017, pp. 77–85.
[2] W. Wu, Z. Qi, and F. Li, “Pointconv: Deep convolutional ne tworks
on 3d point clouds,” in CVPR . Computer Vision Foundation / IEEE,
2019, pp. 9621–9630.
[3] M. Tatarchenko, J. Park, V . Koltun, and Q. Zhou, “Tangent convo-
lutions for dense prediction in 3d,” in CVPR . Computer Vision
Foundation / IEEE Computer Society, 2018, pp. 3887–3896.
[4] H. Thomas, C. R. Qi, J. Deschaud, B. Marcotegui, F. Goulet te, and
L. J. Guibas, “Kpconv: Flexible and deformable convolution for point
clouds,” in ICCV . IEEE, 2019, pp. 6410–6419.
[5] C. B. Choy, J. Gwak, and S. Savarese, “4d spatio-temporal convnets:
Minkowski convolutional neural networks,” in CVPR . Computer
Vision Foundation / IEEE, 2019, pp. 3075–3084.
[6] L. P. Tchapmi, C. B. Choy, I. Armeni, J. Gwak, and S. Savare se,
“Segcloud: Semantic segmentation of 3d point clouds,” in 3DV. IEEE
Computer Society, 2017, pp. 537–547.
[7] X. Zhu, H. Zhou, T. Wang, F. Hong, Y . Ma, W. Li, H. Li, and
D. Lin, “Cylindrical and asymmetrical 3d convolution netwo rks for
lidar segmentation,” in CVPR . Computer Vision Foundation / IEEE,
2021, pp. 9939–9948.
[8] B. Wu, A. Wan, X. Yue, and K. Keutzer, “Squeezeseg: Convol utional
neural nets with recurrent CRF for real-time road-object se gmentation
from 3d lidar point cloud,” in ICRA . IEEE, 2018, pp. 1887–1893.
[9] B. Wu, X. Zhou, S. Zhao, X. Yue, and K. Keutzer, “Squeezese gv2:
Improved model structure and unsupervised domain adaptati on for
road-object segmentation from a lidar point cloud,” in ICRA . IEEE,
2019, pp. 4376–4382.
[10] A. Milioto, I. Vizzo, J. Behley, and C. Stachniss, “Rang enet ++: Fast
and accurate lidar semantic segmentation,” in IROS . IEEE, 2019, pp.
4213–4220.
[11] H. Radi and W. Ali, “V olmap: A real-time model for semant ic segmen-
tation of a lidar surrounding view,” arXiv preprint arXiv:1906.11873 ,
2019.
[12] Y . Zhang, Z. Zhou, P. David, X. Yue, Z. Xi, B. Gong, and H. F oroosh,
“Polarnet: An improved grid representation for online lida r point
clouds semantic segmentation,” in CVPR . Computer Vision Foun-
dation / IEEE, 2020, pp. 9598–9607.
[13] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep
hierarchical feature learning on point sets in a metric spac e,” in NIPS ,
2017, pp. 5099–5108.
[14] L. Landrieu and M. Simonovsky, “Large-scale point clou d semantic
segmentation with superpoint graphs,” in CVPR . Computer Vision
Foundation / IEEE Computer Society, 2018, pp. 4558–4567.
[15] Q. Hu, B. Yang, L. Xie, S. Rosa, Y . Guo, Z. Wang, N. Trigoni , and
A. Markham, “Randla-net: Efﬁcient semantic segmentation o f large-
scale point clouds,” in CVPR . Computer Vision Foundation / IEEE,
2020, pp. 11 105–11 114.
[18] F. Duerr, M. Pfaller, H. Weigel, and J. Beyerer, “Lidar- based recurrent
3d semantic segmentation with temporal memory alignment,” in3DV.
IEEE, 2020, pp. 781–790.[16] R. Cheng, R. Razani, E. Taghavi, E. Li, and B. Liu, “(af)2 -s3net:
Attentive feature fusion with adaptive feature selection f or sparse
semantic segmentation network,” in CVPR . Computer Vision Foun-
dation / IEEE, 2021, pp. 12 547–12 556.
[17] H. Shi, G. Lin, H. Wang, T. Hung, and Z. Wang, “Spsequence net:
Semantic segmentation network on 4d point clouds,” in CVPR . Com-
puter Vision Foundation / IEEE, 2020, pp. 4573–4582.
[19] H. Cao, Y . Lu, B. Pang, C. Lu, A. L. Yuille, and G. Liu, “Asa p-net:
Attention and structure aware point cloud sequence segment ation,” in
BMVC . BMV A Press, 2020.
[20] H. Tang, Z. Liu, S. Zhao, Y . Lin, J. Lin, H. Wang, and S. Han ,
“Searching efﬁcient 3d architectures with sparse point-vo xel convo-
lution,” in ECCV (28) , ser. Lecture Notes in Computer Science, vol.
12373. Springer, 2020, pp. 685–702.
[21] D. Barnes, W. Maddern, G. Pascoe, and I. Posner, “Driven to distrac-
tion: Self-supervised distractor learning for robust mono cular visual
odometry in urban environments,” in ICRA . IEEE, 2018, pp. 1894–
1900.
[22] P. W. Patil, K. M. Biradar, A. Dudhane, and S. Murala, “An end-to-end
edge aggregation network for moving object segmentation,” inCVPR .
Computer Vision Foundation / IEEE, 2020, pp. 8146–8155.
[23] J. Yan, D. Chen, H. Myeong, T. Shiratori, and Y . Ma, “Auto matic
extraction of moving objects from image and LIDAR sequences ,” in
3DV. IEEE Computer Society, 2014, pp. 673–680.
[24] G. Postica, A. Romanoni, and M. Matteucci, “Robust movi ng objects
detection in lidar data exploiting visual cues,” in IROS . IEEE, 2016,
pp. 1093–1098.
[25] G. Kim and A. Kim, “Remove, then revert: Static point clo ud map
construction using multiresolution range images,” in IROS . IEEE,
2020, pp. 10 758–10 765.
[26] S. Pagad, D. Agarwal, S. Narayanan, K. Rangan, H. Kim, an d V . G.
Yalla, “Robust method for removing dynamic objects from poi nt
clouds,” in ICRA . IEEE, 2020, pp. 10 765–10 771.
[27] J. Schauer and A. N¨ uchter, “The peopleremover - removi ng dynamic
objects from 3-d point cloud data by traversing a voxel occup ancy
grid,” IEEE Robotics Autom. Lett. , vol. 3, no. 3, pp. 1679–1686, 2018.
[28] P. Ruchti and W. Burgard, “Mapping with dynamic-object probabilities
calculated from single 3d range scans,” in ICRA . IEEE, 2018, pp.
6331–6336.
[29] X. Chen, S. Li, B. Mersch, L. Wiesmann, J. Gall, J. Behley , and
C. Stachniss, “Moving object segmentation in 3d lidar data: A
learning-based approach exploiting sequential data,” IEEE Robotics
Autom. Lett. , vol. 6, no. 4, pp. 6529–6536, 2021.
[30] P. He, P. Emami, S. Ranka, and A. Rangarajan, “Learning s cene
dynamics from point cloud sequences,” Int. J. Comput. Vis. , vol. 130,
no. 3, pp. 669–695, 2022.
[31] T. Cortinhal, G. Tzelepis, and E. E. Aksoy, “Salsanext: Fast,
uncertainty-aware semantic segmentation of lidar point cl ouds,” in
ISVC (2) , ser. Lecture Notes in Computer Science, vol. 12510.
Springer, 2020, pp. 207–222.
[32] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for auto nomous
driving? the KITTI vision benchmark suite,” in CVPR . IEEE
Computer Society, 2012, pp. 3354–3361.
[33] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke , C. Stach-
niss, and J. Gall, “Semantickitti: A dataset for semantic sc ene under-
standing of lidar sequences,” in ICCV . IEEE, 2019, pp. 9296–9306.
[34] M. Everingham, L. V . Gool, C. K. I. Williams, J. M. Winn, a nd
A. Zisserman, “The pascal visual object classes (VOC) chall enge,”
Int. J. Comput. Vis. , vol. 88, no. 2, pp. 303–338, 2010.
[35] J. Behley and C. Stachniss, “Efﬁcient surfel-based SLA M using 3d
laser range data in urban environments,” in Robotics: Science and
Systems , 2018.
[36] X. Liu, C. R. Qi, and L. J. Guibas, “Flownet3d: Learning s cene ﬂow
in 3d point clouds,” in CVPR . Computer Vision Foundation / IEEE,
2019, pp. 529–537."
2207.00081,D:\Database\arxiv\papers\2207.00081.pdf,"If the cost of renewable energy sources continues to decrease, what impact might this have on the carbon footprint of computing, and what challenges might arise in achieving a truly carbon-neutral computing environment?","Decreasing renewable energy costs could significantly reduce computing's carbon footprint by incentivizing the adoption of cleaner energy sources. However, achieving true carbon neutrality presents challenges, as the grid's energy mix is complex and requires careful accounting for embodied carbon emissions throughout the supply chain.","increases in computing’s energy-efﬁciency on its energy con-
sumption is largely an economic, and not technical, question.
Of course, improving computing’s energy-efﬁciency is al-
ways beneﬁcial, as it increases productivity and economic out-
put, i.e., enables more to be done with less energy at lower cost.
Thus, as with improving algorithmic efﬁciency, industry has a
strong ﬁnancial incentive to improve energy-efﬁciency. This
incentive has likely driven the incredible energy-efﬁciency im-
provements over the past ﬁfty years. However, improvements
in computing’s energy-efﬁciency do not necessarily, and have
not historically, led to reductions in its energy consumption. In
fact, if Jevons Paradox occurs, improving computing’s energy-
efﬁciency may contribute to increasing energy consumption.
Key Point. Computing’s energy-efﬁciency is continuing to
increase, although its rate may be slowing. Improvements
to computing’s energy-efﬁciency are bounded, and do not
necessarily, and have not historically, led to reductions in
computing’s energy consumption, due to faster growth in de-
mand both from new applications and lower costs.
4 Energy’s Carbon-Efﬁciency
Unlike algorithmic- and energy-efﬁciency, there is no funda-
mental limit to energy’s carbon-efﬁciency, since it is possi-
ble to use zero-carbon energy sources, such as solar, wind,
geothermal, hydroelectric, nuclear, etc. The cost for solar
and wind renewable energy sources, in particular, have also
been decreasing exponentially for some time. Swanson’s law,
which captures this trend for solar energy, refers to the obser-
vation that solar photovoltaic PV module prices have tended
to drop 20% for every doubling in production volume [45].
As a result, solar energy’s cost (in $/watt) has dropped ∼10%
each year on average over the past ﬁfty years [19]. Renew-
able energy sources also have massive energy potential that
could fuel exponential growth for the foreseeable future. For
example, the amount of solar energy the earth receives each
hour is more than global annual energy consumption [28].
As mentioned in §1, energy’s carbon-efﬁciency has been
steadily increasing for the past 20 years, mostly due to the
adoption of natural gas and wind. This trend has been inde-
pendent of any efforts by the computing industry to reduce its
carbon footprint. However, isolating and capturing the trend
the carbon-efﬁciency of computing’s energy is more chal-
lenging, as it depends on the strictness of carbon accounting
and attribution methods used. Carbon offsets are the loosest,
and most widely used, method of carbon accounting: they en-
able “offsetting” the use of carbon-intensive grid energy with
zero-carbon renewable energy generated at another location
and time. Technology companies have led in the adoption of
carbon offsets, and many have used them to eliminate their
net carbon footprint, which is often referred to as running
on 100% renewable energy [3, 23, 37]. However, while car-
bon offsets are beneﬁcial in subsidizing renewable energy,
they are only a temporary mechanism as society transitions tolower carbon energy, since near zero-carbon there will not be
any carbon left to offset. In addition, the use of carbon offsets
means that even net zero companies are still responsible for a
signiﬁcant amount of direct carbon emissions.
To reach zero-carbon, companies must progressively adopt
stricter forms of carbon accounting. To this end, Google re-
cently announced that it aims to be “carbon free” by 2030,
in part, by piloting a stricter form of carbon offset, called
Time-based Energy Attribute Certiﬁcates (TEACs), which
have an hourly location-speciﬁc accounting regime [7]. How-
ever, TEACs are still carbon offsets, just at a higher temporal
and spatial resolution than typical offsets, which are usually 1
year and the entire earth, respectively. That is, TEACs match
consumption of grid energy within an hour to renewable gen-
erated that hour within the same grid. Thus, while TEACs are
an improvement upon existing annualized location-agnostic
carbon offsets, they, by deﬁnition, also cannot be used to reach
zero-carbon. Of course, since the grid cannot physically iso-
late different energy sources, in reality, all loads that consume
grid energy share in its carbon emissions. Thus, the strictest
form of carbon accounting attributes the grid’s carbon emis-
sions to all its loads based on their energy use. As a result,
reducing and ultimately eliminating computing’s carbon emis-
sions will require changing its operations to be responsive to
variations in grid energy’s carbon emissions and availability.
Thus far, we have focused on trends in operational car-
bon, i.e., carbon emissions from using grid energy. There has
also been an increasing focus on accounting for and reducing
“embodied carbon,” which represents the carbon emissions
from producing a product or service [22,26,27]. For example,
computing’s embodied carbon emissions are based on the
carbon emissions from manufacturing the facilities and IT
equipment that host it. Importantly, though, one company’s
embodied carbon is another company’s operational carbon.
For example, a cloud platform’s embodied carbon is, in part, a
chip manufacturer’s operational carbon. The primary purpose
in accounting for embodied carbon is to provide an incentive
in the supply chain for companies to reduce their operational
carbon. That is, if companies made purchasing decisions to
reduce their embodied carbon, it would incentive upstream
suppliers to in-turn reduce their operational carbon. Account-
ing for embodied carbon is akin to a value added tax (V AT),
as carbon emissions, similar to a V AT, are associated with the
value added at each production stage of a good or service.
Unfortunately, unlike with algorithmic- and energy-
efﬁciency, there are not yet strong ﬁnancial incentives for
companies to reduce their operational or embodied carbon
emissions, as energy prices do not yet incorporate the cost of
carbon’s negative externalities to the environment. As a result,
while energy’s carbon-efﬁciency has been improving, and is
unbounded, its long-term trend is unclear.
Key Point. Energy’s carbon-efﬁciency is increasing, although
its long-term trend is unclear due to the lack of ﬁnancial incen-
tive to improve it. Improvements to energy’s carbon-efﬁciency
4
are unbounded, as it is possible to only use zero-carbon en-
ergy. Since there will be no carbon offsets at zero-carbon,
eliminating computing’s carbon emissions will ultimately re-
quire eliminating the grid’s carbon emissions.
5 Implications for Sustainable Computing
The trends above have important implications for sustain-
able computing moving forward. Speciﬁcally, given the fun-
damental limits to improving computing’s algorithmic- and
energy-efﬁciency, the only way to sustain exponential growth
in its demand, while also eliminating its carbon footprint, is
to improve its energy’s carbon-efﬁciency. However, the trends
in energy’s carbon-efﬁciency are not yet clear. In particular,
the terminology above around different forms of carbon ac-
counting, e.g., “100% renewable energy,” “carbon-neutral,”
“carbon-free,” “zero-carbon,” ‘embodied carbon,” etc., is com-
plex and fully understanding it requires some non-trivial tech-
nical background on how society’s energy system works. To
anyone without such a background, which includes much of
the general public as well as many computing researchers,
the use of the terms above may inadvertently convey the false
impression that computing’s carbon emissions are already at
zero, or soon will be. Such messaging is often pejoratively
referred to as “greenwashing.” False impressions of com-
puting’s carbon footprint are a signiﬁcant issue, as they can
diminish the perception of progress in decarbonizing comput-
ing, or even discourage further research altogether.
In the end, as we discuss, the various forms of carbon
accounting and offsets are temporary measures that, by deﬁ-
nition, will not be applicable at zero-carbon. To reach zero-
carbon, computing, and more generally society, will have to
signiﬁcantly change how it operates to directly use renewable
and low-carbon energy. Of course, the problem with renew-
able energy is that, while it is potentially plentiful, cheap, and
clean, it is also highly unreliable. In particular, solar and wind
vary widely and uncontrollably over time based on the earth’s
movement and weather. As a result, transitioning the grid to
operate entirely on zero-carbon energy will require either i)
signiﬁcant over-provisioning within the energy system, e.g.,
of batteries, solar, wind, etc., which is likely cost-prohibitive,
or ii) signiﬁcant ﬂexibility in the system’s loads.
Fortunately, compared to other loads, computing is
uniquely ﬂexible with substantial performance, temporal, and
spatial ﬂexibility, enabling it to shift the intensity, time, and lo-
cation of its execution to better align with when and where re-
newable and other low-carbon energy is available. To the best
of our knowledge, computing is the only load with substan-
tial spatial ﬂexibility that is capable of migrating its energy
consumption over long distances. In addition, computing can
also leverage numerous software-based fault-tolerance tech-
niques, e.g., checkpointing, replication, and recomputation, to
continue execution despite unexpected renewable shortages,
which may require throttling or shutting down servers. Thus,computing has the potential to leverage its multiple dimen-
sions of ﬂexibility to not only lower its direct carbon footprint,
but offset variations in renewable energy’s availability. As a
result, computing is not just another grid load, as it can also
act as an energy resource, akin to a battery, that the grid can
deploy to balance demand with a variable supply [13]. In
some sense, improving energy’s carbon-efﬁciency is related
to improving computing’s energy-ﬂexibility by enabling it to
adapt to when and where low-carbon energy is available.
While many have recognized computing’s unique dimen-
sions of energy ﬂexibility, there has been much less research
on exercising them to optimize energy’s carbon-efﬁciency
compared to computing’s energy-efﬁciency, even though, as
Equation 2 shows, carbon-efﬁciency is just as important as
energy-efﬁciency in determining computing’s carbon foot-
print. One reason for the lack of research is likely that, unlike
with algorithmic- and energy-efﬁciency, there is neither a di-
rect nor strong ﬁnancial incentive to improve energy’s carbon-
efﬁciency, although this may change as renewable energy
prices drop. That said, there is a weak, but increasing, indi-
rect incentive to track and improve energy’s carbon-efﬁciency
both to appeal to environmentally-conscious consumers (and
employees), and as a hedge against future changes in the en-
ergy system, such as energy constraints due to geopolitical
events, stricter carbon regulations imposed by governments,
or further signiﬁcant drops in renewable or battery prices.
Another reason for the lack of research may also be that
optimizing carbon-efﬁciency requires deeper visibility into
energy’s carbon emissions, which has historically not been
available. Recently, carbon information services, such as elec-
tricityMap [4] and WattTime [6], have emerged, and are be-
ginning to address this issue by tracking grid energy’s carbon
emissions for different regions over time, and making them
available online. The data shows that grid energy’s carbon
emissions vary signiﬁcantly by region and over time. Cloud
platforms have started adopting these services to enable their
users to estimate the carbon emissions of their energy con-
sumption, and adjust their operations to reduce emissions [9].
Ultimately, the primary implication for achieving sustain-
able computing from the trends above is that research should
emphasize improvements to the carbon-efﬁciency of both
computing’s energy (by adapting to when and where low-
carbon energy is available), as well as the grid’s energy (by
leveraging computing as an energy resource). The former is
important for reducing computing’s direct carbon emissions,
while the latter is important for reducing society’s carbon
emissions, which are related and also affect embodied car-
bon. Given the lack of a strong ﬁnancial incentive to improve
carbon-efﬁciency, academic research in this area is especially
important. Indeed, historically, an explicit purpose of aca-
demic research has been to focus on problems that industry
does not address due to lack of a near-term ﬁnancial incentive.
Acknowledgements. This research is supported by NSF
grants 2105494, 2021693, 2020888, as well as VMware.
5
References
[1]EPA Report to Congress on Server and Data Center
Energy Efﬁciency. Technical report, U.S. Environmental
Protection Agency, August 2007.
[2]OpenAI Blog, AI and Compute. https://openai.
com/blog/ai-and-compute/ , March 16th 2018.
[3]Reuters, Amazon Vows to be Carbon Neutral by
2040, buying 100,000 Electric Vans. https://www.
reuters.com/article/us-amazon-environment/
amazon-vows-to-be-carbon-neutral-by-2040-/
buying-100000-electric-vans-idUSKBN1W41ZV ,
September 19th 2019.
[4]Electricity Map. https://www.electricitymap.
org/map , Accessed September 2020.
[5]Hyperscale Data Center Count Reaches 541 in Mid-
2020; Another 176 in the Pipeline. Technical report,
Synergy Research Group, 2020.
[6]WattTime. https://www.watttime.org/ , Accessed
September 2020.
[7]24/7 by 2030: Realizing a Carbon-free Fu-
ture. https://www.gstatic.com/gumdrop/
sustainability/247-carbon-free-energy.pdf ,
Accessed May 2022.
[8]Digiconomist, Bitcoin Energy Consump-
tion Index. https://digiconomist.net/
bitcoin-energy-consumption , Accessed May
2022.
[9]Google Cloud Carbon Footprint Console. https:
//cloud.google.com/carbon-footprinthttps:
//cloud.google.com/carbon-footprint , Ac-
cessed June 2022.
[10] Google Data Centers: Efﬁciency. http:
//google.com/about/datacenters/efficiency/ ,
Accessed May 2022.
[11] Uptime Institute Global Data Center Survey
2021: Growth Stretches an Evolving Sector.
https://uptimeinstitute.com/resources/
asset/2021-data-center-industry-survey ,
Accessed May 2022.
[12] VentureBeat, report: 75% of Compa-
nies are Focusing on Cloud-Native Apps.
https://venturebeat.com/2022/05/04/
report-75-of-companies-are-focusing-on-/
cloud-native-apps/ , May 4th 2022.[13] A. Agarwal, J. Sun, S. Noghabi, S. Iyengar, A. Badam,
R. Chandra, S. Seshan, and S. Kalyanaraman. Virtual
battery: Redesigning cloud computing for renewable
energy. In HotNets , November 2021.
[14] International Energy Agency. Global Data Centre En-
ergy Demand by Data Centre Type, 2010-2022. https:
//www.iea.org/data-and-statistics/charts/
global-data-centre-energy-demand-by-data-/
centre-type-2010-2022 , March 2021.
[15] B. Alcott. Jevons’ Paradox. Ecological Economics ,
54(1):9–21.
[16] Anders SG Andrae. Projecting the Chiaroscuro of the
Electricity Use of Communication and Computing from
2018 to 2030. Preprint , 2019.
[17] Anders SG Andrae and Tomas Edler. On Global Elec-
tricity Usage of Communication Technology: Trends to
2030. Challenges , 2015.
[18] Lotﬁ Belkhir and Ahmed Elmeligi. Assessing ICT
Global Emissions Footprint: Trends to 2040 & Rec-
ommendations. Journal of Cleaner Production , 2018.
[19] A. de la Tour, M. Glachant, and Y . Ménière. What Cost
for Photovoltaic Modules in 2020? Lessons from Expe-
rience Curve Models. Technical report, Interdisciplinary
Institute for Innovation, May 2013.
[20] Peter J. Denning and Ted G. Lewis. Exponential Laws
of Computing Growth. Communications of the ACM ,
60(1):54–65, January 2017.
[21] Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong,
Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi
Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam
Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang,
Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin
Robinson, Kathy Meier-Hellstern, Toju Duke, Lucas
Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng
Chen, and Claire Cui. Glam: Efﬁcient scaling of lan-
guage models with mixture-of-experts. Technical report,
Google Inc., December 2021.
[22] Carole-Jean Wu et al. Sustainable AI: Environmental
Implications, Challenges and Opportunities. In MLSys ,
August 2022.
[23] Darrell Etherington. TechCrunch, Google Claims
Net Zero Carbon Footprint over its Entire Life-
time, Aims to only use Carbon-Free Energy by
2030. https://techcrunch.com/2020/09/14/
google-claims-net-zero-carbon-footprint-/
over-its-entire-lifetime-aims-to-only-use/
-carbon-free-energy-by-2030/ , September 14th
2020.
6
[24] Hugues Ferreboeuf. LEAN ICT- Towards Digital Sobri-
ety. https://theshiftproject.org/en/article/
lean-ict-our-new-report/ , March 2019.
[25] Michael P. Frank. Foundations of Generalized Re-
versible Computing. In Conference on Reversible Com-
putation , June 2017.
[26] Udit Gupta, Mariam Elgamal, Gage Hills, Gu-Yeon Wei,
Hsien-Hsin S. Lee, David Brooks, and Carole-Jean Wu.
ACT: Designing Sustainable Computer Systems with
an Architectural Carbon Modeling Tool. In ISCA , June
2022.
[27] Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse,
Hsien-Hsin S. Lee, Gu-Yeon Wei, David Brooks, and
Carole-Jean Wu. Chasing Carbon: The Elusive Envi-
ronmental Footprint of Computing. In HPCA , February
2021.
[28] R. Harrington. Business Insider, This incred-
ible fact should get you psyched about solar
power. https://www.businessinsider.com/
this-is-the-potential-of-solar-power-2015-9 ,
September 29th 2022.
[29] Ralph Hintemann. Efﬁciency Gains are Not Enough:
Data Center Energy Consumption Continues to Rise
Signiﬁcantly. Technical report, Borderstep Institute for
Innovation and Sustainability, 2018.
[30] Stephen P. Holland, Matthew J. Kotchen, Erin T. Mansur,
and Andrew J. Yates. Why Marginal CO2 Emissions
Are Not Decreasing for U.S. Electricity: Estimates and
Implications for Climate Policy. Proceedings of the
National Academy of Sciences , 119(8):e2116632119,
2022.
[31] J. Koomey. Growth in Data Center Electricity Use 2005
to 2010. https://www.koomey.com/research.html ,
2011.
[32] Jonathan Koomey, Stephen Berard, Maria Sanchez, and
Henry Wong. Implications of Historical Trends in the
Electrical Efﬁciency of Computing. IEEE Annals of the
History of Computing , 33(3):46–54, March 2010.
[33] Jonathan Koomey and Samuel Naffziger. Moore’s Law
Might be Slowing Down, but not Energy Efﬁciency.
IEEE spectrum , 2015.
[34] R. Landauer. Irreversibility and Heat Generation in
the Computing Process. IBM Journal of Research and
Development , 5(3):183–191, July 1961.
[35] David MacKay. Sustainable Energy - Without the Hot
Air. UIT cambridge, 2008.[36] Eric Masanet, Arman Shehabi, Nuoa Lei, Sarah Smith,
and Jonathan Koomey. Recalibrating Global Data Cen-
ter Energy-use Estimates. Science , 367(6481):984–986,
February 2020.
[37] Kevin O’Sullivan. The Irish Times, Facebook Commits
to Net-Zero Carbon Emissions by 2030. https:
//www.irishtimes.com/news/environment/
facebook-commits-to-net-zero-carbon-/
emissions-by-2030-1.4354701 , September 15th
2020.
[38] David Patterson, Joseph Gonzalez, Urs Hölzle, Quoc Le,
Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild,
David So, Maud Texier, and Jeff Dean. The Carbon
Footprint of Machine Learning Training Will Plateau,
Then Shrink. Technical report, Google Inc., April 2022.
[39] Raymond Perrault, Yoav Shoham, Erik Brynjolfsson,
Jack Clark, John Etchemendy, Barbara Grosz, Terah
Lyons, James Manyika, Saurabh Mishra, and Juan Car-
los Niebles. The AI Index 2019 Annual Report. AI
Index Steering Committee, Human-Centered AI Institute,
Stanford University, Stanford, CA , 2019.
[40] Greg Schivley, Ines Azevedo, and Constantine Sama-
ras. Assessing the Evolution of Power Sector Carbon
Intensity in the United States. Environmental Research
Letters , 13(064018), June 2018.
[41] Arman Shehabi, Sarah Josephine Smith, Dale A. Sar-
tor, Richard E. Brown, Magnus Herrlin, Jonathan G.
Koomey, Eric R. Masanet, Nathanial Horner, Ines Lima
Azevedo, and William Linter. United States Data Cen-
ter Energy Usage Report. Technical Report LBNL-
1005775, Lawrence Berkeley National Lab (LBL), June
2016.
[42] Brad Smith. Ofﬁcial Microsoft Blog, Microsoft
will be Carbon Negative by 2030. https:
//blogs.microsoft.com/blog/2020/01/16/
microsoft-will-be-carbon-negative-by-2030/ ,
January 16th 2020.
[43] David R. So, Wojciech Manke, Hanxiao Liu, Zihang
Dai, Noam Shazeer, and Quoc V . Le. Primer: Searching
for Efﬁcient Transformers for Language Modeling. In
NeurIPS , December 2021.
[44] S. Sorrell. Jevons’ Paradox Revisited: The Evidence for
Backﬁre from Improved Energy Efﬁciency. 37(4):1456–
1469, 2009.
[45] R. Swanson. A Vision for Crystalline Silicon Photo-
voltaics. Progess in Photovoltaics: Research and Appli-
cations , 14(5), August 2006.
7"
2211.14405,D:\Database\arxiv\papers\2211.14405.pdf,How does the proposed probabilistic method GNN model address the limitations of existing approaches for designing loss functions in combinatorial optimization problems?,"The proposed model avoids the drawbacks of previous methods by minimizing the expected value of the objective function divided by the probability of finding a solution, ensuring a greater probability measure for solutions with lower objective function values. This approach also eliminates the need for a pre-set parameter and avoids the issue of upper bounds affecting convergence.","tree is inherently a heuristic search algorithm while ours is an exact backtracking
solver. Li et al.’s search tree is parameterized with a pre-set branching factor
M(which they evaluate experimentally and suggest an ideal value of M= 32),
while our search tree branches on choices made in local structures. As we
are interested in exact search, an important metric we use is in counting the
branches expanded in our search tree, rather than counting the fraction of solved
problems like in [5].
In the following, we denote by N(v) the open neighborhood of a vertex v
(i.e. the subset of vertices that are adjacent to v) andN[v] =N(v)∪{v}the
closed neighborhood. We also denote by [ n] the set of integers {1,2,...,n}.
3 Branching Heuristic from New Probabilistic-
Method GNN
In this section, we show the restrictions in the design of the loss function of
the GNN model in [8]. To avoid such restrictions, we propose a new approach
to design loss functions using techniques and tools in the probabilistic method.
We then demonstrate the utility of our approach in the maximum-clique prob-
lem. In the hope of enhancing exact algorithms for combinatorial optimization
problems, we show a method to extract the branching heuristic from the learned
probability space output by our GNN model. Then, we apply this method to the
dominating-clique problem, and the experimental results show that the learned
probability space yields to a better branching heuristic for the dominating-clique
problem.
Our probabilistic-method GNN gives rise to a probability space similar to
the way in [8]. In our probabilistic-method GNN, the node feature associated
with each vertex is a 1-dimensional vector and is interpreted as the parameter of
a Bernoulli distribution. Intuitively, this Bernoulli distribution characterizes the
likelihood for the vertex to be in a solution. We also assume that the collection
of Bernoulli distributions associated with the vertices are mutually independent.
For a given graph G(V,E), the collection of Bernoulli distributions give rise to a
probability space (Ω ,F,P). The sample space Ω is the power set of V; the event
spaceFis the power set of Ω; for a subset SofV,P(S) =(∏
ipi)(∏
j(1−pj))
withvi∈Sandvj/∈Swherepiandpjare the the parameters of the Bernoulli
distributions associated with vertices viandvj.
3.1 New Probabilistic-Method GNN Model
The idea of the design of loss functions in [8] comes from the ﬁrst-moment
method in the probabilistic method
P(X <a )>1−E(X)/a
whereXis a random variable under a certain probability distribution and ais
a positive number. Applying a=E(X)/(1−t) with a strictly positive t <1,
4
we get
P(
X <E(X)/(1−t))
>t,
which tells us that with a strictly positive probability t,Xis smaller than
E(X)/(1−t). Using the ﬁrst-moment method, given a combinatorial optimiza-
tion problem with the quality of a solution Smeasured by a function f(such
as the size of S), Karalias et al. in [8] set f(S) + 1 S /∈Sβas a random variable
where S is a randomly selected subset according to the distribution from the
GNN output,Sis the set of solutions, 1 S /∈Sis an indicator function, and βis
a positive constant strictly greater than max S∈S(
f(S))
. This random variable
leads the loss function of GNN models to be
L=E(
f(S) + 1 S /∈Sβ)
=E(
f(S))
+P(S /∈S)β.
After training, if L<(1−t)β, then with a strictly positive probability t,
there is an Swith
f(S) + 1 S /∈Sβ <β
which implies that 1 S /∈Sis false, i.e. S∈Sandf(S)<β.
This approach is quite interesting, but it suﬀers from the following two
drawbacks.
•We wanttto be close to 0 so that L<(1−t)βhas more likelihood after
training, but it also implies the less likelihood (i.e. the probability t) of
the event that there is an Swithf(S) + 1 S /∈Sβ <β ;
•Choosing the value of βneeds special care. Since it is usually hard to
get the closed form of P(S /∈S) for the given combinatorial optimization
problem, we use an upper bound of P(S /∈S) to replace it in the loss
function, which makes less likely that the loss function will converge to a
small number after training if βis too great.
To avoid these issues, we propose a new way to design loss functions. Our
idea is straightforward: minimizing E(
f(S))
P(S∈S)−1to let the probability
measure of the solutions with low values of fbe as great as possible. Our loss
function is
ln(
E(
f(S)))
−ln(
P(S∈S))
.
In practice, the diﬀerence between E(
f(S))
andP(S∈S) might be large, which
may cause the loss function to weigh E(
f(S))
too strongly compared to P(S∈
S). We use ln(·) in hopes that the loss function would weigh the terms as
proportionally as possible.
3.2 New Loss Function for Maximum-Clique Problem
In this section, we show an application of our GNN model for the maximum-
clique problem. For the probability P(Sis a clique ), we use the correlation
inequality in the probabilistic method to get its lower bound.
5
Deﬁnition 1. [12] LetMbe a ﬁnite universal set {x1,x2,...,x n}andRbe a
random subset of Msampled by P(xi∈R) =pi. Suppose these samples are
mutually independent. An event Ais a collection of subsets of M.Ais an
increasing event if a set Sis inAimplies that every superset of Sis inA.
Similarly,Ais a decreasing event if a set Sis inAimplies that every subset of
Sis inA.
Given two increasing events AandBand two decreasing events CandD,
the correlation inequality [12] shows that
P(A∩B )≥P(A)·P(B),
P(C∩D )≥P(C)·P(D),
P(A∩C )≤P(A)·P(C).(1)
By induction, if{Ai}i∈[n]are all increasing or all decreasing events,
P(⋀
i∈[n]Ai)≥∏
i∈[n]P(Ai).(2)
Given a graph G(V,E), denote by CSthe event that Sis a clique. For a
non-adjacent pair of vertices viandvj, denote by Bijthe event that viandvj
are both in S, soP(Bij) =pipj. Clearly,CSis equivalent to⋀
i,jBij, andBij’s
are decreasing events. Thus, by (2),
P(CS) =P(⋀
{vi,vj}/∈EBij)≥∏
{vi,vj}/∈EP(Bij).(3)
Clearly, maximizing∏
{vi,vj}/∈EP(Bij) helps in maximizing P(CS). Hence,
our loss function for the maximum-clique problem is
L=−ln(
E(|S|))
−∑
{vi,vj}/∈Eln(
P(Bij))
.(4)
We apply the probability distributions output from the GNN model using
the loss function (4) to the greedy approximation algorithm for the maximum-
clique problem in [8]. See Table 1 for the experimental results.
3.3 Loss Function for (Minimum) Dominating-Clique Prob-
lem
The maximum-clique problem belongs to the kind of combinatorial problems
that the properties of solutions are deﬁned locally. We would like to turn our
focus towards solving combinatorial problems whose solutions have both local
and global conditions. Also, previous studies of applying GNNs for combinato-
rial optimization problems usually use the probability distributions from GNN
models for greedy approximation algorithms. Thus, we also look for a method
to apply the probability distributions from GNN models to exact algorithms.
6
The dominating-clique problem is a good problem to be studied since 1) it is
a combinatorial optimization problem with both local and global conditions;
2) checking the existence of a dominating clique is NP-complete; 3) there is a
powerful exact solver for the dominating-clique problem in [9] to which we can
apply our improvement.
We ﬁrst design a loss function for ﬁnding a dominating clique and then
extend it seamlessly for ﬁnding the minimum dominating clique. Denote by DS
the event that Sis a dominating set, i.e. SdominatesV\S. The closed form of
P(DS∩CS) is a good candidate as a loss function for ﬁnding a dominating clique,
but it is harder to evaluate the closed form compared to evaluating the closed
form of P(CS). Thus, we alternatively analyze the upper and lower bounds of
P(DS∩CS). Our idea is to increase the upper and lower bounds of P(DS∩CS)
simultaneously to optimize its value. For a node vi, denote by Aithe event that
the vertices in N[vi] are all not in S, soP(Ai) =∏
vj∈N[vi](1−pj). It is easy to
see thatDSis equivalent to⋀
iAiandAi’s are increasing events. By (1) and
(2), we get
P(DS∩CS)≤P(DS)P(CS),
P(DS) =P(⋀
i∈[n]Ai)≥∏
i∈[n]P(Ai).
With (3), we have
exp(∑
i∈[n]ln(
P(Ai))
+∑
{vi,vj}/∈Eln(
P(Bij)))
=∏
i∈[n]P(Ai)∏
{vi,vj}/∈EP(Bij)
≤P(DS)P(CS).
This inequality tells us that increasing∑
i∈[n]ln(
P(Ai))
+∑
{vi,vj}/∈Eln(
P(Bij))
is meanwhile increasing the upper bound of P(DS∩CS).
For the lower bounds of P(DS∩CS), we have
P(DS∩CS)≥P(DS) +P(CS)−1
≥2 exp(1
2(
lnP(DS) + ln P(CS)))
−1 (as exis a convex function)
≥2 exp(1
2(∑
i∈[n]ln(
P(Ai))
+∑
{vi,vj}/∈Eln(
P(Bij))))
−1.
Therefore, we set the loss function for the dominating-clique problem as
L=−(∑
i∈[n]ln(
P(Ai))
+∑
{vi,vj}/∈Eln(
P(Bij)))
(5)
trying to optimize the upper and lower bounds of P(DS∩CS) simultaneously.
To ﬁnd the minimum dominating clique, we can simply add ln(
E(|S|))
into
the loss function as
L=−(∑
i∈[n]ln(
P(Ai))
+∑
{vi,vj}/∈Eln(
P(Bij)))
+ ln(
E(|S|))
. (6)
7
By Equation (6), we make an eﬀort to increase the probability measure of
small-sized dominating cliques so that it is easier to identify them. However,
calculating E(|S|) on Ω (i.e. E(|S|) =∑n
i=1pi) indeed aﬀects all small-sized
subsets ofVrather than small-sized dominating cliques. An improvement is to
ﬁnd an event in the event space Fwhich is close to the exact event consisting
of dominating cliques only. One way to accomplish this is as follows.
In each iteration during the GNN training, we generate a random permu-
tation{v1,v2,...,v n}ofVand generate an event (i.e. a set of subset of V)
iteratively. We initialize the event as an empty set. For i= 1 ton, we ﬁrst
exclude the subsets of Vthat contain any vertex of v1,...,v i−1. Then, we con-
tinue to exclude the subsets of Vthat do not contain vi. After that, we exclude
the subsets of Vthat contain any non-adjacent vertex to vi. At the end of the
current iteration, we add the remaining subsets of Vinto the event.
This event is much more closer to the event of dominating cliques only than
the event Ω. Thus, instead of minimizing E(|S|) on Ω, we try to minimize E(|S|)
on this event as
n∑
i=1(
pii−1∏
j=1(1−pj)∏
vk∈{vi+1,..,vn}\N(vi)(1−pk)(
1 +∑
vr∈N(vi)\{v1,..,vi−1}pr))
(7)
in hopes that it gives the loss function a more accurate expected size of domi-
nating cliques. The term piis the probability of the event that viis inS. The
term∏i−1
j=1(1−pj) is the probability of the event that v1,...,v i−1are not inS.
The term∏
vk∈{vi+1,..,vn}\N(vi)(1−pk) is the probability of the event that Sdo
not contain any vertex that is behind viin the permutation and non-adjacent
tovi. The term(
1 +∑
vr∈N(vi)\{v1,..,vi−1}pr)
is the conditional expectation of
|S|given the above three events happen.
3.4 New Branching Heuristic for Dominating-Clique Solvers
Exact solvers for combinatorial optimization problems are generally based on
backtracking search. To improve this framework, branching heuristics are ap-
plied to give the most promising direction during the search, and branch-
ing heuristics are designed by the properties of speciﬁc problems. To im-
prove branching heuristics by our GNN model, our idea is to deﬁne a function
f(Var b,Θ) to measure the quality of branches in the search tree where Var bis the
set of unsigned variables of a branch band Θ is the probability space from the
output of our GNN model. We next utilize this idea for the dominating-clique
problem.
Culberson et al. [9] propose an eﬃcient solver for the dominating-clique
problem. From their experiments, the solver performs better than other gen-
eral SAT solvers, including BerkMin, MarchEq, and SATzilla. The solver is
a backtracking-based algorithm. They encode a given graph G= (V,E) with
V={v1,v2,...,v n}to a CNF formula as follows.
•There arenvariables{Xi}i∈[n]whereXi= 1 means that the correspond-
ingviis in the solution;
8
•Deﬁne a clause Ci={Xj}vj∈N[vi]for each vertex vito indicate that the
vertex is in the solution or at least one of its neighbors is in the solution;
With the encoded CNF formula, the solver works as Algorithm 1.
Algorithm 1 [9] An algorithm for the dominating-clique problem
1:procedure DomClq (D,S,U,G (V,E))
2: ifUis∅then
3:DOMCLQ←D
4: return TRUE
5: else
6: FindC∈Usuch that|C∩S|= min C′∈U|C′∩S|
7: ifC∩Sis not∅then
8: S′′←S
9: forXi∈C∩Sdo
10: Xi←1;D←D∪{vi}
11: S′←{Xj|Xj∈S′′,{vi,vj}∈E};U′←U\{C′|C′∈
U,X i∈C′}
12: ifDomClq(D,S′,U′,G(V,E))then
13: return TRUE
14: else
15: Xi←0;D←D\{vi};S′′←S′′\{Xi}
16: return FALSE
The solver has three parameters: a potential dominating clique D, a setS
of the unassigned variables such that S={Xu|∀v∈D,{u,v}∈E}, and a set
Uof unsatisﬁed clauses. The three parameters are initialized as ∅,{Xi}i∈[n],
and{Ci}i∈[n]respectively.
From the perspective of CSP solvers, this algorithm uses the MRV heuristic.
This heuristic is optimal if every sub-problem by adding one vertex into Dhas
the same amount of search space. However, it is false for most cases. To improve
it, we can see{Xi}i∈[n]as random variables under the probability distributions
output from our probabilistic-method GNN. We use the information entropy of
these random variables to predict the amount of the search space of unsatisﬁed
clauses. In particular, instead of the cardinality of C′∩S, we measure the joint
information entropy of the random variables in C′∩S. We therefore replace
Line 6 in Algorithm 1 by ﬁnding C∈Usuch that ( C∩S) has the minimum
joint information entropy. In other words, we deﬁne the function fas the joint
information entropy of unassigned variables of a branch to measure the quality of
branches. Note that the probability distributions of unassigned variables should
not be ﬁxed during the backtracking search. The reason is that S,D, andU
are changing during the search, so the correspondingly unexplored subgraph
which consists of SandUis also changing. We apply the softmax function
on the distributions of the unassigned variables to re-weigh them during the
backtracking search. We also try the Z-score normalization, but its performance
9
is worse than the softmax function.
To calculate the joint information entropy of random variables in C′∩S=
{X1,...,X m}, we try two ways. The ﬁrst one, called the fast version because
the calculation of this way is fast but not accurate, is∑m
i=1H(Xi)−(∏m
i=1(1−
pi))
log2(∏m
i=1(1−pi))
.Xi’s are mutually independent, so their joint infor-
mation entropy is the sum of each one’s information entropy. In addition, if
there is a dominating clique, at least one variable in C′∩Sis 1; we thus minus
the information entropy of the case that all variables in C′∩Sare 0.
The second way, called the accurate version but with a slow calculation,
is similar to the improvement on Equation (6). Instead of calculating the joint
information entropy of random variables in ( C′∩S), we calculate the joint infor-
mation entropy under a more precise case. Given a clause C′∩S={X1,..,X m},
fori= 1 tom, we iteratively set X1,...,X i−1as 0 andXias 1. We know that
{Xi+1,...,X m}\{Xj}vj∈N(vi)must be 0 from the properties of dominating
cliques. Thus, we only need to consider the joint information entropy of the
random variables in N[vi]\{v1,...,v i−1}. Similar to Equation (7), we calculate
the joint information entropy as
m∑
i=1(
p(
−log2(p) +∑
vr∈N(vi)\{v1,..,vi−1}H(Xr)))
(8)
wherep=pi∏i−1
j=1(1−pj)∏
vk∈{vi+1,..,vm}\N(vi)(1−pk).
4 Experimental Results
4.1 Finding Maximum Cliques
Table 1 shows the experimental results of approximation ratio (i.e. the ratio of
the size of the clique we ﬁnd versus the size of the maximum clique). We use the
greedy approximation algorithm in [8] to ﬁnd large cliques, and the maximum
clique is from Gurobi [13]. The greedy approximation algorithm has two ways,
fast and slow, to decode cliques using the learned probability distributions. The
fast way returns quickly but with a less use of the input probability distributions
compared to the slow way. We run experiments for two GNN models. These two
models have the same neural network architecture as the model in [8]. The only
diﬀerence is that one uses our loss function (4) and another uses the original
loss function in [8]. The datasets Twitter, COLLAB, and IMDB used in the
experiments are from [8] as well.
4.2 Finding Dominating Cliques
We run experiments on three dominating-clique solvers: the original solver in [9]
and the GNN-enhanced solvers (the output distributions from the GNN model
with the loss function (5) as input) using the fast and accurate calculations of
joint information entropy respectively.
10
Table 1: Results on test dataset: approximation ratios on real-world datasets
andG(n,p) instances; the average number of nodes of G(n,p) instances is similar
to the average number of nodes of graphs in Twitter (about 120 nodes per
graph); Sparse means that the instances are generated with 0 .2≤p≤0.4; Dense
means that the instances are generated with 0 .6≤p≤0.8; italics indicates the
standard deviation.
Loss function as (4) Loss function in [8]
Dataset Slow Fast Slow Fast
Twitter 0.931 ±0.099 0.927±0.112 0.956±0.077 0.914±0.141
COLLAB 1.000 ±0.000 1.000±0.000 1.000±0.000 1.000±0.000
IMDB 1.000 ±0.000 1.000±0.000 1.000±0.000 1.000±0.000
G(n,p)(Sparse) 0.910 ±0.100 0.908±0.102 0.922±0.099 0.901±0.104
G(n,p)(Dense) 0.901 ±0.050 0.900±0.047 0.886±0.062 0.893±0.053
(a)
 (b)
(c)
 (d)
Figure 1: Experimental results on test data for ﬁnding dominating cliques; X
axis forn
Figure 1 shows the experimental results on test data. To be speciﬁc, Fig-
ure 1a shows the winning ratios of the three solvers on the instances that domi-
nating cliques do not exist, and Figure 1c shows the winning ratios of the three
solvers on the instances that dominating cliques exist. Figure 1b shows the ra-
tios of the GNN-enhanced solvers to the original algorithm in terms of geometric
11"
2307.00165,D:\Database\arxiv\papers\2307.00165.pdf,"How can the proposed framework be used to understand the factors influencing a user's decision to purchase a specific item, and what are the limitations of this approach?","The framework can identify key historical items that, if changed, would alter the user's recommendation. This provides insight into the factors driving the recommendation. However, the framework relies on the accuracy of the sampler model, which may not always be perfect, potentially leading to inaccurate explanations.","WSDM ’23, February 27-March 3, 2023, Singapore, Singapore Jianchao Ji et al.
positive, then the explicit reasoning expression is:
¬𝒆𝒗1𝒖∧𝒆𝒗2𝒖∧···∧ 𝒆𝒗𝒏𝒖→𝒆𝒗𝒏+1𝒖 (3)
where{𝒆𝒗1𝒖,𝒆𝒗2𝒖,···,𝒆𝒗𝒏𝒖}represents the user’s history event and
𝒆𝒗𝒏+1𝒖 is the next item the user interacts with. To put explicit feed-
back information into consideration, 𝒆𝒗𝒖is used to represent that
user𝑢interacted with item 𝑣with positive feedback and and ¬𝒆𝒗𝒖is
used to represent that user 𝒖interacted with item 𝑣with negative
feedback. Ideally, the sequential recommendation procedure can
predict 𝒆𝒗𝒏+1𝒖 based on{𝒆𝒗1𝒖,𝒆𝒗2𝒖,···,𝒆𝒗𝒏𝒖}. Based on the definition
of material implication1, the expression is equivalent to:
(¬¬𝒆𝒗1𝒖∨¬𝒆𝒗2𝒖∨···∨¬ 𝒆𝒗𝒏𝒖)∨𝒆𝒗𝒏+1𝒖 (4)
The recommendation score of a candidate item 𝑣𝑛+1is calculated
based on the similarity between the logical expression and the
constant True (T) vector. Based on the score, the model will decide
whether the item should be recommend to the user (if the expression
is close to True) or not (if the expression is close to False).
4 COUNTERFACTUAL COLLABORATIVE
REASONING (CCR)
We build an counterfactual collaborative reasoning (CCR) frame-
work to generate explicit counterfactual examples for data augmen-
tation and improve the performance of sequential recommendation
models. The main idea of the proposed data augmentation frame-
work is to discover slight changes Δon users’ explicit feedback
via solving a counterfactual optimization problem which will be
formulated in the following. Meanwhile, the process of generating
explicit counterfactual data can also provide explanations for items
in the top-𝐾recommendation.
4.1 Explicit Counterfactual Data Sampler
As shown in figure 1(b), besides a sequential recommendation model
A, our CCR framework introduces a sampler Sto generate explicit
counterfactual examples. Firstly, both AandSin our model are
pre-trained based on the original dataset. Then, the explicit coun-
terfactual data generated by the sampler will be used to re-optimize
the anchor modelA. After that, the re-optimized anchor model
will provide the final recommendation list for the user.
To generate explicit counterfactual data, we use NCR as the
sampler to conduct counterfactual reasoning. This is because NCR
can consider the counterfactual of explicit feedback with the help
of logical negations ( ¬). The first step of the sampler is to decide
which explicit feedback of a user 𝑢𝑖’s historical items should be
changed. We use a binary vector 𝚫𝒊={0,1}|𝑩𝒊|to represent the
intervention, where the the vector size is equal to the size of the
user’s explicit feedback vector 𝑩𝒊. Then, we apply the intervention
on𝑩𝒊:
𝑩∗
𝒊=(1−𝑩𝒊)⊙𝚫𝒊+𝑩𝒊⊙(1−𝚫𝒊) (5)
For each𝛿𝑡∈𝚫𝒊, if𝛿𝑡=1, then the corresponding feedback
is reversed; otherwise, the feedback remains the same. For exam-
ple, if 𝑩𝒊=[0,1,1]and𝚫𝒊=[1,1,0], it means that the user’s
feedback on the first and second items should be reversed, thus
1Material Implication ( →) can be represented as: 𝑥→𝑦⇔¬𝑥∨𝑦
LikeLikeLike
CameraiPadSD CardiPad  CaseExplicit Counterfactual DataOriginal Data∆𝒊𝟑=𝟎∆𝒊𝟏=𝟎∆𝒊𝟐=𝟏LikeDislikeLike
CameraiPadSD CardCamera Lens
Figure 3: An illustration of explicit counterfactual data gen-
eration progress. In the original data, the user likes the iPad
and then they purchased an iPad case which is compatible
with the iPad. Suppose Δ2
𝑖=1and the user’s explicit feed-
back on iPad is changed, then the sampler will generate a
new item as the next item.
𝑩∗
𝒊=[1,0,1]. To decide which feedback should be changed, we
design an optimization function for 𝚫𝒊:
𝚫𝒊=argmin𝚫𝒊∥𝚫𝒊∥0+𝛼·S(𝒗𝒏+1|𝑯𝒊,𝑩∗
𝒊) (6)
where∥𝚫𝒊∥0is the zero-norm of the intervention vector 𝚫𝒊that
represents the amount of changed feedback, 𝛼is a hyper-parameter,
and𝒗𝒏+1is the item embedding vector of the ground-truth next
item.S(𝒗𝒏+1|𝑯𝒊,𝑩∗
𝒊)is the ranking score of the sampler model
for user𝑢𝑖on item𝑣𝑛+1under the counterfactual user feedback 𝑩∗
𝒊.
In Eq. (6), the first term aims to minimize the amount of intervened
feedback between the original data and the explicit counterfactual
data. The second term tries to find the explicit feedback that can
alter the output the sequence, i.e., the ranking score of item 𝑣𝑛+1
under the counterfactual feedback is decreased so that a new item
appears as the output. However, the 𝚫𝒊is not differentiable since it
is discrete. Thus, we will introduce a relaxed optimization method
later.
To get the new next item, we take the history items 𝑯𝒊and the
intervened feedback 𝑩∗
𝒊into the sampler to derive the next item
ˆ𝒗𝑛+1that the user may interact with:
ˆ𝒗𝑛+1=argmax𝑣∈𝐼S(𝒗|𝑯𝒊,𝑩∗
𝒊) (7)
where𝐼is a set of items in the dataset, which can be the whole item
set𝑉or another set involving prior knowledge. Finally, an explicit
counterfactual sequence is generated as (𝐻𝑖,𝐵∗
𝑖,ˆ𝑣𝑛+1). Together
with the original sequence (𝐻𝑖,𝐵𝑖,𝑣𝑛+1), the anchor model Awill
be optimized over the augmented training dataset.
The intuition of the explicit counterfactual data augmentation
procedure is that the sampler model creates “difficult” examples that
leads to changes in the next item even if the historical feedback is
just slightly changed. Such difficult examples, once included as aug-
ment examples to enhance the training data, will help the training
of sequential recommendation models to distinguish the influence
of minor changes in users’ historical feedback [ 53]. Take Figure
3 as an example, in the original sequential data, user purchased
an iPad case as the next item since it is compatible with the iPad
Counterfactual Collaborative Reasoning WSDM ’23, February 27-March 3, 2023, Singapore, Singapore
that the user already purchased and the user actually likes the iPad.
However, if the user dislike the iPad, then the next item would not
be an iPad case but could be a camera lens since it is compatible
with the camera that the user liked before.
4.2 Relaxed Optimization
One difficulty in optimizing Eq. (6)is that the binary vector 𝚫𝒊and
the first term of the optimization function ∥𝚫𝒊∥0are not differen-
tiable since they are discrete. In the implementation, we relax 𝚫𝒊
to a real-valued vector and relax the ℓ0-norm∥𝚫𝒊∥0toℓ1-norm
∥𝚫𝒊∥1to make the intervention vector 𝚫𝒊learnable. This method
has been shown to be effective in prior research [ 2,3] and helps to
minimize the number of changed explicit feedback in the sequence.
𝚫𝒊=argmin𝚫𝒊∥𝚫𝒊∥1+𝛼·S(𝒗𝒏+1|𝑯𝒊,𝑩∗
𝒊) (8)
As mentioned before, the sampler Sis implemented with NCR
to accommodate the explicit feedback in the sequence. More specif-
ically, for each user-item interaction event 𝑒in the history 𝐻𝑖, sup-
pose 𝒆is the corresponding event vector for the event (which could
be negated if this event is a negative feedback in the original data),
and suppose event 𝑒’s corresponding value in the intervention
vector is𝛿𝑒, then the intervened event vector 𝒆∗is:
𝒆∗=¬𝒆·𝛿𝑒+𝒆·(1−𝛿𝑒) (9)
These intervened event vectors constitute the counterfactual
history{𝑯𝒊,𝑩∗
𝒊}for the NCR sampler to calculate the ranking score
of item𝑣𝑛+1, as shown in Eq. (8). After optimization, the values in
the learned intervention vector 𝚫𝒊may not be exactly equal to 0
or1. As a result, a threshold will be applied to binarize 𝚫𝒊as the
final output. In the experiments, we set the threshold as 0.5, i.e., for
those elements in 𝚫𝒊larger than 0.5, we set them to 1, otherwise, we
set them to 0. Finally, the binarized intervention vector 𝚫𝒊is used
to generate the new next item ˆ𝑣𝑛+1for the explicit counterfactual
example according to Eq.(7).
4.3 Reduce Noisy Examples
As mentioned above, for the generated explicit counterfactual data,
the next interaction item is selected based on Eq. (7). However, since
no sampler model is perfectly accurate, it may generate inaccurate
predictions. Both of the accurate and inaccurate counterfactual
examples generated by the sampler will be used to re-optimized
the anchor modelA, as a result, if we do not set some constraints
to reduce the amount of inaccurate counterfactual examples, the
performance of the re-optimized anchor model may be harmed.
Inspired by [ 53], we set a confidence parameter 𝜅∈[0,1)to mitigate
this issue. We accept the generated explicit counterfactual data only
whenS(ˆ𝒗𝑛+1|𝑯𝒊,𝑩∗
𝒊)>𝜅. This means that we will only accept a
counterfactual example when the sampler is sufficiently confident
of it. Otherwise, the model will discard the example.
4.4 Learning Algorithm
To make the whole process clear, we summarize the learning algo-
rithm of our framework in Algorithm 1. At first, we train both the
samplerSand the anchor model Abased on the original dataset
𝑇. Then for each training example in 𝑇, the sampler will learn the
intervention vector Δ𝑖and generate counterfactual data based onAlgorithm 1 CCR Learning Algorithm
Input: the original dataset 𝑇
Input: Pre-train sampler Sand anchor modelA
Initialize the counterfactual dataset 𝑇𝑐=∅
foreach training example from 𝑇do
Randomly initialize the intervention vector 𝚫𝒊
Learn 𝚫𝒊by Eq.(8)
Generate new sequence (𝐻𝑖,𝐵∗
𝑖,ˆ𝑣𝑛+1)based on Eq.(7)
ifS(ˆ𝒗𝑛+1|𝑯𝒊,𝑩∗
𝒊)>𝜅then
𝑇𝑐←𝑇𝑐∪(𝐻𝑖,𝐵∗
𝑖,ˆ𝑣𝑛+1)
end if
end for
Re-optimizeAbased on𝑇∪𝑇𝑐
Output: the final recommendations based on re-optimized A
Eq.(7). If the sampler has enough confidence of the generated coun-
terfactual data, it will be added into the counterfactual dataset 𝑇𝑐.
When the model finishes the data augmentation process, the anchor
modelAwill be re-optimized based on 𝑇𝑐∪𝑇to provide the final
recommendations for each user.
4.5 Counterfactual Explanations
During the process of generating explicit counterfactual data, our
framework can also provide explanations to show why the model
recommends the item to the user. Previous counterfactual expla-
nation methods for recommendation [ 25,49,51] mostly focus on
implicit counterfactual explanation based on implicit behaviors.
However, one contribution of our work is that our framework can
generate explicit counterfactual explanations.
In Eq. (8), we are trying to explore an intervention vector 𝚫𝒊.
Because of the first term in Eq. (8), only a few explicit feedback will
be changed. Meanwhile, the second term of Eq. (8)will penalize the
probability of interacting with the current item. Therefore, only
the most essential history items’ feedback will be changed. These
items can be used to generate counterfactual explanations for the
recommended item. Take Figure 4 as an example, since Δ2
𝑖=1,
the corresponding item is the counterfactual explanation: the iPad
is the reason for recommending the iPad case, because if the user
disliked the iPad, we would not have recommended the iPad case
but would have recommended the camera lens instead. We store
all explanations in the set 𝐸.
To evaluate if our explanation correctly explains the recom-
mended item, inspired by recent work on counterfactual explain-
able recommendation [ 48], we use Probability of Necessity (PN)
and Probability of Sufficiency (PS) to evaluate our explanations. In
logic and mathematics, if 𝑋happens then 𝑌will happen, we say 𝑋
is a sufficient condition for 𝑌. Similarly, if 𝑋does not happen then
𝑌will not happen, we say 𝑋is a necessary condition for 𝑌.
4.5.1 Probability of Necessity .Suppose a set of items 𝐸𝑖𝑗⊂𝑉
constitute the explanation for the recommended item 𝑣𝑗to user
𝑢𝑖. The idea of the PN score is: if the items in 𝐸𝑖𝑗are reversed (for
explicit explanation) or removed (for implicit explanation), then
what is the probability that item 𝑣𝑗would not be recommended for
user𝑢𝑖. We calculate the percentage of the generated explanations
WSDM ’23, February 27-March 3, 2023, Singapore, Singapore Jianchao Ji et al.
that meet the above PN condition:
PN=Í
ui∈UÍ
vj∈Ri,KPNijÍ
ui∈UÍ
vj∈Ri,KI(Eij≠∅),PNij=1,if vj∉R∗
i,K
0,else(10)
where Ri,Kis the original top-K recommendation list for user 𝑢𝑖. Let
𝑣𝑗∈Ri,Kbe a recommended item that our model has a nonempty
explanation 𝐸𝑖𝑗≠∅. Then for the original sequence data, we in-
tervene (reverse or remove) the item(s) in 𝐸𝑖𝑗and get the new rec-
ommendation list R∗
i,Kfor user𝑢𝑖from the recommendation model.
𝐼(𝐸𝑖𝑗≠∅)is an identity function: when 𝐸𝑖𝑗≠∅,𝐼(𝐸𝑖𝑗≠∅)=1.
Otherwise,𝐼(𝐸𝑖𝑗≠∅)=0.
4.5.2 Probability of Sufficiency .Similar to the definition of PN,
the idea of PS score is: if the items in 𝐸𝑖𝑗are maintained while other
items are reversed (for explicit explanation) or removed (for implicit
explanation), then what is the probability that item 𝑣𝑗would still
be recommended for user 𝑢𝑖. We calculate the percentage of the
generated explanations that meet the above PS condition:
PS=Í
ui∈UÍ
vj∈Ri,KPSijÍ
ui∈UÍ
vj∈Ri,K(Eij≠∅),PSij=1,if vj∈R′
i,K
0,else(11)
where R′
i,Kis the new recommendation list after the intervention is
applied, and other notations have similar meanings as above.
5 EXPERIMENTS
In this section, we conduct experiments on three real-world datasets
and compare the results of (1) the original sequential recommen-
dation model without data augmentation, (2) models with implicit
counterfactual data augmentation, and (3) models with our Coun-
terfactual Collaborative Reasoning (CCR) framework. Furthermore,
the counterfactual explanation results show our framework’s ability
to generate higher quality explanations.
5.1 Dataset
We use three real-world datasets in the experiments.
ML100K [27]: The MovieLens-100K (ML100K) dataset stores
users’ preference for various movies. It contains 100,000 movie
ratings from 1 to 5 stars of 943 users to 1,682 movies.
Amazon [43]: This is the Amazon e-commerce dataset. We take
Movies &TVandElectronics datasets as two examples for ex-
periments. Movies &TV contains 123,961 users, 50,053 products
and 1,697,533 product ratings. Electronics contains 192,404 users,
63,002 products and 1,689,188 product ratings.
Some basic statistic of the datasets can be found in Table 1. We
consider 1-3 ratings as negative feedback with label as 0, and 4-5
ratings as positive feedback with label as 1. We use positive Leave-
One-Out [ 8,69] to create the training, validation and testing dataset.
For each user, we put the last positive interaction and its following
negative interactions into the testing set, and we put the last but
one positive interaction and its following negative interactions into
the validation set. Then, we put all of the rest interactions into the
training set. If a user has less than 5 interactions, we put all of the
interactions into the training set to avoid cold-start.Dataset #users #items #interaction Density
ML100K 943 1,682 100,000 6.30%
Movies & TV 123,961 50,053 1,697,533 0.027%
Electronics 192,404 63,002 1,689,188 0.014%
Table 1: Basic statistics of the datasets
LikeLikeLike
CameraiPadSD CardiPad  CaseExplicit Counterfactual DataOriginal Data∆𝒊𝟑=𝟎∆𝒊𝟏=𝟎∆𝒊𝟐=𝟏LikeDislikeLike
CameraiPadSD CardCamera Lens
Figure 4: An example of generating counterfactual explana-
tions. Since Δ2
𝑖=1, the corresponding item is the counterfac-
tual explanation for the recommended item.
5.2 Baselines
We consider both standalone sequential recommendation models
and implicit data augmentation methods for comparison:
GRU4Rec [30]: GRU4Rec is a sequential recommendation model
based on Recurrent Neural Networks (RNN).
STAMP [42]: STAMP is a sequential recommendation model
based on attention mechanism, which can capture users’ long-term
and short-term preferences for recommendation.
SASRec [33]: SASRec is a sequential recommendation model
based on self-attention mechanism
NCR [8]: NCR is a sequential recommendation model based on
neural logical reasoning, which captures the logical relationship
between user-item interactions for recommendation.
CASR [53]: CASR is a state-of-the-art implicit counterfactual
data augmentation method for sequential modeling.
Both CASR and our CCR frameworks can be applied on all of
the four recommendation models.
5.3 Implementation Details
The learning rate is searched in [0.0001,0.001,0.01,0.1] for all meth-
ods. We apply ReLU as the activation function. For all methods, the
embedding size is 64. We optimize the methods using mini-batch
[34] and the batch size is 128. The hyper-parameter 𝛼is searched in
[10−3,10−2,10−1,1,101,102,103], and finally set to 10 for the results
we report in the paper. The confidence parameter 𝜅is searched
from 0 to 1. The influence of different 𝜅on the performance will
be discussed in the following experiments. We tune each model’s
parameters to its own best performance on the validation set. For
both CASR (implicit counterfactual data augmentation) and CCR
(explicit counterfactual data augmentation), we generate one coun-
terfactual example for each sequence in the training set.
Counterfactual Collaborative Reasoning WSDM ’23, February 27-March 3, 2023, Singapore, Singapore
Dataset ML100K Movies & TV Electronics
Metric NDCG@5 NDCG@10 HR@5 HR@10 NDCG@5 NDCG@10 HR@5 HR@10 NDCG@5 NDCG@10 HR@5 HR@10
STAMP 0.342 0.402 0.503 0.665 0.406 0.427 0.521 0.657 0.301 0.341 0.412 0.542
CASR-STAMP 0.351 0.406 0.511 0.676 0.412 0.445 0.532 0.661 0.307 0.349 0.425 0.553
CCR-STAMP 0.365∗0.418∗0.527∗0.693∗0.435∗0.463∗0.552∗0.689∗0.329∗0.364∗0.453∗0.570∗
GRU4Rec 0.340 0.403 0.502 0.672 0.411 0.431 0.538 0.661 0.312 0.354 0.432 0.554
CASR-GRU4Rec 0.349 0.411 0.509 0.680 0.414 0.453 0.542 0.671 0.326 0.369 0.447 0.560
CCR-GRU4Rec 0.370∗0.429∗0.522∗0.689∗0.428∗0.466∗0.557∗0.695∗0.344∗0.386∗0.476∗0.581∗
SASRec 0.348 0.411 0.508 0.678 0.412 0.456 0.543 0.667 0.322 0.357 0.439 0.558
CASR-SASRec 0.357 0.415 0.518 0.685 0.420 0.461 0.549 0.673 0.335 0.365 0.450 0.575
CCR-SASRec 0.376∗0.427∗0.531∗0.701∗0.438∗0.479∗0.562∗0.699∗0.358∗0.390∗0.471∗0.592∗
NCR 0.359 0.412 0.514 0.680 0.415 0.457 0.551 0.673 0.332 0.366 0.441 0.557
CASR-NCR 0.362 0.419 0.518 0.689 0.417 0.458 0.555 0.682 0.339 0.374 0.451 0.569
CCR-NCR 0.376∗0.434∗0.535∗0.705∗0.433∗0.472∗0.568∗0.702∗0.354∗0.395∗0.469∗0.588∗
Table 2: Experimental results on Normalize Discounted Cumulative Gain (NDCG) and Hit Ratio (HR). For each model, we
present the performance of the original model, the results of applying implicit counterfactual data augmentation method
CASR on the model, and the results of applying our CCR method on the model. Bold numbers represent best performance.
We use * to indicate that the performance is significant better than other baselines. The significance is at 0.05 level on paired
t-test.
5.4 Evaluation Metrics
We use Normalized Discounted Cumulative Gain at rank 𝐾(NDCG@K)
and Hit Ratio at rank 𝐾(HR@K) to evaluate recommendation perfor-
mance. To evaluate the explanation performance, we use Probability
of Necessity (PN), Probability of Sufficiency (PS) and their harmonic
mean FNS=2·PN·PS
PN+PS. For each user-item pair in the validation set
and the test set, we randomly sample 100 irrelevant items and rank
all of these 101 items for recommendation.
5.5 Compatible with the Baselines
One issue in comparison with baselines is that our framework can
generate explanations in the counterfactual generation progress
while the baseline methods can not. We use two approaches to
make the baselines compatible for explanation evaluation.
First, we apply CASR on each of the four recommendation mod-
els to generate explanations. Since CASR manually selects one item
to intervene, we intervene each item in each example’s history and
select the one that achieves the highest FNSscore as the CASR expla-
nation. Second, to get stronger explanations for each model, we use
our framework as a guideline to tell the baseline methods how many
items they should use to generate explanations. Then, we apply the
counterfactual explainable recommendation (CountER) framework
[49] to each of the recommendation model (STAMP, GRU4Rec, SAS-
Rec, NCR) to generate explanations for them: based on the number
of items, we search all of the combinations of users’ history items
as candidate explanations and take the one that gives highest FNS
score as the explanation of the model under CountER framework.
Finally, since the CCR framework works in the “augment once,
apply to all” paradigm and directly produces explicit explanation
with NCR sampler during data augmentation, we directly take its
output explanation for evaluation.5.6 Performance on Recommendation
The experimental results on NDCG and Hit Ratio (HR) are shown
in Table 2. Based on the results, we have following observations.
First and most importantly, compared with the original model
and the model under implicit counterfactual data augmentation, our
CCR framework achieves significantly better performance than the
baseline methods on all of these three datasets. Compared with the
original model, CCR can get better results based on the the gener-
ated explicit counterfactual data, which alleviates the data scarcity
and encodes informative examples into the training dataset. Com-
pared with the implicit counterfactual data augmentation method
CASR, the explicit counterfactual data generated by CCR are more
effective since CCR takes advantages of the explicit feedback.
5.7 Performance on Explanation
The experimental results on explanation are shown in Table 3. Based
on the experiment results, we have following observations.
In terms of the overall explanation performance ( FNS), CountER-
based explanations are better than CASR-based explanations. This
is understandable since CountER uses the optimal number of items
from CCR to generate explanations while CASR selects one and
only one item as explanation. Furthermore, by considering explicit
feedback, CCR generates even better explanations than CountER-
based methods.
Besides, by considering explicit feedback, the CCR explanations
are better than CASR explanations on both PN and PS and thus
better overall explanation quality FNS. This means that the CCR aug-
mented examples have higher quality since they are more sufficient
and necessary, and when these better examples are used to aug-
ment the dataset, it helps CCR to achieve better recommendation
performance than CASR.
WSDM ’23, February 27-March 3, 2023, Singapore, Singapore Jianchao Ji et al.
Dataset ML100K Movies & TV Electronics
Top N N=1 N=5 N=1 N=5 N=1 N=5
Metric PN% PS% FNS% PN% PS% FNS% PN% PS% FNS% PN% PS% FNS% PN% PS% FNS% PN% PS% FNS%
CASR-STAMP 25.6 7.7 11.8 22.1 12.6 11.8 29.2 17.8 22.2 18.4 17.1 17.7 30.9 8.7 13.6 24.9 12.1 16.3
CASR-GRU4Rec 21.7 8.2 11.9 18.4 13.1 15.3 23.9 3.5 6.1 19.8 8.4 11.8 22.4 9.9 13.8 20.6 14.0 16.7
CASR-SASRec 23.9 9.3 13.4 19.3 13.5 15.8 25.7 18.2 21.3 17.4 18.9 18.1 25.3 10.2 14.5 19.4 14.5 16.6
CASR-NCR 17.2 32.5 22.5 14.7 36.8 21.0 19.4 38.6 25.8 10.8 44.3 17.4 19.9 39.0 26.3 16.3 40.3 23.2
CountER-STAMP 53.2 17.0 25.8 38.3 26.4 31.2 58.6 36.8 45.2 47.9 43.8 45.8 59.6 18.6 28.3 48.1 27.5 34.9
CountER-GRU4Rec 40.8 19.1 26.0 34.5 29.5 31.8 46.1 6.9 12.0 41.3 14.9 21.9 43.5 19.2 26.6 39.7 29.6 33.9
CountER-SASRec 45.3 21.9 29.5 36.0 30.1 32.7 48.3 39.7 43.5 40.5 45.5 42.8 50.7 23.9 32.4 41.2 36.6 38.7
CountER-NCR 34.7 52.4 41.7 28.1 54.5 37.1 42.7 53.7 47.5 34.9 59.0 43.8 36.7 52.3 43.2 32.2 57.8 41.3
CCR 42.1 60.3 49.6 36.1 66.7 46.8 50.1 64.7 56.5 41.9 73.2 53.3 45.8 74.8 56.8 41.0 79.6 54.1
Table 3: Results on PN, PS and FNS. Bold numbers are best performance. All numbers are percentage numbers with %omitted.
When CCR achieves the best result, its improvements against the best baseline are significant at 𝑝<0.01.
00.10.20.30.40.50.60.70.80.91.00.60.650.70.75
CCR-NCR results on 𝑀𝐿100𝐾𝐻𝑅@10
00.10.20.30.40.50.60.70.80.91.00.40.420.440.46
𝑁𝐷𝐶𝐺 @10
00.10.20.30.40.50.60.70.80.91.00.660.680.70.72
CCR-NCR results on 𝑀𝑜𝑣𝑖𝑒𝑠 &𝑇𝑉𝐻𝑅@10
00.10.20.30.40.50.60.70.80.91.00.460.470.480.490.5
𝑁𝐷𝐶𝐺 @10
00.10.20.30.40.50.60.70.80.91.00.540.560.580.6
CCR-NCR results on 𝐸𝑙𝑒𝑐𝑡𝑟𝑜𝑛𝑖𝑐𝑠𝐻𝑅@10
00.10.20.30.40.50.60.70.80.91.00.380.40.42
𝑁𝐷𝐶𝐺 @10
Figure 5: Performance on HR@10 (Blue Line) and NDCG@10 (Red Line) on different 𝜅with different datasets.
1 2 3 4 5 60.650.70.75
CCR-NCR results on 𝑀𝐿100𝐾𝐻𝑅@10
1 2 3 4 5 60.40.420.440.460.48
𝑁𝐷𝐶𝐺 @10
1 2 3 4 5 60.680.690.70.710.72
CCR-NCR results on 𝑀𝑜𝑣𝑖𝑒𝑠 &𝑇𝑉𝐻𝑅@10
1 2 3 4 5 60.460.480.5
𝑁𝐷𝐶𝐺 @10
1 2 3 4 5 60.560.580.6
CCR-NCR results on 𝐸𝑙𝑒𝑐𝑡𝑟𝑜𝑛𝑖𝑐𝑠𝐻𝑅@10
1 2 3 4 5 60.360.380.40.420.440.46
𝑁𝐷𝐶𝐺 @10
Figure 6: Performance on HR@10 (Blue Line) and NDCG@10 (Red Line) with different iterations.
5.8 Impact of Hyper-Parameters
5.8.1 Impact of𝜅.In the generation process of the explicit coun-
terfactual data, we have a confidence parameter 𝜅. We accept the
generated counterfactual data only when the ranking score of the
data is larger than 𝜅. The results on the influence of 𝜅is shown
in Figure 5. From the figure, we can see that our framework will
have the best performance when 𝜅is set around 0.7. When 𝜅is
very small, it does not change the performance of the framework
because all of the generated data can pass the constraint of 𝜅. When
𝜅is too big, the performance will decrease because only a few coun-
terfactual data can pass through the constraint and thus we cannot
get enough counterfactual data to re-optimize the anchor model.
5.9 Impact of Iterative Re-optimization
Since the re-optimized anchor model A′achieves better perfor-
mance than the original anchor model A, a natural idea is that
if we use the re-optimized anchor model to generate a set of new
augmented data and optimize A′again toA′′in a boosting way,whether the performance can be even better. We use NCR as an
example anchor model to test the idea. Unfortunately, as we can
see in Figure 6, the performance on HR and NDCG decreases with
the number of rounds of iterative re-optimization. This observa-
tion is consistent with previous research [ 53]. The reason is that
since the sampler is not perfectly accurate, the generated counter-
factual examples can contain noise and such noise is learned into
the anchor model. As a result, multiple rounds of augmentation
and re-optimization may propagate such noise and thus decrease
the performance. This means that even though data augmentation
can improve the model performance, it cannot boost the model
performance infinitely.
6 CONCLUSION
In this paper, we propose a Counterfactual Collaborative Reasoning
(CCR) framework, which integrates the power of logical reasoning
and counterfactual reasoning and generates explicit counterfactual
data to enhance the performance of sequential recommendation
Counterfactual Collaborative Reasoning WSDM ’23, February 27-March 3, 2023, Singapore, Singapore
models. Experiments on three real-world datasets verified the effec-
tiveness of the framework. Furthermore, a unique advantage of the
CCR framework is that it can also generate explicit counterfactual
explanations to better understand the user behavior sequence. In
this work, we take recommender system as an example to explore
the joint ability of logical and counterfactual reasoning, which are
two important types of reasoning abilities for machine learning.
On the other hand, they can also be considered to improve other
intelligent tasks beyond recommendation, such as vision and lan-
guage processing tasks, which we will explore in the future.
Acknowledgment . This work was supported in part by NSF
IIS 1910154, 2007907, 2046457, 2127918 and CCF 2124155. Any opin-
ions, findings, conclusions or recommendations expressed in this
material are those of the authors and do not necessarily reflect
those of the sponsors.
REFERENCES
[1]Q. Ai, V. Azizi, X. Chen, and Y. Zhang. 2018. Learning heterogeneous knowledge
base embeddings for explainable recommendation. Algorithms (2018).
[2]Emmanuel J Candes, Justin K Romberg, and Terence Tao. 2006. Stable signal
recovery from incomplete and inaccurate measurements. CPAM (2006).
[3]Emmanuel J Candes and Terence Tao. 2005. Decoding by linear programming.
IEEE transactions on information theory 51, 12 (2005), 4203–4215.
[4]Hanxiong Chen, Xu Chen, Shaoyun Shi, and Yongfeng Zhang. 2019. Generate
natural language explanations for recommendation. In EARS 2019 at SIGIR .
[5]Hanxiong Chen, Yunqi Li, Shaoyun Shi, Shuchang Liu, He Zhu, and Yongfeng
Zhang. 2022. Graph collaborative reasoning. In WSDM . 75–84.
[6]Hongxu Chen, Yicong Li, Xiangguo Sun, Guandong Xu, and Hongzhi Yin. 2021.
Temporal meta-path guided explainable recommendation. In WSDM . 1056–1064.
[7]H. Chen, Y. Li, H. Zhu, and Y. Zhang. 2022. Learn Basic Skills and Reuse: Modu-
larized Adaptive Neural Architecture Search (MANAS). In CIKM . 169–179.
[8]Hanxiong Chen, Shaoyun Shi, Yunqi Li, and Yongfeng Zhang. 2021. Neural
collaborative reasoning. In Proceedings of the Web Conference 2021 . 1516–1527.
[9]Shulong Chen and Yuxing Peng. 2018. Matrix factorization for recommendation
with explicit and implicit feedback. Knowledge-Based Systems 158 (2018), 109–117.
[10] X. Chen, H. Chen, H. Xu, Y. Zhang, Y. Cao, Z. Qin, and H. Zha. 2019. Personalized
fashion recommendation with visual explanations based on multimodal attention
network: Towards visually explainable recommendation. In SIGIR . 765–774.
[11] X. Chen, Z. Wang, H. Xu, J. Zhang, Y. Zhang, X. Zhao, et al .2022. Data Augmented
Sequential Recommendation based on Counterfactual Thinking. In TKDE .
[12] X. Chen, H. Xu, Y. Zhang, J. Tang, Y. Cao, Z. Qin, and H. Zha. 2018. Sequential
recommendation with user memory networks. In WSDM . 108–116.
[13] Xu Chen, Yongfeng Zhang, and Ji-Rong Wen. 2022. Measuring “Why” in Rec-
ommender Systems: a Comprehensive Survey on the Evaluation of Explainable
Recommendation. arXiv preprint arXiv:2202.06466 (2022).
[14] Gabriel de Souza Pereira Moreira, Sara Rabhi, Jeong Min Lee, Ronay Ak, and
Even Oldridge. 2021. Transformers4Rec: Bridging the Gap between NLP and
Sequential/Session-Based Recommendation. In RecSys . 143–153.
[15] Tim Donkers, Benedikt Loepp, and Jürgen Ziegler. 2017. Sequential user-based
recurrent neural network recommendations. In RecSys . 152–160.
[16] Kai Epstude and Neal J Roese. 2008. The functional theory of counterfactual
thinking. Personality and social psychology review 12, 2 (2008), 168–192.
[17] Amir Feder, Nadav Oved, Uri Shalit, and Roi Reichart. 2021. Causalm: Causal
model explanation through counterfactual language models. Comp. Ling. (2021).
[18] T. Fu, X. Wang, M. Peterson, S. Grafton, M. Eckstein, and W. Wang. 2020. Coun-
terfactual vision-and-language navigation via adversarial path sampler. In ECCV .
[19] Zuohui Fu, Yikun Xian, Ruoyuan Gao, Jieyu Zhao, Qiaoying Huang, et al .2020.
Fairness-aware explainable recommendation over knowledge graphs. In SIGIR .
[20] Yingqiang Ge, Shuchang Liu, Zuohui Fu, Juntao Tan, Zelong Li, Shuyuan Xu,
Yunqi Li, Yikun Xian, and Yongfeng Zhang. 2022. A survey on trustworthy
recommender systems. arXiv:2207.12515 (2022).
[21] Yingqiang Ge, J. Tan, Y. Zhu, Y. Xia, J. Luo, S. Liu, Z. Fu, S. Geng, Z. Li, and Y.
Zhang. 2022. Explainable Fairness in Recommendation. In SIGIR .
[22] Shijie Geng, Z. Fu, Y. Ge, L. Li, G. de Melo, and Y. Zhang. 2022. Improving
Personalized Explanation Generation through Visualization. In ACL. 244–255.
[23] Shijie Geng, Z. Fu, J. Tan, Y. Ge, G. De Melo, and Y. Zhang. 2022. Path Language
Modeling over Knowledge Graphsfor Explainable Recommendation. In WWW .
[24] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.
Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized
Prompt & Predict Paradigm (P5). In RecSys .[25] Azin Ghazimatin, Oana Balalau, Rishiraj Saha Roy, and Gerhard Weikum. 2020.
Prince: Provider-side interpretability with counterfactual explanations in recom-
mender systems. In WSDM . 196–204.
[26] Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019.
Counterfactual visual explanations. In ICML . PMLR, 2376–2384.
[27] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History
and context. TIIS5, 4 (2015), 1–19.
[28] Ruining He and Julian McAuley. 2016. Fusing similarity models with markov
chains for sparse sequential recommendation. In ICDM . IEEE, 191–200.
[29] Balázs Hidasi and Alexandros Karatzoglou. 2018. Recurrent neural networks
with top-k gains for session-based recommendations. In CIKM . 843–852.
[30] Balázs Hidasi, A. Karatzoglou, L. Baltrunas, and D. Tikk. 2016. Session-based
recommendations with recurrent neural networks. ICLR (2016).
[31] Wenyue Hua and Yongfeng Zhang. 2022. System 1 + System 2 = Better World:
Neural-Symbolic Chain of Logic Reasoning. In EMNLP .
[32] Jin Huang, X. Zhao, H. Dou, J. Wen, and E. Chang. 2018. Improving sequential
recommendation with knowledge-enhanced memory networks. In SIGIR .
[33] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-
mendation. In ICDM . IEEE, 197–206.
[34] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic opti-
mization. 3rd International Conference on Learning Representations (2015).
[35] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017.
Neural attentive session-based recommendation. In CIKM . 1419–1428.
[36] Lei Li, Yongfeng Zhang, and Li Chen. 2020. Generate neural template explanations
for recommendation. In CIKM . 755–764.
[37] Lei Li, Yongfeng Zhang, and Li Chen. 2021. Extra: Explanation ranking datasets
for explainable recommendation. In SIGIR . 2463–2469.
[38] Lei Li, Yongfeng Zhang, and Li Chen. 2021. Personalized Transformer for Ex-
plainable Recommendation. In ACL. 4947–4957.
[39] Lei Li, Yongfeng Zhang, and Li Chen. 2022. Personalized prompt learning for
explainable recommendation. arXiv preprint arXiv:2202.07371 (2022).
[40] Yunqi Li, Hanxiong Chen, Shuyuan Xu, Yingqiang Ge, Juntao Tan, Shuchang Liu,
and Yongfeng Zhang. 2022. Fairness in Recommendation: A Survey. In arXiv .
[41] Zelong Li, Jianchao Ji, and Yongfeng Zhang. 2022. From Kepler to Newton:
Explainable AI for Science Discovery. In ICML 2022 AI for Science .
[42] Qiao Liu, Yifu Zeng, R. Mokhosi, and H. Zhang. 2018. STAMP: short-term
attention/memory priority model for session-based recommendation. In KDD .
[43] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations
using distantly-labeled reviews and fine-grained aspects. In EMNLP .
[44] Massimo Quadrana, Alexandros Karatzoglou, et al. 2017. Personalizing session-
based recommendations with hierarchical recurrent neural networks. In RecSys .
[45] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factor-
izing personalized markov chains for next-basket recommendation. In WWW .
[46] Shaoyun Shi, Hanxiong Chen, Weizhi Ma, Jiaxin Mao, Min Zhang, and Yongfeng
Zhang. 2020. Neural logic reasoning. In CIKM . 1365–1374.
[47] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.
2019. BERT4Rec: Sequential recommendation with bidirectional encoder repre-
sentations from transformer. In CIKM . 1441–1450.
[48] Juntao Tan, Shijie Geng, Zuohui Fu, Yingqiang Ge, Shuyuan Xu, Yunqi Li, and
Yongfeng Zhang. 2022. Learning and Evaluating Graph Neural Network Expla-
nations based on Counterfactual and Factual Reasoning. In WWW . 1018–1027.
[49] Juntao Tan, Shuyuan Xu, Yingqiang Ge, Yunqi Li, Xu Chen, and Yongfeng Zhang.
2021. Counterfactual explainable recommendation. In CIKM . 1784–1793.
[50] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation
via convolutional sequence embedding. In WSDM . 565–573.
[51] Khanh Hiep Tran, Azin Ghazimatin, and Rishiraj Saha Roy. 2021. Counterfactual
Explanations for Neural Recommenders. In SIGIR . 1627–1631.
[52] Nan Wang, Hongning Wang, Yiling Jia, and Yue Yin. 2018. Explainable recom-
mendation via multi-task learning in opinionated text data. In SIGIR . 165–174.
[53] Zhenlei Wang, Jingsen Zhang, Hongteng Xu, Xu Chen, Yongfeng Zhang,
Wayne Xin Zhao, and Ji-Rong Wen. 2021. Counterfactual data-augmented se-
quential recommendation. In SIGIR . 347–356.
[54] Qingsong Wen, Liang Sun, Fan Yang, Xiaomin Song, Jingkun Gao, et al .2021.
Time series data augmentation for deep learning: A survey. IJCAI (2021).
[55] Chao-Yuan Wu, Amr Ahmed, Alex Beutel, Alexander J Smola, and How Jing.
2017. Recurrent recommender networks. In WSDM . 495–503.
[56] Liwei Wu, Shuqing Li, Cho-Jui Hsieh, and James Sharpnack. 2020. SSE-PT:
Sequential recommendation via personalized transformer. In RecSys .
[57] Yikun Xian, Zuohui Fu, et al .2020. CAFE: Coarse-to-fine neural symbolic rea-
soning for explainable recommendation. In CIKM .
[58] Yikun Xian, Z. Fu, S. Muthukrishnan, G. De Melo, and Y. Zhang. 2019. Reinforce-
ment knowledge graph reasoning for explainable recommendation. In SIGIR .
[59] Yikun Xian, Tong Zhao, Jin Li, Jim Chan, Andrey Kan, Jun Ma, Xin Luna Dong,
Christos Faloutsos, George Karypis, Shan Muthukrishnan, and Yongfeng Zhang.
2021. Ex3: Explainable attribute-aware item-set recommendations. In RecSys .
[60] Kun Xiong, Wenwen Ye, X. Chen, Y. Zhang, X. Zhao, B. Hu, Z. Zhang, and J.
Zhou. 2021. Counterfactual Review-based Recommendation. In CIKM .
WSDM ’23, February 27-March 3, 2023, Singapore, Singapore Jianchao Ji et al.
[61] Shuyuan Xu, Yunqi Li, Shuchang Liu, Zuohui Fu, Yingqiang Ge, Xu Chen, et al .
2021. Learning causal explanations for recommendation. In CSR.
[62] Aobo Yang, Nan Wang, Renqin Cai, Hongbo Deng, and Hongning Wang. 2022.
Comparative Explanations of Recommendations. In WWW . 3113–3123.
[63] Mengyue Yang, Q. Dai, Z. Dong, X. Chen, X. He, and J. Wang. 2021. Top-N
Recommendation with Counterfactual User Preference Simulation. In CIKM .
[64] Feng Yu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. 2016. A dynamic
recurrent model for next basket recommendation. In SIGIR . 729–732.
[65] Shengyu Zhang, D. Yao, Z. Zhao, T. Chua, and F. Wu. 2021. Causerec: Counter-
factual user sequence synthesis for sequential recommendation. In SIGIR .
[66] Wei Zhang, Junbing Yan, Z. Wang, and J. Wang. 2022. Neuro-Symbolic Inter-
pretable Collaborative Filtering for Attribute-based Recommendation. In WWW .[67] Yongfeng Zhang, Xu Chen, et al .2020. Explainable recommendation: A survey
and new perspectives. Foundations and Trends ®in Information Retrieval (2020).
[68] Yongfeng Zhang, Guokun Lai, Min Zhang, Yi Zhang, Yiqun Liu, and Shaoping
Ma. 2014. Explicit factor models for explainable recommendation based on
phrase-level sentiment analysis. In SIGIR . 83–92.
[69] Wayne Xin Zhao, Junhua Chen, Pengfei Wang, Qi Gu, and Ji-Rong Wen. 2020.
Revisiting Alternative Experimental Settings for Evaluating Top-N Item Recom-
mendation Algorithms. In CIKM . 2329–2332.
[70] Kun Zhou, Hui Wang, et al .2020. S3-rec: Self-supervised learning for sequential
recommendation with mutual information maximization. In CIKM .
[71] Yaxin Zhu, Yikun Xian, Zuohui Fu, Gerard de Melo, and Yongfeng Zhang. 2021.
Faithfully explainable recommendation via neural logic reasoning. In NAACL ."
2203.01874,D:\Database\arxiv\papers\2203.01874.pdf,"The paper describes a method for predicting the time evolution of a dynamical system using a combination of graph neural networks and a thermodynamically consistent integrator.  What are the potential limitations of this approach, and how might these limitations be addressed in future work?","The paper acknowledges that the computational complexity of the model can be a limitation, particularly for large simulations with fine grids.  Future work could address this by combining graph representations with model order reduction techniques, such as autoencoders or U-net architectures, to reduce the number of parameters and computational cost.","Thermodynamics-informed Graph Neural Networks
3.3 Geometric structure: Graph Neural Networks
LetG= (V,E,u)be a directed graph, where V={1,...,n}is a set of|V|=nvertices,E⊆V×V is a set of|E|=e
edges andu∈RFgis the global feature vector. Each vertex and edge in the graph is associated with a node and a
pairwise interaction between nodes respectively in a discretized physical system. The global feature vector deﬁnes the
properties shared by all the nodes in the graph, such as gravity or elastic properties. For each vertex i∈Vwe associate
a feature vector vi∈RFv, which represents the physical properties of each individual node. Similarly, for each edge
(i,j)∈Ewe associate an edge feature vector eij∈RFe.
In practice, the positional state variables of the system ( qi) are assigned to the edge feature vector eijso the edge
features represent relative distances ( qij=qi−qj) between nodes, giving a distance-based attentional ﬂavour to the
graph network [31, 32, 33] and translational invariance [34, 35]. The rest of the state variables are assigned to the
node feature vector vi. The external interactions, such as forces applied to the system, are included in an external load
vectorfi. A simpliﬁed scheme of the graph codiﬁcation of a physical system is depicted in Fig. 1.
fi
qijziu
zj
eijvi
vju
Figure 1: Physical system domain discretized in a mesh with node state variables zi, relative nodal distances qij,
external interactions fiand global properties u(top). Graph representation of the same system, with node and edge
attributes:viandeij(bottom).
These features are fed into an encode-process-decode scheme [23], which consists on several multilayer perceptrons
(MLPs) shared between all the nodes and edges of the graph. The algorithm consists of ﬁve steps (Fig. 2):
3.3.1 Encoding
We use two MLPs ( εv,εe) to transform the vertex and edge initial feature vectors into higher-dimensional embeddings
xi∈RFhandxij∈RFhrespectively,
εe:RFe−→RFh
eij↦−→xijεv:RFv−→RFh
vi↦−→xi.(9)
3.3.2 Processing
The processor is the core task of the algorithm, as it shares the nodal information between vertices via message passing
and modiﬁes the hidden vectors in order to extract the desired output of the system. First, a MLP ( πe) computes the
updated edge features x′
ijfor each graph edge, based on the current edge features, global features, and sending and
recieving node,
πe:R3Fh+Fg−→RFh
(xij,xi,xj,u)↦−→x′
ij.(10)
Then, for each node the messages are pooled with a permutation invariant function φbased on the neighborhood
Ni={j∈V|(i,j)∈E} of the node i. Last, the node embeddings are updated with a second MLP ( πv) using the
current node features, the pooled messages, the external load vector and the global features,
πv:R2Fh+Ff+Fg−→RFh
(xi,φ(x′
ij),fi,u)↦−→x′
i(11)
where (·,·)denotes vector concatenation and x′
iandx′
ijare the updated nodal and edge latent vectors.
4
Thermodynamics-informed Graph Neural Networks
The processing step is equivalent to the message passing [36] of 1-step adjacent nodes. In order to get the inﬂuence of
further graph nodes, the process can be recurrently repeated with both shared or unshared parameters in Mprocessing
blocks and optionally using residual connections [37]. In this approach, we use both unshared parameters and residual
connections to each message passing block and sum as aggregation function φ. Note that the computed messages
φ(x′
ij)represent a hidden embedding of the intermolecular interactions of the system (internal messages) whereas the
vectorfiaccounts for the external interactions (external messages).
3.3.3 Decoding
The last block extracts the relevant physical output information yi∈RFyof the system from the node latent feature
vector, implemented with a MLP ( δv). In this work, we predict for each particle the GENERIC energy Eand entropy
Spotentials and the ﬂattened operators landm:
δv:RFh−→RFy
x′
i↦−→yi= (l,m,E,S ).(12)
3.3.4 Reparametrization
A last processing step is needed to get the GENERIC parameters before integrating the state variables. Both operators
in matrix form LandMare constructed using the ﬂattened output of the Graph Neural Network landmrespectively,
reshaped in lower-triangular matrices. The skew-symmetric and positive semi-deﬁnite conditions are imposed by
construction using the following parametrization:
L=l−l⊤,M=mm⊤. (13)
BothEandSare directly predicted for every node. Then, these potentials can be differentiated with respect to the
network input in order to get the gradients∂E
∂zand∂S
∂zneeded for the GENERIC integrator. These gradients are easily
obtained using automatic differentiation [38], and ensures the integrability of the energy and entropy gradients [39].
Considering the dimensions of the lower triangular matrices and the scalar value of both potentials, the output dimen-
sion of the decoder network is
Fy=n(n+ 1)
2+n(n−1)
2+ 1 + 1 (14)
wherenrepresent the dimension of the state variables z.
3.3.5 Integration
The single-step integration of the state variables of the system zt→zt+1is then performed using Eq. (8).
πeu
vi
eijV′
E′πv
φf
εeεv δ
×M...
...
(b) Processor (a) Encoder (c) Decoder(l,m)
Node block Edge blockzt
(E,S )
(d) Reparametrization (e) Integrator(L,M)

∂E
∂zt,∂S
∂ztzt+1
Figure 2: Algorithm block scheme used to predict a single-step state variable change in time. (a) The encoder trans-
forms the node and edge features to a learnt embedding. (b) The processor shares the nodal information through the
graph viaMmessage passing modules. (c) The decoder extracts the GENERIC ﬂattened operators and potentials from
the processed node embeddings. (d) The reparametrization step builds the symmetries of the LandMoperators and
computes the potential gradients with respect to the network input. (e) The integrator predicts the next time step state
variables based on the GENERIC formulation. The whole process is repeated iteratively to get the dynamical rollout
of the physical system.
5
Thermodynamics-informed Graph Neural Networks
3.4 Learning procedure
The complete dataset Dis composed by Nsimmultiparametric simulation cases of a dynamical system evolving in
time. Each caseDicontains the labelled pair of a single-step state vector ztand its evolution in time zt+1for each
node of the system
D={Di}Nsim
i=1,Di={(zt,zt+1)}T
t=0, (15)
where the datasetDis disjointly partitioned in 80% training, 10% test and 10% validation sets:Dtrain,DvalandDtest
respectively.
The training is performed in a single-snapshot supervision, which has two main advantages: (i) enables parallelization
between snapshots, which decreases training time, and (ii) avoids intensive memory usage due to a several-snapshot
recursive training. The loss function is divided in two terms:
• Data loss: This term accounts for the correct prediction of the state vector time evolution using the GENERIC
integrator. It is deﬁned as the MSE along the graph nodes and state variables between the predicted and the
ground-truth time derivative of the state vector in a given snapshot,
Ldata
n=dzGT
dt−dznet
dt2
2, (16)
where∥·∥ 2denotes the L2-norm. The choice of the time derivative instead of the state vector itself is to
regularize the global loss function to a uniform order of magnitude with respect to the degeneracy terms, as
shown in Eq. (8).
• Degeneracy loss: This condition is added to the optimization in order to force the degeneracy conditions
of the Poisson and dissipative operators, which ensure thermodynamical consistency of the integrator. It is
deﬁned as the MSE along the graph nodes and state variables of two residual terms corresponding to the
energy and entropy degeneracy conditions,
Ldeg
n=L∂S
∂zn2
2+M∂E
∂zn2
2. (17)
Alternative approaches are found in the literature to impose this degeneracy restrictions, such as a speciﬁc
tensor parametrization of the brackets [17] or forcing ortogonality using additional skew-symmetric matrices
[18]. However, we decide to include it as a soft constraint in order to allow more ﬂexibility in the learning
process and improve convergence while maintaining the degeneracy conditions up to an admissible error.
The global loss term is a weighted mean of the two terms over the shufﬂed Nbatchbatched snapshots,
L=1
NbatchNbatch∑
n=0(λLdata
n+Ldeg
n). (18)
As the energy and entropy are supervised only by their gradients, we remark that (i) they are learnt up to an integration
constant value and (ii) the activation functions must have a sufﬁcient degree of continuity. To meet this second
requirement, one must select activations with non-zero second derivative in order to have a correct backpropagation
of the weights and biases. Thus, linear or rectiﬁed units (ReLU, Leaky ReLU, RReLU) are not appropriate for this
task. It is well known [40] that logistic functions such as sigmoid and hyperbolic tangent are universal approximators
of any derivative arbitrarily well, but are not optimal for very deep neural networks architectures, as they suffer from
several problems such as vanishing gradients. Then, the correct activation functions suitable for learning gradients are
the ones which combine both non-zero second derivatives and ReLU-type non-linearities, such as Softplus, Swish [41]
or Mish [42]. In the present work we use the Swish activation function.
The inputs and outputs of the networks are standardized using the training dataset statistics. Gaussian noise is also
added to the inputs during training in order to model the accumulation of error during the time integration [25], which
is not contemplated in a single-snapshot training, with the variance of the noise σ2
noiseas a tunable hyperparameter and
zero mean value. All the cases are optimized using Adam [43] and a multistep learning rate scheduler.
The code is fully implemented in Pytorch. Our datasets and trained networks are publicly available online at https:
//github.com/quercushernandez .
6
Thermodynamics-informed Graph Neural Networks
3.5 Evaluation metrics
Two ablation studies are performed to evaluate the method presented in this work. The ﬁrst case is performed using
only Graph Neural Networks (from now, GNN) with similar architecture and learning procedure used in prior works
[24, 25] and no metriplectic integrator. In the second case, we impose the metriplectic structure [15, 16] (from now,
SPNN), using standard MLPs with no graph computations. Both alternative methods are tuned for equal parameter
count in order to get a fair comparison of the results.
All the results are computed with the integration scheme in Eq. (8) iteratively from the initial conditions to the pre-
scribed time horizon T, denoted as rollout. The rollout prediction error is quantiﬁed by the relative L2 error, computed
with Eq. (19) for each snapshot and simulation case,
ε=∥zGT−znet∥2
∥zGT∥2. (19)
The results are represented in Fig. 9, 10 and 11 showing the rollout statistics for all the snapshots divided in train and
test simulations, state variables and method used (Ours, GNN or SPNN).
4 Numerical experiments
4.1 Couette ﬂow of an Oldroyd-B ﬂuid
4.1.1 Description
The ﬁrst example is a shear (Couette) ﬂow of an Oldroyd-B ﬂuid model (Fig. 3). This is a constitutive model for
viscoelastic ﬂuids, considering linear elastic dumbbells as a proxy representation of polymeric chains immersed in a
solvent.
xy
NRe, We
H
V
Figure 3: Couette ﬂow in an Oldroyd-B ﬂuid. The simulations span different Reynolds and Weissenberg numbers to
obtain different ﬂow proﬁles with a ﬁxed lid velocity.
The state variables chosen are the position of the ﬂuid on each node of the mesh q, its velocity vin thexdirection,
internal energy eand the conformation tensor shear component τ,
S={z= (q,v,e,τ )∈R2×R×R×R}. (20)
The edge feature vector contains the relative position of the nodes whereas the rest of the state variables are part of the
node feature vector. An additional one-hot vector nis added to the node features in order to represent the boundary
and ﬂuid nodes. The global feature vector urepresent the Weissenberg and Reynolds numbers of each simulation,
resulting in the following feature vectors:
eij= (qi−qj,∥qi−qj∥2),vi= (v,e,τ,n),u= (Re,We). (21)
4.1.2 Database and Hyperparameters
The training database for the Couette ﬂow is generated with the CONNFFESSIT technique [44], based on the Fokker-
Plank equation [45], using a Monte Carlo algorithm. The ﬂuid is discretized in the vertical direction with Ne= 100
elements and N= 101 nodes in a total height of H= 1. A total of 10,000 dumbells are considered at each nodal
location in the model. The lid velocity is set to V= 1, with variable Weissenberg We ∈[1,2]and Reynolds number
7
Thermodynamics-informed Graph Neural Networks
Re∈[0.1,1], summing a total of Nsim= 100 cases. The simulation is discretized in NT= 150 time increments of
∆t= 6.7·10−3.
Following Eq. (21), the dimensions of the graph feature vectors are Fe= 3,Fv= 5 andFg= 2. The hidden
dimension of the node and edge latent vectors is Fh= 10 . The learning rate is set to lr= 10−3with decreasing order
of magnitude on epochs 2000 and4000 , and a total number of Nepoch = 6000 . The training noise variance is set to
σ2
noise= 10−2.
4.1.3 Results
The rollout results for the Couette ﬂow are presented in Fig. 9. A substantial improvement is shown in the present
approach over the two other methods, which remain in a similar performance. Note that the skewed distributions
towards higher errors on each box is due to the error accumulation on snapshots further in time from the starting
conditions, where errors are lower. Fig. 8 (left) shows that the degeneracy conditions imposed by our method ensure
the thermodynamical consistency of the learnt energy and entropy potentials.
4.2 Viscoelastic bending beam
4.2.1 Description
The next example is a viscoelastic cantilever beam subjected by a bending force. The material is characterized by a
single-term polynomial strain energy potential, described by the following equation
U=C10(I1−3) +C01(I2−3) +1
D1(Jel−1)2(22)
whereUis the strain energy potential, Jelis the elastic volume ratio, I1andI2are the two invariants of the left
Cauchy-Green deformation tensor, C10andC01are shear material constants and D1is the material compressibility
parameter. The viscoelastic component is described by a two-term Prony series of the dimensionless shear relaxation
modulus,
gR(t) = 1−¯g1(1−e−t
τ1)−¯g2(1−e−t
τ2), (23)
with relaxation coefﬁcients of ¯g1and¯g2, and relaxation times of τ1andτ2.
Figure 4: Viscoelastic beam problem with a load case. The load position and direction are modiﬁed on each simulation,
obtaining different stress ﬁelds.
The state variables for the viscoelastic beam on each node are the position q, velocityvand stress tensor σ,
S={z= (q,v,σ)∈R3×R3×R6}. (24)
The relative deformed position is included into the edge feature vector whereas the rest of the variables are part of the
node feature vector. An additional one-hot vector nis added to the node features in order to represent the encastre and
beam nodes. The external load vector Fis included in the node processor MLP as an external interaction. No global
feature vector is needed in this case, resulting in the following feature vectors:
eij= (qi−qj,∥qi−qj∥2),vi= (v,σ,n). (25)
8
Thermodynamics-informed Graph Neural Networks
4.2.2 Database and Hyperparameters
The prismatic beam dimensions are H= 10 ,W= 10 andL= 40 , discretized in Ne= 500 hexahedral linear brick
elements and N= 756 nodes. The material hyperelastic and viscoelastic parameters are C10= 1.5·105,C01= 5·103,
D1= 10−7and¯g1= 0.3,¯g2= 0.49,τ1= 0.2,τ2= 0.5respectively. A distributed load of F= 105is applied
inNsim= 52 different positions with an orientation perpendicular to the solid surface. The quasi-static simulation is
discretized in NT= 20 time increments of ∆t= 5·10−2.
Following Eq. (25), the dimensions of the graph feature vectors are Fe= 4,Fv= 11 andFg= 0. The hidden
dimension of the node and edge latent vectors is Fh= 50 . The learning rate is set to lr= 10−4with decreasing order
of magnitude on epochs 600and1200 , and a total number of Nepoch = 1800 . The training noise variance is set to
σ2
noise= 10−5.
4.2.3 Results
The rollout results for the bending viscoelastic beam are presented in Fig. 10. The errors achieved by the present
approach are again below the other two methods. The beam deformed conﬁguration of three different test simulation
snapshots are represented in Fig. 5, with the color code representing the xxcomponent of the stress tensor. Similarly
to the previous case, Fig. 8 (center) shows the thermodynamical consistency of our dynamical integration.
Figure 5: (a), (b) and (c): Representation of a snapshot of three test simulations, i.e. not seen by the network on
training, of the bending beam problem. (c), (d) and (e): Their respective ground truth simulations. The color code
represents the xxcomponent of the dimensionless stress tensor, scaled ×0.001.
4.3 Flow past a cylinder
4.3.1 Description
The last example consists of a viscous unsteady ﬂow past a cylinder obstacle. The ﬂow conditions are set to obtain
varying Reynolds regimes, which result in Kármán vortex street and therefore a periodic behaviour in the steady state.
The state variables for the ﬂow past a cylinder are the velocity vand the pressure ﬁeld P,
S={z= (v,P)∈R2×R}. (26)
9
Thermodynamics-informed Graph Neural Networks
Figure 6: Unsteady ﬂow past a cylinder obstacle. The ﬂow velocity and cylinder obstacle position are varied to obtain
different Reynolds numbers and ﬂow proﬁles.
The ﬂow is computed with an Eulerian description of the output ﬁelds. Thus, the nodal coordinates ( q0) are ﬁxed
in space and considered as edge features, whereas the whole state variables are assigned to the node features. An
additional one-hot vector nis added to the node features in order to represent the inlet/outlet, walls or ﬂuid nodes. No
global feature vector is needed in this case, resulting in the following feature vectors:
eij= (q0
i−q0
j,∥q0
i−q0
j∥2),vi= (v,P,n). (27)
4.3.2 Database and Hyperparameters
The ground truth simulations are computed solving the 2D Navier Stokes equations. Six different obstacle positions
are simulated with varying ﬂuid discretization, which consist of approximately Ne= 1100 quadrilateral elements and
N= 1200 nodes. No-slip conditions are forced in the stream walls and the cylinder obstacle. The ﬂuid has a density
ofρ= 1 and a dynamic viscosity of µ= 10−3. The variable freestream velocity is contained within the interval
v∈[1,2], summing a total of Nsim= 30 cases. The unsteady simulation is discretized in NT= 300 time increments
of∆t= 10−2.
Following Eq. (27), the dimensions of the graph feature vectors are Fe= 3,Fv= 8 andFg= 0. The hidden
dimension of the node and edge latent vectors is Fh= 128 . The learning rate is set to lr= 10−4with decreasing
order of magnitude on epochs 600and1200 , and a total number of Nepoch= 2000 . The training noise variance is set
toσ2
noise= 4·10−4.
4.3.3 Results
The rollout results for the ﬂow past a cylinder problem are presented in Fig. 11. In this example the domain varies
signiﬁcantly, using a different unstructured mesh for each simulation. Thus, the graph-based architectures outperform
the vanilla SPNN, which is meant for ﬁxed structured problems. Considering the other two methods, our approach
outperforms the standard GNN architecture due to the metriplectic structure imposition over the dynamical problem,
as depicted in Fig. 8. A single snapshot of the whole rollout of three different test simulations are represented in Fig. 7,
with the color code representing the xcomponent of the velocity ﬁeld.
5 Conclusions
We have presented a method to predict the time evolution of an arbitrary dynamical system based on two inductive
biases. The metriplectic bias ensures the correct thermodynamic structure of the integrator based on the GENERIC
formalism, whose operators and potentials are estimated using computations over graphs, i.e. exploiting the geometric
structure of the problem. The results show relative mean errors of less than 3% in all the tested examples, outperform-
ing two other state-of-the-art techniques based on only physics-informed and geometric deep learning respectively.
These results conﬁrm that both biases are necessary to achieve higher precision in the predicted simulations. The use
of both techniques combine the computational power of geometric deep learning with the rigorous foundation of the
GENERIC formalism, which ensure the thermodynamical consistency of the results.
The limitations of the presented technique are related to the computational complexity of the model. Large simulations
with ﬁne grids require a high amount of message passing to get the information across the whole domain, or a very
10
Thermodynamics-informed Graph Neural Networks
Figure 7: (a), (b) and (c): Representation of a snapshot of three test simulations, i.e. not seen by the network on
training, of the cylinder ﬂow problem. (c), (d) and (e): Their respective ground truth simulations. The color code
represents the xcomponent of the dimensionless velocity ﬁeld.
0 50 100 15005001,0001,500
(a) Couette ﬂowE,S [-]
0 10 2001020
(b) Bending beam0 100 200 3000200400
(c) Cylinder ﬂowEnergy
Entropy
Figure 8: Conservation of energy and non-decreasing entropy potentials for a test case of the (a) Couette ﬂow, (b)
bending beam and (c) cylinder ﬂow. Both quantities are averaged across all graph nodes for visualization.
ﬁne time discretization, which both result in a high computational cost. Similarly, high-speed phenomena in relation
to the wave velocity of the medium might be impossible to model.
Future work may overcome the stated limitations by combining graph representations with model order reduction tech-
niques, such as autoencoders or U-net architectures [46, 47]. The idea is to replace deep message passing with various
coarse-graining steps, allowing the boundary information to reach every node in the simulation domain while reducing
the number of parameters of the neural network. Another interesting topic to extend our work is to improve the general-
ization and decrease the amount of training data via equivariant arquitectures [48, 49], which avoid data augmentation
by exploiting the invariance to certain groups such as rotations SO(3)or general Euclidean transformations E(3). As
the present work is only limited to in-silico experiments, future work may extend the proposed method to measured
datasets in real-world applications, such as digital twins of industrial processes or real-time augmented/virtual reality
environments.
References
[1] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning
framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal
of Computational Physics , 378:686–707, 2019.
[2] Miles Cranmer, Alvaro Sanchez Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, and Shirley
Ho. Discovering symbolic models from deep learning with inductive biases. Advances in Neural Information
Processing Systems , 33:17429–17442, 2020.
11"
2404.08709,D:\Database\arxiv\papers\2404.08709.pdf,"While the Fβ metric is commonly used to evaluate imbalanced data classifiers, it has been criticized for its limited interpretability and potential ambiguity. What are the specific limitations of the Fβ metric that make it difficult to interpret and potentially misleading, and how do these limitations relate to the use of simple metrics like TPR and PPV?","The Fβ metric aggregates TPR and PPV, which can lead to ambiguity because the same Fβ value can be achieved with different combinations of TPR and PPV. This makes it difficult to determine the specific trade-off between these two metrics, which is crucial for understanding the classifier's performance in imbalanced datasets.","4 Szymon Wojciechowski et al.
Acc=TP+TN
TP+FN+TN+FP(1)
However, it is easy to show that Accis heavily biased towards the majority
class and could lead to misleading conclusions. It is a good metric only for tasks
that use the so-called 0−1loss function, i.e., when the cost of errors committed
on each class is the same. This observation has led the imbalanced data science
community to look for other metrics.
Often we are interested in classifier evaluation on only a part of the data,
i.e, positive or negative data. True Positive Rate (TPR) also known as Recall
orSensitivity ),True Negative Rate (TNR) known as Specificity , andPositive
Predictive Value (PPV) also called Precision :
TPR =TP
TP+FN, (2)
TNR =TN
TN+FP, (3)
PPV =TP
TP+FP(4)
It is well known that people do not like to make comparisons based on mul-
tiple criteria (metrics). Moreover, solving such a problem leads to multi-criteria
optimization, which may return not one but several solutions, so-called non-
dominated solutions, i.e., solutions for which none of the metrics can be im-
proved without degrading some of the other criteria. Usually, to select a solution
that suits the user’s preferences, a multiple criteria decision analysis (MCDA)
is employed [4]. However, that is a difficult task that has been researched for
years. MCDA solutions focus on designing a decision-making process to assist
the user in deciding. Such a decision-making process primarily helps identify
the user’s preferences, which could be used in a decision model. An example of
such a process is PROMETHEE, which relies on pairwise comparisons to rank
alternatives evaluated on multiple criteria [17].
An alternative approach is to reduce all criteria to a single one, a function
of all used criteria, and select a solution according to its value. This approach
has been widely accepted in the scientific community and focuses on imbalanced
data classification [16].
A number of metrics have been proposed, generally aggregating TPRand
TNR, orTPRandPPV. Examples of such metrics are the arithmetic, geometric
or harmonic means between the two components: recallandspecificity . There
are also other proposals trying to enhance one of the two components of the
mean, for example Index of Balanced Accuracy [9] or Fβscore[21]. This work
focuses on using Fβ, thus let us present its definition
Fβ=(β2+ 1)×PPV ×TPR
β2×PPV +TPR(5)
Fβ-plot - a visual tool for evaluating imbalanced data classifiers 5
Theβparameter expresses the trade-off between selected simple metrics. It
will express how much more critical TPRis to the user than PPV. Improper
selection of the value of the mentioned parameter can lead to the choice of
an inappropriate classifier, e.g., favor the majority class for imbalanced data
classification task [1].
Interestingly, many practical recommendations for quality metrics note the
importance of the βparameter and suggest using indicators for several values
(usually 0.5, 1, and 2). Unfortunately, papers comparing the quality of classifiers
of imbalanced data typically provide only the F1metric.
3Fβ-plot analysis
Theideaof Fβ-plotistovisualizethe Fβvaluesforeachoftheanalyzedclassifiers
depending on the βvalue then determine the βranges to indicate which classifier
takes the best values.
The ranking of the evaluated methods will vary with the value of the β
parameter. Classifiers with a preference for the majority class (and, therefore,
achieving a higher precision value) will be superior as the value of the βparam-
eter decreases. While the βis converging to 0, the Fβvalue will approach the
precision score of the classifier. Similarly, the value of the Fβwill approximate
theTPRcomponent, with a higher value of the βparameter increasing to in-
finity. The point of balance between the components is 1 – the value for which
theFβfunction is equivalent to the harmonic mean.
Fβ-plot is a tool for observing changes in ranking depending on the βpa-
rameter. A simulation of such a relationship is shown in Figure 1. The βvalues
are presented on a logarithmic scale to make the results more readable.
101
100101
0.00.20.40.60.81.0F score
0.22
0.65
1.53
4.58
TPR:0.1 PPV:0.9 TPR:0.3 PPV:0.7 TPR:0.5 PPV:0.5 TPR:0.7 PPV:0.3 TPR:0.9 PPV:0.1
Fig. 1.Example of relations between Fβandβ
Fivedifferentscenariosconsideringdifferentconfigurationsof TPRandPPV
were simulated. As one can observe, it is affirmed that for values of β= 1, a
6 Szymon Wojciechowski et al.
perfectly balanced classifier is preferred. However, even with slight differences
in the βparameter’s value, the curves’ ranking changes significantly, preferring
results more biased towards one of the parameters. Biased solutions are preferred
as well for strongly deviating βvalues.
Noteworthy is the fact that βvalues where curves are intersecting are easily
determined
(1 +β2)·PPV A·TPR A
(β2·PPV A) + TPR A=(1 +β2)·PPV B·TPR B
(β2·PPV B) + TPR B(6)
β=s
(TPR A·TPR B·(PPV B−PPV A)
(PPV A·PPV B·(TPR A−TPR B)(7)
where indexes A and B refer to exemplary classifiers.
Nevertheless, the representation of the achieved scores in their entire range,
along with an indication of the intersections (thus, ranking changes), allows a
more thorough evaluation of the quality of tested models. At the same time, the
Fβ-plot provides guidelines to the system designer – indicating the method that
achieves the best performance according to the preference of the system’s end
user, which should determine the proportion between the cost of errors.
To give a practical example of Fβ-plot application, we conducted a simple
experiment on a well-known imbalanced dataset – Thyroid Disease [19]. We
trained 92 models using k-nearest neighbors algorithm ( k0NN) combined with a
various SMOTE-based oversamplers [14].
Firstly, the performance of the models was determined on a single data split
with a 20% hold-out. The Fβ-plot for the obtained results is presented in Fig-
ure 2. A scatter plot was also included, showing the obtained values for TPR
-PPVspace. The colors mark out the methods that achieved the best result
over the analyzed range of βvalues.
0.0 0.2 0.4 0.6 0.8 1.0
TPR0.00.20.40.60.81.0PPV
101
100101
0.00.20.40.60.81.0F score
0.33
0.50
1.26
3.05
6.42
DEAGO
Borderline_SMOTE2
ADOMSLVQ_SMOTE
LN_SMOTE
DSMOTE
Fig. 2. Fβ-plot for Thyroid Disease with hold-out evaluation.
Fβ-plot - a visual tool for evaluating imbalanced data classifiers 7
We may observe that, depending on the configuration of the βparameter,
it is possible to distinguish six oversampling methods that result in the best
Fβvalues. The balanced method is the AHCalgorithm, for which we can also
notice it has a slight majority class preference. The other algorithms that pre-
fer the majority class are CCRandGaussian_SMOTE , although their curve
characteristics are similar and are reflected in the close distance in the scatter
plot. From β= 1.26, which we can describe as a slight preference for the mi-
nority class, the NT_SMOTE ,NRSBoudary_SMOTE , and ISOMTE , re-
spectively, start to dominate. We should also point out that the selected models
constitute a boundary relative to the point <1,1>, which would be considered
aperfectmodel.
Let us notice that many experiments use more rigorous experimental proto-
cols generally based on k-fold cross-validation. Fβ-plot could be easily adapted
to the new protocol, as shown in Figure 3. The plot was expanded to include
standard deviations, and the scatter plot marked all pairs of values obtained by
methods. The color indicates the methods whose average Fβwas the highest in
the given range.
0.0 0.2 0.4 0.6 0.8 1.0
TPR0.00.20.40.60.81.0PPV
101
100101
0.00.20.40.60.81.0F score
0.74
1.07
1.18
2.36
5.72
7.56Lee
ADOMS
G_SMOTERandom_SMOTE
ADASYN
DSMOTE
Fig. 3. Fβ-plot for Thyroid Disease with cross-validation.
As expected, the list of selected methods has changed repeated evaluation.
The reoccurring algorithms are AHCstaying within the range containing β= 1
andISMOTE performing better at higher βvalues. The remaining methods
have been superseded by others that achieve similar results within the standard
deviation range of their Fβ. The observed change reflects using a broad set of
resampling algorithms, which tend to generate similar synthetic samples for the
learningset.Inordertodistinguishstatisticallybetteralgorithmsonthesampled
interval, a paired T-test was performed. The result is presented as a black line
near the bottom axis, marking the area where the best algorithm is statistically
significantly better than other algorithms. From this, it can be determined that
in the case of an extreme preference against recall ( β >7.56),ISMOTE is the
only one to achieve the highest score. This information might be practically
useful, if the aim of the system designer is to provide a screening test system.
8 Szymon Wojciechowski et al.
In other cases, we have to consider that there is a method similar to the marked
one. One should remember that based on the statistical tests performed, it is
possible to indicate a group of similar methods, which can also be important
information for the system designer.
4 Experiments
The experimental study will present the characteristics of Fβplots for selected
benchmark imbalanced datasets [6]. A vast pool of oversampling algorithms will
be compared [15], which will be used to preprocess the data, followed by the
k-NN classifier. For the evaluation protocol we choose 2x5-fold stratified cross-
validation . Table 2 presents the eight datasets chosen for this experiment. The
code for experiments reproduction, as well as Fβplot code is publicly available2.
The experiment aims to investigate observable relationships and discuss their
interpretation. The experiment results are presented in Figure 4.
Table 2. Main characteristics of the chosen benchmark datasets
Name Samples Features IR
vehicle1 846 18 2.90
segment0 2308 19 6.06
yeast-0-2-5-6_vs_3-7-8-9 1004 8 9.14
cleveland-0_vs_4 173 13 12.31
ecoli4 336 7 15.80
glass-0-1-6_vs_5 184 9 19.44
abalone-21_vs_8 581 8 40.50
poker-8-9_vs_5 2075 10 82.00
For most datasets, no statistically significant best classifier was observed
throughout the analyzed range, and it is only possible to highlight AMSCO
on the β > 2.36interval for the vehicle1 dataset. Other methods achieve the
best values on the analyzed ranges, but it should be remembered that other
oversampling can be identified to obtain similar results.
Additionally, it could be observed that for most problems, the value of the Fβ
remains high, not falling below 0.6. However, the exception is the strongly im-
balanced set of poker-8-9_vs_vs_5 , for which it is difficult to identify a method
to achieve a satisfactory result, preferring precision. Such behavior may be re-
lated to the pool of preprocessing methods themselves, which - by design - seek
to equalize (or outweigh) the model’s bias against the minority class. Another
factor that may also affect the observed result is the problem’s difficulty, which
is not always related to the degree of imbalance, but, for example, the number
2https://github.com/w4k2/fb-plot
Fβ-plot - a visual tool for evaluating imbalanced data classifiers 9
0.00.20.40.60.81.0
0.49
0.59
1.02
1.42
2.36vehicle1
ROSE
DEAGO
polynom_fit_SMOTE_starpolynom_fit_SMOTE_bus
AMSCO
0.29
2.06segment0
DSMOTE
E_SMOTEpolynom_fit_SMOTE_bus
0.00.20.40.60.81.0
0.21
0.59
1.07
1.87
2.60
4.53yeast-0-2-5-6_vs_3-7-8-9
SMOTE_RSB
DSMOTE
SOMO
MSYNSupervised_SMOTE
SMOTE_PSO
MWMOTE
0.46
0.89
2.36cleveland-0_vs_4
ADG
SUNDOGaussian_SMOTE
AND_SMOTE
0.00.20.40.60.81.0
1.48
2.60ecoli4
DEAGO
NRASDE_oversampling
0.27
1.07
1.29glass-0-1-6_vs_5
NoSMOTE
Stefanowskipolynom_fit_SMOTE_star
polynom_fit_SMOTE_poly
101
1001010.00.20.40.60.81.0
0.56
1.87
2.98abalone-21_vs_8
Lee
CCREdge_Det_SMOTE
ProWSyn
101
1001012.72
6.58poker-8-9_vs_5
PDFOS
SMOTE_CosineGazzah
Fig. 4. Fβ-plots of selected datasets
10 Szymon Wojciechowski et al.
of objects of the borderline class [18,20]. A similar effect is also observed for the
setsvechicle1 andglass-1-6_vs_5 , and the opposite effect is observed for the
setsegment0 , where we can almost always indicate the method for which the
Fβis near 1. As for the other sets, the curve assembled from best algorithms
forms a""V""shaped curve, where the lowest Fβvalues are achieved in the near
surroundings of β= 1with a slightly higher tendency towards the minority class.
5 Conclusion
The issue of selecting suitable metrics for imbalanced data problems remains
pertinent. On one hand, it is recommended to avoid aggregated metrics and
instead rely on simple metrics for analysis. On the other hand, it should be ac-
knowledged that analyzing multiple criteria simultaneously may be challenging,
especially for less experienced users or those who cannot determine the costs
of incorrect decisions related to selected data fractions. This paper presents a
simple Fβ−plotsmethod to visualize the results of the experiments, allowing
simultaneous evaluation of the quality of multiple methods for different values
ofβ, i.e., the end-user’s expectation of the validity of TPRagainst PPV. The
Fβ−plotsmethod identifies the costs at which a particular method is beneficial.
Additionally, it indicates the tasks for which a given classifier may be suitable,
such as values of imbalance ratio that are proportional to costs between simple
metrics, as suggested by Brzezinski et al. [2]. Such an analysis provides a broader
perspective on the quality and scope of the tested classifiers.
0.0 0.2 0.4 0.6 0.8 1.0
PPV0.00.20.40.60.81.0TPR
0.20.40.60.8
Fig. 5.Example of different F1values in PPV-TPRspace.
However, selecting an aggregate metric can be challenging due to its limited
interpretability [12] and potential ambiguity, i.e., even if we use the different β
Fβ-plot - a visual tool for evaluating imbalanced data classifiers 11
values, we still face the problem of aggregated metric ambiguousness because
the same Fβvalue may be taken for different PPVandTPRvalues (see Fig-
ure 5). Thus, one might suspect that machine learning methods that use such an
aggregated metric as a criterion will be biased toward specific values of simple
metrics without providing the information that there are equally good solutions
(in terms of a given metric) for other values of PPVandTPR. Additionally, Fβ
ignores the number of true negatives [3]. The issues mentioned above were not
discussed in this paper and are still waiting to be properly addressed
Acknowledgements This work was supported by the Polish National Science
Centre under the grant No. 2019/35/B/ST6/04442.
References
1. Brzezinski, D., Stefanowski, J., Susmaga, R., Szczech, I.: Visual-based
analysis of classification measures and their properties for class im-
balanced problems. Information Sciences 462, 242 – 261 (2018).
https://doi.org/https://doi.org/10.1016/j.ins.2018.06.020 , http:
//www.sciencedirect.com/science/article/pii/S0020025518304602
2. Brzezinski, D., Stefanowski, J., Susmaga, R., Szczech, I.: On the dynamics of
classification measures for imbalanced and streaming data. IEEE Transactions
on Neural Networks and Learning Systems 31(8), 2868–2878 (2020). https:
//doi.org/10.1109/TNNLS.2019.2899061
3. Christen, P., Hand, D.J., Kirielle, N.: A review of the f-measure: Its history,
properties, criticism, and alternatives. ACM Comput. Surv. 56(3) (oct 2023).
https://doi.org/10.1145/3606367 ,https://doi.org/10.1145/3606367
4. Cinelli, M., Kadziński, M., Gonzalez, M., Słowiński, R.: How to support the
application of multiple criteria decision analysis? let us start with a com-
prehensive taxonomy. Omega 96, 102261 (2020). https://doi.org/https:
//doi.org/10.1016/j.omega.2020.102261 ,https://www.sciencedirect.com/
science/article/pii/S0305048319310710
5. Demšar, J.: Statistical comparisons of classifiers over multiple data sets. Journal
of Machine Learning Research 7, 1–30 (2006)
6. Derrac, J., Garcia, S., Sanchez, L., Herrera, F.: Keel data-mining software tool:
Datasetrepository,integrationofalgorithmsandexperimentalanalysisframework.
J. Mult. Valued Logic Soft Comput 17, 255–287 (2015)
7. Devijver, P., Kittler, J.: Pattern recognition: a statistical approach. Prentice/Hall
International (1982)
8. García, S., Fernández, A., Luengo, J., Herrera, F.: Advanced nonparametric tests
for multiple comparisons in the design of experiments in computational intelligence
and data mining: Experimental analysis of power. Inf. Sci. 180(10), 2044–2064
(2010)
9. García, V., Mollineda, R.A., Sánchez, J.S.: Index of balanced accuracy: A perfor-
mance measure for skewed class distributions. In: Iberian conference on pattern
recognition and image analysis. pp. 441–448. Springer (2009)
10. García, S., Herrera, F.: An extension on ""statistical comparisons of classifiers over
multiple data sets"" for all pairwise comparisons. Journal of Machine Learning Re-
search 9, 2677–2694 (2009), http://www.jmlr.org/papers/volume9/garcia08a/
garcia08a.pdf"
1909.08258,D:\Database\arxiv\papers\1909.08258.pdf,"How does the use of default reasoning in the described system contribute to a deeper understanding of text passages, and what are the potential limitations of this approach?","The system utilizes default reasoning to extract semantic relations from text, which provides a more nuanced understanding than simply identifying keywords. However, the reliance on default rules may lead to inaccuracies if the system encounters unexpected or ambiguous information.","Kinjal Basu 399
to be true in case of absence of enough information. As an example, consider the following example
where we state that if we are not able to prove that q(a) succeeds then p(a) succeeds:
p(a) :-not q(a).
So, in the above rule we assumed that p(a) has succeeded based on the absence of information about q(a).
Default reasoning is very useful in modelling human reasoning as we can draw conclusions even in
the absence of information by defaulting to the default rule. Default reasoning thus plays an important
role in common-sense reasoning and understanding. In case of ASP, a default d stated as Normally
elements of class C have property P is represented as the following rule:
p(X) :-c(X), not ab(d(X)), not -p(X).
Here, ab(d(X)) can be read as “X is abnormal with respect to the default assumption d” and not p(X) can be
read as “We cannot successfully prove that p(X) is false” or “ p(X) may be true”. Default reasoning uses two
kinds of exceptions viz strong exceptions and weak exceptions. Weak exception makes the default inapplicable
and stop the agent from making a default conclusion. For example, in the above-mentioned default rule we can
apply a weak exception e(X) by adding the following rule to the program
ab(d(X)) :-not -e(X).
The exception states that X may not be applicable to d if e(X) may be true. Similarly, Strong Exceptions refute
the defaults conclusion by allowing the agent to derive the opposite to be true. This can be demonstrated by adding
the following rule to the program
-p(X) :-e(X).
The above rule states that p(X) is false if e(X) succeeds, which allows us to defeat ds conclusion that normally
class C elements have the property P.
4 Current Status of the Research
The goal directed ASP engine - s(ASP)[2] has been developed in our lab which is available for download ( https:
// sourceforge. net/ projects/ sasp-system/ ). Also our lab developed the CASPR system (for open do-
main question answering), which uses s(ASP) for reasoning. We have developed an unique way to extract semantic
relations from text passage using default reasoning (discussed earlier). Those semantic relations represents more
deeper understanding of a text passage. Also we have incorporated Microsoft Concept Graph [16] to CASPR
for generating relevant commonsense knowledge. Our current goal is to enhance the CASPR system to handle
complex questions and apply the system in different applications (like, image question answering).
5 Preliminary Result
CASPR system [13] was tested on Stanford’s SQuAD dataset and the results are very promising. The SQuAD
Dataset [14] contains more than 100,000 reading comprehensions along with questions and answers for those
reading passages. SQuAD dataset uses the top 500+ articles from the English Wikipedia. These articles are then
divided into paragraphs. The Dev Set v1.1 of the SQuAD Dataset has been used to obtain comprehension passages
for building a prototype for the proposed approach. This dataset has around 48 different articles with each article
having around 50 paragraphs each. Out of the 48 different articles in the SQuAD dev set, 20 articles were chosen
from different domains to help build the CASPR system. Using the 20 different articles mentioned above, the ASP
program was generated on one paragraph from each article. Then, ASP queries were generated for all the questions
in the dataset for these paragraphs. The results show the percentage of questions for which the answer generated
from the ASP solver was present in the list of answers speciﬁed for the question in the SQuAD dataset. This result
can be viewed in terms of the Table 1. The result shows that approximately 80% of the questions can be answered.
This shows that most of the knowledge, if not all, has been captured successfully in the ASP program generated
for the passage. The ASP queries generated for the questions are very similar to the original question and convey
the same meaning.
400 Conversational AI
No ArticleResultPercentCorrect Question Count
1 ABC 5 5 100.00
2 Amazon Rainforest 12 14 85.71
3 Apollo 4 5 80.00
4 Chloroplasts 4 5 80.00
5 Computational Complexity 3 3 100.00
6 Ctenophora 9 12 75.00
7 European Union Law 13 13 100.00
8 Genghis Khan 3 5 60.00
9 Geology 4 5 80.00
10 Immune System 13 15 86.67
11 Kenya 5 5 100.00
12 Martin Luther 2 5 40.00
13 Nikola Tesla 6 7 85.71
14 Normans 4 5 80.00
15 Oxygen 8 15 53.33
16 Rhine 5 8 62.50
17 Southern California 3 5 60.00
18 Steam Engine 4 5 80.00
19 Super Bowl 50 25 29 86.21
20 Warsaw 3 5 60.00
Total 135 171 78.95
Average Result 77.76
Table 1: Results of Question Answering
6 Open Issues and Expected Achievements
In our approach we have two major challenges - (i) Sentence Parsing and (ii) Commonsense Knowledge Genera-
tion. Both of them are equally important to be solved. In the following sections we will discuss about the open
issues.
6.1 Sentence Parsing
In this research we are dealing with lots of natural language sentences (mainly in English). So, parsing the sen-
tences syntactically and semantically are very important to know the meaning of the whole sentences and the
context. Currently we are using Stanford Parser [11] to tag the Parts-of-Speech (POS) and get the dependency
graph. Then we use our default rules to fetch the semantic relations of the text. Eventually we will create a
semantic parser to get deeper semantic relations of the sentence.
6.2 Commonsense Knowledge Generation
The commonsense knowledge are the backbone of this research. In a human conversation we assume many untold
things. These back ground assumptions are actually the commonsense knowledge we gathered from our day to
day life. There are many projects like Cyc [9] , ConceptNet [10], FrameNet [3], Microsoft Concept Graph [16]
etc. Most of these data are crowd-sourced or collected as a ﬂat knowledge. Also, when we retrieve these data,
Kinjal Basu 401
we do not have any control over the commonsense knowledge generated in the background. Instead of generating
relevant information, it may generate bunch of unnecessary information, which we may not require. So, our vision
is to represent commonsense knowledge in a more human-like way.
7 Conclusion
We are standing in an era where the world is gradually moving from touch-screen systems to voice-controlled
devices. In the current time, the usage of voice driven system is increasing very fast. Already we have started
using voice driven digital assistant from the big four systems - Apples Siri, Microsofts Cortana, Amazons Alexa
and Googles Assistant. These systems are very user friendly as it can handle natural languages and also it does not
need any expertise to use them.
Our long term goal is to create a social-bot, that can communicate with human as a friend. It will not be an
information retrieval bot, but beyond that it can understand emotion and intention of a human from the conversation
using commonsense.
References
[1] Weronika T Adrian, Mario Alviano, Francesco Calimeri, Bernardo Cuteri, Carmine Dodaro, Wolfgang Faber,
Davide Fusc `a, Nicola Leone, Marco Manna, Simona Perri et al. (2018): The ASP system DLV: advancements
and applications .KI-K ¨unstliche Intelligenz 32(2-3), pp. 177–179, doi:10.1007/s13218-018-0533-0.
[2] Joaquin Arias, Manuel Carro, Elmer Salazar, Kyle Marple & Gopal Gupta (2018): Constraint answer
set programming without grounding .Theory and Practice of Logic Programming 18(3-4), pp. 337–354,
doi:10.1017/S1471068418000285.
[3] Collin F Baker, Charles J Fillmore & John B Lowe (1998): The berkeley framenet project . In: Proceedings
of the 17th international conference on Computational linguistics-V olume 1 , Association for Computational
Linguistics, pp. 86–90, doi:10.3115/980451.980860.
[4] Zhuo Chen, Gopal Gupta et al. (2018): An AI-Based Heart Failure Treatment Adviser System .IEEE journal
of translational engg. in health and medicine 6, pp. 1–10, doi:10.1109/JTEHM.2018.2883069.
[5] Kenneth Mark Colby, Sylvia Weber & Franklin Dennis Hilf (1971): Artiﬁcial paranoia .Artiﬁcial Intelligence
2(1), pp. 1–25, doi:10.1016/0004-3702(71)90002-6.
[6] David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A Kalyanpur, Adam
Lally, J William Murdock, Eric Nyberg, John Prager et al. (2010): Building Watson: An overview of the
DeepQA project .AI magazine 31(3), pp. 59–79, doi:10.1609/aimag.v31i3.2303.
[7] Martin Gebser, Roland Kaminski, Benjamin Kaufmann & Torsten Schaub (2019): Multi-shot ASP solving
with clingo .TPLP 19(1), pp. 27–82, doi:10.1017/S1471068418000054.
[8] Michael Gelfond & Yulia Kahl (2014): Knowledge representation, reasoning, and the design
of intelligent agents: The answer-set programming approach . Cambridge University Press,
doi:10.1017/CBO9781139342124.
[9] Douglas B Lenat (1995): CYC: A large-scale investment in knowledge infrastructure .Communications of
the ACM 38(11), pp. 33–38, doi:10.1145/219717.219745.
[10] Hugo Liu & Push Singh (2004): ConceptNeta practical commonsense reasoning tool-kit .BT technology
journal 22(4), pp. 211–226, doi:10.1023/B:BTTJ.0000047600.45421.6d.
[11] Christopher Manning et al. (2014): The Stanford CoreNLP natural language processing toolkit . In: Proc.
52nd ACL , pp. 55–60, doi:10.3115/v1/P14-5010.
[12] George A Miller (1995): WordNet: a lexical database for English .Communications of the ACM 38(11), pp.
39–41, doi:10.1145/219717.219748.
402 Conversational AI
[13] Dhruva Pendharkar & Gopal Gupta (2019): An ASP Based Approach to Answering Questions for Natural
Language Text . In: Proc. PADL 2019 , pp. 46–63, doi:10.1007/978-3-030-05998-9 4.
[14] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev & Percy Liang (2016): Squad: 100,000+ questions for
machine comprehension of text .arXiv preprint arXiv:1606.05250 , doi:10.18653/v1/D16-1264.
[15] Joseph Weizenbaum (1966): ELIZA—a computer program for the study of natural language communication
between man and machine .CACM 9(1), pp. 36–45, doi:10.1145/357980.357991.
[16] Zheng Yu, Haixun Wang, Xuemin Lin & Min Wang (2016): Understanding short texts through semantic
enrichment and hashing .IEEE TKDE 28(2), pp. 566–579, doi:10.1109/TKDE.2015.2485224."
2209.08984,D:\Database\arxiv\papers\2209.08984.pdf,"What are the limitations of using deep learning models for generating synthetic satellite images, and how do these limitations impact the reliability of the generated images for specific remote sensing applications?","Deep learning models for generating synthetic satellite images can struggle with accurately representing complex spatial relationships and subtle spectral variations, which can limit their usefulness for applications requiring high fidelity or precise measurements.","4 LYDI A ABADY ,et al .
data collections acquired through popular data acquisition
missions and available through different online portals.
1) Airborne Visible/Infrared Imaging
Spectrometer (A VIRIS) Data Portal
A VIRIS refers to an optical sensor realized and operated
by NASA that gathers spectral radiance of 224 contigu-
ous visible and Near Infrared (NIR) spectral bands with
wavelengths ranging from 400 to 2500µm[35]. The data is
gathered using four aircrafts platforms. Until now, the area
of coverage is North America, Europe, portion of South
America, and Argentina. The A VIRIS project is focused on
studies related to climate change and global environment.
2) Copernicus Open Access Hub
The Copernicus Open Access Hub [36] is the online portal
provided by the European Space Agency (ESA) to down-
load products generated by one of the Sentinel missions.
In particular, Sentinel-1 and Sentinel-2 have gained a lot of
popularity.
Sentinel-1 mission provides C-band SAR imaging [37]
with two satellites, Sentinel-1A and Sentinel-1B, with a
revisit frequency of 6 days for both and 12 days for a sin-
gle satellite. Sentinel-1 images are available through the
Copernicus Open Access Hub [36] in different products.
For instance, different acquisition modes (i.e., patterns
of movements through which the antenna emits electro-
magnetic pulses) are available. The simplest one is the
Stripmap, where pixel carpets are sensed with a ﬁxed
antenna pattern, while a more complex variations is the
Interferometric Wide swath (IW), where the system emits
three chirps steering the antenna in the platform moving
direction. Other differences between products are related
to the level of processing they undergo. The Open Access
Hub offers them in 3 different levels: level 0 consists of
unfocused SAR echo signals; level 1 consists of focused
SAR images, provided either as complex signals as Simple
Look Complex (SLC) or as amplitude only signals as GRD
products; additional levels offer even more processing.
Sentinel-2 mission provides MSI in 13 bands for land
monitoring usage [38]. As for Sentinel-1, the images ase
provided by a pair of twin satellites that has a revisit fre-
quency of 5 days for regular coverage areas. Table 1 shows
the spatial resolution of all the multi-spectral bands: 4
bands have a resolution of 10m Ground Sampling Distance
(GSD), 6 of 20m GSD, and 3 of 60m GSD. Sentinel-2 MSI
is offered in 5 different products, 3 of which are not pub-
licly available (i.e., level-0, level-1A and level-1B). The
orthorectiﬁed products level-1C (i.e., Top-of-Atmosphere
(TOA)) and level-2A (i.e., Bottom-of-Atmosphere (BOA))
are freely available to all users.
3) Maxar DigitalGlobe Portal
The DigitalGlobe Discover portal [39] is an online plat-
form for downloading satellite imagery produced by Maxar
technologies. Maxar counts 7 satellites in its constellation
providing EO imagery, 4 on orbit (i.e., WorldView1-2-
3 and GeoEye1) and 3 decommissioned (i.e., QuickBird,Spatial Resolution (m) Band Number
2 - Blue
3 - Green
4 - Red10
8 - NIR
5 - Vegetation Red Edge 1
6 - Vegetation Red Edge 2
7 - Vegetation Red Edge 3
8a - Narrow NIR
11 - SWIR 120
12 - SWIR 2
1 - Coastal Aerosol
9 - Water vapour 60
10 - SWIR Cirrus
Table 1. Spatial Resolution of Sentinel-2 MSI bands.
Ikonos and WorldView4), whose images, however, are still
available in the portal archive. According to the technical
guide [40], the GSD resolution can vary according to the
type of imagery produced: for panchromatic products, the
ground resolution varies from 50 cm to2 m, while for MSI
it ranges from 2to2,4 m.
4) U.S. Geological Survey Landsat Data Access
The United States Geological Survey (USGS) offers a por-
tal [41] to download all the imagery produced by the
Landsat program. The Landsat program [42] is a joint mis-
sion by NASA and USGS aiming at monitoring the Earth
surface for any remote sensing application, which has been
active now for more than 50 years. It comprehends two twin
satellites (i.e., Landsat8 and Landsat9) providing MSI in
the visible, near and short-wave infrared spectra, as well
as thermal infrared wavelengths, with a ground sampling
distance of 30 m .
5) SEN12MS
The SEN12MS dataset [43] is a large scale satellite dataset
aimed at training deep learning architectures for land-
cover related applications (e.g., land-cover classiﬁcation,
segmentation, etc.). It is made up of 180,662 256×256
triplets of Sentinel-1 SAR image patches, multispectral
Sentinel-2 image patches, and MODIS [44] land cover
maps. The images span various locations and seasons and
have been processed to provide the information related to
the same exact location, i.e., to have similar GSD and geo-
reference information. In particular, since Sentinel-2 MSI
is orthorectiﬁed, Sentinel-1 data has been orthorectiﬁed
too, while the MODIS land cover maps have been upsam-
pled to reach a resolution of 10 m GSD from the original
resolution of 500 m . [43]
ANOVERVIEW ON THE GENERATION AND DETECTION OF SYNTHETIC AND MANIPULATED SATELLITE IMAGES 5
III. GENERATIVE MODELS
A GAN [45] architecture provides a game theoretic frame-
work where two networks, namely a generator and a dis-
criminator, are trained in an adversarial manner. Starting
from an input noise sample z, the generator synthesizes
new samples with a distribution pgthat is similar to the
distribution of some source data pxon which the discrim-
inator is trained. The discriminator analyzes these samples
trying to judge whether they are real or synthesized samples
produced by the generator. The goal of the generator is to
produce samples that can not be distinguished from the real
ones by the discriminator. Formally, the generator aims at
minimizing the following loss function, i.e., the adversarial
loss,
LG(ΦD,ΦG) =Ez∼pz(z)[log(1−ΦD(ΦG(z)))],(1)
where ΦGis the generator network function and ΦDis the
discriminator network function. The discriminator’s goal,
instead, is to distinguish between generated samples and
real ones. Hence, it tries to minimize the following loss
function, where xis the input sample, drawn from px:
LD(ΦD,ΦG) =−Ez∼pz(z)[log(1−ΦD(ΦG(z)))]−
Ex∼px(x)[log(Φ D(x))],(2)
where the ﬁrst term makes sure that the discriminator rec-
ognizes generated samples ΦG(z)as such, and the second
term makes sure that it recognizes samples xas original
ones belonging to px. GANs have been applied success-
fully to a variety of different domains, from text [46],
to biomedical images [47] and of course natural images.
In particular, a great effort have been paid to continu-
ously improve the quality and realism of synthetic natural
images [48] [49] [50]. Due to the impressive results they
got and the high realism of the generated images, many
variants of the basic GANs framework were developed,
going beyond image generation from scratch, that are
widely used in various computer vision tasks, and also for
satellite imagery applications. In the following, we describe
the most relevant architectures used in the literature.
One of the most relevant GANs-based frameworks is
image-to-image translation [8] [9], wherein the input image
is translated from a semantic domain to another. Image-
to-image translation is based on Conditional Generative
Adversarial Network (cGAN) [51], an evolution of the
basic GAN architecture where the training procedure is
modiﬁed with the addition of a condition on the inputs
of either the generator or the discriminator or both. This
condition can derive from any kind of additional informa-
tion. In [8], a popular cGAN, named pix2pix is proposed
to improve the quality of the generated images. To train
the pix2pix architecture, a paired dataset is used where
each input image has its corresponding representation in
the target domain, namely a reference image rfollowing a
distributionpxr. For example, if we want to transfer images
from summer to winter, we would need images of the sameplace belonging to both the summer and winter domains
where one will act as the input (summer) to be transferred
by the generator and the second is the reference that the
network will try to simulate from the input. The authors
add a term corresponding to the L1distance between the
reference images and the generated images, to the loss of
the generator, which is now expressed as:
LG(ΦD,ΦG) =Ex∼pxr(x,r)[log(1−ΦD(x,ΦG(x)))]+
λExr∼pxr(x,r),[∥(r−ΦG(x)))∥1],
(3)
whereλis a weight parameter balancing the importance of
the two loss terms LDis the same as in (2).
Another very popular cGAN is the Cycle Genera-
tive Adversarial Network (CycleGAN) [9], whose general
architecture is shown in Fig. 2. It replaces the L1distance
loss term of pix2pix with a so called cycle consistency
loss term, computed by resorting to two generators and two
discriminators. The cycle consistency loss is deﬁned as
Lcyc(ΦG,ΦG2) =Ex∼px(x)[∥(ΦG2(ΦG(x)))−x)∥1]+
Ey∼py(y)[∥(ΦG(ΦG2(y)))−y))∥1],
(4)
wherex(drawn from px) denotes a sample from the ﬁrst
domain, given as input to the ﬁrst generator ΦGand the ﬁrst
discriminator ( ΦD), andy(drawn from py) is a sample from
the second domain, given as input to the second generator
ΦG2and the second discriminator ( ΦD2).
An advantage with respect to pix2pix is that with Cycle-
GAN there is no need for a paired dataset for training
(that can be difﬁcult, if not impossible, to collect in many
applications).
An additional constraint, known as identity loss, can also
be added to the loss of the generator. The goal of the iden-
tity loss is to ensure that the output of the generator is equal
to its input, when a sample xof the ﬁrst domain is fed at
the input of the second generator ΦG2. The same applies
when a sample of the second domain is fed to the ﬁrst
generator ΦG. Formally, the identity loss has the following
expression:
Lidentity (ΦG,ΦG2) =Ex∼px(x)[∥(ΦG2(x))−x)∥1]+
Ey∼py(y)[∥(ΦG(y))−y))∥1].
(5)
Fig. 2 shows the cycleGAN architecture.
A variant of CycleGAN is the No Independent Compo-
nent for Encoding GAN (NICE-GAN) [52], where, instead
of designing a dedicated encoder for the generator, the
ﬁrst layers of the discriminators are used as the generator
encoding layers. Hence, the generator and the discriminator
share some common layers.
Another line of research aims at improving not only the
generated sample quality, but also the training process, to
mitigate the problems of convergence instability that often
affects GANs. One of the methods proposed to achieve this
6 LYDI A ABADY ,et al .
  Domain A Domain B
Generated  
or 
PristineGenerated  
or 
PristineAdversarial Loss Path
Domain A  
ReconstructedCyclic Loss Path
Domain B
Reconstructed
Domain A  
Identity
Domain B  
IdentityIdentity Loss Path
Domain A paths
Domain B paths
Fig. 2.: cycleGAN Architecture
goal is the Wasserstein GAN with Gradient Penalty loss
(WGAN-GP) [53], where the Wasserstein loss formulation,
in which the discriminator acts as a critic and increase the
distance between the real and fake samples instead of clas-
sifying the images as real or fake, is considered (WGAN
[54]) and a gradient penalty is added to the discrimina-
tor’s loss to fulﬁll a Lipschitz constraint. The loss for the
generator and the discriminator are deﬁned as
LG(ΦD,ΦG) =−Ez∼pz(z)[ΦD(ΦG(z))], (6)
and
LD(ΦD,ΦG) =Ez∼pz(z)[ΦD(ΦG(z))]−
Ex∼px(x)[ΦD(x)]+
λEw∼pw(w)[(||∇wΦD(w)||2−1)2],(7)
whereλis the gradient penalty tradeoff and wis a random
sample either produced by the generator or taken from the
distribution of real samples.
Another approach that allows to mitigate the instability
of GANs training, to enhance the quality of the generated
images, and also to speed up the training, is the progressive
training methodology. The main feature of the Progres-
sive Generative Adversarial Network (ProGAN) [48] is the
incremental approach, with the size of the model increas-
ing incrementally during training. The training starts on
small resolution data, typically 4×4pixel images, then,
during training, additional convolutional layers are added
to both the generator model and the discriminator models
to increase the resolution.
In addition to GANs, another widely used generation
framework builds upon Variational Autoencoders (V AEs)
[55]. An autoencoder [56] Ais a neural network trained to
reconstruct at the output the same data given as input, after
processing it with a series of operations that avoid learning
the identity function by respecting some constraints (e.g.,
reducing the dimensionality of the data at some point in the
network). The autoencoder is composed by two blocks:•the encoder Aethat maps the input x, to a hidden repre-
sentationh(i.e.,h= Φ Enc(x)).
•the decoder Adthat has a specular architecture compared
to the encoder, and that maps the hidden representation to
an approximate version of the input ˜x(i.e.,˜x= Φ Dec(h)).
In case of tensor data, the input can be a RGB image XRGB
and the hidden representation a vector h, with the encoder
and decoder trained together to minimize a reconstruction
loss, typically, an L2 loss term, between the input samples
and the output (decoded) sample.
In V AEs, the input is not only being encoded into a vec-
torial representation, but the hidden variable is forced to
follow a Gaussian distribution N(f(x);g(x)), with mean
f(·)and variance g(·)being functions of the input imple-
mented by the encoder network. At the decoding stage, a
sample from the hidden representation variable distribu-
tion is drawn and used as input to the decoder which in
turn outputs a reconstruction of the initial input data. The
main intuition behind this approach is to allow the decoder
to generate new data by sampling from the hidden variable
distribution. From this perspective, the hidden variable dis-
tribution can be assumed to have some desirable properties,
e.g., being a Gaussian normal distribution. In this scenario,
the total loss iused during training is equal to:
L(x,˜x) =∥x−˜x∥2
2+
βLkl(N(f(x),g(x)),N(0,Id)),(8)
where the ﬁrst term represents a “data ﬁdelity term"", i.e., a
L2 loss between the input sample xand the estimated sam-
ple˜x. The second term, applies a kind of “regularization"",
by forcing the network to minimize the Kullback–Leibler
divergenceLklbetween the learned hidden variable distri-
bution and a desired normal N(0;Id)distribution, with Id
being the identity matrix of ddimensionality, and βa hyper
parameter weight.
After training the V AE, the decoder is used to generate
new images by picking random samples from the learned
distribution.
IV . SATELLITE FORGERIES VIA DEEP
NEURAL NETWORKS
In this section, we overview the most relevant methods for
the generation and manipulation of satellite imagery, with
particular attention to those based on GANs and V AEs. As
mentioned in the introduction, due to the different charac-
teristics of satellite images and to the different needs of
remote sensing applications, a number of dedicated meth-
ods have been developed, which are suitedfor this kind of
images.
In the following, we classify the various methods based
on the type of forgery they aim at. In particular, we are
considering the following types of forgeries: i) generation
from scratch of synthetic satellite images (addressed in
Section A) and ii) modiﬁcation of the semantic content of
pre-existing satellite images (Section B).
ANOVERVIEW ON THE GENERATION AND DETECTION OF SYNTHETIC AND MANIPULATED SATELLITE IMAGES 7
Reference Year Task Data type Description
[57] 2017 Generation from scratch SAR Generate simulated SAR images
[58] 2018 Generation from scratch HSI Generate HSI to aid later on in classiﬁcation
[59] 2020 Generation from scratch Multispectral Generate multi-spectral images
[59] 2020 Semantic Modiﬁcation Multispectral Convert the land cover of multi-spectral images
[60] 2021 Semantic Modiﬁcation Multispectral Convert the land cover of multi-spectral images
[61] 2021 Semantic Modiﬁcation RGB Convert the landscape of a source city to that of a target city
Table 2. List of techniques discussed in Section IV and their characteristics.
Table 2 summarizes all the generation methods consid-
ered in Section IV of this overview, categorized accord-
ing to the proposed classiﬁcation. Additionally, Figure 3
reports a visual representation of the timeline of the sem-
inal works related to satellite image generation for each
datatype.
2017 2020 2021
Guo et al
[57]Generation of  
SAR images
Zhao et al
[61]Abady et al
[59]2018Generation of  
HSI pixels
Zhan et al
[58]Generation  
and  
manipulation  
 of multispectral  
Images  Manipulation  
of 
satellite Images  
Fig. 3.: Timeline showing the ﬁrst satellite generation
methods for each datatype
A) Generation of Synthetic Images from
Scratch
In [57] , the authors use a conventional GAN architecture
to generate synthetic SAR images. The generator is imple-
mented by a deconvolutional network that takes as input
observation parameters, that are directly measured from
the images, e.g., platform azimuth and target depression
angle, and a latent vector characterizing other observation
conditions, that is, the target position and environmental
factors such as clouds and rain. The discriminator is fed
with real samples and generated ones having the same
observation parameters. A cluster normalization procedure
is implemented to reduce the inﬂuence of the clutter, caus-
ing convergence instability. Speciﬁcally, segmentation is
applied to the images in the training set to separate tar-
get and clutter. Then, the images are normalized so that
the clutter levels are all the same. Thanks to normalization,
the discriminator learns to ignore the clutter and focuses on
the target. The generated images are evaluated by applying
to them a Convolutional Neural Network (CNN) classiﬁer
considering 10 selected target categories from the Mov-
ing and Stationary Target Acquisition and Recognition
(MSTAR) dataset. The classiﬁcation accuracy on the syn-
thetic images is similar to that achieved on pristine images,
thus proving the plausibility of the synthetic images. The
visual quality of the generated SAR images is also assessedand compared with that of images simulated by means of
ray tracing [62], and that of real samples, for speciﬁc obser-
vation parameters. The authors show that both simulators
(GAN and ray-tracing) are able to predict images close to
real ones.
In [58], the authors propose a semisupervised algorithm
for HSI classiﬁcation exploiting images produced by
GANs, to overcome the difﬁculty of gathering a large
labeled HSI training dataset. The authors propose a 1D-
GAN, called hyper spectral GAN (HSGAN), inspired by
the architecture described in [63] (with a difference in the
input dimension, set to 1 instead of 2). The proposed frame-
work enables automatic extraction of spectral features
needed for HSI classiﬁcation. The HSGAN is trained by
using unlabeled hyperspectral data, so that it learns to gen-
erate hyperspectral samples similar to the pristine samples.
Once the GAN is trained, the discriminator is modiﬁed
by replacing the last sigmoid layer with a 16 class sig-
moid layer and then is ﬁne-tuned on a small labeled dataset
for hyperspectral classiﬁcation. Both HSGAN training and
ﬁne-tuning of the discriminator is performed on the Indian
Pines dataset, gathered by A VIRIS sensor in June 1992 over
the Indian Pines region in northwestern Indiana. The size of
the original images is 145 ×145 pixels with 220 spectral
bands. The noisy and water-absorption bands are ﬁltered
out, getting 200 bands, that correspond to the output size
of the 1D-GAN.
Fig. 4 shows 128 examples of real and synthetic spectral
bands, where each line corresponds to the values assumed
by one pixel on the 200 bands. The ﬁgure also shows a real
and a synthetic waveform. The authors have shown exper-
imentally the superior performance of the HSI classiﬁer
trained on synthetic images with respect to state-of-the-art
methods. They also assessed the impact of the GAN train-
ing dataset size on the classiﬁcation performance, proving
that the size of the HSGAN has a noticeable impact on
the classiﬁcation accuracy; the more data the HSGAN is
trained on, the more accurate the classiﬁer is.
In [59], the authors use a ProGAN architecture [48] to
generate 13 bands Sentinel-2 level-1C images of 256×256
resolution. For training, they have used all 180k samples of
the SEN12MS dataset. Similar to [48], a WGAN-GP loss
function is used.
B) Semantic Modiﬁcations
In [59] mentioned before, the authors propose an image-
to-image translation solution tailored to the remote sensing
8 LYDI A ABADY ,et al .
(a) Real Spectral Samples.
 (b) Real Spectral Waveform.
(c) Generated Spectral Sam-
ples.
(d) Generated Spectral Wave-
form.
Fig. 4.: Examples of real and generated HSI bands from
[58]. 4a and 4c show 128 samples of real and generated
spectral lines . 4b and 4d show a sample of real and gen-
erated spectral waveforms respectively, with 200 values
each.
scenario. They trained a network for land cover transfer,
i.e., a network able to change the content of images to move
them from one land-cover class to another. In particular,
the paper focuses on transferring a sample from the vegeta-
tion class into the barren class and vice versa. They rely on
the NICE-GAN [52] architecture to perform unpaired style
transfer. The model was trained on 4 bands, i.e., RGB and
NIR (10 meters bands) images of size 480×480cropped
from Sentinel-2 level-1C products, with no cloud cover-
age. For the vegetation domain, 20k images were gathered
from Congo, El Salvador, Montenegro, Gabon and Guyana,
while for the barren domain 20k images were gathered
from Western Sahara. Samples were split into training and
testing. Speciﬁcally, 18K images from each domain are
used for training, while the remaining 2K are left for test-
ing. Fig. 5 shows an example of real and GAN-transferred
images for each transfer direction. The authors also verify
that the expected correlation between the bands is pre-
served in the generated images, by looking at the spectral
view of pixels belonging to different land cover classes for
both real and generated images.
A similar task is pursued by Ren et al in [60]. In this
work, the authors exploit a CycleGAN architecture to trans-
late 10 bands of Sentinel-2 level-1C image, namely the 10m
and 20m bands, from drought to vegetation and vice versa.
In [61], the CycleGAN architecture is used for a differ-
ent semantic modiﬁcation: the creation of synthetic images
having the urban structure of a given city (i.e., Tacoma
in Washington, U.S.) but with the landscape features of
For the land-cover transfer task, an unpaired dataset has to be used
(given an image from a source domain, the corresponding image in the
target domain is not available).
(a) Image translation from barren to vegetation domain.
(b) Image translation from vegetation to barren domain.
Fig. 5.: Land-cover image translation examples taken from
[59].
another city (i.e., Seattle in Washington, U.S. and Beijing,
China). To achieve this task, they train a CycleGAN model
on a given city to generate an image with the landscape fea-
tures of this speciﬁc city, starting from an input base-map
with the desired city structure. For the map domain, the
authors use the cartoDB basemaps, which provides basic
urban structural information. As for the satellite imagery
domain, they rely on Google Earth’s satellite imagery. They
use the QTile plugin in QGIS open source software [64]
to collect their datasets for both domains. All datasets
have a resolution of 256×256. 1196 pairs of images are
collected, that is satellite images and their correspond-
ing basemaps, for Seattle, 1122 pairs for Beijing and 758
pairs for Tacoma. Fig. 6 shows an example of a satellite
image generated from a basemap from Tacoma, showing
the landscape features of Seattle and Beijing.
V . BEYOND FORGERIES
In this section, we overview additional methods for the
generation and manipulation of satellite imagery, consid-
ering techniques proposed to edit an image content without
a necessarily malevolent goal. This is the case, for exam-
ple, of image enhancement techniques. Despite the fact that
these methods are not meant to be harmful or used in a
deceptive way, they still undermine image integrity to some
extent. For instance, a colorized image obtained syntheti-
cally from a gray scale one could be considered as altered
from the data integrity point of view.
ANOVERVIEW ON THE GENERATION AND DETECTION OF SYNTHETIC AND MANIPULATED SATELLITE IMAGES 9
Fig. 6.: An example of generated satellite images using the
basemap image in (a) from Tacoma, with the landscape fea-
tures of Seattle (c) and Beijing (d). (b) shows the ground
truth satellite image that belongs to the basemap in (a) [61].
In the following, we consider two types of generation
and manipulation: i) generation of satellite images of a
given type from a different type of data, e.g., the gen-
eration of an EO image starting from a SAR image and
vice versa (addressed in Section A); and ii) modiﬁcations
aiming at quality enhancement, such as colorization and
cloud removal (see Section B). Table 3 summarizes all the
generation methods considered in this section, categorized
according to the proposed classiﬁcation. Figure 7 shows the
timeline of the works described in this section.
2017 2020
Enomoto et al
[76]Cloud Removal
2018
Merkle et al
[66]He et al 
[65]Schmitt et al  
[74] Generate RGB  
from SARGenerate SAR  
 from RGBColorize  
SAR Images
Andrade et al
[71] Vandal et al
[73] Generate certain  
bands from  
other bandsGenerate RGB  
from  
Historical maps
Fig. 7.: Timeline highlighting the main works related to
synthetic image generation that can not be categorized as
malevolent forgeries.
A) Datatype Transfer
One of the main applications of remote sensing data is
Earth monitoring and change analysis. For these tasks, both
EO (optical) imagery and SAR are usually exploited, con-
sidering data captured at different times. Being able to
generate one type of data or modality from the other facil-
itates these tasks since only a type of data would need
to be acquired. We refer to this kind of image translation
with the term inter-modality datatype transfer. As described
in Section 1, several methods have been proposed in this
category.Another remote sensing application for datatype transfer
comes from land cover classiﬁcation and object detection.
Typically, different spectral bands are used for these tasks.
Instead of incurring in the costs of acquiring all the bands,
only some of them are acquired while the others are syn-
thetized automatically. Similarly, we can generate missing
spectral bands relying on existing spectral bands. We refer
to this kind of transfer as intra-modality datatype transfer
(Section 2).
1) Inter-modality Transfer
the prediction of optical images using SAR images is ﬁrst
considered in [65],
Speciﬁcally, [65] addresses the problem of generating
optical images that represent a prediction of the foreseen
land-cover changes using different combinations of remote
sensing data as input. Two different architectures are pro-
posed. The best performing method resorts to a pix2pix
architecture, that adopts a ResNet-like architecture [84] for
the generator, and a patchGAN [8] with 5 layers for the
discriminator. The patchGAN classiﬁes the patches of an
input image, providing a score matrix at the output; the ﬁnal
score on the whole image is taken by the discriminator by
averaging all the outputs.
Let T1 be a given acquisition time or period, and T2 the
target period (corresponding to a later time - the images
are considered of the same period if the collection dates’
difference is less than 5 days). Two combinations of the
input samples are considered in [65] and their impact on the
networks’ ability to predict the optical samples assessed.
Speciﬁcally, the authors consider providing as input: i)
only SAR images, from both T1 and T2; ii) both SAR
and optical images from T1 and SAR images from T2. In
the following, the two networks trained with the two input
combinations will be denoted as cGAN and MTcGAN,
respectively.
The data for training and testing are gathered from the
Copernicus hub [36]. Three different regions are consid-
ered for the experiments: Iraq, Jianghan, and Xiangyang.
For each area, 4 images are downloaded from the Coperni-
cus hub, that is two Sentinel-1 images (i.e., SAR) and two
Sentinel-2 images (i.e., EO), from T1 and T2. SAR images
were pre-processed using Sentinel Application Platform
(SNAP). For Sentinel-2 products, only 4 bands are consid-
ered, i.e., RGB and NIR, with a GSD of 10 m . The SAR and
optical images are co-registered by reprojection in order to
provide information on the same geographical area. Then
the images were divided into 256×256patches and split
into training and test sets.
With the assessment made in [65], it is argued that using
the optical data as additional input is beneﬁcial, leading to
an improvement of the networks’ capability of predicting
the optical samples corresponding to T2.
Inspired by [65], considerable research has been dedi-
cated to the generation of optical images from SAR images
and vice versa, for a different or also the same acquisi-
tion time. The most relevant approaches are described in
the following.
10 LYDI A ABADY ,et al .
Reference Year Task Data type Description
[65] 2018 Datatype transfer / Inter-modality RGB-SAR Generate optical images from SAR input or
fused optical-SAR input
[66] 2018 Datatype transfer / Inter-modality SAR-RGB Simulate SAR from optical images
[67] 2018 Datatype transfer / Inter-modality RGB-SAR Simulate optical images from SAR images
[68] 2018 Datatype transfer / Inter-modality RGB-SAR Simulate optical images from SAR images and
vice versa
[69] 2019 Datatype transfer / Inter-modality RGB-SAR Simulate optical images from SAR images
[70] 2019 Datatype transfer / Inter-modality RGB-SAR Simulate optical images from SAR images
[71] 2020 Datatype transfer / Inter-modality RGB Generate optical images from historical maps
[72] 2020 Datatype transfer / Intra-modality Multispectral Generate NIR images from RGB images
[73] 2020 Datatype transfer / Intra-modality Multispectral Generate certain bands using other bands as
input
[74] 2018 Quality Improvement / Colorization RGB-SAR Generate colorized SAR images from SAR-
optical fused image
[75] 2019 Quality Improvement / Colorization RGB Adapt color distribution of a testing dataset to
match that of a classiﬁer training dataset
[76] 2017 Qaulity Improvement / Cloud Removal Multispectral Remove clouds from RGB images using NIR
band as auxiliary information
[77] 2018 Quality Improvement / Cloud Removal RGB Remove clouds from RGB images
[78] 2018 Quality Improvement / Cloud Removal Multispectral-SAR Remove thick clouds from multispectral images
[79] 2019 Quality Improvement / Cloud Removal RGB Remove clouds from RGB images
[80] 2020 Quality Improvement / Cloud Removal RGB-SAR Remove clouds from RGB images
[81] 2020 Quality Improvement / Cloud Removal RGB-SAR Remove clouds from RGB images
[82] 2020 Quality Improvement / Cloud Removal Multispectral Remove clouds using temporal data of RGB and
NIR bands
[83] 2021 Quality Improvement / Cloud Removal RGB Remove clouds from RGB images
Table 3. List of methods discussed in Section V and their characteristics.
The opposite transfer with respect to that considered in
[65], that is, the generation of SAR images from optical
images, is addressed in [66], with the goal of improving
the matching between optical and SAR images, to improve
the geolocation accuracy of optical satellite images. The
authors use a pix2pix architecture to generate SAR patches
from the corresponding optical image using three different
losses: the original GAN loss, a variation of the adversar-
ial loss adopting the mean square error in place of the log
likelihood [85], and ﬁnally the conditional Wasserstein loss
[54]. Training is performed on 201 ×201 paired patches
from TerraSAR-X (SAR) and PRISM (optical), gathered
all over Europe.
Finally, they assess the matching between the SAR
images and the generated SAR images using three SOTA
image-matching techniques: Normalized cross-correlation
(NCC), Scale Invariant Feature Transform (SIFT) and
Binary Robust Invariant Scalable Key (BRISK). All met-
rics prove that the generated SAR images were beneﬁcial
to improve the matching.
Other methods have been proposed addressing similar
tasks. In [67], the pix2pix architecture is used to address
the transfer from SAR images to optical images. A Cycle-
GAN architecture, instead of a pix2pix, is used in [69] for
the same task of translating SAR into optical images. In
[68], a CycleGAN architecture is proposed to perform both
translation, from optical to SAR images, and vice versa.
Finally, in [70], the authors address the generation of opti-
cal images from SAR images at different times, to mitigate
the impact of heavy clouds on optical images (i.e., gener-
ating optical images that do not contain clouds). The sameapproach introduced in [65] is used, with the difference that
a pix2pix archiutecture is considered instead of the cGAN
architecture that is adopted in [65].
A last application of inter-modality transfer is the gen-
eration of optical images starting from maps or auxil-
iary raster data. In [71], the authors generate satellite-like
imagery from historical maps using a pix2pix network.
They train the pix2pix architecture on satellite images
extracted from Google and the corresponding segmented
images, providing the optical image and the segmented
image as input images from the two domains. During test-
ing, segmentation is applied to the historical maps, then the
generator is fed with the segmented image to get the opti-
cal image representing the area (what the area might have
looked like). They train two models, one on urban areas and
the second on rural and natural areas. They argue that merg-
ing the output of the two models based on the label (that is,
taking the urban labeled pixels from the model trained on
urban area, and the rural labels from the model trained on
rural areas) yields a better representation of the area.
In [86], the authors generate optical RGB and SAR
imagery from land cover segmentation maps and auxil-
iary raster data, To this purpose, they use a variation of
the pix2pix architecture where all the normalization layers
are replaced by spatially adaptive normalization (SPADE)
layers [87]. The auxiliary data is used as an input to the
generator while the land cover is passed as an input to
all SPADE layers. For training and testing, two datasets
are used: i) GeoNRW [88], containing aerial photographs,
terraSAR-X, DEMs and land cover with 10 classes; ii)
DFC2020 [89], containing Sentinel-1, Sentinel-2 and land
ANOVERVIEW ON THE GENERATION AND DETECTION OF SYNTHETIC AND MANIPULATED SATELLITE IMAGES 11
cover data with 10 classes. For the GeoNRW, SAR images
or optical images is generated from land cover maps and
Digital Elevation Models (DEM) [90] used as auxiliary
data, while for the DFC2020 (that lacks DEM data), SAR
and optical imagery are generated using either solely land
cover maps or a combination of land cover with optical (for
SAR generation) or SAR imagery (for optical generation)
as auxiliary input. To judge the effectiveness of the genera-
tion in terms of land cover coverage, the authors analyze the
land cover segmentation maps obtained by a U-Net trained
on real data and compare the land cover map obtained from
the generated images with the ground truth map, that is
used for the generation. The results show that the gener-
ated images are comparable to the real images in term of
land cover maps. Moreover, the authors show that this type
of image-to-image translation can also be used to modify
the semantic content of an image. For instance, by apply-
ing a threshold to a certain height in the DEM image, and
modifying the pixels of the land cover map with a value
in the DEM image below that height and labeling them as
water, we can get a modiﬁed image with a larger area cov-
ered by water. An example of this semantic modiﬁcation
obtained with the transfer network in [86] is shown in Fig.
8.
Fig. 8.: An example of generated datatype with semantic
modiﬁcation [86]. The ﬁrst column shows a real DEM,
optical and SAR image. The second column shows the
original land cover map with the corresponding generated
optical and SAR image. The third and fourth column show
the modiﬁed land cover with threshold set to height 32m
and 33m respectively with the corresponding generated
optical and SAR images.
2) Intra-modality Transfer
A different kind of datatype transfer regards the generation
of a subset of EO bands starting from a different subset of
bands. In [72], the authors resort to a pix2pix network for
generating the NIR band of Sentinel-2 samples using the
RGB bands as input. The model is trained using a subset of
SEN12MS dataset.In [73], the authors synthesize multiple spectral bands
from other bands, using unsupervised image-to-image
transfer [91]. More speciﬁcally, they rely on a V AE-GAN,
that is a combination between a V AE and a GAN, proposed
in [92], where a discriminator is used to learn to differenti-
ate between V AE output and real samples, and is used in an
adversarial fashion to improve the reconstruction error of
the V AE. To improve the reconstruction, the authors intro-
duce skip connections in the network and a shared spectral
reconstruction loss, encouraging the decoder to reconstruct
identical spectral wavelengths with similar distributions
while still synthesizing dissimilar bands. The shared recon-
struction loss exploits the availability of shared spectral
bands from different satellites. Data for training and test-
ing are gathered from 3 geostationary satellites: GOES-16,
GOES-17, and Himawari-8, with 16 spectral bands, 15 of
which overlap, with similar information content (GOES-
16 and GOES-17 include two visible -blue, red-, four
near-infrared -including cirrus-, and ten thermal infrared
bands. H8 captures three visible -blue, green, red-, three
near-infrared -missing cirrus band -, and the same ten
thermal infrared bands as GOES-16 and GOES-17). The
authors ﬁrst evaluate the ability of the network to gener-
ate an individual band from the other 15 bands acquired
by the same satellite and the full set of bands from the
other two satellites. This approach is applied to GOES-16,
hence each model takes 15 bands of GOES-16 and 16 of
GOES-17 and Himawari-8. The improvement in the recon-
struction error brought by the modiﬁcations introduced by
the authors, with respect to the use of a standard V AE, is
proven experimentally. The reconstruction mean absolute
error (MAE) obtained with the proposed solution is then
compared against the use of cross sensor and UNIT [93]
as a baseline proving the superior performance of the pro-
posed method in terms of MAE and precision. They also
assess the capability of the network to generate multiple
missing bands from satellites with a limited number of
spectral bands (older generation satellites often have fewer
channels hence being able to generate additional frequency
bands can be very useful). Models are trained on GOES-
16, removing bands one by one until just one band was
left, and reconstructing the missing bands. All 16 GOES-
17 and Himawari-8 bands were kept. They observe that
the reconstruction MAE decreases monotonically (approx-
imately) as more bands are given as inputs. These results
show that few bands (3-4) are needed to synthesize images
with an acceptable MAE.
B) Quality Improvement
Editing remote sensing images using deep learning is not
conﬁned to changes in the semantic content or in the
datatype. Methods have also been developed to change the
properties of the image itself, e.g., performing colorization
or reduction of cloud coverage."
2001.01902,D:\Database\arxiv\papers\2001.01902.pdf,How does the design of interactive interfaces for data exploration change when considering the constraints of different screen sizes?,"The design of interactive interfaces for data exploration must adapt to different screen sizes, utilizing widgets like dropdown menus for narrower screens and radio buttons for wider screens to accommodate the available space and ensure optimal user experience.","(a) All queries, wide screen
(b) All queries, narrow screen(c) Queries 6-8
(e) Original SDSS interface(d) Interface w/ low reward
Figure 6: (a-d) Generated interfaces from queries in Listing 1. (e) the pre-existing Sloan Digital Sky Survey search form. Screenshots only
show widgets and do not include the visualizations.
Preliminary Results
We now present preliminary results when running our ap-
proach on the query log in Listing 1, which is derived from
the Sloan Digital Sky Survey (SDSS 2017) query log. We
run MCTS for around 1 minute to generate each interface.
Figure 6 shows that the layout and widget selections are sen-
sitive to the input queries and screen constraints. (a) uses all
queries in Listing 1 as input, and generates a layout for a
wider screen. It ﬁnds that the queries vary in the attributes
that are selected (objid, count), as well as the number of re-
sults to return (top), and takes advantage of the wider screen
to enumerate them as two sets of radio buttons. In contrast,
(b) chooses dropdown widgets due to the narrower screen.
Figure 6(c) shows the interface is much simpler when
queries 6–8 are use as input. These queries have the same
WHERE clauses; since the three queries all have a TOP
clause, the user is only asked to pick the number of rows
to return (10, 100, 1000). (d) shows a low-reward interface,
and illustrates that that poor interface choices are easily pos-
sible. Finally, (e) shows the original SDSS form.
1select top 10 objid from stars
where ubetween 0and 30and gbetween 0and 30and
rbetween 0and 30and ibetween 0and 30
2select top 100 objid from galaxies
where ubetween 1and 29and gbetween 10and 30and
rbetween 9and 30and ibetween 3and 28
3select top 1000 objid from quasars where ...
4select count (*)from stars where ...
5select objid from galaxies where ...
6select top 10 objid from quasars where ...
7select top 100 objid from stars where ...
8select top 1000 objid from galaxies where ...
9select count (*)from quasars where ...
10select objid from stars where ...
Listing 1: Example queries used in experiments. All queries have
the same WHERE clause structure; for space considerations, we
only show the full queries for the ﬁrst two.
Ongoing Work
Although we have shown that the top-down approach can
generate layout-sensitive interactive interfaces, there are a
number of improvements needed for it to be practically use-
ful in terms of functionality and performance.A current limitation is that some combinations of widget
choices may not make semantic sense; one approach is to
integrate with a query engine to beneﬁt from its query analy-
sis phase, another is to leverage co-occurrence of subtrees in
the query log to identify likely and unlikely combinations of
widget choices. This can also inform the search phase. Fur-
ther, we are extending the widgets to support parameterized
sizes—for instance, a button or dropdown can be resized de-
pending on the available screen space.
This work has not been optimized for performance—
many of the algorithms perform exhaustive enumeration,
and can beneﬁt from optimizations such as parallelization,
incremental computation of the difftree and cost func-
tions, and search pruning. A key optimization opportunity is
to accelerate the transformation rules, which become slow
to evaluate as the difftree becomes large. Our goal is
interactive run-times.
Acknowledgements: This work was supported by NSF
IIS 1527765, 1564049, 1845638, and Amazon and Google
awards.
References
[Browne et al. 2012] Browne, C.; Powley, E. J.; Whitehouse, D.;
Lucas, S. M.; Cowling, P. I.; Rohlfshagen, P.; Tavener, S.; Liebana,
D. P.; Samothrakis, S.; and Colton, S. 2012. A survey of monte
carlo tree search methods. IEEE Transactions on Computational
Intelligence and AI in Games 4:1–43.
[Comber and Maltby 1997] Comber, T., and Maltby, J. R. 1997.
Layout complexity: Does it measure usability? In INTERACT .
[Gajos and Weld 2004] Gajos, K. Z., and Weld, D. S. 2004. Supple:
automatically generating user interfaces. In IUI.
[Mackinlay, Hanrahan, and Stolte 2007] Mackinlay, J. D.; Hanra-
han, P.; and Stolte, C. 2007. Show me: Automatic presentation
for visual analysis. IEEE Transactions on Visualization and Com-
puter Graphics 13.
[SDSS 2017] SDSS. 2017. Sloan digital sky survey.
http://www.sdss.org/.
[Sievert et al. 2017] Sievert, C.; Parmer, C.; Hocking, T.; Chamber-
lain, S.; Ram, K.; Corvellec, M.; and Despouy, P. 2017. plotly:
Create interactive web graphics via plotly. js. R package version
4(1):110.
[Zhang, Sellam, and Wu 2017] Zhang, H.; Sellam, T.; and Wu, E.
2017. Mining precision interfaces from query logs. In SIGMOD
Conference ."
2310.20599,D:\Database\arxiv\papers\2310.20599.pdf,"If the brain relies on a similar neural substrate for generating and processing sensory information, how might this shared mechanism contribute to the subjective experience of hallucinations and mental imagery, and what are the potential implications for understanding the neural basis of these phenomena?","The shared neural substrate for generating and processing sensory information suggests that hallucinations and mental imagery might arise from similar neural processes, potentially involving feedback loops and internal representations. This understanding could lead to new insights into the neural mechanisms underlying these subjective experiences and their potential role in mental disorders.","through feedforward and feedback-dominated phases, as we suggest in FFA, has an anatomical and
physiological basis.
Algorithm 1 Training a simple three-layer network by FFA
Learning
parameters: Wf1,Wf2,Wb1,Wb2
forepoch <n_epochs do
Y=Wf2·hf;hf=Wf1·X
ef=T−Y
Loss f=1
2eT
f·ef
∆Wf2=−eT
f·hT
f,∆Wf1=−Wb2·ef·XT; // forward updates
ˆX=Wb1·hb;hb=Wb2·Y
eb=X−ˆX
Loss b=1
2eT
b·eb
∆Wb1=−eT
b·hT
b,∆Wb2=−Wf1·eb·YT; // backward updates
end
Internally generated perceptual experiences, such as hallucinations, dreams, and mental imagery
evoke vivid experiences that mimic the perception of real-world stimuli. Neuroimaging studies
demonstrate an overlap in neural activation between internally generated experiences and perception
suggesting a shared neural substrate for generating and processing sensory information (Ganis et al.,
2004; Pearson, 2019; Pearson et al., 2008; Abid et al., 2016; Dijkstra et al., 2017). While studies
have provided insights into the brain regions involved in these phenomena, the neural mechanisms
and computations underlying hallucination and imagery remain a topic of ongoing research and
debate. One challenge is that hallucinations and imagery are subjective experiences that are difficult
to objectively measure and study (Pearson et al., 2008). Additionally, the neural correlates of these
experiences can vary across individuals and different types of hallucinations (Suzuki et al., 2017,
2023). Thus, computational modeling of how comparable phenomena can emerge in neural networks,
without explicitly training for complex non-bio-plausible generative objective functions (Goodfellow
et al., 2014), would help elucidate the neural mechanisms that may underpin these internally-generated
perceptions.
2.2 Bio-plausible training
Our work also falls within the class of bio-plausible extensions of the original BP algorithm that
try to avoid the weight transport problem (Grossberg, 1987). In an effort to mitigate the weight
transport problem, (Lillicrap et al., 2016) developed an algorithm known as Feedback Alignment
(FA). This algorithm employs random and fixed feedback connections to propagate errors, initiating
a line of research that explored different variations of random, yet fixed, feedback connections for
credit assignment (Nøkland, 2016; Crafton et al., 2019; Liao et al., 2015). Another line of work
uses a strategy that still aims for BP-like symmetric weights while circumventing weight transport
by designing a training objective for the feedback path that encourages symmetry (Akrout et al.,
2019). For example, augmenting a reconstruction loss with weight decay will constrain solutions
to the transpose in the linear setting (Kunin et al., 2019, 2020). However, our method differs in
two key ways. First, those methods required invoking a separate gradient pass to train the feedback
weights whereas we accomplish the training of feedback with the same feedforward network, thus
adding no other hidden paths. Second, those methods explicitly seek symmetry whereas we do not
constrain the stage-wise feedback weights, only their end-to-end goal. Our algorithm resembles the
stage-wise reconstruction in target propagation (TP) which could also result in end-to-end propagation
of latent representations back to inputs if noise at each local propagation step is sufficiently small
(Bengio, 2014; Lee et al., 2014). Unlike the layerwise implementation of TP, our training approach is
end-to-end, and we do not use any BP training on the penultimate layer of the discriminator.
4
MNIST CIFAR10
CIFAR10MNISTA B
C
CIFAR10
%
% %Figure 2: Co-optimization in FFA. A) Accuracy and reconstruction performance for FFA and
control algorithms as a function of epochs. B) Dual-task performance for a variety of feedforward
discriminative and autoencoder architectures trained under BP or FA compared to FFA training
(details for architecture in Suppl. 8.1). The shaded area represents the desired corner. C) Robustness
to input Gaussian noise ( µ= 0and varying σ2between 0 and 1) as measured by test accuracy on the
noisy input.
3 Feedback-Feedforward Alignment
During the training, BP uses a computational graph to backpropagate the error to the hidden layers
(Figure 1). This computational graph is a linear neural network that is the transpose of the forward
neural network and is constantly updated every time the forward weights are updated. FA and in
general the family of the random feedback gradient path such as DFA, use random values and do not
update the backward weights during the training. FFA in essence runs two FA algorithms to train
the forward pass and backward pass alternatively. The FFA diagram in Figure 1 highlights its two
distinguishing features: feedback (decoder) has an end-to-end goal and co-opting of the forward
discriminator path (encoder) to train this decoder. A pseudo-code for a simple three-layer neural
network to clarify the parameter updates in FFA is in algorithm 1.
Below, we compare how FFA operates on MNIST across two architectures (fully connected and
convolutional) and on CIFAR10 using a ResNet architecture by directly reconstructing from the
ten-dimensional discriminator output. For details on the architecture please refer to Supplementary
material 8.1. For each architecture, we compare FFA to BP and feedback alignment (FA) (Lillicrap
et al., 2016) training of a single objective (feedforward discrimination or an autoencoder loss)
resulting in 5 control models: FFA, BP, FA, BP-AE, and FA-AE. The purpose of these controls was
to verify that the properties of gradient descent on a single loss at the output does not trivially invoke
reconstruction of input for example in BP-trained discriminative networks.
3.1 FFA achieves the co-optimization of discrimination and reconstruction
We highlight performance results on a convolutional architecture but also report results on a fully
connected architecture. Convolutional architectures are potentially of greater interest because they
are used for scaling up algorithms to larger datasets. Furthermore, convolutional architectures tend to
expose greater performance gaps between BP and FA (Bartunov et al., 2018). FFA-trained networks
achieved digit discrimination performance on par with FA but slightly below BP (Figure 2). However,
on MNIST, reconstruction performance is exceedingly high. Critically, we were also interested in
5
inputsitr 0 itr 1 itr 2 itr 3reconstructed images
FFA BP-Autoencoder FA-Autoencoder
AA BFigure 3: Denoising in FFA. Closed-loop inference on noisy inputs ( σ2= 0.4) performed by FFA
and control algorithms assuming a static read-out for discrimination set by iteration 0. Shown at right,
the sample reconstructions recovered by FFA and control autoencoders over 4 iterations (no clipping
or other processing was performed on these images).
seeing if FFA could co-train, using only the discriminator weights for credit assignment, a digit
reconstruction path. We found that FFA produced reconstruction on par with a BP-trained autoencoder
for convolutional architectures while slightly lagging the autoencoder standard for reconstruction
on fully connected architectures. Thus, within the same network, FFA co-optimizes two objectives
at levels approaching the high individual standards set by a BP-trained discriminator and a BP-
trained reconstructor (see Figure 2 and Figure Suppl. 7). In FFA, like FA, the feedforward and
feedback weights aligned over training (Lillicrap et al., 2016), but only in FFA, alignment is useful
for reconstruction, presumably because both paths are free to align to each other which breaks the
random feedback constraint of FA. In examining discrimination versus reconstruction performance,
these can be mutually exclusive. For example, single objective networks tend to improve along
one axis or the other. In contrast, FFA-trained networks moved toward the top-right corner of the
plot indicating co-optimization along both axes (Figure 2, scatter plot). As shown in Figure 2B
for CIFAR10, FFA and FA both struggle to keep up with BP, so for the rest of the paper regarding
inference, we focus on MNIST.
3.2 FFA induces robustness to image noise and adversarial attacks
Although in FFA training, we did not use any noise augmentation, as we show in this section, the
network trained under FFA developed robustness to noise and adversarial attacks relative to the
BP control. Previous works showed that BP networks are vulnerable to noise and highlighted that
FA-trained networks are surprisingly robust (Goodfellow et al., 2015; Akrout, 2019). When pixel
noise was used to degrade input characters, we found that FFA was more robust than BP conferring
some of the same robustness seen in FA (Figure 2C). This advantage of FFA and FA over BP was
also true for gradient-based white-box adversarial attacks (Figure Suppl. 9).
4 Flexible visual inference through recursion
While FFA is not explicitly a recurrent network, by coupling the feedforward and feedback pathways
through mutual learning of dual, complementary losses, it may indirectly encourage compatibility
in their inference processes. That is, we can run the network in a closed loop, passing ˆxfrom the
decoder back in as input to the encoder (replacing x) (see Figure 1). In this section, we explore the
capabilities of FFA in dealing with missing information (noise or occlusion) and in generation (visual
imagery, hallucinations, or dreams). It is worth noting that FFA was not trained to perform any of
these tasks and was only trained for discrimination and reconstruction of clean images, conditioned
on this discrimination. The inference algorithm we use in this section relies on two main components:
6
iterations iterationsFigure 4: Resolving occlusion. A 15x15 black square occludes the digits in the first columns as shown
in the second column. Each row shows a sample occluded digit (5, 8, and 9) and the corresponding
resolved images under high noise and low noise conditions. For high noise and low noise visual
inference, the resolved digit is depicted in 5th and the last columns, respectively. For details regarding
the sample intermediate iterations refer to Figure Suppl. 10.
recursion and noisiness of inference in each recursion. The algorithm was developed in (Kadkhodaie
and Simoncelli, 2021) for denoising autoencoders based on Empirical Bayes Theorem (Miyasawa,
1961). Although FFA is not trained as a denoiser autoencoder (no noisy input was used during
training), we hypothesized that since it exhibits robustness to noise properties, then the theory applies
here and the algorithm can be adapted to draw effective inferences from the representation learned by
FFA. We especially focused on the effect of the noisiness of inference to inform the computational
role of noisy responses in actual biological neurons, as the utility of noisy responses remains largely
unknown despite extensive active research (Echeveste and Lengyel, 2018; Findling and Wyart, 2021;
McDonnell and Ward, 2011).
Figure 5: Hallucination. Without external input, we let the inference algorithm run on the FFA-trained
network until convergence (the last column) for high noise (upper) and low noise (lower) inference.
The sample iterations are linearly spaced and for high noise, there are typically twice as many
iterations needed. Refer to Section 8.8 for iteration values.
7
4.1 Denoising
As a first step toward future recurrent processing within FFA, we simply ran the network in a closed
loop, passing ˆxfrom the decoder back in as input to the encoder (replacing x) (see Figure 1) and
found that both discrimination and reconstruction performance is sustained over iterations similar
to an autoencoder whereas BP and FA discriminators change over multiple closed-loop iterations
– when their feedback path from training was used for generative inference in runtime – and thus
would require a dynamic decoder to recover any performance (Figure 3).
4.2 Resolving occlusions
We occlude parts of an input image by a blank square and run the network inference. The assumption
here is that the occluded image was briefly presented and during the inference, the original image is
not accessible throughout inference. Figure 4 shows examples of completion of the pattern using
FFA. Even though for high noise inference more iterations were needed, the generated samples do
not reflect any superiority compared to low noise inference which took fewer iterations to converge.
4.3 Hallucination
Visual hallucinations refer to the experience of perceiving objects or events even when there is no
corresponding sensory stimulation that would typically give rise to such perceptions. As mentioned
above, the spontaneous activity in V1 is linked to the vividness of hallucinated patterns. Here, we
let the FFA-trained network run through the inference algorithm starting from Gaussian noise at
the input and adding noise in each iteration. As shown in Figure 5, when in the high noise regime
(β= 0.2), the quality of hallucinated digits is better compared to the low noise regime ( β= 0.99,
for the definition of βsee Section 8.5). Given that the noise in the inference algorithm controls the
convergence rate (Kadkhodaie and Simoncelli, 2021), these results suggest that the computational
role of spontaneous activity in generating stronger hallucinated percepts may be the refinement of the
hallucinated patterns.
4.4 Mental imagery
Visual mental imagery refers to the ability to create mental representations or pictures of visual
information in the absence of actual sensory input (Pearson et al., 2015; Colombo, 2012). A key
distinction between mental imagery and hallucinations is that mental imagery involves voluntarily
creating mental images through imagination, while hallucinations are involuntary sensory perceptions.
To implement the voluntary, top-down activation of a percept (e.g. ’9’), we add the average activation
pattern of the category in the latent layer to each recursion in the inference algorithm. Presumably, the
brain has a recollection of the category which can be read out from memory during mental imagery.
Figure 6 shows that as noise in the inference goes higher, so does the quality of the imagined digits.
5 Limitations
We acknowledge several limitations of the Feedback-Feedforward Alignment (FFA) framework
in its current form. One key limitation is the difficulty of scaling FFA to larger datasets such as
ImageNet. While we observed gaps in performance compared to classical backpropagation (BP)
on CIFAR10, we found little difference compared to the Feedback Alignment (FA) baseline in
discrimination performance on smaller datasets such as CIFAR10. However, it is yet possible that
FFA could be more suitable at large scale for specific architectures, such as transformers, where
layer sizes do not decrease towards the output layer. Scaling up FFA requires further theoretical and
empirical exploration. Another limitation is related to the assessment of the generated inferences.
Currently, the evaluation relies primarily on visual inspection. Although we included classifier
accuracy reports for denoising, it assumes that perception arises solely from top activations and
that bottom hierarchy activation (such as V1) does not directly contribute to perception. Enhancing
the evaluation methodology to incorporate more objective measures and quantitative assessments
of generated inferences would strengthen the framework. Furthermore, while FFA demonstrates a
balance between discrimination performance, efficient learning, and robust recurrent inference, it is
important to acknowledge that FFA may not fully capture all aspects of the biological brain. The
8
Higher noiseImagine ‘5’ Imagine ‘3’
Higher noiseiterations iterationssamples samples
Figure 6: Visual imagery. Generated samples (upper panels) using the inference algorithm on the FFA-
trained network when top-down signal ’5’ (left) and ’3’ (right) was activated. The sample iterations
(equally spaced) for sample generations were shown in the lower panel. Each row corresponds to an
inference noise level. Refer to Section 8.7 for iteration and βvalues
framework represents a step towards understanding the brain’s mechanisms but may still fall short
in faithfully replicating the intricacies of neural processing such as the precise dynamics of neural
responses or differences in tuning among feedforward and feedback neural subtypes, though it takes
a step toward those directions. Overall, these limitations highlight the need for further research and
development to address the scalability of FFA, refine evaluation methodologies, and gain deeper
insights into the biological plausibility of the framework. Overcoming these limitations will pave the
way for more effective and robust alternatives to BP, advancing the understanding and application of
neural network training algorithms to neurobiology.
6 Conclusions
In moving beyond classical error backpropagation training of a single-objective, feedforward network,
we have presented a feedforward-feedback algorithm that trains neural networks to achieve mutualistic
optimization of dual objectives. Co-optimization provides attendant advantages: avoids weight
transport, increases robustness to noise and adversarial attack, and gives feedback its own runtime
function that allows closed-loop inference. Through our experiments, we demonstrated that the
network trained using the FFA approach supports various visual inference tasks.
7 Broader Impacts
This work has broader impacts that include advancing our understanding of human perception,
enhancing the robustness and performance of neural networks, helping to identify the emergence
of closed-loop inference in larger networks for real-time applications, and potential implications
for clinical research of mental disorders. By studying the neural mechanisms underlying visual
perception, this research contributes to our understanding of natural and artificial vision.
9
Acknowledgments
T.T. is supported by NIH 1K99EY035357-01. This work was supported by NIH RF1DA056397,
NSF 1707398, Gatsby Charitable Foundation GAT3708, NIH R34 (NS116739), Klingenstein-Simons
fellowship, Sloan Foundation fellowship and Grossman-Kavli Scholar Award, as well as a NVIDIA
GPU grant, and was performed using the Columbia Zuckerman Axon GPU cluster.
References
Abid, H., Ahmad, F., Lee, S. Y ., Park, H. W., Im, D., Ahmad, I., and Chaudhary, S. U. (2016). A
functional magnetic resonance imaging investigation of visual hallucinations in the human striate
cortex. Behav. Brain Funct. , 12(1):31.
Akrout, M. (2019). On the Adversarial Robustness of Neural Networks without Weight Transport.
arXiv:1908.03560 [cs, stat] . arXiv: 1908.03560.
Akrout, M., Wilson, C., Humphreys, P. C., Lillicrap, T., and Tweed, D. (2019). Deep Learning
without Weight Transport. arXiv:1904.05391 [cs, stat] . arXiv: 1904.05391.
Bartunov, S., Santoro, A., Richards, B. A., Marris, L., Hinton, G. E., and Lillicrap, T. (2018).
Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures.
arXiv:1807.04587 [cs, stat] . arXiv: 1807.04587.
Bengio, Y . (2014). How Auto-Encoders Could Provide Credit Assignment in Deep Networks via
Target Propagation. arXiv:1407.7906 [cs] . arXiv: 1407.7906.
Choksi, B., Mozafari, M., O’May, C. B., Ador, B., Alamia, A., and VanRullen, R. (2021). Predify:
Augmenting deep neural networks with brain-inspired predictive coding dynamics.
Clark, A. (2013). Whatever next? predictive brains, situated agents, and the future of cognitive
science. Behav. Brain Sci. , 36(3):181–204.
Colombo, B. (2012). Mental imagery. In Encyclopedia of the Sciences of Learning , pages 2187–2191.
Springer US, Boston, MA.
Crafton, B., Parihar, A., Gebhardt, E., and Raychowdhury, A. (2019). Direct feedback alignment
with sparse connections for local learning. Front. Neurosci. , 13:525.
Debes, S. R. and Dragoi, V . (2023). Suppressing feedback signals to visual cortex abolishes attentional
modulation. Science , 379(6631):468–473.
Dijkstra, N., Bosch, S. E., and van Gerven, M. A. J. (2017). Vividness of visual imagery depends on
the neural overlap with perception in visual areas. J. Neurosci. , 37(5):1367–1373.
Echeveste, R. and Lengyel, M. (2018). The redemption of noise: Inference with neural populations.
Trends Neurosci. , 41(11):767–770.
Findling, C. and Wyart, V . (2021). Computation noise in human learning and decision-making: origin,
impact, function. Curr. Opin. Behav. Sci. , 38:124–132.
Friston, K. (2009). The free-energy principle: a rough guide to the brain? Trends Cogn. Sci. ,
13(7):293–301.
Ganis, G., Thompson, W. L., and Kosslyn, S. M. (2004). Brain areas underlying visual mental
imagery and visual perception: an fMRI study. Cognitive Brain Research , 20(2):226–241.
Gilbert, C. D. and Sigman, M. (2007). Brain states: top-down influences in sensory processing.
Neuron , 54(5):677–696.
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.,
and Bengio, Y . (2014). Generative adversarial networks.
Goodfellow, I. J., Shlens, J., and Szegedy, C. (2015). Explaining and Harnessing Adversarial
Examples. arXiv:1412.6572 [cs, stat] . arXiv: 1412.6572.
10
Grossberg, S. (1987). Competitive learning: From interactive activation to adaptive resonance.
Cognitive Science , 11(1):23–63.
He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition.
arXiv:1512.03385 [cs] . arXiv: 1512.03385.
Helmholtz, H. v., Gullstrand, A., Kries, J. v., Nagel, W. A., and University College, L. L. S. (1909).
Handbuch der Physiologischen Optik [electronic resource] . Hamburg ; Leipzig : verlag von
Leopold V oss.
Kadkhodaie, Z. and Simoncelli, E. (2021). Stochastic solutions for linear inverse problems using
the prior implicit in a denoiser. In Ranzato, M., Beygelzimer, A., Dauphin, Y ., Liang, P., and
Vaughan, J. W., editors, Advances in Neural Information Processing Systems , volume 34, pages
13242–13254. Curran Associates, Inc.
Kar, K., Kubilius, J., Schmidt, K., Issa, E. B., and DiCarlo, J. J. (2019). Evidence that recurrent
circuits are critical to the ventral stream’s execution of core object recognition behavior. Nat.
Neurosci. , 22(6):974–983.
Keller, G. B. and Mrsic-Flogel, T. D. (2018). Predictive Processing: A Canonical Cortical Computa-
tion. Neuron , 100(2):424–435.
Khaligh-Razavi, S.-M. and Kriegeskorte, N. (2014). Deep supervised, but not unsupervised, models
may explain IT cortical representation. PLoS Comput. Biol. , 10(11):e1003915.
Kietzmann, T. C., Spoerer, C. J., Sörensen, L. K. A., Cichy, R. M., Hauk, O., and Kriegeskorte,
N. (2019). Recurrence is required to capture the representational dynamics of the human visual
system. Proc. Natl. Acad. Sci. U. S. A. , 116(43):21854–21863.
Kreiman, G. and Serre, T. (2020). Beyond the feedforward sweep: feedback computations in the
visual cortex. Ann. N. Y. Acad. Sci. , 1464(1):222–241.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolu-
tional Neural Networks. In Pereira, F., Burges, C. J. C., Bottou, L., and Weinberger, K. Q., editors,
Advances in Neural Information Processing Systems 25 , pages 1097–1105. Curran Associates, Inc.
Kunin, D., Bloom, J. M., Goeva, A., and Seed, C. (2019). Loss Landscapes of Regularized Linear
Autoencoders. arXiv:1901.08168 [cs, stat] . arXiv: 1901.08168.
Kunin, D., Nayebi, A., Sagastuy-Brena, J., Ganguli, S., Bloom, J., and Yamins, D. L. K. (2020). Two
Routes to Scalable Credit Assignment without Weight Symmetry. arXiv:2003.01513 [cs, q-bio,
stat]. arXiv: 2003.01513.
Lee, D.-H., Zhang, S., Fischer, A., and Bengio, Y . (2014). Difference Target Propagation.
arXiv:1412.7525 [cs] . arXiv: 1412.7525.
Lee, T. S. and Mumford, D. (2003). Hierarchical bayesian inference in the visual cortex. JOSA A ,
20(7):1434–1448.
Liao, Q., Leibo, J. Z., and Poggio, T. (2015). How important is weight symmetry in backpropagation?
Lillicrap, T. P., Cownden, D., Tweed, D. B., and Akerman, C. J. (2016). Random synaptic feedback
weights support error backpropagation for deep learning. Nature Communications , 7:13276.
Lillicrap, T. P. and Santoro, A. (2019a). Backpropagation through time and the brain. Curr. Opin.
Neurobiol. , 55:82–89.
Lillicrap, T. P. and Santoro, A. (2019b). Backpropagation through time and the brain. Current
Opinion in Neurobiology , 55:82–89.
Lindsay, G. W. (2021). Convolutional neural networks as a model of the visual system: Past, present,
and future. J. Cogn. Neurosci. , 33(10):2017–2031.
Markov, N. T., Ercsey-Ravasz, M., Essen, D. C. V ., Knoblauch, K., Toroczkai, Z., and Kennedy, H.
(2013). Cortical High-Density Counterstream Architectures. Science , 342(6158):1238406.
11"
1901.11333,D:\Database\arxiv\papers\1901.11333.pdf,"The paper describes a method for transferring text attributes using a sequence-to-sequence model.  What are the limitations of using automatic evaluation metrics to assess the quality of the generated text, and how does the paper address these limitations?","Automatic evaluation metrics like BLEU and perplexity can be limited in their ability to capture nuanced aspects of text quality, such as fluency and attribute correctness. The paper addresses this by supplementing automatic evaluation with human evaluation, which provides a more comprehensive assessment of the generated text.","Figure 1: Iterative process of the algorithm to transfer the text style from positive to negative reviews.
malized below in Algorithm 1.
Algorithm 1 Iterative Matching and Translation
Input: Two corpora X,Y with different attributes.
ˆX={}
fort= 0,···,Tdo
ˆY(t)={}
forxi∈Xdo
ift== 0 then
ifmax y∈YSim(xi,y)>γ then
ˆyi= argmax
y∈YSim(xi,y)
ˆX.append( xi)
ˆY(0).append( ˆyi)
end if
else
match i←argmax
y∈YSim( ˆyi,y)
ˆyi← argmin
y∈{ˆyi,match i}WMD( xi,y)
ˆY(t).append( ˆyi)
end if
end for
Train a Seq2Seq model M(t):ˆX→ˆY(t)
forxi∈ˆXdo
trans i←M(t)(xi)
ˆyi← argmin
y∈{ˆyi,trans i}WMD( xi,y)
ˆy(t)
i←ˆyi
end for
end for
Output: Attribute transfer model M(T):X→ˆY(T)
3.3 Method Details
Word Mover Distance In our algorithm, WMD
is used to measure the content shift from the
source sentence to the rewrite. WMD is a metric
of “travel cost” from sentence satosb. The de-
tailed explanations and calculation of the distance
is in the paper (Kusner et al., 2015). In brief, each
sentence is represented as a weighted point cloud
of embedded words. The distance between the
sentence satosbis the minimum cumulative dis-
tance that words from sentence saneed to travel to
match exactly the point cloud of sentence sb. De-
note the vocabulary size as n, the travel distancefrom the word iin sentence sato the word jin
sentence sbasT(i,j), and the corresponding cost
of this “word travel” as c(i,j). The distance cal-
culation is formulated as
WMD( sa,sb) = min
T≥0n∑
i,j=1Tij·c(i,j).(3)
Since the initial construction of pseudo-parallel
corpus is already guaranteed to have good target
attribute and grammaticality, the only remaining
criteria to fulﬁll is the minimal content change
from the original sentence to the resulted output.
WMD is used as a decision factor whenever an
update occurs. Keeping the sentence pair with the
smallest cost from each other approximates mini-
mization of the content shift.
Advantages of the WMD over other basic mea-
sures of sentence similarity include the fact that
it has no hyperparameters to tune, appropriately
handles sentences with unequal number of words
(via weighted word matchings that sum to 1), ac-
counts for synonymic-similarity at the word-level
(through use of pretrained word embedding vec-
tors), and considers the entire contents of each
sentence (every word must be somehow matched).
Furthermore, WMD has produced high accuracy
in information retrieval (Brokos et al., 2016; Kim
et al., 2017), where measuring content-similarity
is critical (as in our attribute transfer task).
Semantic Sentence Representation For the
matching process in Step 1, the cosine similar-
ity is computed between each pair of sentences.
There is no perfect way of semantic representa-
tion of a sentence, but a state-of-the-art method is
to use the sentence embeddings obtained by aver-
aging the ELMo embeddings of all the words in
the sentence (Peters et al., 2018). ELMo uses a
deep, bi-directional LSTM model to create word
representations within the context that they are
used. Perone et al. (2018) have shown that this ap-
proach can efﬁciently represent semantic and lin-
guistic features of a sentence, outperforming more
elaborate approaches such as Skip-Thought (Kiros
et al., 2015), InferSent (Conneau et al., 2017a) and
Universal Sentence Encoder (Cer et al., 2018).
4 Experiments
4.1 Datasets
We evaluate the proposed model on two represen-
tative tasks: sentiment modiﬁcation on the Y ELP
dataset, and text formality conversion on the F OR-
MALITY dataset. A careful human assessment in
Section 4.1.1 shows that these two datasets are sig-
niﬁcantly more suitable than the other three popu-
lar “style transfer” datasets, namely the political
slant transfer dataset (Prabhumoye et al., 2018;
Tian et al., 2018), gender transfer dataset (Prab-
humoye et al., 2018), and humorous-to-romantic
transfer dataset (Li et al., 2018).
Dataset Style Train Dev Test
YELPPositive 270K 2K 0.5K
Negative 180K 2K 0.5K
FORMALITYFormal 90K 4.6K 2.7K
Informal 96K 5.6K 2.1K
Table 2: Statistics of Y ELPand F ORMALITY datasets.
YELP The commonly used Y ELPreview dataset
for sentiment modiﬁcation (Shen et al., 2017; Li
et al., 2018; Prabhumoye et al., 2018) contains a
positive corpus of reviews rated above three and a
negative corpus of reviews rated below three. This
task requires ﬂipping high polarity sentences such
as “The food is good” and “The food is bad”. We
use the same train/test split as Shen et al. (2017);
Li et al. (2018) (see Table 2).
FORMALITY The F ORMALITY dataset stems
from an aligned set of formal and informal sen-
tences (Rao and Tetreault, 2018). It demands
changes in subtle linguistic features such as “want
to” to “wanna”. We obtained a non-parallel dataset
by shufﬂing the two originally aligned corpora (re-
moving duplicates and sentences that exceed 100
words). Table 2 describes statistics of the resulting
two corpora. The development/test sets are pro-
vided with four human-generated attribute transfer
rewrites for each sentence (i.e. the gold standard).4.1.1 Dataset Quality Assessment
In order to identify the best datasets for evalua-
tion, we asked human judges to assess ﬁve popular
text attribute rewriting datasets, namely Y ELPsen-
timent modiﬁcation dataset, F ORMALITY dataset,
POLITICAL slant transfer dataset (Prabhumoye
et al., 2018; Tian et al., 2018), G ENDER trans-
fer dataset (Prabhumoye et al., 2018), and H U-
MOROUS -to-R OMANTIC transfer dataset (Li et al.,
2018). For each dataset, we randomly extracted
100 sentences (i.e. 50 per style). For each sen-
tence, we asked two native English speakers to an-
notate them with one of three options, i.e. either of
the two attributes or “Cannot Decide”. Based on
the collected annotations, we calculate three met-
rics: (1) undecidable rate , which is the percentage
of “Cannot Decide” answers (we report the aver-
age percentage between the two annotators), (2)
disagreement rate , which is the percentage of dif-
ferent opinions between the two annotators, and
(3)F1 score between the human annotations and
the gold labels in the original dataset (“Cannot De-
cide” answers were not considered).
Dataset Unde. Rate Dis. Rate F1
POLITICAL 49.0 33.0 67.2
GENDER 88.0 14.0 53.6
HUMOROUS 66.0 31.0 79.3
YELP 11.0 16.0 93.2
FORMALITY 0.5 17.0 87.6
Table 3: Comparison of dataset quality. “Unde. Rate”
indicates the undecidable rate, “Dis. Rate” the dis-
agreement rate between annotators.
The scores for each dataset are summarized in
Table 3. A quick comparison shows that Y ELP
and F ORMALITY obtain signiﬁcantly lower unde-
cidable and inter-annotator disagreement rates , in-
dicating that the style of the sentences in these two
datasets are less ambiguous to humans. In addi-
tion, Y ELPand F ORMALITY have much higher F1
score than all the other datasets, which conﬁrms
the correctness of the source-target attribute split.
This comparison points out that the three
datasets, including political slant, gender and
romantic-to-humorous caption datasets, are am-
biguous and noisy, therefore not only adding com-
plexity to the task and its evaluation (because even
human annotators struggle to identify the correct
style), but also leading to models that are not use-
ful in real practice.
YELP FORMALITY
Content Grammar Attribute Success (%) Content Grammar Attribute Success (%)
CA 2.30±1.45 2.65±1.32 2.77±1.69 5.5 2.45±1.31 2.67±1.57 1.97±1.26 0.5
MD 2.12±1.52 2.38±1.49 2.25±1.54 2.0 1.79±1.17 2.37±1.50 2.42±1.56 0.0
DAR 2.93±1.45 2.95±1.51 3.09±1.65 14.5 2.89±1.39 3.02±1.59 2.95±1.56 7.0
Ours 3.07±1.49 4.32±1.07 3.43±1.65 21.0 3.60±1.55 4.42±1.08 3.11±1.58 29.0
Human 4.66±0.77 4.33±1.07 4.22±1.29 63.5 4.62±0.82 4.62±0.94 3.91±1.57 59.5
Table 4: The mean and standard deviation of human ratings on Content preservation, Grammaticality, and at-
tribute correctness on both datasets for different systems: CROSS ALIGNMENT (CA), MULTI DECODER (MD),
DELETE ANDRETRIEVE (DAR), Ours , and Human reference. Ratings are on a 1 to 5 scale.
4.2 Evaluation Strategies
Human Evaluation Following Li et al. (2018);
Lample et al. (2019), we asked human judges to
evaluate outputs of different models in terms of
content preservation, grammaticality and attribute
change correctness on a Likert scale from 1 to 5.
We randomly selected 100 sentences from each
corpus (100 positive and 100 negative sentences
from Y ELP; 100 formal and 100 informal sen-
tences from F ORMALITY ). Each of the twelve
human judges passed a test batch prequaliﬁcation
before they could start evaluating, and we veriﬁed
they spent a reasonable amount of time in the task.
Automatic Evaluation In addition to the hu-
man evaluation, we also programatically gauge
rewriting quality via automated methods, follow-
ing the practice in (Li et al., 2018; Lample et al.,
2019). Here, content preservation is measured by
the BLEU score between model outputs and mul-
tiple human references for each sentence.2Since
the previous work only introduces single human
reference for Y ELP, we enriched the test set with
four extra human references for each test sentence,
in total 800 rewrites. The diverse human rewrites
ensures a more variety-tolerant measurement of
model outputs. Attribute accuracy is assessed via
the classiﬁcation results of a CNN-based text clas-
siﬁer (Kim, 2014). To automatically gauge ﬂu-
ency, we use pretrained language models (LM) to
get the perplexity of output sentences. The details
of the classiﬁer, LM and the collection of human
references are elaborated in Appendix A.1.
4.3 Baselines
To compare against multiple baselines, we re-
implemented three recently-proposed methods
2We used the script to calculate BLEU after detok-
enization at https://github.com/moses-smt/
mosesdecoder/blob/master/scripts/
generic/multi-bleu-detok.perl.(described in Sections 1 and 2): C ROSS ALIGN -
MENT (CA) from Shen et al. (2017), M ULTI DE-
CODER (MD) from Fu et al. (2018), and D ELETE -
ANDRETRIEVE (DAR) from Li et al. (2018).
4.4 Experimental settings
For the translation process, we used an off-the-
shelf Seq2Seq model, a 2-layer LSTM encoder-
decoder with 1024 hidden dimensions and atten-
tion. We focused on the novelty of the pro-
posed iterative framework, so a standard and clas-
sic Seq2Seq model is used. More experimental de-
tails are described in Appendix A.2.
5 Results
5.1 Human Evaluation
In terms of human evaluation, the proposed ap-
proach shows signiﬁcant gains over all baselines
in terms of attribute correctness, content preser-
vation, grammaticality, and success rate as shown
in Table 4 (p < 0.05using bootstrap resampling
(Koehn, 2004)).
The largest improvement is in grammaticality,
where we achieve an average of 4.32on Y ELP
and4.42out of 5.0on the F ORMALITY dataset.
These scores are close to those of human ref-
erences, prevalently outperforming the baselines.
On attribute correctness, the model scores 3.43on
YELP and3.11on F ORMALITY dataset, exceed-
ing the previous best methods by 0.33and0.16re-
spectively. Moreover, in content preservation, we
show an improvement of 0.14and0.72over the
previous state-of-the-art DeleteAndRetrieve model
on the two datasets. Finally, we follow the prac-
tice of Li et al. (2018) to evaluate the Success Rate
— an aggregate of the three previous metrics, in
which a sample is successful only if it is rated 4or
5on all the three metrics. Our model demonstrates
an improvement of 6.5%in success rate over the
previous best method.
From Positive to Negative (Y ELP)
Input Thank you ladies for being awesome !
CA Thank you charge up for this food business.
MD Thank you for it $ poor company.
DAR Thank you ladies for being didn’t .
Ours Thank you for wasting my time you idiots.
Human Thank you ladies for being the worst !
From Informal to Formal (F ORMALITY )
Input i tried to like him, bu i just can’t .
CA I was to him, but I just like I can’t .
MD itried to him like, but I just should too .
DAR I do not know if you like him but I do not know he can’t .
Ours I tried to like him, but I cannot.
Human I attempted to like him, however , I am unable .
Table 5: Example outputs of different systems.
YELP FORMALITY
Acc. BLEU PPL Acc. BLEU PPL
CA 73.40 17.54 46.92 46.41 12.65 53.89
MD 66.80 17.34 69.51 66.46 7.46 109.41
DAR 88.00 31.07 79.70 78.87 21.29 63.61
Ours 95.90 22.46 14.89 72.07 38.16 32.63
Table 6: Automatic evaluation results on both datasets.
Acc. is the style/attribute classiﬁcation accuracy (%).
To further inspect performance of different
methods, we show some typical outputs in Ta-
ble 5. First, the output of our model is
nearly as grammatical as the human-written sen-
tence, compared with the loss of sentence struc-
ture in C ROSS ALIGNMENT (e.g. “I tried to him
like”) and D ELETE ANDRETRIEVE (e.g. “for be-
ing didn’t”). Second, in terms of attribute correct-
ness, D ELETE ANDRETRIEVE suffers from fail-
ure to convert the attribute-bearing words (as
it wrongly converts the word “awesome” to
“didn’t”), and C ROSS ALIGNMENT is prone to
missing attribute words. Third, although our
method enforces content alignment, the preser-
vation of content is an inevitable challenge for
the previous approaches such as M ULTI DECODER
and D ELETE ANDRETRIEVE .
5.2 Automatic Evaluation
We also assess each method’s performance via the
automatic measures of attribute-accuracy, BLEU,
and perplexity (Table 6). A highlight is its per-
plexity score, outperforming the previous meth-
ods by a large margin. This advantage owes to the
fact that the Seq2Seq model is trained on pseudo-
pairs similar to real samples, which can guaran-
tee the translation quality. However, a tradeoff
between the other two aspects, attribute accuracyand BLEU, can be clearly seen on both Y ELP
and F ORMALITY datasets. This is common when
tuning all models, as targeting at a higher BLEU
score will result in a lower attribute correctness
score (Shen et al., 2017; Fu et al., 2018; Li et al.,
2018). Similarly, in iterations of our model, the
BLEU score gradually increases while the accu-
racy decreases. Therefore, the reported outputs
are balanced based on all three aspects. An analy-
sis on the limitations of automatic evaluation is in
Appendix A.3.
6 Performance Analysis
Here, we analyze the performance of our model
with regard to various aspects in order to under-
stand what factors underlie its success.
6.1 Initialization of Pseudo-Parallel Corpus
The proposed model starts from the construction
of an initial pseudo-parallel corpus by our match-
ing step. Note that this initial pairing is practicable
in most domains. For example, Yelp reviews natu-
rally have positive and negative comments on the
same food; different translations of the same book
also shares the same content; different news agen-
cies report with different tones on the same events.
After construction, there are three properties of
this pseudo-parallel corpus: First, it is a subset of
the original corpus. Second, all retrieved target
sentences contain the desired attribute information
and are of perfect grammaticality. This property
is retained throughout our iterations and is key to
the high attribute transfer accuracy and ﬂuent lan-
guage generation capabilities of our model. Third,
the sentence pairs are still imperfect in terms of
content preservation, often similar in meaning but
with certain content words altered. This is reme-
died by subsequent reﬁnement steps.
Although our model needs the matched pseudo-
parallel corpus as a starting point, it has high
tolerance to recover from occasional low-quality
matches. To demonstrate this, we randomly
picked 100 sentences and their initial pseudo-pairs
from both source and target corpus, and asked hu-
man judges to rate them. For each sentence pair,
three judges decided whether the sentence pair
forms a good rewrite, a bad rewrite, or an am-
biguous one. We mark the pseudo-pair as either
good or bad if at least two annotators agree on
such a judgment. The percentage of bad rewrites is
38.2%and48.2%on Y ELPand F ORMALITY , re-
spectively. This indicates that subsequent iterative
reﬁnement indeed allows for 30−50% low-quality
pairs in the initial pseudo-parallel corpus.
6.2 Effective Denoising Translation
One of the most important gains from the iterative
translation is to encourage more content preserva-
tion. We illustrate the effectiveness of translation
by using automatic evaluation to gauge how the
bad matches in the initial pseudo-parallel corpus
change immediately after the ﬁrst translation step.
We ﬁnd that after the ﬁrst translation, the BLEU
score of these bad matches shows a clear improve-
ment, increasing from 9.40 to 13.18 on Y ELP, and
5.44 to 28.25 on F ORMALITY . This shows that
the translation model recognizes the noise in the
ﬁrst matching process and generates more proper
transfer candidates, providing a good foundation
for subsequent iterative reﬁnement. An illustrative
example of this reﬁnement can be seen in the sen-
tence (A) in Figure 1, where a bad match of “Worst
burrito ever ever” was denoised and replaced with
“Worst pizza ever ever” after the ﬁrst translation.
6.3 Iterative Reﬁnement
The essential increase of content preservation
owes to the iterative reﬁnement step. This pro-
cess reduces erroneous alignments in the pseudo-
parallel corpus by updating each existing pseudo-
pair with newly generated translation-pair. Thanks
to the denoising effect of the translation model,
this updating could improve the quality of pseudo-
pairs. Furthermore, we use WMD to measure the
content shift between sentences in the pseudo-pair
and the translation-pair, and accept the update only
when the translation-pair possesses smaller con-
tent shift so as to avoid worsening updates. Asthe iteration goes on, the accuracy and perplexity
stay high from the beginning, but as in Figure 2,
the BLEU score keeps increasing. This shows
that the new iteration outputs retains more content
from the original sentences. More importantly,
this reﬁnement can prevent the model from totally
relying on the ﬁne-quality matching pairs, which
contributes to the high tolerance capability on the
matching quality as discussed in Section 6.1.
0 1 2 3 4 5010203040
IterationBLEU
plot 10 1 2 3 4 5
010203040
YELP
FORM .
Figure 2: BLEU score evolution across training itera-
tions on Y ELPand F ORMALITY datasets.
The pseudo-parallel corpus in Iteration t≥0is
composed of matched pairs from the original cor-
pora and translated pairs from the trained trans-
lation model. Both of them contribute to the
good performance of our model. For further in-
vestigation, we look into the pseudo-parallel cor-
pusˆXandˆY(4)in the ﬁnal training iteration for
the F ORMALITY dataset. Since this dataset has
ground-truth parallel pairs, we are able to calcu-
late how many pseudo-parallel pairs are the same
as the ground-truth parallel pairs, and the percent-
age turns out to be around 52%, which is actually
not high. This reveals that the reﬁnement step can
provide the model with better pseudo-pairs even
than original gold pairs and our model can still
stand out without ﬁne-quality matches.
7 Conclusion
In this work, we proposed a Seq2Seq paradigm
for text attribute transfer applications, suggesting
a simple but strong method for overcoming lack of
parallel data. We construct a pseudo-parallel cor-
pus by iteratively matching and updating in a way
that increasingly reﬁnes the ﬁnal transfer function.
Our framework can employ any Seq2Seq model
and outperforms previous methods under all mea-
sured criteria (content preservation, ﬂuency, and
attribute correctness) in both human and automatic
evaluation. The simplicity and ﬂexibility of our
approach can be useful in applications that require
intricate edits or complete sentence rewrites.
Acknowledgments
We thank Professor Peter Szolovits for support-
ing and funding the work, and Prof Benjamin
Kao for advice on evaluation metrics. We also
appreciate Tianxiao Shen, Jiang Guo, Professor
Regina Barzilay, and MIT Natural Language Pro-
cessing Group for insightful discussions and sup-
port. We thank Adam Fisch, Benson Chen, Jin-
gran Zhou, Wui Wang Lui, Shuailin Yuan, Zhutian
Yang, Yilun Du and many other helpers for human
evaluation.
References
Martin Arjovsky and L ´eon Bottou. 2017. Towards
principled methods for training generative adversar-
ial networks. arXiv preprint arXiv:1701.04862 .
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2017. Unsupervised neural ma-
chine translation. arXiv preprint arXiv:1710.11041 .
Miguel Ballesteros, Bernd Bohnet, Simon Mille, and
Leo Wanner. 2015. Data-driven sentence generation
with non-isomorphic trees. In Proceedings of the
2015 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies .
Konstantinos Bousmalis, Nathan Silberman, David
Dohan, Dumitru Erhan, and Dilip Krishnan. 2017.
Unsupervised pixel-level domain adaptation with
generative adversarial networks. In The IEEE Con-
ference on Computer Vision and Pattern Recognition
(CVPR) .
Georgios-Ioannis Brokos, Prodromos Malakasiotis,
and Ion Androutsopoulos. 2016. Using centroids
of word embeddings and word mover’s distance for
biomedical document retrieval in question answer-
ing. arXiv preprint arXiv:1608.03905 .
Julian Brooke, Tong Wang, and Graeme Hirst. 2010.
Automatic acquisition of lexical formality. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics , pages 90–98. Associa-
tion for Computational Linguistics.
Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,
Nicole Limtiaco, Rhomni St John, Noah Constant,
Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,
et al. 2018. Universal sentence encoder. arXiv
preprint arXiv:1803.11175 .Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ıc
Barrault, and Antoine Bordes. 2017a. Supervised
learning of universal sentence representations from
natural language inference data. In EMNLP .
Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Herv ´e J´egou.
2017b. Word translation without parallel data.
arXiv preprint arXiv:1710.04087 .
Giuseppe Di Fabbrizio, Amanda J Stent, and Srini-
vas Bangalore. 2008. Trainable speaker-based re-
ferring expression generation. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning , pages 151–158.
Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan
Zhao, and Rui Yan. 2018. Style transfer in text: Ex-
ploration and evaluation. In AAAI .
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Compu-
tational Linguistics , pages 414–420. Association for
Computational Linguistics.
Albert Gatt and Emiel Krahmer. 2018. Survey of the
state of the art in natural language generation: Core
tasks, applications and evaluation. Journal of Artiﬁ-
cial Intelligence Research , 61:65–170.
Albert Gatt, Francois Portet, Ehud Reiter, Jim Hunter,
Saad Mahamood, Wendy Moncur, and Somayajulu
Sripada. 2009. From data to text in the neonatal in-
tensive care unit: Using nlg technology for decision
support and information management. Ai Commu-
nications , 22(3):153–186.
Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P Xing. 2017. Toward con-
trolled generation of text. In International Confer-
ence on Machine Learning , pages 1587–1596.
Harsh Jhamtani, Varun Gangal, Eduard H. Hovy, and
Eric Nyberg. 2017. Shakespearizing modern lan-
guage using copy-enriched sequence-to-sequence
models. CoRR , abs/1707.01161.
Patrick Juola and Darren Vescovi. 2011. Analyzing
stylometric approaches to author obfuscation. In
IFIP Int. Conf. Digital Forensics .
Jad Kabbara and Jackie Chi Kit Cheung. 2016. Stylis-
tic transfer in natural language generation systems
using recurrent neural networks. pages 43–47.
Gary Kacmarcik and Michael Gamon. 2006. Ob-
fuscating document stylometry to preserve author
anonymity. In ACL.
Sun Kim, Nicolas Fiorini, W John Wilbur, and Zhiy-
ong Lu. 2017. Bridging the gap: Incorporating a
semantic similarity measure for effectively mapping
pubmed queries to documents. Journal of Biomedi-
cal Informatics , 75:122–127.
Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In EMNLP .
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In Ad-
vances in Neural Information Processing Systems ,
pages 3294–3302.
Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander Rush. 2017. OpenNMT:
Open-source toolkit for neural machine translation.
InProceedings of ACL, System Demonstrations ,
pages 67–72.
Philipp Koehn. 2004. Statistical signiﬁcance tests for
machine translation evaluation. In EMNLP .
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. Computational Linguistics , 38(1):173–218.
Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian
Weinberger. 2015. From word embeddings to docu-
ment distances. In International Conference on Ma-
chine Learning , pages 957–966.
Guillaume Lample, Ludovic Denoyer, and
Marc’Aurelio Ranzato. 2017. Unsupervised
machine translation using monolingual corpora
only. arXiv preprint arXiv:1711.00043 .
Guillaume Lample, Sandeep Subramanian, Eric Smith,
Ludovic Denoyer, Marc’Aurelio Ranzato, and Y-
Lan Boureau. 2019. Multiple-attribute text rewrit-
ing. In International Conference on Learning Rep-
resentations .
Juncen Li, Robin Jia, He He, and Percy S. Liang. 2018.
Delete, retrieve, generate: A simple approach to sen-
timent and style transfer. In NAACL-HLT .
Lajanugen Logeswaran, Honglak Lee, and Samy Ben-
gio. 2018. Content preserving text generation with
attribute controls. In Advances in Neural Informa-
tion Processing Systems .
Thang Luong, Hieu Quang Pham, and Christopher D.
Manning. 2015. Effective approaches to attention-
based neural machine translation. In EMNLP .
Franc ¸ois Mairesse and Marilyn A Walker. 2011. Con-
trolling user perceptions of linguistic style: Train-
able generation of personality traits. Computational
Linguistics , 37(3):455–488.
Igor Melnyk, Cicero Nogueira dos Santos, Kahini
Wadhawan, Inkit Padhi, and Abhishek Kumar. 2017.
Improved neural text attribute transfer with non-
parallel data. arXiv preprint arXiv:1711.09395 .
Jonas Mueller, David Gifford, and Tommi Jaakkola.
2017. Sequence to better sequence: continuous re-
vision of combinatorial structures. In International
Conference on Machine Learning , pages 2536–
2544.Dragos Stefan Munteanu, Alexander Fraser, and Daniel
Marcu. 2004. Improved machine translation perfor-
mance via parallel sentence extraction from compa-
rable corpora. In HLT-NAACL .
Tong Niu and Mohit Bansal. 2018. Polite dialogue gen-
eration without parallel data. Transactions of the As-
sociation of Computational Linguistics , 6:373–389.
Shereen Oraby, Lena Reed, Shubhangi Tandon,
S. SharathT., Stephanie M. Lukin, and Marilyn A.
Walker. 2018. Controlling personality-based stylis-
tic variation with neural natural language generators.
InSIGDIAL Conference .
Ellie Pavlick and Joel R. Tetreault. 2016. An empir-
ical analysis of formality in online communication.
Transactions of the Association for Computational
Linguistics , 4:61–74.
Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP , pages 1532–1543.
Christian S Perone, Roberto Silveira, and Thomas S
Paula. 2018. Evaluation of sentence embeddings
in downstream and linguistic probing tasks. arXiv
preprint arXiv:1806.06259 .
Matthew E. Peters, Mark Neumann, Mohit Iyyer,
Matt Gardner, Christopher Clark, Kenton Lee, and
Luke S. Zettlemoyer. 2018. Deep contextualized
word representations. In NAACL-HLT .
Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan R.
Salakhutdinov, and Alan W. Black. 2018. Style
transfer through back-translation. In ACL.
Sudha Rao and Joel Tetreault. 2018. Dear sir or
madam, may i introduce the yafc corpus: Corpus,
benchmarks and metrics for formality style transfer.
arXiv preprint arXiv:1803.06535 .
Tim Salimans, Ian Goodfellow, Wojciech Zaremba,
Vicki Cheung, Alec Radford, and Xi Chen. 2016.
Improved techniques for training gans. In Advances
in Neural Information Processing Systems , pages
2234–2242.
C´ıcero Nogueira dos Santos, Igor Melnyk, and Inkit
Padhi. 2018. Fighting offensive language on social
media with unsupervised text style transfer. In ACL.
Fadi Abu Sheikha and Diana Inkpen. 2011. Generation
of formal and informal sentences. In Proceedings of
the 13th European Workshop on Natural Language
Generation , pages 187–193. Association for Com-
putational Linguistics.
Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2017. Style transfer from non-parallel text
by cross-alignment. In Advances in Neural Informa-
tion Processing Systems , pages 6833–6844.
Jason R Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics , pages
403–411. Association for Computational Linguis-
tics.
Youzhi Tian, Zhiting Hu, and Zhou Yu. 2018. Struc-
tured content preservation for unsupervised text
style transfer. arXiv preprint arXiv:1810.06526 .
Zhirui Zhang, Shuo Ren, Shujie Liu, Jianyong Wang,
Peng Chen, Mu Li, Ming Zhou, and Enhong Chen.
2018. Style transfer as unsupervised machine trans-
lation. arXiv preprint arXiv:1808.07894 ."
2002.05259,D:\Database\arxiv\papers\2002.05259.pdf,"How does the use of a reward system based on complexity, rather than traditional game scores, affect the learning process of the agent and the generation of levels in the Generative Playing Networks framework?","This reward system encourages the agent to learn a more general policy, as it is not incentivized to exploit specific game mechanics or score-based rewards. This, in turn, pushes the generator to create more challenging and diverse levels that test the agent's general problem-solving abilities.","Algorithm 1 Generative Playing Networks (Optional components
in blue)
Input: Differentiable policy πθ(a|S), utility function
Qω(a|S), encoderEψ(S), and state generator Gφ(z).
Algorithm parameters: απ: 2.5e−4,αQ: 2.5e−5,
αR: 5e−5,αG:−4,m: 128 ,policyupdates : 1m,
generator updates: 10, diversity updates: 90, pre-training
steps: 20m
Extensions: update model every n steps (n=5), train with
small entropy, train with 16 parallel workers, 30% of elite
envs kept, human envs sampled 50% of episodes
Initialize weights and pre-train agent
repeat
z∼minibatchmfrompz(z)
S←Gφ(z)
Create new environments from generated states
Keep topklevels from previous loop {see III-A3}
fornumber of policy updates do
Select environment
S←initialize environment
H←Eψ(S)
repeat
A∼πθ(·|H)
Take action A, observeR,S′{see equation 5}
H′←Eψ(S′)
V′←πθ(·|H′)·Qω(·|H′){IfS′is terminal, 0}
δ←R+V′−Qω(A|H)
V←πθ(·|H)·Qω(·|H) +πθ(A|H)·δ
Adv←V′−V{IfS′is terminal, R−V}
ω,ψ←ω,ψ+αQδ2∇Qω(A|Eψ(S))
θ,ψ←θ,ψ+απAdv∇logπθ(A|Eψ(S))
Rec← −∑
cSclogGφ(Eψ(S))c
φ,ψ←φ,ψ+αRRec∇Gφ(Eψ(S))
H←H′
untilS′is terminal
fornumber of generator updates do
z∼minibatchmfrompz(z)
H←Eψ(Gφ(z))
V←πθ(·|H)·Qω(·|H)
φ←φ+αG(V
m)2∇φV
end for
fornumber of diversity updates do
z∼minibatchmfrompz(z)
H←Eψ(Gφ(z))
D←(Ha−Hb)2
mformrandom pairings of Hi
φ←φ+αGD∇GEψ(Gφ(z))
end for
end for
until User deems environments acceptable
The gradient-based updates can use any standard gradient-
based learning rule. We used Adam in our experiments.a typical TD update, the Q function learns this expected value
through small updates, basically keeping a running average.
This then requires many visits to a state to learn the true
expectation of R(St), and many more to learn the true utility
whenγ= 1. Since we know the policy distribution π, we can
calculate
E[R(Sn)] =∑
a∈Aπ(a|S)·E[R(Sa
n+1)]. (4)
Then the expectation of Sa
t+1can be represented by Q and
the probability of an action is π.
Algorithm 1 outlines the update step as variable V. By
bootstrapping the expected reward from the policy function,
the agent can more easily learn it. In practice, the agent should
also be more likely to learn the optimal path. Policy functions
are almost never 100% certain about an action and thus taking
multiple steps will naturally discount a future reward from the
longer route due to the small uncertainty the policy holds.
While not the main focus of the work, this approach also
seems less sensitive to the scale of environment rewards. There
is no discount factor that needs to be tuned to match the scale
of the rewards to make sure the reward can propagate far
enough back in time.
2) Agent Reward: For the game, laid out in equation 2,
to work, the agent cannot be allowed to learn from the
environment’s built-in rewards (e.g. game score). Instead the
simulator’s rewards should be captured and the agent simply
given a reward for winning or losing. If a level doesn’t compile,
the agent instantly loses. In its purest form, this approach is
environment independent with 1 point for winning and -1 point
for losing. The agent is able to pursue an expected reward of 1
while the generator is able to try and keep the agent’s expected
reward at 0.
R(Sn) =

1 if agent wins
−1if agent loses
0 else(5)
If the environment is too difﬁcult to learn in this restricted
reward setup, it can be modiﬁed to allow more frequent rewards.
One modiﬁcation, that is still domain agnostic, is to scale the
reward for winning and losing by two, and then provide a
reward of1
Nanytime the environment tries to return a reward.
The agent could also be rewarded for surviving longer or
winning quickly. These modiﬁcations maintain that a positive
score is still only achievable by winning and the environment
is able to help the agent with more frequent rewards. The
generator will also be able to target these rewards as well
when learning to make levels with a given expected value. If
necessary, the win and loss rewards could be further scaled
and custom rewards could be used for a given domain. This
is one way that domain knowledge could be added into this
approach.
3) Environment Selection: During the agent update loop,
the agent will play one episode of a environment and then
randomly select a new environment from the mini-batch of
environments generated. The agent will keep re-sampling from
the given minibatch until it has completed a speciﬁed number
of updates and then the generator will be updated and the
minibatch of levels will be replaced with a new set.
To assist with training, two methods can be used with
the standard selection; pretraining and elitism. Pretraining
is the case where you have access to some example data
and can train the networks in a semi-supervised setup. With
pretraining, the agent can only select from a curated set of
well designed environments until the agent has learned to solve
them. This gives the generator a valid direction to learn from
at the beginning, otherwise it is essentially generating random
environments at the beginning.
The second assistance comes from an elitism mechanism
that keeps the most useful environments around. There are
two components to this elitism. The ﬁrst is to assume that
the hand-curated environments are elite and to re-sample them
periodically after the pretraining stage. The second is to persist
environments that the agent is still learning to play.
After the agent has ﬁnished its update loop, the environments
can be ranked according to the average reward the agent
received. The levels are ranked by the distance to zero of
their average score. This is to simulate the generator which is
attempting to output levels with an expected reward of zero
but this is more stable as the levels are known for sure to have
that reward. Once the environments are ranked, the top ones
are kept based on a speciﬁed percentage of environments to
recycle each time.
4) State Reconstruction: To give the generated environments
a more human-designed appearance when trained in a semi-
supervised setting, the generator can also learn directly from
the curated environments. This is done during the agent loop.
When the agent encodes the state, this encoding can be passed
into the generator, as a latent variable, and the generator can
be tasked with reconstructing the input like an autoencoder.
For this reason, we actually deﬁne the agent as three networks:
a policy network, a utility network, and an encoder network
that encodes the state and feeds into the ﬁrst two networks.
This can help constrain the generator to the manifold of
possible levels (according to the agent) that look similar to
human designed levels. This has been shown to be true with
adversarial examples for classiﬁers, that random noise can
cause a network to activate in the same way as the natural
images it is trained on [19]. For this reason, if a classier is
used to train a generator, the most likely outcome is a random
pattern [20]. Training the generator to also be a decoder can
help mitigate this potential problem. In practice this did not
seem to be too big of a problem.
We do not put any constraints on the output of the encoder
network, so the generator network is likely learning two
separate tasks for two different input distributions. This allows
the two tasks to not interfere with each other, while still
affecting each other. One could train the decoder as a variational
autoencoder to have generating and decoding have the same
inputs [21].B. Generator Loop
The generator loop simply consists of updating the generator
to create environments with an expected value of 0. The
generator is updated by sampling a minibatch of mrandom
latent variables and mapping them to environments which
are evaluated by the agent. The generator’s weights are then
updated. This can be repeated a few times but doing too many
updates seems to be detrimental to the generator’s diversity.
1) Diversity Update: To help combat the collapse of
diversity, a proposed extension for the main algorithm is to
update the network a few times explicitly with the goal of
increasing diversity. Here, similarity is calculated as the L2
difference between the encoding values of two environments.
Environments from a minibatch are randomly paired and their
similarities calculated. Since every two environments in a
minibatch are independently generated, one can simply compare
every other sample for similarity. The network is then updated
to maximize the average distance between samples. It was found
that it is most effective to do the diversity update separate from
the primary update target.
IV. E XPERIMENTS
Fig. 2. Zelda from the GVGAI framework is a simple dungeon crawler. To
beat a level, the player has to collect the key and open the door while avoiding
or killing the monsters. Shown above are the ﬁve levels provided from the
game.
To validate the algorithm proposed here, we test Generative
Playing Networks on Zelda from the General Video Game
AI (GVGAI) framework [22]. GVGAI is based on the Video
Game Description Language (VGDL) which is a language for
describing video games, mostly within the family of arcade
games. There is a large corpus of simple games written for
this language with each game deﬁnition also describing the set
of level designs that can exist for it. The framework has been
used extensively as a test-bed for game and level PCG research.
GVGAI also has an OpenAI gym interface for interfacing with
reinforcement learning algorithms [23], [24].
This is a perfect test-bed for Generative Playing Networks,
the agent receives the game-state as a tile representation of the
game, where each tile is a one-hot vector of the object in a given
location. The generator creates this tile representation state
which is directly converted to a level map for the environment
to load. If a level does not have an avatar, key, or door, the
game engine can’t load it and it is presented to the agent as an
instant loss after one frame. We run two experiments here, one
is the unsupervised setting and the other is semi-supervised
using 5 human made levels.
We train Generative Playing Networks on this game and
each level with the objective of getting interesting new levels.
We do one experiment on the core unsupervised approach, this
is the part deﬁned in black in Algorithm 1 or the full arrows
in Figure 1. We do a second experiment to show how this
approach can take advantage of even a few datapoints. For
this experiment we use the additional parts of the algorithm:
the blue lines in Algorithm 1 and dashed arrows in Figure 1.
Below we detail some of the experiment parameters. The rest
of the parameters we used can be found in algorithm 1.
A. Model Architectures
The levels being generated for the experiment are 12 by
16 tiles by a one-hot encoding of 14 possible objects. Since
the space of legal VGDL levels is smaller than the number
of possible game states, we mask out cell types that are not
allowed for a level design e.g. an object that must be spawned
ﬁrst. Another approach we could have taken would have been
to let the level design compiler automatically select a blank
tile when an object is not possible.
Our generator model consists of a fully connected layer that
expects a latent vector, of size 512, sampled from a standard
normal distribution and transforms it to an output of 3 by 4 by
512. This is processed through two convolutional layers, with
kernels of size three, before being passed through a dropout
layer and then transformed with a sub-pixel convolution layer
to double the output size. This convolution and upsampling is
repeated until the expected output size is reached. Each layer
contains 512 ﬁlters and LeakyReLu is used as the activations.
The ﬁnal output is passed through a Softmax activation to
better learn the one-hot encoding. We found that the dropout
layer is another helpful tool in maintaining diverse results.
It was found that the level style was fairly susceptible to the
up-sampling choice for the architecture of the generator. We got
good results with nearest-neighbor upsampling, with transposed
convolutions, and with sub-pixel convolutions [25], [26]. We
chose to do all our experiments with sub-pixel convolutions as
it was our favorite aesthetic.
For the agent, we use a nine layer residual network [27]
followed by a gated recurrent unit. The encoded state is a
vector of size 512. The policy and utility function are each a
single, fully connected layer.
V. R ESULTS
Figure 4 and 3 are a random selection of GVGAI Zelda
levels generated after training on the game for 50 million
frames. After that much training the results tend to collapse
into a single design. For both experiments, all of the generated
Fig. 3. Random Selection of Self-Supervised Generated GVGAI Levels
Trained from Nothing
Fig. 4. Random Selection of Semi-Supervised Generated GVGAI Levels
Trained from 5 Examples
levels are playable/winnable. Only two levels in Figure 4 and
one in Figure 3 have enemies.
Training from no examples, the agent seems to learn
that open spaces are good for ensuring it doesn’t create an
impossible level. Outside of one example, it doesn’t create
obstacles for getting to the door but it does keep the key
away from the agent forcing it to ﬁnd the key for each level.
Looking closely at the semi-supervised results, it’s clear that the
generator learned to emulate the general style of the provided
examples but that it’s outputs are all unique. All of the playable
levels would be considered very simple for a human player. It
is understandable that the levels are mostly open, as the agent
likely struggled to tell the difference between solvable and
unsolvable maze-like environments.
While we were successfully able to train a generator network
to create new GVGAI Zelda levels with no example levels
and no domain knowledge, we were only able to learn simple
levels. This suggests the agent never learned a truly general
policy as it could otherwise quickly beat these levels and the
generator would have to learn to make more challenging levels
to lower the agent’s average. This suggest that the algorithm
would beneﬁt from beneﬁt from reinforcement learning agents
that generalize better across levels.
A. Curriculum
One of the ways in which the generator and agent work
together is that the generator should provide a curriculum
for the agent to learn from easy levels and then improve.
What our results seem to indicate though, is that the generator
ﬁrst generates complex, unsolvable, designs. Then it generates
simple solvable designs and ﬁnally very easy designs. This is
demonstrated from left to right in ﬁgure 5.
Fig. 5. Three sample generated levels chosen from early in the learning
process to late in the process to show how the generator learns to match the
agent’s skill. These were taken from the semi-supervised learning experiment.
Figure 6 shows that the generator has learned to successfully
keep the agent at a reward of 0. Therefore the curriculum we’re
seeing is from the collaborative half of the algorithm. If the
agent is continuously improving, the levels should be getting
more difﬁcult according to ﬁgure 6.
10m 20m 30m 40m 50m
Training Steps0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
Reward
Fig. 6. Real average rewards of generated environments converging toward
their estimated target.
B. Agent Results
We also include here two metrics to show that the agent
is learning what it is intended to learn. Figure 7 shows a
plot of the agent’s estimated level value, only from frame 0,
versus each level’s actual reward at the end of the episode. It’s
clear that the agent’s estimates closely track the agent’s actual
rewards meaning that it’s learning accurately. Though there is
much less variance in the estimates than in the real values, this
either is reﬂecting that the estimates are not affected by the
stochastic environment or that the estimates are only accurate
for the most average levels. Figure 8 instead shows where the
agent’s estimates are failing. In the early stages of training, the
agent is correctly estimating that impossible-levels have a value
0m 10m 20m 30m 40m 50m
Training Steps0.0
0.2
0.4
0.6
0.8
1.0
RewardReal Reward
Initial State EstimateFig. 7. Agent initial state value estimate in red vs real level reward in blue.
of -1, but as it encounters less of them (the red line) it starts
to value them higher (the blue line). This either means that
later impossible-levels are more exotic and unique, or, more
likely, that the agent is forgetting after millions of updates.
0m 10m 20m 30m 40m 50m
Training Steps0.50
0.45
0.40
0.35
0.30
Value EstimatesValue Estimates
Failed Environments
101520253035
Failed Environments (%)
Fig. 8. Percentage of failed, uncompilable, environments vs estimated value
for the failed environments. As there are less failures, the agent does not
accurately value these environments
VI. C ONCLUSION
Generative Playing Networks is a novel framework for
procedural content generation informed by the behavior and
value estimates of a learning agent. The method requires no
data, nor domain knowledge, but is computationally intensive.
The process is fully differentiable, allowing the agent to directly
communicate with the generator about what designs it “wants”.
By playing levels it designs for itself it can learn a distribution
of playable levels at a difﬁculty that match its skill. As
reinforcement learning agents increase in performance, so will
the complexity of the levels that GPN can discover.
In this paper, we have introduced an algorithm with a reward
system based around complexity and an RL update rule that
allows for efﬁcient value estimates. The framework can also
allow for richer and more interesting reward functions based
around the agent’s interactions with the environment.
SOFTWARE AND DATA
The Codebase for all these experiments can be found here:
https://github.com/pbontrager/GenerativePlayingNetworks
ACKNOWLEDGEMENTS
We would like to acknowledge Ahmed Khalifa for helpful
discussions.
REFERENCES
[1]N. Shaker, J. Togelius, and M. J. Nelson, Procedural content generation
in games . Springer, 2016.
[2]S. Risi and J. Togelius, “Procedural content generation: From automati-
cally generating game levels to increasing generality in machine learning,”
arXiv preprint arXiv:1911.13071 , 2019.
[3]K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman, “Quan-
tifying generalization in reinforcement learning,” arXiv preprint
arXiv:1812.02341 , 2018.
[4]C. Zhang, O. Vinyals, R. Munos, and S. Bengio, “A study on overﬁtting
in deep reinforcement learning,” arXiv preprint arXiv:1804.06893 , 2018.
[5]N. Justesen, R. R. Torrado, P. Bontrager, A. Khalifa, J. Togelius, and
S. Risi, “Illuminating generalization in deep reinforcement learning
through procedural level generation,” in AAAI Workshop on Reinforcement
Learning in Games , 2019.
[6]J. Togelius, G. N. Yannakakis, K. O. Stanley, and C. Browne, “Search-
based procedural content generation: A taxonomy and survey,” IEEE
Transactions on Computational Intelligence and AI in Games , vol. 3,
no. 3, pp. 172–186, 2011.
[7]A. Summerville, S. Snodgrass, M. Guzdial, C. Holmg ˚ard, A. K. Hoover,
A. Isaksen, A. Nealen, and J. Togelius, “Procedural content generation
via machine learning (pcgml),” IEEE Transactions on Games , vol. 10,
no. 3, pp. 257–270, 2018.
[8]P. Bontrager, A. Roy, J. Togelius, N. Memon, and A. Ross, “Deepmaster-
prints: Generating masterprints for dictionary attacks via latent variable
evolution,” in 2018 IEEE 9th International Conference on Biometrics
Theory, Applications and Systems (BTAS) . IEEE, 2018, pp. 1–9.
[9]V . V olz, J. Schrum, J. Liu, S. M. Lucas, A. Smith, and S. Risi, “Evolving
mario levels in the latent space of a deep convolutional generative
adversarial network,” in Proceedings of the Genetic and Evolutionary
Computation Conference , 2018, pp. 221–228.
[10] A. Khalifa, P. Bontrager, S. Earle, and J. Togelius, “Pcgrl: Proce-
dural content generation via reinforcement learning,” arXiv preprint
arXiv:2001.09212 , 2020.
[11] J. Togelius and J. Schmidhuber, “An experiment in automatic game
design,” in 2008 IEEE Symposium On Computational Intelligence and
Games . IEEE, 2008, pp. 111–118.
[12] R. Wang, J. Lehman, J. Clune, and K. O. Stanley, “Poet: open-
ended coevolution of environments and their optimized solutions,” in
Proceedings of the Genetic and Evolutionary Computation Conference ,
2019, pp. 142–151.
[13] M. Dennis, N. Jaques, E. Vinitsky, A. Bayen, S. Russell, A. Critch, and
S. Levine, “Emergent complexity and zero-shot transfer via unsupervised
environment design,” arXiv preprint arXiv:2012.02096 , 2020.
[14] L. Gissl ´en, A. Eakins, C. Gordillo, J. Bergdahl, and K. Tollmar,
“Adversarial reinforcement learning for procedural content generation,”
arXiv preprint arXiv:2103.04847 , 2021.
[15] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y . Bengio, “Generative adversarial nets,” in
Advances in neural information processing systems , 2014, pp. 2672–2680.
[16] V . Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep
reinforcement learning,” in International conference on machine learning ,
2016, pp. 1928–1937.
[17] N. Justesen, P. Bontrager, J. Togelius, and S. Risi, “Deep learning for
video game playing,” IEEE Transactions on Games , 2019.
[18] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction .
MIT press, 2018.
[19] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks are easily
fooled: High conﬁdence predictions for unrecognizable images,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition , 2015, pp. 427–436.[20] A. Nguyen, A. Dosovitskiy, J. Yosinski, T. Brox, and J. Clune, “Synthesiz-
ing the preferred inputs for neurons in neural networks via deep generator
networks,” in Advances in neural information processing systems , 2016,
pp. 3387–3395.
[21] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” stat,
vol. 1050, p. 1, 2014.
[22] D. Perez-Liebana, J. Liu, A. Khalifa, R. D. Gaina, J. Togelius, and S. M.
Lucas, “General video game ai: A multitrack framework for evaluating
agents, games, and content generation algorithms,” IEEE Transactions
on Games , vol. 11, no. 3, pp. 195–214, 2019.
[23] R. R. Torrado, P. Bontrager, J. Togelius, J. Liu, and D. Perez-Liebana,
“Deep reinforcement learning for general video game ai,” in 2018 IEEE
Conference on Computational Intelligence and Games (CIG) . IEEE,
2018, pp. 1–8.
[24] G. Brockman, V . Cheung, L. Pettersson, J. Schneider, J. Schul-
man, J. Tang, and W. Zaremba, “Openai gym,” arXiv preprint
arXiv:1606.01540 , 2016.
[25] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation
learning with deep convolutional generative adversarial networks,” arXiv
preprint arXiv:1511.06434 , 2015.
[26] W. Shi, J. Caballero, F. Husz ´ar, J. Totz, A. P. Aitken, R. Bishop,
D. Rueckert, and Z. Wang, “Real-time single image and video super-
resolution using an efﬁcient sub-pixel convolutional neural network,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition , 2016, pp. 1874–1883.
[27] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition , 2016, pp. 770–778."
2007.13993,D:\Database\arxiv\papers\2007.13993.pdf,How does the proposed system handle the challenge of estimating object motion when objects are small and sparsely distributed in the scene?,"The system addresses this challenge by densifying the object point set through sampling every third pixel within the object mask, providing more features for robust motion estimation.","DsandΦsand dynamic{Io,Do,Φo}parts of the scene.
B. Ego-motion Estimation
To achieve fast ego-motion estimation, we construct a
sparse feature set Psin each frame. Since dense optical
ﬂow is available, we use optical ﬂow to match those sparse
features across frames. Those sparse features are only de-
tected on regions of the image other than labeled objects. To
ensure robust estimation, a motion model generation method
is applied for initialisation. Speciﬁcally, the method generates
two models and compares their inlier numbers based on re-
projection error. One model is generated by propagating the
previous camera motion, while the other by computing a new
motion transform using P 3P [17] algorithm with RANSAC.
The motion model that produces most inliers is then selected
for initialisation.
C. Object Motion Tracking
The process of object motion tracking consists of three
steps. The ﬁrst step is to classify all the objects into dynamic
and static objects. Then we associate the dynamic objects
across the two frames. Finally, individual object motion is
estimated.
1) Classifying Dynamic Object: Instance level object seg-
mentation allows us to separate objects from background.
Although the algorithm is capable of estimating the motions
of all the segmented objects, dynamic object identiﬁcation
helps reduce computational cost of the proposed system.
This is done using scene ﬂow estimation as shown in Fig. 2
(center). Speciﬁcally, after obtaining camera motionk−1
k−1Tk,
the scene ﬂow vectork−1fidescribing the motion of a 3D
pointk−1mi
k−1between frame k−1andk, can be calculated
as [18]:
k−1fi=k−1mi
k−1−(k−1
k−1Tkkmi
k) (11)
Unlike optical ﬂow, the scene ﬂow can directly decide
whether the scene structure is moving or not. Ideally, the
magnitude of the scene ﬂow vector should be zero for static
3D point. However, noise or error in depth and matching
complicates the situation in real scenarios.
To robustly tackle this, we compute the scene ﬂow mag-
nitude of all the sampled points on each object, and separate
them into two sets (static and dynamic) via thresholding. An
object is recognised dynamic if the proportion of “dynamic”
points is above a certain level, otherwise static. Table I
demonstrates the performance of classifying dynamic and
static objects using this strategy. Overall, the proposed ap-
proach achieves good accuracy among the tested sequences.
Notice that, in sequence 20, we have relatively high false
negative cases. That is because most cars throughout se-
quence 20, move slowly (nearly static) due to trafﬁc jams.
2) Object Tracking: Instance-level object segmentation
only provides labels frame by frame, therefore objects need
to be tracked between frames and their motion models
propagated over time. To manage this, we propose to use
optical ﬂow to associate point labels in accross frames. For
that, we introduce and maintain a ﬁnite tracking label setTABLE I
PERFORMANCE OF DYNAMIC /STATIC OBJECT CLASSIFICATION OVER
VIRTUAL KITTI DATASET .
Sequence 01 02 06 18 20
Total Detection 1383 150 266 970 2091
Dynamic/Static 117/1266 73/77 257/9 970/0 1494/597
False Positive 3 0 9 0 3
False Negative 6 0 0 57 292
L⊂Nwherel∈L starts froml= 1, when the ﬁrst moving
object appears in the scene. The number of elements in L
increases as more objects are being detected. Static objects
and background are labeled with l= 0.
Ideally, for each detected object in frame k, the labels of
all its points should be uniquely aligned with the labels of
their correspondences in previous frame k−1. However, in
practice this is affected by the noise, image boundary and
occlusions. To overcome this, we assign all the points with
the label that appears most in their correspondences. For a
dynamic object, if the most frequent label in the previous
frame is 0, it means that the object starts to move, appears
in the scene at the boundary, or reappears from occlusion. In
this case, the object is assigned with a new tracking label.
3) Object Motion Estimation: As mentioned before, ob-
jects normally appear in small sizes in the scene, which
makes it hard to get sufﬁcient sparse features to track and
estimate their motions robustly. Therefore we densify the
object point set Povia sampling every 3rdpixel within object
mask in practice. Similar to the ego-motion estimation, an
initial object motion model is generated for initialisation. The
model with most inliers is reﬁned using (10) to get the ﬁnal
object motion and the best point matching.
V. E XPERIMENTS
In this section, experimental results on two public datasets
are demonstrated. For detailed analysis we use virtual KITTI
dataset [19], which provides ground truth of ego/object
poses, depth, optical ﬂow and instance level object seg-
mentation. KITTI tracking dataset [20] is used to show
the applicability of our algorithm in real life scenarios.
We adopt a learning-based method, Mask R-CNN [4], to
generate object segmentation in both datasets. The model
of this method is trained on COCO dataset [21], and it is
directly used without ﬁne-tuning. For dense optical ﬂow, we
use a state-of-the-art method, PWC-Net [6]. The model is
trained on FlyingChairs dataset [22], and then ﬁne-tuned
on Sintel [23] and KITTI training datasets [20]. Feature
detection is done using FAST [24].
We use pose change error to evaluate the estimated SE(3)
motion, i.e., given ground truth motion Xand estimated ˆX,
where X∈SE(3)can be either camera or object motion.
The pose change error is obtained as: E=ˆX−1X. Trans-
lation error Etis computed as the L2norm of translational
component in E. Rotation error ERis measured as the angle
in axis-angle representation of rotation part of E. We also
evaluate object velocity error. According to [25], given an
object motion H, the object velocity vcan be calculated as:
Fig. 4. Average error of rigid motion with regard to noise level of depth (left), and to End-point Error of optical ﬂow (right). Curves represent translation
error that are corresponding to left-Y axis, and bars represent rotation error that are corresponding to right-Y axis.
v=||t−(I−R)c||whereRandtare the rotation and
translation part of the motion of points in global reference
frame.Iis identity matrix and cis centroid of object. Then
error of velocity Evbetween estimated ˆvand ground truth
vcan be represented as: Ev=|ˆv−v|. The optical ﬂow is
evaluated using end-point error (EPE) [26].
TABLE II
AVERAGE OPTICAL FLOW END -POINT ERROR (EPE) OF STATIC
BACKGROUND AND OBJECTS IN S18-F124-134.
Static Obj1 Obj2 Obj3
Object Distance (m) − 7.52 16.52 24.67
Object Area (%) − 6.29 0.73 0.29
EPE X-axis (pix) 1.34 0.35 0.34 0.15
EPE Y-axis (pix) 0.27 0.24 0.22 0.18
A. Virtual KITTI Dataset
This dataset is used to analyse the inﬂuence of the
optical ﬂow and depth accuracy on the estimation of the
ego and object motion. Moving objects appears scatteredly
within a sequence, which makes it hard to perform in-depth
tests using the whole sequence. Therefore, we selected a
representative set that contains multiple moving objects for
analysis. The set is part of the sequence 18and the frame
IDs are between 124-134 (S18-F124-134). It contains 10
frames of the agent car with camera moving forward, and
three observed vehicles. Two of them are moving alongside
in the left lane, with one closer to the camera and the other
farther. The third car is moving upfront and it is furthest
from the camera.
TABLE III
AVERAGE ERROR OF OBJECT MOTIONS OF DIFFERENT SETS .
Motion only Joint
Et(m) ER(deg) Et(m) ER(deg)
S01-F225-235 Ego 0.0117 0.0354 0.0043 0.0310
S01-F410-418 Obj 0.0647 0.2811 0.0470 0.2286
S18-F124-134Ego 0.0367 0.1012 0.0052 0.0315
Obj1 0.0169 0.1016 0.0132 0.0804
Obj2 0.1121 0.2720 0.1008 0.1907
Depth : Ground truth depth is corrupted with zero mean
Gaussian noise, with σfollowing standard depth accuracy
of a stereo camera system expressed as: σ=z2
f·b·△dwhere
zis depth,ffocal length, bbaseline and ∆dthe disparity
accuracy. We set b= 0.5m and control ∆dto get the noise
level of depth. Normally, ∆dvaries from 0.1to0.2for astandard industrial stereo camera. Fig. 4 (left) demonstrates
the average error of rigid motion over all selected frames.
Note that our algorithm is robust to depth noise within
reasonable range. The translation error grows gradually with
the depth error for both camera and objects, but stays in
low range ( Et<0.02m). Rotation error ﬂuctuates slightly
but still in low range ( ER<0.04deg).
S01-F225-235 (ego)S01-F410-418 (obj)S18-F124-134 (ego)S18-F124-134 (obj1)S18-F124-134 (obj2)Avg End-point Error (pix)00.070.140.210.28
InitialOptimized
Fig. 5. Average end-point error between initial and optimized optical ﬂow,
among different tested sets.
OpticalFlow : The ground truth optical ﬂow is cor-
rupted with zero mean Gaussian noise with σdecided
by the end-point error (EPE). Table II demonstrates av-
erage EPE of PWC-Net results for the static and object
points among this sequence. Since the errors among static
background and objects are different, we set ﬁve intervals
in increasing order and use these average errors as the
middle value. For instance, for static points, σyis set to
[0.09 0.18 0.27 0.36 0.45].
As illustrated in Fig. 4 (right) and Table II, camera and
object 1motion errors are relatively low and stable for
different EPEs. However, objects 2and3motion errors
increase reaching nearly 0.09meter in translation and 0.2
degree in rotation and this is because they are far away from
the camera and occupy quite small area of the image ( <1%).
Consequently, the object motion estimation is sensitive to
optical ﬂow error if the objects are not well distributed in the
scene. To avoid unreliable estimation, our system addresses
only objects within 25m, and 0.5%image presence.
Overall Results : Now overall results without ground
truth are demonstrated. Because vKITTI does not provide
stereo images, we can not generate depth map. Instead, we
use ground truth depth map and add noise with ∆d= 0.2.
As the objects in S 18-F124-134 are mainly translating,
we introduce two more sets with obvious rotation. One of
them (S 01-F225-235) contains the agent car (camera) turning
left into the main street. The other (S 01-F410-418) contains
TABLE IV
AVERAGE VELOCITY ERROR OF SEQUENCES WITH MULTIPLE MOVING OBJECTS .
Sequence 00 01 02 03 04 05 06 18 20
Detected Objects van cyclist 5 cars 6 cars wagon suv 20 cars 12 cars 10 cars 18 cars 46 cars
Num. of Tracks 44 90 76 39 44 49 109 57 137 431 489
Avg. Velocity (km/h) 18.92 16.06 14.07 34.29 54.44 52.23 30.12 45.22 32.82 20.95 11.73
Avg. Error Ev(km/h) 3.04 2.01 2.02 5.22 2.70 2.63 5.13 5.52 4.26 1.96 2.18
050100150200250300350400X[m]020406080100120Z[m]04GROUND TRUTHPROPOSEDORB-SLAM2Start Position
-50050100150200X[m]-400-300-200-1000100200Z[m]09
GROUND TRUTHPROPOSEDORB-SLAM2Start Position
-80-60-40-200204060X[m]-1000100200300400500600700800Z[m]20
GROUND TRUTHPROPOSEDORB-SLAM2Start Position
Fig. 6. Top view of camera trajectories of three tested KITTI sequences.
static camera observing one car turning left at the crossroads.
To prove the effectiveness of jointly optimising motion and
optical ﬂow, we set a baseline method that only optimises for
motion (Motion Only) using (5) for camera or (9) for object,
and the improved method that optimises for both motion and
optical ﬂow with prior constraint (Joint) using (10).
As illustrated in Table III, optimising for the optical
ﬂow jointly with the SE(3)motions improve the results,
about 300% for the camera motion, and 10-20% on object
motion. Besides, the corresponding optical ﬂow error after
optimisation is also reduced, see Fig. 5.
B. Real KITTI Dataset
In KITTI tracking dataset, there are 21sequences with
ground truth camera and object poses. For camera motion,
we compute the ego-motion error on all the sequences ( 12
in total) except the ones that the camera is not moving at all.
We also generate results of a state-of-the-art method, ORB-
SLAM2 [27] for comparison. Fig. 6 illustrates the camera
trajectory results on three sequences. Compared with ORB-
SLAM2, our proposed method is able to produce smooth
trajectories that are more consistent with the ground truth,
given the fact that our method conducts only frame-by-frame
tracking, while ORB-SLAM2 integrates more complex mod-
ules, such as local map tracking and local bundle adjustment.
In particular, the result of Seq. 20in Fig. 6 (right) shows
that ORB-SLAM2 obtains bad estimates in ﬁrst half of the
sequence, mainly because this part contains dynamic scenes
of trafﬁc on the highway. Nevertheless, our algorithm is
robust against this case. Table V illustrates average motion
error over all the 12tested sequences. The results prove our
improved performance over ORB-SLAM2.
TABLE V
AVERAGE EGO -MOTION ERROR OVER 12TESTED SEQUENCES .
PROPOSED ORB-SLAM2
Et(m) 0.0642 0.0730
ER(deg) 0.0573 0.0622For object motion, we demonstrate the results of object
velocity error among 9sequences that contains considerable
number of moving objects, since vehicle velocity is important
information for autonomous and safety driving applications.
As demonstrated in Table IV, the number of tracks refers
to how many frames those objects are being tracked. This
indicates our pipeline is able to simultaneously and robustly
track multiple moving objects for long distances. The average
velocity error Evis computed over all the tracks among one
or all objects (see the second row in Table IV). Overall, our
method gets around 2-5km/h error, which is considerably
accurate for the velocity ranging from 11-55km/h.
The computational cost of our algorithm is around 6fps
when run on an i 7 2.6Ghz laptop. The main cost lies in
denser points tracking on multiple objects. This can be
improved by employing parallel implementation to achieve
real-time performance.
VI. C ONCLUSION
In this paper we present a novel framework to simul-
taneously track camera and multiple object motions. The
proposed framework detects moving objects via combining
instance-level object segmentation and scene ﬂow, and tracks
them over frames using optical ﬂow. The SE(3) motions
of the objects, as well as the camera are optimised jointly
with the optical ﬂow in a uniﬁed formulation. We carefully
analyse and test our approach on virtual KITTI dataset,
and demonstrate its effectiveness. Furthermore, we perform
extensively test on the real KITTI dataset. The results show
that our method is able to obtain robust and accurate camera
trajectories in dynamic scene, and track the velocity of
objects with high accuracy. Further work will integrate the
proposed motion estimation within a SLAM framework.
ACKNOWLEDGMENT
This research is supported by the Australian Research Coun-
cil through the Australian Centre of Excellence for Robotic
Vision (CE140100016).
REFERENCES
[1] D. Scaramuzza and F. Fraundorfer, “Visual Odometry [Tutorial],”
IEEE Robotics & Automation Magazine , vol. 18, no. 4, pp. 80–92,
2011.
[2] W. Tan, H. Liu, Z. Dong, G. Zhang, and H. Bao, “Robust Monocular
SLAM in Dynamic Environments,” in Mixed and Augmented Reality
(ISMAR), 2013 IEEE International Symposium on . IEEE, 2013, pp.
209–218.
[3] P. F. Alcantarilla, J. J. Yebes, J. Almaz ´an, and L. M. Bergasa, “On
Combining Visual SLAM and Dense Scene Flow to Increase the Ro-
bustness of Localization and Mapping in Dynamic Environments,” in
Robotics and Automation (ICRA), 2012 IEEE International Conference
on. IEEE, 2012, pp. 1290–1297.
[4] K. He, G. Gkioxari, P. Doll ´ar, and R. Girshick, “Mask R-CNN,”
inProceedings of the IEEE International Conference on Computer
Vision , 2017, pp. 2961–2969.
[5] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox, “FlowNet 2.0: Evolution of Optical Flow Estimation
with Deep Networks,” in IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) , Jul 2017. [Online]. Available:
http://lmb.informatik.uni-freiburg.de//Publications/2017/IMKDB17
[6] D. Sun, X. Yang, M.-Y . Liu, and J. Kautz, “PWC-NET: CNNs
for Optical Flow using Pyramid, Warping, and Cost V olume,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , 2018, pp. 8934–8943.
[7] M. R. U. Saputra, A. Markham, and N. Trigoni, “Visual SLAM and
Structure from Motion in Dynamic Environments: A Survey,” ACM
Computing Surveys (CSUR) , vol. 51, no. 2, p. 37, 2018.
[8] D. Zou and P. Tan, “CoSLAM: Collaborative Visual SLAM in
Dynamic Environments,” IEEE Transactions on Pattern Analysis and
Machine Intelligence , vol. 35, no. 2, pp. 354–366, 2013.
[9] A. Kundu, K. M. Krishna, and C. Jawahar, “Realtime Multibody Visual
SLAM with A Smoothly Moving Monocular Camera,” in Computer
Vision (ICCV), 2011 IEEE International Conference on . IEEE, 2011,
pp. 2080–2087.
[10] N. D. Reddy, I. Abbasnejad, S. Reddy, A. K. Mondal, and V . Devalla,
“Incremental Real-time Multibody vSLAM with Trajectory Optimiza-
tion using Stereo Camera,” in Intelligent Robots and Systems (IROS),
2016 IEEE/RSJ International Conference on . IEEE, 2016, pp. 4505–
4510.
[11] A. Dewan, T. Caselitz, G. D. Tipaldi, and W. Burgard, “Motion-
based Detection and Tracking in 3D Lidar Scans,” in 2016 IEEE
International Conference on Robotics and Automation (ICRA) . IEEE,
2016, pp. 4508–4513.
[12] M. A. Fischler and R. C. Bolles, “Random Sample Consensus: A
Paradigm for Model Fitting with Applications to Image Analysis and
Automated Cartography,” Communications of the ACM , vol. 24, no. 6,
pp. 381–395, 1981.
[13] K. M. Judd, J. D. Gammell, and P. Newman, “Multimotion Visual
Odometry (MVO): Simultaneous Estimation of Camera and Third-party Motions,” in 2018 IEEE/RSJ International Conference on Intel-
ligent Robots and Systems (IROS) . IEEE, 2018, pp. 3949–3956.
[14] J. Zhang and V . Ila, “Multi-frame Motion Segmentation for Dynamic
Scene Modelling,” in The 20th Australasian Conference on Robotics
and Automation (ACRA) . Australian Robotics & Automation Asso-
ciation, 2018.
[15] M. Henein, J. Zhang, R. Mahony, and V . Ila, “Dynamic SLAM: The
Need For Speed,” IEEE International Conference on Robotics and
Automation (ICRA). To appear , 2020.
[16] K. Yamaguchi, D. McAllester, and R. Urtasun, “Efﬁcient Joint
Segmentation, Occlusion Labeling, Stereo and Flow Estimation,” in
European Conference on Computer Vision . Springer, 2014, pp. 756–
771.
[17] T. Ke and S. I. Roumeliotis, “An Efﬁcient Algebraic Solution to the
Perspective-three-point Problem,” in CVPR , 2017.
[18] Z. Lv, K. Kim, A. Troccoli, D. Sun, J. M. Rehg, and J. Kautz,
“Learning Rigidity in Dynamic Scenes with A Moving Camera for 3D
Motion Field Estimation,” in Proceedings of the European Conference
on Computer Vision (ECCV) , 2018, pp. 468–484.
[19] A. Gaidon, Q. Wang, Y . Cabon, and E. Vig, “Virtual Worlds as Proxy
for Multi-Object Tracking Analysis,” in CVPR , 2016.
[20] A. Geiger, P. Lenz, and R. Urtasun, “Are We Ready for Autonomous
Driving? The KITTI Vision Benchmark Suite,” in Conference on
Computer Vision and Pattern Recognition (CVPR) , 2012.
[21] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll ´ar, and C. L. Zitnick, “Microsoft COCO: Common Objects in
Context,” in European Conference on Computer Vision . Springer,
2014, pp. 740–755.
[22] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy,
and T. Brox, “A Large Dataset to Train Convolutional Networks for
Disparity, Optical Flow, and Scene Flow Estimation,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition ,
2016, pp. 4040–4048.
[23] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black, “A Natural-
istic Open Source Movie for Optical Flow Evaluation,” in European
Conference on Computer Vision (ECCV) , ser. Part IV , LNCS 7577, A.
Fitzgibbon et al. (Eds.), Ed. Springer-Verlag, Oct. 2012, pp. 611–625.
[24] E. Rosten and T. Drummond, “Machine Learning for High-speed
Corner Detection,” in European Conference on Computer Vision .
Springer, 2006, pp. 430–443.
[25] G. S. Chirikjian, R. Mahony, S. Ruan, and J. Trumpf, “Pose Changes
from A Different Point of View,” in Proceedings of the ASME Inter-
national Design Engineering Technical Conferences (IDETC) 2017 .
ASME, 2017.
[26] D. Sun, S. Roth, and M. J. Black, “A Quantitative Analysis of Current
Practices in Optical Flow Estimation and the Principles behind Them,”
International Journal of Computer Vision , vol. 106, no. 2, pp. 115–
137, 2014.
[27] R. Mur-Artal and J. D. Tard ´os, “ORB-SLAM2: An Open-source
SLAM System for Monocular, Stereo, and RGB-D Cameras,” IEEE
Transactions on Robotics , vol. 33, no. 5, pp. 1255–1262, 2017."
2401.03158,D:\Database\arxiv\papers\2401.03158.pdf,"Given the paper's focus on short text classification (STC) and its exploration of various methods, what are the specific challenges posed by STC tasks that necessitate the development of specialized approaches like those presented in the paper?","STC tasks present challenges related to limited context and reliance on external knowledge bases, particularly due to the concise and informal nature of short texts. These challenges necessitate the development of specialized approaches that can effectively extract meaning and classify short texts without relying heavily on external resources.","counterparts, a process exemplified by approaches such as
MINILLM[49]andGKD[50].Movingbeyondconventional
methods, some strategies leverage the emergent capabilities
ofLLMs,suchasGPT-3[39],whichexhibitsuniqueabilities
thatcanbeharnessedthroughin-contextlearning,chainsof
thought,andexplicitinstructionfollowing.Techniquessuch
as the CoT method incorporate intermediate reasoning into
prompts, enhancing models under small-sample learning
paradigms, such as Meta-Context Tuning and Multitask-
Context Tuning, as seen with MT-COT[51]. Furthermore,
approaches such as the IF method, as demonstrated by
Lion[52], exploit LLMs’ adaptability of LLMs to gener-
ate challenging instructions that fortify the capabilities of
student models. These innovative methods provide new in-
sights into knowledge distillation and expand the horizons
for model compression, marking a significant stride in the
efficient utilization of LLMs.
3. Method
In section 3.1, we define the STC task and then reiter-
ate its primary challenges. Section 3.2 introduces Quartet
Logic: A Four-step Reasoning Framework, a specialized
CoTmethoddesignedforLLMstoaddresstheSTCtask.Fi-
nally, we detail the CoT-Driven Multi-task learning method
developed to enable smaller models to tackle the STC task
effectively in section 3.3.
3.1. Task Description
Given a short text 𝑋𝑖belonging to 𝐷and a dataset
consisting of N short texts, the STC task aims to classify
𝑋𝑖as relevant label 𝑙from a predefined set of labels 𝐿.
Semantic and syntactic challenges are inherent in STC
tasks owing to their concise and informal nature. State-
of-the-art methods based on GCN are constrained by their
dependenceonanexternalknowledgebase.Toaddressthese
limitations, we propose distinct frameworks for LLMs and
smaller models that target the issues of limited context and
reliance on external knowledge.
3.2. Quartet Logic: A Four-Step Reasoning
Framework
To improve the performance of LLMs in STC tasks,
this study introduces Quartet Logic: A Four-Step Reason-
ing (QLFR) framework, a CoT-prompt-based approach for
multi-stepreasoning.Ratherthanusingconventionalsingle-
step prompting, which usually yields immediate label pre-
dictions without intermediary reasoning stages, we aimed
toelicitinherentknowledgeandabilitiesbeforedetermining
the predicted label.
Our CoT method, called Syntactic and Semantic En-
richment CoT (SSE-CoT), provides a four-step reasoning
approach, as shown in Figure 2. The first step involves
identifying the key concepts within the input text 𝑋𝑖. Next,
the method retrieves the relevant common sense knowledge
𝑆embedded within the LLM. Subsequently, the text was
revised to𝑅to enhance the clarity and context, enrichingits interpretative depth. Finally, the revised text is used as a
query to determine the predicted label, ̂ 𝑦.
We conduct our four-step prompts as follows.
Step 1. Key Concept Identification
In the initial step, a specified template is used to query
the LLM and pose questions, utilizing its proficiency in
identifying relevant concepts.
𝐶1
1[Given the short text 𝑋𝑖], identify key
concepts.
Here,𝐶1
1comprises the context of the first step, and the
followingcontentconstitutesthereasoninginstruction 𝐼1
1for
the first step. This stage was designed to focus the model
on essential content in preparation for the next steps. The
process can be formally expressed as:
𝐾1=𝑓identify (𝐶1
1,𝐼1
1) (1)
where𝑓identifydenotesafunctionthatcapturesthecapability
of the models to extract concepts.
Step 2. Common-sense Knowledge Retrieval
With the fundamental concepts 𝐾1identified in Step 1,
this step involves retrieving the associated common-sense
knowledge from the inherent knowledge base of the model
using the following template:
𝐶1
2[𝐶1
1,𝐾1], retrieve related common knowledge.
Inthisphase,concatenate 𝐶1
1and𝐾1toformthecontext,
and use the following content 𝐼1
2as the retrieval directive:
𝑆=𝑓retrieve (𝐶1
2,𝐼1
2) (2)
Here,𝑓retrieveis a function enabling the model to recall
pertinentinformationfromaninternalknowledgerepository.
Knowledge retrieval mitigates the semantic gap in STC
tasks because of its brevity and facilitates the provision of
contextually rich and comprehensive responses.
Step 3. Text Rewriting
Followingtheretrievalofpertinentcommon-senseknowl-
edge,thisstepentailsassimilating 𝑆intoacohesiveandpol-
ishedshorttext.Thecontextisformedbyconcatenating 𝐶1
2
and𝑆,where𝐼1
3servesasthemodificationdirective.
𝐶1
3[𝐶1
2,𝑆]. Refine and enhance the language to
guarantee precision, fluidity, and legibility,
whilst preserving the accuracy and wholeness of
the integrated information.
Integration, represented by function 𝑔, is essential for
converting raw data into an easily understandable format. It
overcomesthesyntacticconstraintsofshorttexts,producing
Page 4 of 15
Given the short text ""Del Potro says make French Open"", identify key concepts.Del Potro is a professional tennis player known for his powerful baseline play and has achieved success in major tennis tournaments. The French Open, also known as Roland Garros, is one of the four Grand Slam tennis tournaments and is held annually in Paris, France. It is the premier clay court tennis championship event in the world.Given the original text “Del Potro says make French Open."" The key concepts from the short text “Del Potro says make French Open” are: Del Potro and French Open. Del Potro is a professional tennis player known for his powerful baseline play and has achieved success in major tennis tournaments. The French Open, also known as Roland Garros, is one of the four Grand Slam tennis tournaments and is held annually in Paris, France. It is the premier clay court tennis championship event in the world. Refine and enhance the language to guarantee precision, fluidity, and legibility, whilst preserving the accuracy and wholeness of the integrated information.Del Potro, the esteemed professional tennis player, has made statements regarding the French Open, one of the four major Grand Slam tournaments held annually in Paris on its iconic clay courts.The key concepts from the short text “Del Potro says make French Open” are: Del Potro and French Open.Given the short text ""Del Potro says make French Open ”. The key concepts from the short text “Del Potro says make French Open” are: Del Potro and French Open, retrieve related common knowledge.
Given the short text “Del Potro, the esteemed professional tennis player, has made statements regarding the French Open, one of the four major Grand Slam tournaments held annually in Paris on its iconic clay courts.”, classify it into one of the categories. The categories are ‘health’, ‘sport’, ‘entertainment’, ‘business’, ‘sci_tech’, ‘U.S.’ and  ‘world’.The category is sport.
DelPotro says make French Open𝑋:
𝑌:sportSSE-CoT1!""step2#$step3%$step4""&stepConcatenationConcatenationConcatenationFigure 2: This diagram presents the Syntactic and Semantic Enrichment CoT (SSE-CoT) , as applied to the short text ‘ Del
Potro says make French Open ’. It begins by identifying key concepts, ‘ Del Potro ’ and the ‘ French Open ’, then combines them to
contextualize ‘ Del Potro ’ as a tennis player and the ‘ French Open ’ as a major tournament. The third step refines this information
for accuracy and integration. Finally, the process classifies the outcome under ‘ sport’. The framework offers a novel solution that
effectively addresses STC task challenges.
a structured output 𝑅that simplifies comprehension and
subsequent categorization by LLM.
𝑅=𝑔(𝐶1
3,𝐼1
3) (3)
Step 4. Short Text Classification
Given the short text 𝑅. classify it into
one of the categories. The categories are
‘health’, ‘sport’, ‘entertainment’, ‘business’,
‘sci_tech’, ‘U.S.’ and ‘world’.
Afterintegratingcommon-senseknowledgeandrefining
the short text 𝑋𝑖as𝑅, we prompted the LLM with instruc-
tion𝐼1
4to generate the final predicted label.
̂ 𝑦𝑖= argmax𝑝(𝑦|𝑋′,𝐼4) (4)
The label with the highest output probability is designated
as the predicted label ̂ 𝑦𝑖.3.3. CoT-Driven Multi-task Learning Method
We propose a novel approach, the CoT-Driven Multi-
Task learning (QLFR-CML) framework, specifically de-
signed for smaller models to address STC tasks. The ar-
chitecture of this framework is illustrated in Figure 3. Our
framework consists of two stages, as detailed below.
In the initial stage, we employed two specialized CoT
prompt processes to generate a rationale from an LLM.
TheSyntacticandSemanticEnrichmentCoT(SSE-CoT)en-
hancestheclarityandcoherenceofshorttextsbyaddressing
their syntactic and semantic shortcomings. Simultaneously,
Domain Augmentation CoT (DA-CoT) broadens the textual
context by integrating domain-specific knowledge.
In the second stage, knowledge transfer occurs from
the LLM to a smaller model through a multi-task learning
strategy. The smaller model was trained to predict labels
andaccuratelygeneratecommon-senseanddomain-specific
rationales. This integrated training approach improves the
reasoning capabilities of the model and strengthens its abil-
ity to classify short texts with precision and depth.
We introduce rationale generation in section3.3.1, fol-
lowed by an overview of Explicit Category Context Aug-
mentation in section 3.3.2, providing a direct and efficient
prompt. Finally, we present our multi-task learning strategy
in section3.3.3.
Page 5 of 15
Domain Augmentation CoT (DA-CoT)multi-task fine-tuneworldsporthealth……supervised-signalSmaller ModelSyntactic and Semantic Enrichment CoT (SSE-CoT)1. ‘syria tightens security following protests’2. Del Potro says make French Open3. ‘study: 17 strokes occur sleeping’……
LLMtraining data
First StageSecond Stage1. ‘syria tightens security following protests’2. Del Potro says make French Open3. ‘study: 17 strokes occur sleeping’……training data
…………SSE-CoTreasoning……………………thinkingDA-CoTreasoninglabelrationales…………transferFigure 3: Overview of the QLFR-CML method. In the first stage, the framework employs SSE-CoT and DA-CoT to prompt
LLM with training data for rationale generation. In the second stage, the generated rationales guide the training of a smaller,
specialized model. This stage involves multi-task fine-tuning that incorporates a supervised signal, which includes a label and two
distinct rationales derived from SSE-CoT and DA-CoT reasoning.
3.3.1. Rationale generation
We introduce two distinct CoTs to generate rationales
to enhance knowledge transfer in smaller models. First,
SSE-CoT, introduced in Section ??, addresses common-
senseknowledge,encompassingbroad,universallyacknowl-
edgedinformation,suchasfundamentallifefactsandrecog-
nized concepts. Second, Domain Augmentation CoT (DA-
CoT)focusesondomain-specificknowledge,whichincludes
detailed field-specific information, specialized jargon, and
unique concepts. The DA-CoT uses a two-step reasoning
process,asshowninFigure4.Wetaketheoutput 𝑅fromthe
thirdstepoftheSSE-CoTandtheoutput 𝑂fromthesecond
step of the DA-CoT as rationales.
We conduct the two-step DA-CoT prompts as follows.
Step 1. Key Concept Identification
In the first step, we used a designated template to query
the LLM, capitalizing on its capacity to pinpoint pertinent
concepts. Unlike the first step of the SSE-CoT, the DA-CoT
incorporatesdomain-specificcuewords.Forexample,inthe
newsdomainillustratedinFigure.4,itisessentialtoinclude
critical entities, actions, and events. Details of the other
domains are provided in Appendix A. 𝐶2
1comprises the
contextofthefirststep,andthefollowingcontentconstitutes
the reasoning instruction 𝐼2
1for the first step.
𝐶2
1[Given the short text 𝑋𝑖], identify the key
components, consider the main entities, actions,
and events described.𝐾2=𝑓identify (𝐶2
1,𝐼2
1) (5)
Here,𝐾2are the key snippet concepts extracted from 𝑋𝑖
using𝑓identifyrepresentingthemodel’scapabilitytoidentify
andunderstandkeysnippetconceptsandterminologiesfrom
the text.
Step 2. Domain Knowledge Retrieval
After establishing foundational concepts in the initial
phase, this step prompts the LLM to apply domain-specific
terminology and deeper analytical perspectives to its out-
puts.
𝐶2
2[𝐶2
1,𝐾2]. Provide a summary of the identified
components, including their interrelations and
the overall significance within the context of
the text.
𝑂=𝑓enrich(𝐶2
2,𝐼2
2) (6)
where𝑂represents the enriched knowledge retrieved or
generated by the function 𝑓enrich, integrating the identified
concepts with in-depth, domain-specific information.
3.3.2. Explicit Category Context Augmentation
In contrast to encoder-only models such as BERT, the
smaller model utilized in our study adopts an encoder-
decoder architecture. This generative approach requires the
promptenhancementofthecomprehensionandresponseof
Page 6 of 15
Identify the key components in the following text: Del Potro says make French Open. Consider the main entities, actions, and events described.Del Potro's mention suggests he may be making a statement about participating in or commenting on the French Open, indicating its relevance to him and potentially the tennis community. The French Open is significant as one of the four major annual tennis championships, known for its clay courts.The key components identified are ""Del Potro,"" indicating a specific individual, likely the professional tennis player Juan Martín del Potro, and ""French Open,"" which refers to the well-known tennis Grand Slam event held in Paris.The key components identified are ""Del Potro,"" indicating a specific individual, likely the professional tennis player Juan Martín del Potro, and ""French Open,"" which refers to the well-known tennis Grand Slam event held in Paris. Provide a summary of the identified components, including their interrelations and the overall significance within the context of the text.
DA-CoT
𝟐𝐧𝐝step𝟏𝐬𝐭stepfeeds intoDelPotro says make French Open𝑋:
DA-CoTOutputFigure 4: Thefiguredepictsthetwo-phaseprocedureoftheDA-CoTmethodemployedinthesnippetsdomain.Initially,themethod
discerns essential text elements, including primary entities, actions, and events. Subsequently, it synthesizes the interconnections
and collective importance of these elements, enhancing comprehension of their pertinence and consequences in the context of the
text.
the model to the task. To circumvent the unpredictability
inherent in manually crafted prompts, we introduce the
Explicit Category Context Augmentation (ECCA) method,
whicheliminatestherequirementformanuallycraftedtask-
specificpromptsandenrichestextrepresentation,leadingto
a more accurate model classification.
In the ECCA method, the original input text 𝑋𝑖is aug-
mentedwithcategorylabels 𝐿usinganinjectionfunctionto
form an enhanced input 𝑋′
𝑖, which the model subsequently
uses for classification. This process was designed to infuse
label-specificsemanticcuesintoamodelclassificationtask.
𝑋′
𝑖=inject (𝑋𝑖,𝐿) (7)
The injection can be executed as a simple concatenation,
where # represents the concatenation operation and 𝑙𝑗,𝑗∈
(1,2,⋯,𝑚)denotes each label.
𝑋′
𝑖=𝑙1⊕𝑙2⊕…⊕𝑙𝑚⊕𝑋𝑖 (8)
3.3.3. Multi-task learning
Several methods exist for integrating rationale into the
trainingprocessesofdownstreammodels.Thedirectmethod
usesrationaleasanadditionalinput.However,thisapproach
requiresthatanLLMgeneratearationalebeforethesmaller
model makes a prediction. Therefore, we adopt a multi-task
learningframeworktoenhancethelinkbetweentheinput 𝑋𝑖
and desired output 𝑌𝑖.
The primary task is to predict the correct category label
̂ 𝑦𝑖from augmented input 𝑋′
𝑖. The secondary task involves
processing the original text 𝑋𝑖to output a rationale ̂ 𝑟𝑖, with
𝑟𝑖∈𝑅serving as ground truth. Similarly, the tertiary
task processes the original text 𝑋𝑖to generate the domain-
specific rationale ̂ 𝑜𝑖, where𝑜𝑖∈𝑂is the ground truth. Thismulti-task setup aims to predict the category label directly
andgeneraterationalesthatprovideinterpretabilityandcon-
texttothemodel’sdecisions,leveragingtherationalesfrom
theteachermodelassupplementaryguidance.Thus,theloss
function encompasses the following terms for each task:
𝐿=𝐿𝑙𝑎𝑏𝑒𝑙+𝜆1𝐿SSE+𝜆2𝐿DA (9)
The weights 𝜆1and𝜆2balance the influences of the sec-
ondaryandtertiarytasks,ensuringthatanysingletaskdoes
not dominate the model’s training. The calculation method
for each loss function is as follows:
𝐿𝑙𝑎𝑏𝑒𝑙=1
𝑁𝑁∑
𝑖=1𝓁(𝑓𝑠(𝑋′
𝑖), ̂ 𝑦𝑖) (10)
𝐿𝑆𝑆𝐸 =1
𝑁𝑁∑
𝑖=1𝓁(𝑓𝑠(𝑋𝑖),𝑟𝑖) (11)
𝐿𝐷𝐴=1
𝑁𝑁∑
𝑖=1𝓁(𝑓𝑠(𝑋𝑖),𝑜𝑖) (12)
Here,𝑓𝑠represents the smaller model, and 𝓁denotes the
cross-entropy loss between the predicted and target tokens.
The estimated rationales are unnecessary during testing,
thereby obviating the need for LLM at that stage.
4. Experiment
4.1. Datasets
To ensure a thorough and unbiased evaluation, we
conducted extensive experiments on six widely recognized
Page 7 of 15
#texts avg.length#classes#train(radio)
MR 10662 12.1 2 40(0.38%)
Snippets 12340 17.4 8 160(1.30%)
Ohsumed 7400 8.65 23 460(6.22%)
StackOverflow 20000 5.7 20 400(2.00%)
TagMyNews 32605 6.1 7 140(0.43%)
AGNews 20000 27.8 4 80(0.4%)
Table 1
Summary of short text datasets used.
benchmark short-text datasets: MR, Snippets, Ohsumed,
StackOverflow,TagMyNews,andAGNews.Following[35],
werandomlysampled40labeledshorttextsfromeachclass,
wherehalfformedthetrainingset,andtheotherhalfformed
the validation set. Table 1 provides detailed information
regarding the datasets. We describe the datasets further
below.
1.MR1: introduced by [53], consists of one-sentence
moviereviewsannotatedaseitherpositiveornegative
for sentiment analysis.
2.Snippets2: introduced by [1], are snippets derived
from Google search engine results.
3.Ohsumed3: introduced by [54], this dataset focused
on classifying cardiovascular diseases. In this study,
we utilized the subset defined by [34], which concen-
tratesontheclassificationofshorttextsusingonlythe
titles of single-label documents.
4.StackOverflow4: introduced by [2], contains 20 dis-
tinct types of question titles, derived from the Stack-
Overflow data within the IT Q&A community.
5.TagMyNews5:introducedby[34],comprisesEnglish
news headlines sourced from the Really Simple Syn-
dication (RSS) feeds.
6.AGNews6: introduced by [55], was the source of our
dataset. In this study, we randomly selected 20,000
articles from the collection.
4.2. Experimental Setting
WeselectedLLaMA2-13B[40]asthefoundationalmodel
for our QLFR method and chose the LLaMA2-13B and
Flan-T5-Large[56]modelstorepresenttheLLMandsmaller
modelintheQLFR-CMLmethod,respectively.IntheQLFR
method,theLLaMA2-13Bemploysefficientparameterfine-
tuning with Low-Rank Adaptation (LoRA)[57], which is
conductedwithabatchsizeof10acrossfiveepochs.LoRA
maintains the weights of pretrained LMs while introducing
trainablerankdecompositionmatricesintoeachtransformer
layer, making it feasible to fine-tune larger LMs with fewer
computational resources7. Conversely, in the QLFR-CML,
1https://www.cs.cornell.edu/people/pabo/movie-review-data/
2https://github.com/jacoxu/STC2/tree/master
3https://github.com/yao8839836/text_gcn
4https://github.com/jacoxu/StackOverflow/blob/master/
5https://github.com/cskarthik93/TagMyNews-Classification
6https://github.com/mhjabreel/CharCNN
7Inourexperiment,trainableparametersonlyaccountfor0.24%ofthe
entire LLaMA2-13B parametersFlan-T5-Large has fully fine-tuned parameters with batch
sizes of 5 and 10 epochs. All experiments were conducted
using three A100s and five V100s.
4.3. Evaluation
To assess the efficacy of the proposed approach, we
selectedtwoprincipalevaluationmetrics:accuracy(denoted
asACC) and macro-averaged F1 score (denoted as F1).
Accuracy provides a straightforward measure of the overall
accuracy of the model predictions. Conversely, the macro-
averagedF1scoreiscrucialforimbalanceddatasetsbecause
it uniformly weighs precision and recalls across all classes,
ensuring fair evaluation even with limited samples in some
categories.
4.4. Compared Methods
The baselines can be divided into three primary cate-
gories:group (A) , pre-trained language models; group
(B), GCN-based Models; and group (C) , Large Language
Models. Each group is discussed below.
Group (A). Pre-trained Language Models
Pre-trained Language Models (PLMs) have garnered
considerableattentioninthefieldofNLP,frequentlyserving
as tools for text classification, among other basic tasks. In
thisstudy,wechosethreeestablishedmethodsforcompara-
tive analysis.
1. BERT8[18], pre-trained on extensive corpora, is fur-
ther fine-tuned using a linear classifier for short-text
classification. Each document can be represented by
either the average of its word embeddings (denoted
by-avg) or the embedding of a CLS token (denoted
by-CLS).
2. RoBERTa9[58] is a modified and optimized version
of BERT that is pre-trained on large amounts of text
using adjusted training approaches.
Group (B). GCN-based models
Recently,methodsbasedonGraphConvolutionalNeural
Networks(GCNs)haveachievedoutstandingresultsinSTC
tasks. Five classical models are used in this study.
1. HGAT-inductive10[37]advocatesconnectingeachnew
sample to the existing training and unlabeled data in
the corpus.
2. SimpleSTC11[38] constructs a basic word graph of
common words using an additional corpus to address
STC tasks via inductive learning.
3. ST-Text-GCN12[36] utilizes a self-training method to
extractkeywords,ensuringtheeffectiveuseoflimited
labeled text and many unlabeled texts.
4. HGAT13[34]deploysadual-levelattention-basedGNN
to function on a corpus-level graph encompassing
entities, topics, and documents.
8https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4
9https://github.com/facebookresearch/fairseq
10https://github.com/BUPT-GAMMA/HGAT
11https://github.com/tata1661/SimpleSTC-EMNLP22
12https://github.com/wanggangkun/ST-Text-GCN
13https://github.com/BUPT-GAMMA/HGAT
Page 8 of 15
MR Snipptes Ohsumed TagMyNews StackOverflow AGNews
ACC F1 ACC F1 ACC F1 ACC F1 ACC F1 ACC F1
PLMsBERT-avg 51.69 50.65 79.31 78.47 23.91 4.98 55.13 44.26 72.91∗73.69∗76.52∗76.49∗
BERT-CLS 53.48 46.99 81.53 79.03 21.76 4.81 58.17 41.04 73.74∗74.11∗78.35∗78.42∗
RoBERTa 53.62∗52,27∗79.58∗79.10∗26.95∗19.47∗55.57∗50.45∗64.87∗64.40∗79.33∗79.45∗
GCNsHGAT-inductive 61.18 59.77 79.40 77.69 42.08 25.71 58.20 49.55 72.31∗70.42∗70.23 68.43
SimpleSTC 62.27 62.14 80.96 80.56 43.16∗23.35∗67.17 63.34 73.63∗73.39∗72.62∗71.89∗
ST-Text-GCN 50.23∗34.02∗83.83∗83.15∗33.64∗22.62∗52.33∗47.38∗69.68∗68.94∗86.83∗86.06∗
HGAT 62.75 62.36 82.36 74.44 42.68 24.82 61.72 53.81 75.29∗75.14∗72.10 71.94
SHINE 64.58 63.89 82.39 81.62 45.57 30.98 62.50 56.21 76.81∗76.44∗81.39∗81.45∗
LLMsLLaMA2-7B 71.49∗71.03∗78.47∗78.76∗48.08∗40.21∗56.75∗56.20∗87.36∗88.21∗79.49∗80.67∗
ChatGLM 73.50∗73.31∗80.62∗80.39∗51.28∗36.68∗70.77∗67.79∗86.13∗86.77∗82.51∗82.50∗
OursQLFR-CML 76.10 76.10 84.67 83.89 58.44 44.32 79.61 75.43 84.11 84.21 88.30 88.41
↑2.60 ↑2.88 ↑0.84 ↑0.74 ↑7.16 ↑4.11 ↑8.84 ↑7.64 ↓3.25 ↓4.00 ↑1.47 ↑2.35
QLFR 81.70 81.72 85.75 85.32 61.10 51.85 83.37 79.84 89.67 89.58 89.14 89.28
↑8.20 ↑8.41 ↑1.92 ↑2.17 ↑9.82 ↑11.64 ↑12.60 ↑12.05 ↑2.31 ↑1.37 ↑2.31 ↑3.22
Table 2
Performance evaluation measured on short text datasets. This table presents the comparative performance of baseline models
and ours, measured in terms of ACC (%) and F1 (%). The table highlights the highest scores in bold, with underscores indicating
the highest scores achieved by prior methods. ∗indicates that the result is reproduced by us.
5. SHINE14[35] introduces a hierarchically heteroge-
neous corpus-level graph that optimizes node inter-
actions and captures textual similarities.
Group (C). Large Language Models
Four LLMs were selected, with LLaMA2-7B and Chat-
GLM used for comparative experiments and the GPT-3 and
FLAN-T5 series used in subsequent analytical experiments.
1. LLaMA2-7B15[40],introducedbyMata,encompasses
seven billion parameters. It exhibits superior efficacy
on multiple benchmark datasets and is suitable for
research and commercial applications.
2. ChatGLM16, introduced by Tsinghua University, is a
robust language-generation model that provides ad-
vanceddeep-learningtechnologieswithtrainingusing
extensive corpora.
3. GPT-3[59], note that GPT-3 does not release the
model parameters, and we use them via the API.
Consequently, the supervised fine-tuning of GPT-3 is
not feasible; it is utilized solely for the experiments
described in Section 4.7.
4. FLAN-T5 series17[56], introduced by Google. This
method fine-tunes language models for tasks of an
unprecedented scale owing to the remarkable gener-
alization capacities of these models. Consequently,
a singular model can effectively execute more than
1,000 tasks. In Section 4.9, we perform comparative
experiments using models of various sizes.
4.5. Benchmark Comparison
Table 2 shows the performance comparison. As can
be seen, our QLFR method surpasses other approaches on
14https://github.com/tata1661/SHINE-EMNLP21
15https://github.com/facebookresearch/llama-recipes/
16https://github.com/THUDM/ChatGLM-6B
17https://huggingface.co/google/flan-t5-xxlall six datasets. Similarly, the QLFR-CML method shows
notable performance across five datasets. Specifically, on
the TagMyNews dataset, QLFR achieves an increase of
12.60% in ACC and 12.05% in F1 score compared to the
previously optimal ChatGLM model. QLFR-CML records
an 8.84% improvement in ACC and a 7.64% increase in
F1 score. These results support our hypothesis that LLMs
caneffectivelyutilizetheirinherentknowledgeandabilities
to address traditional NLP tasks, particularly the STC task
discussed in this paper, thereby validating the effectiveness
of our proposed methods.
A comparative analysis revealed that QLFR outper-
formed QLFR-CML, especially on the MR dataset, where
QLFR’s accuracy of QLFR exceeded that of QLFR-CML
by 5.6%. This indicates that models with larger parameters
have greater intrinsic knowledge and enhanced capabilities
tosolvetheSTCtask.AlthoughtheCMLdidnotoutperform
ChatGLM on the StackOverflow dataset, it significantly ex-
ceeded the top-performing model in the GCN-based group.
Moreover, it achieved a 7.3% higher ACC and a 7.77%
greater F1 score than SHINE.
An interesting observation is that for the Snippets and
AGNews datasets, the methods based on LLMs did not
outperform those utilizing GCNs. In the Snippets dataset,
ST-Text-GCN outperformed ChatGLM by 3.21% in ACC
and 2.76% in F1 score. Given the dense entity relationship
characteristics of news datasets, the inherent topological
advantages of GCN methods can enable a more effective
capture of relational data, leading to superior performance.
Consequently,traditionalapproacheshaveretainedtheirrel-
evance in the context of LLMs.
4.6. Ablation Study
QLFR-CMLAblation. Ablationstudieswereperformedon
six benchmark datasets to assess the influence of particular
Page 9 of 15
MR Snipptes Ohsumed TagMyNews StackOverflow AGNews
ACC F1 ACC F1 ACC F1 ACC F1 ACC F1 ACC F1
QLFR-CML 76.10 76.10 84.67 83.89 58.44 44.32 79.61 75.43 84.11 84.21 88.30 88.41
w/o ECCA 75.48 75.46 83.27 81.80 56.91 48.65 77.07 74.06 83.08 83.44 87.52 87.74
w/o SSE-CoT 74.36 74.38 81.55 80.64 56.43 47.96 75.96 72.19 82.87 83.58 86.38 86.80
w/o DA-CoT 74.02 74.02 81.21 80.17 55.18 47.27 75.45 71.73 81.96 82.74 85.87 86.10
Table 3
Performance evaluation of QLFR-CML and its variant on short text datasets. The best results are in bold. ‘w/o ECCA’ illustrates
results without employing the ECCA strategy, while ‘w/o SSE-CoT’ and ‘w/o DA-CoT’ display outcomes when omitting SSE-CoT
and DA-CoT rationales, respectively.
strategies or components on the QLFR-CML method. We
developed three variations of the QLFR-CML, as follows:
•QLFR-CML+w/oECCA:Duringthefine-tuningstage,
theoriginaltext 𝑋isusedasinputtopredictthelabel
without employing the ECCA strategy.
•QLFR-CML+w/o SSE-CoT: The task of generate
SSE-CoT rationales is removed from the multi-task
learning process.
•QLFR-CML+w/oDA-CoT:Similarly,thetaskofgen-
erate DA-CoT rationales is excluded from the multi-
task learning process.
Table 3 displays the outcomes of our ablation study,
highlighting the best-performing metrics in bold. These
findings demonstrate the integral role of each strategy and
component in the effectiveness of our QLFR-CML method.
Notably, ECCA contributes most substantially, with SSE-
CoT and DA-CoT also providing significant enhancements.
The significant improvement provided by ECCA confirms
the importance of a prompt for generative models with an
encoder-decoder architecture. The improvement observed
with SSE-CoT and DA-CoT affirms that our strategy effec-
tively transfers the knowledge and capabilities of LLM to a
smaller model.
TheimpactofSSE-CoTandDA-CoTvariedaccordingto
thedatasettype.Forinstance,innews-relateddatasets,such
as TagMyNews, SSE-CoT exerts a more substantial influ-
encethanDA-CoT,likelybecauseofthefrequentoccurrence
of widely recognized entities in news texts. Conversely, in
specialized datasets, such as Ohsumed for medical content
and StackOverflow for computing science, the importance
of DA-CoT increases, reflecting the prevalence of domain-
specific terminology.
QLFQ Ablation. Ablation studies were performed on six
benchmark datasets to assess the influence of the compo-
nentsoftheQLFQframework.Wedevelopedtwovariations
of the QLFQ as follows.
•QLFR+w/orewriting:Omissionoftherewritingstep,
using the concatenation of the original input with the
retrieved text as input.
•QLFR+w/o retrieval: Omission of the retrieval step,
employing rewritten original inputs as input.•QLFR+w/oboth:Utilizingtheoriginalinputwithout
modification.
Table 4 displays the outcomes of our ablation study,
highlighting the best-performing metrics in bold. The find-
ings indicate that both steps in our SSE-CoT are beneficial.
The rewriting step addresses the challenge of syntactic in-
exactitude in short texts, while the retrieval step effectively
resolvestheissueofsemanticsparsity.Acomparisonreveals
that retrieval offers slightly more advantage than rewriting,
suggestingthatsemanticdeficienciesinshorttextsaremore
critical than syntactic imprecision.
4.7. Analysis of In-Context Learning for QLFR
Inadditiontothesupervisedfine-tuning(SFT)paradigm,
in-context learning has gained popularity. We chose four
LLMs, ChatGLM, LLaMA2-7B, LLaMA2-13B, and GPT-
3,forexperimentationinthezero-shotandone-shotsettings.
Given the rigorous demands of in-context learning, the
QLFRwastestedsolelyontheGPT-3.Wemanuallyselected
a sample from each category in a one-shot setting and
constructed the SSE-CoT as the context. Specific examples
of the input formats are provided in Appendix B. The
experimental results are presented in Figure 5.
Zero-shot setting. In the analysis across the three datasets
under a zero-shot setting, the in-context learning abilities
of the four models were ranked as follows: GPT-3 out-
performed LLaMA2-13B, which, in turn, surpassed both
ChatGLM and LLaMA2-7B. This hierarchy is attributable
to in-context learning aptitude inherent in larger models,
withincreasedparameterscorrelatingwithenhancedperfor-
mance. Applying our QLFR method to GPT-3 resulted in
performance gains across the board. For instance, the ACC
on the Ohsumed dataset improved from 51.6% to 52.9%.
Thesefindingsindicatethatourapproachisnotlimitedtothe
SFTbutisalsoapplicabletothecontextlearningparadigm.
One-shot setting. In the one-shot setting, the findings re-
garding the model performance were consistent with those
observed in the zero-shot setting. Furthermore, our method
achieved improvements across all datasets.
Comparing the results of the zero-shot and one-shot
tasks, contrary to our initial expectations, the hypothesis
thatprovidingtask-relevantexamplesenhancesthecompre-
hension and task response of LLM is not supported. The
findings reveal that only the GPT improves performance
Page 10 of 15
MR Snipptes Ohsumed TagMyNews StackOverflow AGNews
ACC F1 ACC F1 ACC F1 ACC F1 ACC F1 ACC F1
QLFR 81.70 81.72 85.75 85.32 61.10 51.85 83.37 79.84 89.67 89.58 89.14 89.28
w/o rewriting 81.38 81.38 85.28 84.16 60.80 49.63 82.78 78.91 89.08 88.44 88.52 89.04
w/o retrieval 80.18 80.20 83.65 83.72 57.43 42.96 81.04 76.48 87.87 87.58 87.30 88.61
w/o both 79.84 79.84 83.16 83.51 56.99 40.65 80.44 76.12 87.35 87.51 87.02 87.94
Table 4
Performance evaluation of QLFR and its variant on short text datasets. The best results are in bold. ‘w/o rewriting’ and ‘w/o
retrieval’ refer to results without the rewriting step and retrieval step in SSE-CoT, respectively. ‘ w/o both’ refers to the exclusion
of both steps
020406080100Metrics(%)59.765.3 65.874.3 74.7
63.165.664.475.3 74.7
ChatGLMLLaMA2-7BLLaMA2-13BGPT-3
GPT-3+QLFRMR
QLFR-CML
QLFR
ACC
F1
020406080100Metrics(%)
20.427.034.951.652.9
28.2 28.440.742.447.5
ChatGLMLLaMA2-7BLLaMA2-13BGPT-3
GPT-3+QLFROhsumed
QLFR-CML
QLFR
ACC
F1
020406080100Metrics(%)51.3
44.572.874.176.1
56.6 55.675.9
71.773.7
ChatGLMLLaMA2-7BLLaMA2-13BGPT-3
GPT-3+QLFRTagMyNews
QLFR-CML
QLFR
ACC
F1(a) Zero-shot
020406080100Metrics(%)58.1
52.354.878.680.6
50.368.6
52.778.580.6
ChatGLMLLaMA2-7BLLaMA2-13BGPT-3
GPT-3+QLFRMR
QLFR-CML
QLFR
ACC
F1
020406080100Metrics(%)
21.625.637.951.153.4
28.826.742.1 43.048.6
ChatGLMLLaMA2-7BLLaMA2-13BGPT-3
GPT-3+QLFROhsumed
QLFR-CML
QLFR
ACC
F1
020406080100Metrics(%)
26.132.969.576.478.2
34.540.169.972.675.5
ChatGLMLLaMA2-7BLLaMA2-13BGPT-3
GPT-3+QLFRTagMyNews
QLFR-CML
QLFR
ACC
F1(b) One-shot
Figure 5: Performance evaluation of LLMs in zero-shot and one-shot settings is conducted using three representative datasets.
The upper three groups correspond to zero-shot settings, while the lower three pertain to one-shot settings. In each figure, the
best results are highlighted in bold.
in one-shot learning compared with zero-shot learning in
the two datasets. In contrast, a decline is observed in the
remaining datasets. Further analysis of the model outputs
suggested that LLM with approximately 10 billion parame-
terstendedtodemonstrateareducedcapacityforprocessing
instructions with greater input complexity. In contrast, the
GPTconsistentlyexhibitsarobustunderstandingofinstruc-
tionsandsustainsitsperformancedespiteincreasedcontext
length.
Comparison of the results of SFT and in-context learn-
ing paradigm. GPT-3, with over a hundred billion pa-
rameters, exhibits a remarkable in-context learning perfor-
mance. In the one-shot setting, GPT-3 exceeded QLFR-
CML, and GPT-3+QLFR nearly matched the supervisedQLFRmethod.However,formodelswithbillionsofparam-
eters, the SFT paradigm remained predominant.
4.8. Analysis of different base models for
QLFR-CML
InourQLFR-CMLmethod,theinitialstageinvolvesthe
extraction of rationales with the assistance of an LLM, fol-
lowed by a fine-tuning phase that requires a smaller model.
Therefore, we investigated the outcomes of employing the
same LLM with various smaller models and the results of
utilizing the same smaller model with different LLMs.
Differentsmallermodels. AsshowninFigure6,weutilized
LLaMA2-13B as the LLM and three versions of Flan-T5-
Base, Flan-T5-Large, and Flan-T5-XL as smaller models,
Page 11 of 15"
2010.07431,D:\Database\arxiv\papers\2010.07431.pdf,"In the context of maximizing a submodular function subject to fairness constraints, what is the significance of the ""excess ratio"" and how does it relate to the difficulty of finding an approximate solution in a streaming setting?","The excess ratio quantifies the difference between the lower bound and the size of each color group. A higher excess ratio indicates a greater challenge in finding an approximate solution in a streaming setting, as it becomes more difficult to ensure that the lower bounds are met without violating the global cardinality constraint.","a1/2-approximate solution. The advantage of this algorithm compared to the algorithm provided
in [12] based on continuous greedy, is its simplicity and faster running time of O(|V|k). Moreover,
the algorithm and ideas introduced in this section serve as a warm-up for the streaming setting.
The greedy algorithm picks at each step the element that has the largest marginal gain while satisfying
some constraint. We start by observing that if this element was only required to satisfy the upper-
bound and cardinality constraints, the greedy algorithm might not return a feasible solution. It might
reach the global cardinality constraint without satisfying the lower bounds. Therefore, a more careful
selection of the elements is needed. To that end, we deﬁne the following concept.
Deﬁnition 3.1 We call a set S⊆Vextendable if it is a subset S⊆S′of some feasible solution
S′∈F.
For a setSto be extendable, it must satisfy the upper bounds: |S|≤kand|S∩Vc|≤ucfor all
c= 1,...,C . IfSalso satisﬁes the lower bounds ( |S∩Vc|≥ℓcfor allc), thenSis already feasible.
Otherwise, it is necessary to add at least ℓc−|S∩Vc|elements of every color cfor whichSdoes not
yet satisfy the lower bound. This yields a feasible extension as long as it does not violate the global
cardinality constraint k. In short, we have the following simple characterization:
Observation 3.2 A setS⊆Vis extendable if and only if
|S∩Vc|≤ucfor allc= 1,...,C andC∑
c=1max(|S∩Vc|,ℓc)≤k.
Algorithm 1 FAIR-GREEDY
1:S←∅
2:while|S|<kdo
3:U←{e∈V|S+eis extendable}
4:S←S+ argmaxe∈Uf(e|S)
5:returnSTheFAIR-GREEDY algorithm starts with S=∅
and in each step takes the element with highest
marginal gain that keeps the solution extendable.
Fact 3.3 FAIR-GREEDY is a1/2-approximate
algorithm with O(|V|k)running time for fair
monotone submodular maximization.
The analysis of FAIR-GREEDY is deferred to
Appendix A.
4 Monotone Streaming Algorithm
In this section, we present our algorithm for fair monotone submodular maximization in the streaming
setting, and we prove its approximation guarantee. We begin by explaining the intuition behind
our algorithm. If we removed the lower-bound constraints |S∩Vc|≥ℓcinF, then the remaining
constraints would give rise to a matroid (a so-called laminar matroid). There exist efﬁcient streaming
algorithms for submodular maximization under matroid constraint (e.g. [ 14,28]), which we could
use in a black-box manner. A solution obtained from such an algorithm Amay of course violate the
lower-bound constraints. We could hope to augment our solution to a feasible one using “backup”
elements gathered from the stream in parallel to A. As we are dealing with a monotone submodular
function, adding such elements would not hurt the approximation guarantee inherited from A.
However, doing so might violate the global cardinality constraint |S|≤k. Indeed, as we remarked in
Section 3, not every set satisfying the upper-bound constraints can be extended to a feasible solution.
Recall that the right constraint to place was for the solution to be extendable (Deﬁnition 3.1) to a
feasible set. Crucially, we show that such a solution can be efﬁciently found, as extendable subsets of
Vform a matroid.
Lemma 4.1 Let˜F⊆ 2Vbe the family of all extendable subsets of V. Then ˜Fis a matroid.
The proof of Lemma 4.1 can be found in Appendix B.1. Algorithms for submodular maximization
under a matroid constraint require access to a membership oracle for the matroid. For ˜F, membership
is easy to verify, as follows from Observation 3.2.
4
Algorithm 2 FAIR-STREAMING
1:SA←∅,Bc←∅ for allc= 1,...,C
2:forevery arriving element eof colorcdo
3: processewith algorithmA
4: if|Bc|<ℓcthen
5:Bc←Bc+e
6:SA←solution of algorithm A
7:S←SAaugmented with elements in sets Bc
8:returnSNow we are ready to present our algorithm
FAIR-STREAMING for fair monotone submod-
ular maximization. Let Abe a streaming algo-
rithm for monotone submodular maximization
under a matroid constraint. FAIR-STREAMING
runsAto construct an extendable set SAthat
approximately maximizes f. In parallel, for ev-
ery colorcwe collect a backup set Bcof size
|Bc|=ℓc. At the end, the solution SAis aug-
mented to a feasible solution Susing a simple
procedure: for every color such that |SA∩Vc|<ℓc, add anyℓc−|SA∩Vc|elements from Bcto
satisfy the lower bound. The pseudocode of FAIR-STREAMING is given as Algorithm 2. Thus we get
the following black-box reduction, proved in Appendix B.2.
Theorem 4.2 SupposeAis a streaming algorithm for monotone submodular maximization un-
der a matroid constraint. Then there exists a streaming algorithm for fair monotone submodular
maximization with the same approximation ratio and memory usage as A.
Applying Theorem 4.2 to the algorithm of [32] we get the following result.
Theorem 4.3 (Streaming monotone) There exists a streaming algorithm for fair monotone submod-
ular maximization that attains 1/2-approximation and uses kO(k)memory.
We remark that the 1/2approximation ratio is tight even in the simpler setting of monotone streaming
submodular maximization subject to a cardinality constraint [29].
A more practical algorithm to use as AinFAIR-STREAMING is the 1/4-approximation algorithm of
Chakrabarti and Kale [ 14]. It turns out that we can further adapt and optimize our implementation to
make our algorithm extremely efﬁcient and use only 2oracle calls and O(logk)time per element.
We prove Theorem 4.4 in Appendix C, where we also state the algorithm of [14] for completeness.
Theorem 4.4 (Streaming monotone) There exists a streaming algorithm for fair monotone submod-
ular maximization that attains 1/4-approximation, using O(k)memory. This algorithm uses O(logk)
time and 2oracle calls per element.
5 Non-monotone Streaming Case
We now focus on non-monotone functions. One might consider applying the approach from the
previous section, i.e., use a known algorithm for non-monotone submodular maximization under a
matroid constraint to ﬁnd a high quality extendable solution, and then add backup elements to satisfy
the lower-bound constraints. However, this approach is more challenging now, as adding backup
elements to a solution could drastically decrease its value.
For example, consider the following instance with two colors. Let V=A∪B∪{x}where
A={ai|i∈[m1]},B={bi|i∈[m2]}, eache∈A∪Bisblue, andxisred. Letf(S) =|S|for
eachS⊆A∪B, and letx“nullify” the contributions of Bbut not the contributions of A. Formally,
f(S) ={|S| ifx̸∈S,
|S∩A|ifx∈S.
It is easy to verify that fis submodular (a formal proof is given in the Appendix). Suppose that we
have to pick exactly one red element, i.e., ℓred=ured= 1. This renders all elements in Buseless,
and the optimal solution takes only elements in A. However, before xappears, elements in AandB
areindistinguishable , sincef(S) =|S|for anyS⊆A∪B. Therefore, if m1≪m2, andxis last in
the stream, any algorithm that does not store the entirety of Vwill pick only a few elements from A,
thus achieving almost zero objective value once xis included in the solution.
The core difﬁculty here, and in general, is that ℓcis nearly as large as nc=|Vc|for some color c, like
for red in our example. To quantify this we introduce the excess ratio
q= 1−max
c∈[C]ℓc/nc.
5
We show that this quantity is inherent to the difﬁculty of the problem. Indeed, it is impossible to
achieve an approximation ratio better that qwith sublinear space.
Theorem 5.1 (Hardness non-monotone) For any constant ϵ>0andq∈[0,1], any algorithm for
fair non-monotone submodular maximization that outputs a (q+ϵ)-approximation for inputs with
excess ratio above q, with probability at least 2/3, requires Ω(n)memory.
The proof of Theorem 5.1 is deferred to Appendix D.2. Note that in practice qis nearly always large,
as the size of the data is signiﬁcantly larger than the size of the summary. In what follows, we present
a streaming algorithm for fair non-monotone submodular maximization, that nearly matches the
above approximation lower-bound, using only O(k)memory.
5.1 Non-monotone algorithm
Algorithm 3 FAIR-SAMPLE -STREAMING
1:SA←∅,Bc←∅ for allc= 1,...,C
2:forevery arriving element edo
3: processewith algorithmA
4: ife∈Vcthen
5:Bc←Reservoir-Sample( Bc,e)
6:S←SAaugmented with elements in sets Bc
7:returnSOur non-monotone algorithm FAIR-SAMPLE -
STREAMING is a variant of FAIR-STREAMING ,
where we modify the way backup elements are
collected. LetAbe anα-approximation al-
gorithm for non-monotone submodular max-
imization under a matroid constraint. FAIR-
SAMPLE -STREAMING runs algorithmAto con-
struct an extendable set SAthat approximately
maximizesf. In parallel, our algorithm col-
lects for every color ca backup set Bcof size
|Bc|=ℓc, by sampling without replacement ℓcelements inVcusing reservoir sampling [ 43]. Note
that we do not need to know the value of ncto execute reservoir sampling. At the end, the solution
SAis augmented to a feasible solution Susing the same simple procedure as in Section 4. The
pseudocode of FAIR-SAMPLE -STREAMING is given as Algorithm 3. We show that adding elements
from the back-up set reduces the objective value by a factor of at most q.
Theorem 5.2 SupposeAis a streaming α-approximate algorithm for non-monotone submodular
maximization under a matroid constraint. Then, there exists a streaming algorithm for fair non-
monotone submodular maximization with expected qαapproximation ratio, and the same memory
usage, oracle calls, and running time as A.
The proof is provided in Appendix D.1. Combining this with the state of the art 1/5.82-approximation
algorithm of Feldman, et al. [28] (restated in Appendix C for completeness) yields the following.
Theorem 5.3 (Streaming non-monotone) There exists a streaming algorithm for fair non-
monotone submodular maximization that achieves q/5.82-approximation in expectation, using O(k)
memory. This algorithm uses O(k)time andO(k)oracle calls per element.
6 Empirical Evaluation
In this section, we empirically validate our results and address the question: What is the price of
fairness? To this end, we compare our approach against several baselines on four datasets. We
measure: (1) Objective values. (2) Violation of fairness constraints: Given a set S, we deﬁne
err(S) =∑
c∈[C]max{|S∩Vc|−uc,ℓc−|S∩Vc|,0}. A single term in this sum quantiﬁes by
how many elements Sviolates the lower or upper bound. Note that err(S)is in the range [0,2k].
(3) Number of oracle calls, as is standard in the ﬁeld to measure the efﬁciency of algorithms.
We compare the following algorithms:
•FAIR-STREAMING -CK: monotone,A=Chakrabarti-Kale [ 14] (Theorem 4.4 and Ap-
pendix C.1),
•FAIR-STREAMING -FKK : monotone,A=Feldman et al. [28] (Appendix C.2),
•FAIR-SAMPLE -STREAMING -FKK : non-monotone,A=Feldman et al. [ 28] (Theo-
rem 5.3),
6
•UPPER BOUNDS : [28] (Appendix C.2) applied to matroid deﬁning upper bounds only ( uc
andk),
•FAIR-GREEDY : monotone; where data size allows; see Section 3,
•GREEDY : monotone; when data size allows; no fairness constraints, only k,
•SIEVE STREAMING : Badanidiyuru et al. [3]; monotone; no fairness constraints, only k,
•RANDOM : maintain random sample of kelements; no fairness constraints,
•FAIR-RANDOM : maintain random feasible (fair) solution.
We now describe our experiments. We report the results in Fig. 1, and discuss them in Section 6.5.
The code is available at https://github.com/google-research/google-research/tree/
master/fair_submodular_maximization_2020 .
6.1 Maximum coverage
Social inﬂuence maximization [ 37] and network marketing [ 42] are some of the prominent appli-
cations of the maximum coverage problem. The goal of this problem is to select a ﬁxed num-
ber of nodes that maximize the coverage of a given network. Given a graph G= (V,E), let
N(v) ={u: (v,u)∈E}denote the neighbors of v. Then the coverage of S⊆V, denoted by f(S),
is deﬁned as the monotone submodular function f(S) =⏐⏐⋃
v∈SN(v)⏐⏐.We perform experiments
on the Pokec social network [ 41]. This network consists of 1 632 803 nodes, representing users, and
30 622 564 edges, representing friendships. Each user proﬁle contains attributes such as age, height
and weight; these can take value “null”. We impose fairness constraints with respect to (i) age and
(ii) body mass index (BMI).
(i) We split ages into ranges [1,10],[11,17],[18,25],[26,35],[36,45],[46+] and consider each range
as one color. We create another color for records with “null” age (around 30%). Then for every
colorcwe setℓc= max{0,|Vc|/n−0.05}·kanduc= min{1,|Vc|/n+ 0.05}·k, except for the
null color, where we set ℓc= 0. The results are shown in Fig. 1a, 1b, and 1c.
(ii) BMI is computed as the ratio between weight (in kg) and height (in m) squared. Around 60% of
proﬁles do not have set height or weight. We discard all such proﬁles, as well as proﬁles with clearly
fake data (less than 2%of proﬁles). The resulting graph consists of 582 319 nodes and 5 834 695
edges. The proﬁles are colored with respect to four standard BMI categories (underweight, normal
weight, overweight and obese). Lower- and upper-bound fairness constraints are set again to be
within 5%of their respective frequencies. The results are shown in Fig. 1d, 1e, and 1f.
6.2 Movie recommendation
We use the Movielens 1M dataset [ 31], which contains∼1M ratings for 3 900 movies by 6 040 users,
to develop a movie recommendation system. We follow the experimental setup of prior work [ 47,51]:
we compute a low-rank completion of the user-movie rating matrix [57], which gives rise to feature
vectorswu∈R20for each user uandvm∈R20for each movie m. Thenw⊤
uvmapproximates
the rating of mbyu. The (monotone submodular) utility function for a collection Sof movies
personalized for user uis deﬁned as
fu(S) =α·∑
m′∈Mmax(
max
m∈S(
v⊤
mvm′)
,0)
+ (1−α)·∑
m∈Sw⊤
uvm.
The ﬁrst term optimizes coverage of the space of all movies (enhancing diversity) [ 45], and the
second term sums up user-dependent movie scores; αcontrols the trade-off between the two terms.
In our experiment we recommend a collection of movies for α= 0.85andk-values up to 100.
Each movie in the database is assigned to one of 18 genres c; our fairness constraints mandate
a representation of genres in Ssimilar to that in the entire dataset. More precisely, we set ℓc=⌊
0.8|Vc|
|V|k⌋
anduc=⌈
1.4|Vc|
|V|k⌉
. The results are shown in Fig. 1g, 1h, and 1i.
6.3 Census DPP-based summarization
A common reliable method for data summarization is to use a Determinantal Point Process (DPP) to
assign a diversity score to each subset, and choose the subset that maximizes this score. A DPP is
7
a probability measure over subsets, deﬁned for every S⊆V, asP(S) =det(LS)
det(I+L), whereLis an
n×npositive semi-deﬁnite kernel matrix, LSis the|S|×|S|principal submatrix of Lindexed by
S, andIis the identity matrix. To ﬁnd the most diverse representative subset, we need to maximize
the non-montone submodular function f(S) = log det( LS)[40]. To ensure non-negativity (on
non-empty sets), we normalize fby a constant.
We use the Census Income dataset [ 22] which consists of 45 222 records extracted from the 1994
Census database, with 14 attributes such as age, race, gender, education, and whether the income is
above or below 50K USD. We follow the experimental setup of [ 11] to generate feature vectors of
dimension 992 for 5000 randomly chosen records.2We select fair representative summaries with
respect to race, requiring that each of the race categories provided in the dataset (White, Black,
Asian-Pac-Islander, Amer-Indian-Eskimo, and Other) have a similar representation in Sas in the
entire dataset. Accordingly, we set ℓc=⌊
0.9T
nk⌋
anduc=⌈
1.1T
nk⌉
, and varykbetween 50−600.
The results are shown in Fig. 1j, 1k, and 1l.
6.4 Exemplar-based clustering
We consider a dataset containing one record for each phone call in a marketing campaign ran by a
Portuguese banking institution [ 49]. We aim to ﬁnd a representative subset of calls in order to assess
the quality of service. We choose numeric attributes such as client age, gender, account balance,
call date, and duration, to represent each record in the Euclidean space. We require the chosen
subset to have clients in a wide range of ages. We divide the records into six groups according
to age: [0,29],[30,39],[40,49],[50,59],[60,69],[70+] ; the numbers of records in each range are
respectively: 5 273,18 089,11 655,8 410,1 230,554. We set our bounds so as to ensure that each
group comprises 10−20% of the subset. Then we maximize the following monotone submodular
function [38], where Rdenotes all records:
f(S) =C−∑
r∈Rmin
e∈Sd(r,e)whered(x,y) =∥x−y∥2
2.
We letf(∅) = 0 andCbe|V|times the maximum distance.3The results are shown in Fig. 1m, 1n,
and 1o, where the clustering cost refers to C−f(S).
6.5 Results
We observe that in all the experiments our algorithms make smaller or similar number of oracle
calls compared to the baselines in corresponding settings (streaming or sequential). Moreover, the
objective value of the fair solutions obtained by our algorithms is similar to the unfair baseline
solutions, with less than 15% difference.
We also observe that the algorithms that do not impose fairness constraints introduce signiﬁcant
bias. For example, SIEVE STREAMING makes 150errors in the maximum coverage experiment for
k= 200 (see Fig. 1b), and 30errors fork= 70 in the exemplar-based clustering experiment (see
Fig. 1n). Moreover, even though UPPER BOUNDS satisﬁes the upper-bounds constraints, it still makes
a noticeable amount of errors. For instance, it makes 20errors in the maximum coverage experiment
fork= 200 (see Fig. 1b), and 100errors in the DPP-based summarization experiment for k= 600
(see Fig. 1k).
7 Conclusion
We presented the ﬁrst streaming approximation algorithms for fair submodular maximization, for both
monotone and non-monotone objectives. Our algorithms efﬁciently generate balanced solutions with
respect to a sensitive attribute, while using asymptotically optimal memory. We empirically demon-
strate that fair solutions are often nearly optimal, and that explicitly imposing fairness constraints is
necessary to ensure balanced solutions.
2Code available at: https://github.com/DamianStraszak/FairDiverseDPPSampling .
3Note that Cis added to ensure that all values are non-negative. Any Cwith this property would be suitable.
8
50 100 150 20002468101214104
Fair-Streaming-FKK
Fair-Streaming-CK
Sieve-Streaming
Upper-bounds
Random
Fair-Random(a) Pokec-age
50 100 150 200050100150 (b) Pokec-age
50 100 150 20002468107 (c) Pokec-age
50100150200250012345104
Fair-Streaming-FKK
Fair-Streaming-CK
Sieve-Streaming
Upper-bounds
Random
Fair-Random
(d) Pokec-BMI
501001502002500246810 (e) Pokec-BMI
5010015020025002468101214107 (f) Pokec-BMI
20 40 60 80100020406080100Fair-Streaming-FKK
Fair-Streaming-CK
Sieve-Streaming
Upper-bounds
Greedy
Fair-Greedy
Random
Fair-Random
(g) Movielens-genre
20 40 60 80100020406080100120140 (h) Movielens-genre
20 40 60 8010001234105 (i) Movielens-genre
1002003004005006008.928.938.948.958.968.978.988.99104
Fair-Sample-Streaming-FKK
Upper-bounds
Random
Fair-Random
(j) Census-race
100200300400500600020406080100120 (k) Census-race
10020030040050060000.511.522.53105 (l) Census-race
1020304050607000.511.522.531011
Fair-Streaming-FKK
Fair-Streaming-CK
Sieve-Streaming
Upper-bounds
Greedy
Fair-Greedy
Random
Fair-Random
(m) Bank-age
1020304050607005101520253035 (n) Bank-age
1020304050607000.511.52106 (o) Bank-age
Figure 1: Performance of FAIR-STREAMING -CK, FAIR-STREAMING -FKK and FAIR-SAMPLE -
STREAMING -FKK compared to other baselines, in terms of objective value, violation of fairness
constraints, and running time, on Movielens, Pokec, Census, and Bank-Marketing datasets.
9
Broader Impact
Several recent studies have shown that automated data-driven methods can unintentionally lead to
bias and discrimination [ 35,56,5,10,52]. Our proposed algorithms will help guard against these
issues in data summarization tasks arising in various settings – from electing a parliament, over
selecting individuals to inﬂuence for an outreach program, to selecting content in search engines and
news feeds. As expected, fairness does come at the cost of a small loss in utility value, as observed in
Section 6. It is worth noting that this “price of fairness” (i.e., the decrease in optimal objective value
when fairness constraints are added) should not be interpreted as fairness leading to a less desirable
outcome, but rather as a trade-off between two valuable metrics: the original application-dependent
utility, and the fairness utility. Our algorithms ensure solutions achieving a close to optimal trade-off.
Finally, despite the generality of the fairness notion we consider, it does not capture certain other
notions of fairness considered in the literature (see e.g., [ 18,58]). No universal metric of fairness
exists. The question of which fairness notion to employ is an active area of research, and will be
application dependent.
Acknowledgments and Disclosure of Funding
Marwa El Halabi was supported by a DARPA D3M award, NSF CAREER award 1553284, NSF award
1717610, and by an ONR MURI award. The views, opinions, and/or ﬁndings contained in this article
are those of the authors and should not be interpreted as representing the ofﬁcial views or policies,
either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of
Defense. Slobodan Mitrovi ´c was supported by the Swiss NSF grant No. P400P2_191122/1, MIT-IBM
Watson AI Lab and Research Collaboration Agreement No. W1771646, and FinTech@CSAIL. Jakab
Tardos has received funding from the European Research Council (ERC) under the European Union’s
Horizon 2020 research and innovation programme (grant agreement No 759471).
References
[1]Georg Anegg, Haris Angelidakis, Adam Kurpisz, and Rico Zenklusen. A technique for obtaining
true approximations for k-center with covering constraints. In Daniel Bienstock and Giacomo
Zambelli, editors, Integer Programming and Combinatorial Optimization , pages 52–65, Cham,
2020. Springer International Publishing.
[2]Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali Vakilian, and Tal Wagner.
Scalable fair clustering. arXiv preprint arXiv:1902.03519 , 2019.
[3]Ashwinkumar Badanidiyuru, Baharan Mirzasoleiman, Amin Karbasi, and Andreas Krause.
Streaming submodular maximization: Massive data summarization on the ﬂy. In Proceedings
of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining ,
pages 671–680, 2014.
[4]Dan Biddle. Adverse impact and test validation: A practitioner’s guide to valid and defensible
employment testing . Gower Publishing, Ltd., 2006.
[5]Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai.
Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In
Advances in neural information processing systems , pages 4349–4357, 2016.
[6]Markus Brill, Jean-Francois Laslier, and Piotr Skowron. Multiwinner approval rules as appor-
tionment methods. 2017.
[7]N. Buchbinder, M. Feldman, J. Naor, and R. Schwartz. Submodular maximization with
cardinality constraints. SIAM, 2014.
[8]Niv Buchbinder, Moran Feldman, and Roy Schwartz. Online submodular maximization with
preemption. In Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete
algorithms , pages 1202–1216. SIAM, 2014.
[9]Gruia Calinescu, Chandra Chekuri, Martin Pal, and Jan V ondrák. Maximizing a monotone
submodular function subject to a matroid constraint. SIAM Journal on Computing , 40(6):1740–
1766, 2011.
10
[10] Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Semantics derived automatically from
language corpora contain human-like biases. Science , 356(6334):183–186, 2017.
[11] Elisa Celis, Vijay Keswani, Damian Straszak, Amit Deshpande, Tarun Kathuria, and Nisheeth
Vishnoi. Fair and diverse DPP-based data summarization. In Jennifer Dy and Andreas Krause,
editors, Proceedings of the 35th International Conference on Machine Learning , volume 80 of
Proceedings of Machine Learning Research , pages 716–725, Stockholmsmässan, Stockholm
Sweden, 10–15 Jul 2018. PMLR.
[12] L. Elisa Celis, Lingxiao Huang, and Nisheeth K. Vishnoi. Multiwinner voting with fairness
constraints. In Jérôme Lang, editor, Proceedings of the Twenty-Seventh International Joint
Conference on Artiﬁcial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden , pages
144–151. ijcai.org, 2018.
[13] L. Elisa Celis, Damian Straszak, and Nisheeth K. Vishnoi. Ranking with fairness constraints. In
Ioannis Chatzigiannakis, Christos Kaklamanis, Dániel Marx, and Donald Sannella, editors, 45th
International Colloquium on Automata, Languages, and Programming, ICALP 2018, July 9-13,
2018, Prague, Czech Republic , volume 107 of LIPIcs , pages 28:1–28:15. Schloss Dagstuhl -
Leibniz-Zentrum für Informatik, 2018.
[14] Amit Chakrabarti and Sagar Kale. Submodular maximization meets streaming: Matchings, ma-
troids, and more. In Jon Lee and Jens Vygen, editors, Integer Programming and Combinatorial
Optimization , pages 210–221, Cham, 2014. Springer International Publishing.
[15] Chandra Chekuri, Shalmoli Gupta, and Kent Quanrud. Streaming algorithms for submodular
function maximization. In International Colloquium on Automata, Languages, and Program-
ming , pages 318–330. Springer, 2015.
[16] Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. Fair clustering
through fairlets. In Advances in Neural Information Processing Systems , pages 5029–5037,
2017.
[17] Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvtiskii. Matroids, matchings,
and fairness. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics ,
pages 2212–2220, 2019.
[18] Alexandra Chouldechova and Aaron Roth. The frontiers of fairness in machine learning. arXiv
preprint arXiv:1810.08810 , 2018.
[19] Joanne McGrath Cohoon, James P. Cohoon, Seth Reichelson, and Selwyn Lawrence. Effective
recruiting for diversity. In Randa L. Shehab, James J. Sluss, and Deborah Anne Trytten, editors,
IEEE Frontiers in Education Conference, FIE 2013, Oklahoma City, Oklahoma, USA, October
23-26, 2013 , pages 1123–1124. IEEE Computer Society, 2013.
[20] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic
decision making and the cost of fairness. In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining , pages 797–806, 2017.
[21] Anirban Dasgupta, Ravi Kumar, and Sujith Ravi. Summarization through submodularity and
dispersion. In Proceedings of the 51st Annual Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 1014–1022, Soﬁa, Bulgaria, August 2013. Association
for Computational Linguistics.
[22] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
[23] Delbert Dueck and Brendan J. Frey. Non-metric afﬁnity propagation for unsupervised image
categorization. In IEEE 11th International Conference on Computer Vision, ICCV 2007, Rio de
Janeiro, Brazil, October 14-20, 2007 , pages 1–8. IEEE Computer Society, 2007.
[24] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd innovations in theoretical computer science
conference , pages 214–226, 2012.
[25] Khalid El-Arini and Carlos Guestrin. Beyond keyword search: discovering relevant scientiﬁc
literature. In Chid Apté, Joydeep Ghosh, and Padhraic Smyth, editors, Proceedings of the 17th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Diego,
CA, USA, August 21-24, 2011 , pages 439–447. ACM, 2011.
11"
2404.11809,D:\Database\arxiv\papers\2404.11809.pdf,How does the use of conjugate parameters in knowledge graph embedding models affect the geometric properties of the transformed entities in a non-Euclidean space?,"The use of conjugate parameters introduces linear constraints on the embedding parameters, resulting in a more structured and geometrically regular representation of entities in the non-Euclidean space. This can lead to improved visualization and potentially better performance in tasks like link prediction.","(a) Transformed entities by ComplEx (left) and Compl ϵx
(right). In the left graph, a black point describes a trans-
formed entity, and the vector values of a point are unrelated
in each dimension. While in the right graph, half of the value
zi, i∈[1, d/2]of a vector z1, z2, ..., z dthat is describing a
point are constrained as the other half zi, i∈[d/2+1, d]corre-
spondingly. The linear constrain zi=aixi+biyiis illustrated
in the right graph.
(b) Transformed entities by 5⋆E(left) and 5⋆ϵn(right). Note
that we illustrate the negative conjugated model instead of
the positive conjugated one for simplicity in plotting. Blue
traces are the original entities and their projections in the Non-
Euclidean space. Green traces are the multiple copies of the
blue traces under iterations of the Möbius transformation. Red
traces are the inverse of green traces. Apparently, the right
graph has much neater geometric properties.
Figure 1: Transformed entities illustrated in 3D
5⋆ϵOur method transforms the original model
5⋆Einto the parameter-sharing model 5⋆ϵ, where
relations are represented as the real part of low-
rank matrices using conjugate parameters. Specifi-
cally, we set the original square relation embedding
matricesa b
c d
to be half the normal parameters
and the other half their conjugate parameters, i.e.,a b
ba
. In this model, parameters play distinct
roles at different positions, and the best conjuga-
tion positions are the principal and secondary diag-
onal positions. Note that experiments showed that,
the following negative conjugation method, i.e.,a b
−ba
, achieves similar performance as above.
Although the negative conjugation on this model is
equivalent to restricting the original Möbius func-
tion to the unitary Möbius transformation, our ap-
proach is much more general to a variety of repre-
sentations.
3.3 Transformation Analysis
Compl ϵxLeta2=a1, then the transformation
of conjugate model Compl ϵxis

x1x2
→
a1a1
x1x2
→
a1x1a1x2
.
(3)
We can see that the resulted relation embedding is
constrained to
a1a1
other than
a1a2
; the
predicted tail entity is constrained to
a1x1a1x2
instead of
a1x1a2x2
in original model, which
does not narrow the rang of relation or tail embed-
ding since the a1, x2can be any value. Further,
since tail entities also act as head entities, we cansay that the range of both the entities and relations
are not constrained.
5⋆ϵLetc=b, d=a, then the transformation of
conjugate model 5⋆ϵis
x→x
1
→a b
bax
1
→ax+b
bx+a.(4)
The five subsequent transformations turn into:
ϑ1=x+a
bwhich depicts translation bya
b,ϑ2=1
x
which depicts inversion and reflection w.r.t. real
axis, ϑ3=bb−aa
b2xwhich depicts homothety and
rotation, and ϑ4=x+a
bwhich depicts translation
bya
b. We can see that, although the relation pa-
rameters are constrained comparing to the original
model, the five sub-transformations are reserved in
this conjugate model.
Characteristics For this reason, we consider our
conjugate models retain expressiveness in function
level for various relation patterns compared to their
original counterparts. The difference between orig-
inal models and our conjugate models is that, the
latter ones have more linear constrain in its value of
each embedding parameter, as illustrated in Figure
1.
3.4 Reduced Calculation
Sharing half of the parameters also reduces the
computation for the regularization terms into half,
where each parameter of relation is squared to the
sum. For example, the original calculation r2
1+r2
2
is turned into r2
1×2in both baseline models, where
r1, r2denote the real or imaginary part of a com-
plex number, and in which r1represents the shared
Dataset #Train #Valid #Test Ent Rel Exa
FB15K-237 272,115 17,535 20,466 14,541 237 544,230
WN18RR 86,835 3,034 3,134 40,943 11 173,670
YAGO3-10 1,079,040 5,000 5,000 123,188 37 2,158,080
FB15K 483,142 50,000 59,071 14,951 1,345 966,284
WN18 141,442 5,000 5,000 40,943 18 282,884
Table 1: Datasets statistics. #: Split in terms of number of triples; Ent: Entities; Rel: Relations; Exa: Examples.
parameter. However, the final time consumption
depends on multiple aspects, such as formulation
and coding, thus is not necessarily reduced.
4 Experiments
4.1 Experimental Setup
Metrics We followed the standard evaluation pro-
tocal for KGE models. T: the rank set of truth, ri:
the rank position rof the first true entity for the
i-th query. We computed two rank-based metrics:
(i) Mean Reciprocal Rank (MRR), which computes
the arithmetic mean of reciprocal ranks of all true
entities from the ranked list of answers to queries
T, and (ii) Hits@ N(N= 1, 3, 10), which counts
the true entities Iand calculate their proportion in
the truth Tin top Nsorted predicted answers list.
MRR =1
TTX
i=11
ri(5)
Hits@ N=1
TX
r∈T,r≤NI (6)
We also use additional metric Time (sec-
onds/epoch) to measure how many seconds each
training epoch costs to demonstrate the time saved
by our method. To do this, we conducted all exper-
iments using the same GPUs. GeForce GTX 1080
Ti is used for all datasets except for the largest
dataset YAGO3-10 who needs a larger GPU and
we used Tesla V100S-PCIE-32GB for it.
Datasets We evaluated our method on five widely
used benchmark datasets (See Table 1). FB15K
(Bordes et al., 2013) is a subset of Freebase, the
contents of which are general facts. WN18 (Bordes
et al., 2013) is a subset of Wordnet, a database that
features lexical relations between words. YAGO3-
10 (Dettmers et al., 2018) is the largest common
dataset, which mostly describes attributes of per-
sons, and contains entities associated with at least
ten different relations.As was first noted by Toutanova and Chen
(2015), FB15K and WN18 suffer from test leak-
age through inverse relations, e.g., the test set fre-
quently contains triples such as (s, hyponym, o )
while the training set contains its inverse
(o, hypernym, s ). To create a dataset without this
property, they introduced FB15K-237, a subset
of FB15K where inverse relations are removed.
WN18RR was created for the same reason by
Dettmers et al. (2018).
We adopted all of the five datasets for compre-
hensive comparison of models.
Hyperparameter Settings We explored the in-
fluence of hyperparameter settings to our method.
To do this, we used the best hyperparameter set-
tings for the original models (marked as ∇or no
mark), and applied the same settings on our con-
jugate models and ablation models. We adopted
the best hyperparameter settings for ComplEx pro-
vided by Nayyeri et al. (2021), and fine-tuned the
best hyperparameters ourselves for 5⋆Esince there
was no published best hyperparameter settings for
this model at the time we did the experiments. We
also fine-tuned the best hyperparameters for one of
our conjugate model 5⋆ϵ(noted as ♢) to explore
the upper bound.
We selected the hyperparameters based on the
MRR on the validation set. Our grid search
range refered to but was larger than Nayyeri et al.
(2021). The optional optimizers are {Adagrad,
Adam, SGD}. The range of embedding dimensions
are {100, 500} with learning rates range in {1E-02,
5E-02, 1E-01}. The batch sizes attempted range in
{100, 500, 1000, 2000}. Regularization coefficients
are tested among {2.5E-03, 5E-03, 1E-02, 5E-02,
1E-01, 5E-01}.
5 Results
5.1 Main Results and Analysis
The main experimental results are shown in Table
2 and Table 3. The numbers with boldface indicate
Model Time MRR H@1 H@3 H@10
ComplEx 42±8 0.366±4e-4 0.271 0.402 0.558
Compl ϵx 46±11 0.363 ±5e-4 0.268 0.400 0.555
5⋆E 18±3 0.350 ±8e-4 0.257 0.386 0.538
5⋆ϵ∇ 14±4 0.353±7e-4 0.259 0.390 0.541
5⋆ϵ♢ 17±9 0.354±8e-4 0.259 0.391 0.544
(a) FB15K-237
Model Time MRR H@1 H@3 H@10
ComplEx 139±21 0.488 ±1e-3 0.442 0.503 0.579
Compl ϵx 146±45 0.475 ±9e-4 0.433 0.488 0.558
5⋆E 16±1 0.490 ±5e-4 0.444 0.506 0.587
5⋆ϵ∇ 11±1 0.493 ±8e-4 0.442 0.512 0.588
5⋆ϵ♢ - - - - -
(b) WN18RR
Model Time MRR H@1 H@3 H@10
ComplEx 370±20.577±1e-3 0.502 0.622 0.712
Compl ϵx 371±2 0.574 ±2e-3 0.500 0.618 0.707
5⋆E 415±2 0.574 ±2e-3 0.502 0.617 0.701
5⋆ϵ∇ 297±10.576±2e-3 0.505 0.619 0.702
5⋆ϵ♢ - - - - -
(c) YAGO3-10
Table 2: Link prediction results on FB15K-237,
WN18RR, YAGO3-10 datasets. Time, MRR and H@n
are presented as mean ( ±standard deviation).
the best results among all the models.
We mainly tested whether the conjugate mod-
els perform consistent with their original counter-
parts, especially whether they can achieve the same
state-of-the-art results. We conducted one set of
experiments using the best hyperparameters of the
original models (marked as ∇or no mark), and the
other set of experiments tuning the hyperparam-
eters for one of our conjugate model (marked as
♢).
The results show that both Compl ϵxand5⋆ϵ
consistently achieve results comparable to their
original models on the datasets without test set leak-
age, including the largest dataset, i.e., YAGO3-10;
and obtain the same optimal accuracies as the orig-
inal models on all datasets with possibly-required
fine-tuning. From the perspective of training time,
we see 5⋆ϵspends 31% less time on average for all
datasets; and both conjugate models perform sub-
stantially best in training time on datasets FB15K,
who have the most relations.
Compl ϵxUnder the best hyperparameter set-
tings of the original model, the performance
ofCompl ϵxare consistently comparable with
ComplEx on all five datasets. We speculate the
reason for the consistent but tiny performance drop
might come from the computation precision, but
we will leave it as our future studies.Model Time MRR H@1 H@3 H@10
ComplEx 346±124 0.855±1e-3 0.823 0.874 0.910
Compl ϵx 293±16 0.855±1e-3 0.827 0.871 0.907
5⋆E 42±9 0.812 ±1e-3 0.767 0.840 0.889
5⋆ϵ∇ 26±0 0.794±2e-3 0.743 0.827 0.882
5⋆ϵ♢ 29±5 0.813 ±2e-3 0.766 0.844 0.894
(a) FB15K
Model Time MRR H@1 H@3 H@10
ComplEx 57±3 0.951 ±3e-4 0.944 0.954 0.961
Compl ϵx 58±5 0.950 ±3e-4 0.945 0.953 0.960
5⋆E 43±60.952±5e-4 0.946 0.955 0.962
5⋆ϵ∇ 29±60.949±6e-4 0.944 0.953 0.959
5⋆ϵ♢ 26±2 0.952 ±3e-4 0.947 0.955 0.962
(b) WN18
Table 3: Link prediction results on FB15K and WN18
datasets. Instructions for this table are the same as those
in Table 2.
Although our method reduces the computation,
applying the method on this model requires split-
ting and concatenating matrices to keep the shape
of outputs which incurs additional time-consuming
operations. Consequently, the total time cost is not
reduced much. However, training time on dataset
FB15K, who has the most relations, becomes very
stable.
Overall results imply our conjugate model
Compl ϵxis at least comparative with its baseline
model ComplEx .
5⋆ϵUnder the best hyperparameter settings of
the original 5⋆E, the conjugate 5⋆ϵ∇consistently
achieve competitive results on the datasets FB15K-
237, WN18RR and YAGO3-10. The tiny but
consistent accuracy enhancement on these three
datasets is probably caused by similar program-
ming artifacts as observed in ComplEx .
We hypothesize that the accuracy fluctuation of
5⋆ϵ∇on FB15K and WN18 is caused by the test
leakage issue which makes the model sensitive to
its hyperparameter setting. Because the only dif-
ference of these two datasets comparing to their
subsets FB15K-237 and WN18RR is the 81% and
94% inverse relations (Toutanova and Chen, 2015),
i.e.,(s, hyponym, o )and(o, hypernym, s )in the
training set and the test set respectively, which is
known as test leakage. Note that the accuracy fluc-
tuation was simply solved by fine-tuning the hyper-
parameters (See results marked as 5⋆ϵ♢).
Notice that in Table 2, we didn’t report the fine-
tuned results of 5⋆ϵ♢on datasets WN18RR and
YAGO3-10, because the results abtained with the
original settings ∇is already the best.
Model Time MRR H@1 H@3 H@10
5⋆ϵ∇ 14±40.353±7e-4 0.259 0.390 0.541
5⋆ϵn 13±20.353±8e-4 0.259 0.389 0.541
5⋆Er16±0 0.326 ±1e-3 0.238 0.357 0.505
5⋆ϵv 13±1 0.264 ±4e-4 0.192 0.288 0.404
5⋆ϵh 12±0 0.301 ±4e-4 0.221 0.329 0.458
(a) FB15K-237
Model Time MRR H@1 H@3 H@10
5⋆ϵ∇ 11±1 0.493 ±8e-4 0.442 0.512 0.588
5⋆ϵn 14±3 0.485 ±1e-3 0.432 0.506 0.589
5⋆Er16±0 0.410 ±3e-3 0.391 0.417 0.447
5⋆ϵv 12±5 0.026 ±2e-4 0.015 0.025 0.045
5⋆ϵh 14±3 0.026 ±3e-4 0.016 0.025 0.046
(b) WN18RR
Model Time MRR H@1 H@3 H@10
5⋆ϵ∇ 297±10.576±2e-3 0.505 0.619 0.702
5⋆ϵn 298±10.574±1e-3 0.502 0.618 0.701
5⋆Er416±2 0.569 ±2e-3 0.499 0.611 0.695
5⋆ϵv 297±1 0.562 ±8e-4 0.488 0.607 0.695
5⋆ϵh 298±1 0.546 ±1e-3 0.471 0.592 0.680
(c) YAGO3-10
Table 4: Ablation studies on FB15K-237, WN18RR,
YAGO3-10 datasets. Instructions for this table are the
same as those in Table 2.
Training time in this model was reduced by 22%,
31%, 28%, 38% and 33% on each dataset respec-
tively, and 31% on average. 5⋆Ehas eight param-
eter matrices in the coding. By using our method,
the parameter matrices are directly reduced to four
with no additional coding operations, which makes
the significant saved training time.
Above results mean our conjugate model 5⋆ϵ
exceeds the baseline model 5⋆Ein all respect of
accuracy, memory-efficiency and time footprint.
5.2 Ablation Studies
We did two kinds of ablation studies. The results
are shown in Table 4 and Table 5. We know the
reduced calculation is mainly in the regularization
process because we only use half of the parameters.
Thus we experimented where the regularization
term is only half of the parameters on the original
model (See results for 5⋆Er) to explore whether
the effect of our method is similar to the reduced
parameters regularization.
Then we experimented with conjugations in dif-
ferent positions to explore how the models per-
form differently. We set negative conjugation
c=−b, d=ain model 5⋆ϵn, where half of the
conjugate parameters are using negative conjuga-
tion instead of positive conjugation; we set verticalModel Time MRR H@1 H@3 H@10
5⋆ϵ∇ 26±0 0.794±2e-3 0.743 0.827 0.882
5⋆ϵn 31±12 0.799±2e-3 0.750 0.831 0.883
5⋆Er37±1 0.807 ±3e-3 0.760 0.838 0.888
5⋆ϵv 31±7 0.801 ±8e-4 0.753 0.833 0.885
5⋆ϵh 28±2 0.787 ±2e-3 0.735 0.822 0.877
(a) FB15K
Model Time MRR H@1 H@3 H@10
5⋆ϵ∇ 29±6 0.949±6e-4 0.944 0.953 0.959
5⋆ϵn 26±0 0.952 ±3e-4 0.946 0.955 0.962
5⋆Er40±0 0.943 ±9e-4 0.935 0.950 0.954
5⋆ϵv 31±11 0.892 ±2e-3 0.836 0.944 0.958
5⋆ϵh 26±0 0.822 ±2e-3 0.719 0.920 0.949
(b) WN18
Table 5: Ablation studies on FB15K and WN18 datasets.
Instructions for this table are the same as those in Table
2.
conjugation c=a, d=bin model 5⋆ϵv, where
parameters are conjugated in their vertical direc-
tion instead of the diagonal direction; and we let
b=a, d=cin model 5⋆ϵh, the horizontal con-
jugation, where parameters are conjugated in their
horizontal direction.
The studies show that, first, by comparing the
accuracy of 5⋆ϵand5⋆Er, we know that reducing
parameters in the regularization process hurts the
accuracy significantly, which indicates our conju-
gation method indeedly reserves model’s ability
even when the parameters are reduced. Second, the
negative conjugate model 5⋆ϵnperforms as well as
5⋆ϵ. Last but not least, conjugate method should
choose suitable positions, e.g., 5⋆ϵvand5⋆ϵhdo
not perform as well.
5.3 Statistical Methods
To clarify the difference between original models
and their conjugate models, we took the highest
mean as the best result, with the standard devi-
ation as a secondary judgement, and ultimately
two-sample t-tests (See Table 6 in Appendix) are
conducted to decide whether two similar results can
be considered statistically equivalent and which is
the best.
The two-sample t-test estimates if two popula-
tion means are equal. Here we use the t-test to
judge if the Time or MRR means of two models
are equal. We set significance level α= 0.05, and
the null hypothesis assumed that the two data sam-
ples are from normal distributions with unknown
and unequal variances. (h, p)means the result h
andp-value of the hypothesis test. h= 1,0.h= 1
means rejection to the null hypothesis at the sig-
nificance level α.h= 0 indicates the failure to
reject the null hypothesis at the significance level
α.p∈[0,1]is a probability of observing a test
statistic as extreme as, or more extreme than, the
observed value under the null hypothesis. A small
pvalue suggests suspicion on the validity of the
null hypothesis.
To prepare data for the t-tests, experiments
onComplEx ,Compl ϵx,5⋆E,5⋆ϵ∇,5⋆ϵ♢and
5⋆ϵnare conducted 17 times each. Apart from
that, the 5⋆Er,5⋆ϵv,5⋆ϵhapparently perform
worse than the former six models, thus the t-tests
are not needed and their experiments are conducted
5 times each.
Most of our t-tests were done among the orig-
inal model and its conjugate models as the dis-
tribution differs significantly if the base model is
different. However, since the accuracies among
different models are similar on the YAGO3-10 and
WN18 datasets, we did several supplementary t-
tests (indicated in italics ). The supplementary t-
tests showed that the distributions are different in-
deed when based on different original models even
though they appear to be similar. On the contrary,
there exist similar distributions among the results
distribution of the original model and its conjugate
model.
5.4 Advantages of Parameter Sharing
Approching for the best accuracy in link predic-
tion task has the trade off of misinformation effect
or inevitable high memory and time costs. Our
parameter-sharing method by using half conjugate
parameters is very easy to apply and can help con-
trol these costs, and potentially no trade off.
The original ComplEx and5⋆Eeach has their
own strength in the perspective of accuracy on dif-
ferent datasets; while ComplEx costs much more
memory and time than 5⋆Ewhen compared under
similar accuracy.
Our conjugate models consume less memory
and time, and not inferior to the original models in
accuracy, which shows that our parameter-sharing
method makes a complex number represented KGE
model superior to itself.
6 Conclusions
We propose using shared conjugate parameters for
transformations, which suffices to accurately repre-
sent the structures of the KG.Our method can help scaling up KG with less
carbon footprints easily: first, it reduces parame-
ter size and consumes less or at least comparable
training time while achieving consistent accuracy
as the non-conjugate model, including reaching
state-of-the-art results; second, it is easily general-
izable across various complex number represented
models.
7 Future Work
We would like to deal with the interpretation of the
linear constrain of our method. For example, to ex-
plore the effect of this method on different relation
patterns. Moreover, many KG applications like the
work done by Hongwimol et al. (2021) regard vi-
sual appeal as important, where appropriate visuals
can better convey the points of the data and facil-
itate user interaction. We can see that the vector
representations of transformed entities using this
method have more substantial geometric constrains
(See transformed entities illustrated in Figure 1).
We want to explore if our method can obtain better
KG visualization.
Acknowledgements
This work was supported by JSPS KAKENHI
Grant Numbers 21H05054.
References
Ivana Balazevic, Carl Allen, and Timothy Hospedales.
2019. Multi-relational poincaré graph embeddings.
InAdvances in Neural Information Processing Sys-
tems, volume 32. Curran Associates, Inc.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in Neural Information
Processing Systems , volume 26. Curran Associates,
Inc.
Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic
Sala, Sujith Ravi, and Christopher Ré. 2020. Low-
dimensional hyperbolic knowledge graph embed-
dings. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
6901–6914, Online. Association for Computational
Linguistics.
Tim Dettmers, Pasquale Minervini, Pontus Stenetorp,
and Sebastian Riedel. 2018. Convolutional 2d knowl-
edge graph embeddings. In AAAI .
Katsuhiko Hayashi and Masashi Shimbo. 2017. On the
equivalence of holographic and complex embeddings
for link prediction. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers) , pages 554–559,
Vancouver, Canada. Association for Computational
Linguistics.
Pollawat Hongwimol, Peeranuth Kehasukcharoen, Pa-
sit Laohawarutchai, Piyawat Lertvittayakumjorn,
Aik Beng Ng, Zhangsheng Lai, Timothy Liu, and
Peerapon Vateekul. 2021. ESRA: Explainable scien-
tific research assistant. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Con-
ference on Natural Language Processing: System
Demonstrations , pages 114–121, Online. Association
for Computational Linguistics.
Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun
Zhao. 2015. Knowledge graph embedding via dy-
namic mapping matrix. In Proceedings of the 53rd
Annual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers) , pages 687–696, Beijing, China. Asso-
ciation for Computational Linguistics.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015. Learning entity and relation embed-
dings for knowledge graph completion. Proceedings
of the AAAI Conference on Artificial Intelligence ,
29(1).
Mojtaba Nayyeri, Sahar Vahdati, Can Aykul, and
Jens Lehmann. 2021. 5* knowledge graph embed-
dings with projective transformations. Proceedings
of the AAAI Conference on Artificial Intelligence ,
35(10):9064–9072.
Maximilian Nickel, V olker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In ICML .
Natasha Noy, Yuqing Gao, Anshu Jain, Anant
Narayanan, Alan Patterson, and Jamie Taylor. 2019.
Industry-scale knowledge graphs: Lessons and chal-
lenges. Communications of the ACM , 62 (8):36–43.
Kristina Toutanova and Danqi Chen. 2015. Observed
versus latent features for knowledge base and text
inference. In Proceedings of the 3rd Workshop on
Continuous Vector Space Models and their Composi-
tionality , pages 57–66, Beijing, China. Association
for Computational Linguistics.
Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric
Gaussier, and Guillaume Bouchard. 2016. Com-
plex embeddings for simple link prediction. CoRR ,
abs/1606.06357.
Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao,
and Li Deng. 2014. Embedding entities and relations
for learning and inference in knowledge bases.
A Two-sample t-test
Time t-test (h, p)
ComplEx Compl ϵx 5⋆E 5⋆ϵ∇ 5⋆ϵ♢ 5⋆ϵn
ComplEx - (0, 2e-1) - - - -
Compl ϵx - - - - - -
5⋆E - - - (1, 8e-3) (0, 6e-1) (1, 4e-5)
5⋆ϵ∇ - - - - (0, 4e-1) (0, 4e-1)
5⋆ϵ♢ - - - - - (0, 2e-1)
5⋆ϵn - - - - - -MRR t-test (h, p)
ComplEx Compl ϵx 5⋆E 5⋆ϵ∇ 5⋆ϵ♢ 5⋆ϵn
- (1, 8e-17) - - - -
- - - - - -
- - - (1, 1e-12) (1, 1e-14) (1, 1e-10)
- - - - (1, 6e-3) (1, 1e-2)
- - - - - (1, 7e-6)
- - - - - -
(a) FB15K-237
Time t-test (h, p)
ComplEx Compl ϵx 5⋆E 5⋆ϵ∇ 5⋆ϵ♢5⋆ϵn
ComplEx - (0, 6e-1) - - - -
Compl ϵx - - - - - -
5⋆E - - - (1, 4e-11) - (1, 6e-3)
5⋆ϵ∇ - - - - - (1, 2e-2)
5⋆ϵ♢ - - - - - -
5⋆ϵn - - - - - -MRR t-test (h, p)
ComplEx Compl ϵx 5⋆E 5⋆ϵ∇ 5⋆ϵ♢5⋆ϵn
- (1, 3e-28) - - - -
- - - - - -
- - - (1, 9e-11) - (1, 2e-15)
- - - - - (1, 4e-21)
- - - - - -
- - - - - -
(b) WN18RR
Time t-test (h, p)
ComplEx Compl ϵx 5⋆E 5⋆ϵ∇ 5⋆ϵ♢5⋆ϵn
ComplEx - (0, 4e-1) - - - -
Compl ϵx - - - - - -
5⋆E - - - (1, 5e-47) - (1, 1e-46)
5⋆ϵ∇ - - - - - (0, 7e-1)
5⋆ϵ♢ - - - - - -
5⋆ϵn - - - - - -MRR t-test (h, p)
ComplEx Compl ϵx 5⋆E 5⋆ϵ∇ 5⋆ϵ♢5⋆ϵn
- (1, 3e-6) (1, 2e-7) (1, 7e-3) - (1, 3e-8)
- - - - - -
- - (1, 5e-4) - (0, 6e-1)
- - - - - (1, 3e-4)
- - - - - -
- - - - - -
(c) YAGO3-10
Time t-test (h, p)
ComplEx Compl ϵx 5⋆E 5⋆ϵ∇ 5⋆ϵ♢ 5⋆ϵn
ComplEx - (0, 1e-1) - - - -
Compl ϵx - - - - - -
5⋆E - - - (1, 2e-6) (1, 3e-5) (1, 8e-3)
5⋆ϵ∇ - - - - (1, 3e-2) (0, 1e-1)
5⋆ϵ♢ - - - - - (0, 5e-1)
5⋆ϵn - - - - - -MRR t-test (h, p)
ComplEx Compl ϵx 5⋆E 5⋆ϵ∇ 5⋆ϵ♢ 5⋆ϵn
- (0, 8e-2) - - - -
- - - - - -
- - - (1, 2e-21) (1, 2e-2) (1, 6e-21)
- - - - (1, 1e-23) (1, 3e-9)
- - - - - (1, 3e-22)
- - - - - -
(d) FB15K
Time t-test (h, p)
ComplEx Compl ϵx 5⋆E 5⋆ϵ∇ 5⋆ϵ♢ 5⋆ϵn
ComplEx - (0, 5e-1) - - - -
Compl ϵx - - - - - -
5⋆E - - - (1, 1e-7) (1, 4e-9) (1, 6e-9)
5⋆ϵ∇ - - - - (0, 2e-1) (0, 8e-2)
5⋆ϵ♢ - - - - - (0, 1e-1)
5⋆ϵn - - - - - -MRR t-test (h, p)
ComplEx Compl ϵx 5⋆E 5⋆ϵ∇ 5⋆ϵ♢ 5⋆ϵn
- (0, 8e-2) - - (1, 1e-17) (1, 1e-13)
- - - - (1, 5e-21) (1, 2e-16)
- - (1, 3e-14) (1, 5e-5) (0, 2e-1)
- - - - (1, 4e-15) (1, 5e-14)
- - - - - (1, 3e-5)
- - - - - -
(e) WN18
Table 6: t-test (h, p) of Time and MRR on FB15K-237, WN18RR, YAGO3-10, FB15K and WN18 datasets."
1911.02714,D:\Database\arxiv\papers\1911.02714.pdf,"In the context of learning from equivalence or subset queries, what is the relationship between the complexity of learning a concept and the length of the strings used to define the concept? How does the structure of the concept class influence this relationship?","The complexity of learning a concept from equivalence or subset queries is directly related to the length of the strings used to define the concept.  The concept class's structure, where each concept is defined by a string, allows for an adversarial oracle to exploit the learner's inability to determine which dimension a counterexample applies to, leading to an exponential increase in the number of queries required as the string lengths grow.","4 B. Caulﬁeld & S. Seshia
3 Simple Lower Bounds
This section introduces some fairly simple lower bounds. We will start with a lower-bound on
learnability from positive examples.
Proposition 1. There exist concepts C1andC2that are each learnable from constantly many
positive queries, such that C1×C2is not learnable from any number of positive queries.
Proof. LetC1:={{a},{a,b}}and setC2:={N,Z\N}. To learn the set in C1, pose two positive
queries to the oracle, and return {a,b}if and only if both aandbare given as positive examples.
To learnC2, pose one positive query to the oracle and return Nif and only if the positive example
is inN. An adversarial oracle for C1×C2could give positive examples only in the set {a}×N.
Each new example is technically distinct from previous examples, but there is no way to distinguish
between the sets {a}×Nand{a,b}×Nfrom these examples.
Now we will show lower bounds on learnability from EQ,Sub, and Mem . We will see later that
this lower bound is tight when learning from membership queries, but not equivalence or subset
queries.
Proposition 2. There exists a concept Cthat is learnable from #qmany queries posed to Q⊆
{Mem,EQ,Sub}such that learning Ckrequires (#q)kmany queries.
Proof. LetC:={{j}|j∈{0...m}}.
We can learn Cinmmembership, subset, or equivalence queries by querying j∈c∗,{j}⊆c∗,
or{j}=c∗, respectively.
However, a learning algorithm for Ckrequires more than mkqueries. To see this, note that Ck
contains all singletons in a space of size ( m+ 1)k.
So for each subset query {x}⊆c∗, if{j}̸=c∗, the oracle will return jas a counterexample,
giving no new information. Likewise, for each equivalence query {j}=c∗, if{j}̸=c∗, the oracle
can return jas a counterexample. Therefore, any learning algorithm must query x∈c∗,{x}⊆c∗,
or{x}=c∗for (m+ 1)k−1 values of x.
4 Learning From Superset Queries
This section introduces arguably the simplest positive result of the paper: when using superset
queries, learning cross-products of concepts is as easy as learning the individual concepts.
Like all positive results in this paper, this is accomplished by algorithm that takes an oracle for
the cross-product concept∏c∗
iand simulates the learning process for each sublearner Aiby acting
as an oracle for each such sublearner.
Proposition 3. IfQ={Sup}, then there is an algorithm that learns any concept∏c∗
i∈∏Ciin∑#Sup(c∗
i)queries.
Proof. Algorithm 1 learns ⊠Ciby simulating the learning of each Aion its respective class Ci. The
algorithm asks each Aifor superset queries Si⊇c∗
i, queries the product∏Sito the oracle, and
then uses the answer to answer at least one query to some Ai. Since at least one Aireceives an
answer for each oracle query, at most∑#Sup(c∗
i) queries must be made in total.
Modularity in Query-Based Concept Learning 5
We will now show that each oracle query results in at least one answer to an Aiquery (and that
the answer is correct). The oracle ﬁrst checks if the target concept is empty and stops if so. If no
concept class contains the empty concept, this check can be skipped. At each step, the algorithm
poses query∏Sito the oracle. If the oracle returns ’yes’ (meaning∏Si⊇∏c∗
i), thenSi⊇c∗
ifor
eachiby Observation 1, so the oracle answers ’yes’ to each Ai. If the oracle returns ’no’, it will give
a counterexample x= (x1,...,x k)∈∏c∗
i\∏Si. There must be at least one xi̸∈Si(otherwise,
xwould be in∏Si). So the algorithm checks xj∈Sjfor allxjuntil anxi̸∈Siis found. Since
x∈∏c∗
i, we knowxi∈c∗
i, soxi∈c∗
i\Si, so the oracle can pass xias a counterexample to Ai.
Note that once Aihas output a correct hypothesis ci,Siwill always equal ci, so counterexamples
must be taken from some j̸=i.
Result: Learn∏Cifrom Superset Queries
if∅∈Cifor someithen
Query∅⊇∏c∗
i;
if∅⊇∏c∗
ithen
return∅
fori= 1...kdo
SetSito initial subset query from Ai
while SomeAihas not completed do
Query∏Sito oracle;
if∏Si⊇c∗then
AnswerSi⊇c∗
ito eachAi;
Update each Sito new query;
else
Get counterexample x= (x1,...,x k)fori = 1 . . . k do
ifxi̸∈Sithen
Pass counterexample xitoAi;
UpdateSito new query;
fori = 1 . . . k do
ifAioutputscithen
SetSi:=ci;
return∏ci;
Algorithm 1: Algorithm for learning from Subset Queries
5 Learning From Membership Queries and One Positive Example
Ideally, learning the cross-product of concepts should be about as easy as learning all the individual
concepts. The last section showed this is not the case when learning with equivalence, subset, or
membership queries. However, when the learner is given a single positive example and allowed to
make membership queries, the number of queries becomes tractable. This is due to the following
simple observation.
Observation 2 Fix setsS1,S2,...,S k, pointsx1,x2,...,x kand an index i. Ifxj∈Sjfor all
j̸=i, then (x1,x2,...,x k)∈∏Siif and only if xi∈Si.
So, given a positive example p, we can see that p[j←xj]∈∏c∗
iif and only if xj∈c∗
j. This
fact is used to learn using subset or equivalence queries with the addition of membership queries
6 B. Caulﬁeld & S. Seshia
and a positive example. The algorithm is fairly similar for equivalence and subset queries, and is
shown as a single algorithm in Algorithm 2.
Proposition 4. IfQ∈{{Sub},{EQ}}and a single positive example p∈∏c∗
iis given, then∏c∗
i
is learnable in∑#qiqueries from Q(i.e., subset or equivalence queries) and k·∑#qimembership
queries.
Proof. The learning process for either subset or equivalence queries is described in Algorithm 2,
with diﬀerences marked in comments. In either case, once the correct cjis found for any j,Sjwill
equalcjfor all future queries, so any counterexamples must fail on an i̸=j.
We separately show for each type of query that a correct answer is given to at least one learner
Aifor each subset (resp. equivalence) query to the cross-product oracle. Moreover, at most k
membership queries are made per subset (resp. equivalence) query, yielding the desired bound.
Subset Queries: For each subset query∏Si⊆∏c∗
i, the algorithm either returns ‘yes’ or gives a
counterexample x= (x1,...,x k)∈∏Si\∏c∗
i. If the algorithm returns ’yes’, then by Observation
1Si⊆c∗
ifor alli, so the algorithm can return ’yes’ to each Ai. Otherwise, x̸∈∏c∗
i, so there is an
isuch thatxi̸∈c∗
i. By Observation 2 the algorithm can query p[j←xj] for alljuntil thexi̸∈c∗
i
is found.
Equivalence Queries: For each equivalence query∏Si=∏c∗
i, the algorithm either returns
’yes’, or gives a counterexample x= (x1,...,x k). If the algorithm returns ‘yes’, then a valid target
concept is learned. Otherwise, either x∈∏Si\∏c∗
iorx∈∏c∗
i\∏Si. In the ﬁrst case, as with
subset queries, the algorithm uses kmembership queries to query p[j←xj] for allj. Once the
xi̸∈c∗
iis found it is given to Aias a counterexample. In the second case, as with superset queries,
the algorithm checks if xj∈Sjfor alljuntil thexi̸∈c∗
iis found and given to Ai.
Modularity in Query-Based Concept Learning 7
Result: Learn∏c∗
i∈⊠Ci
fori= 1...kdo
SetSito initial query from Ai
while SomeAihas not completed do
Query∏Sito oracle;
ifThe Oracle returns ‘yes’ then
Pass ‘yes’ to each Ai;
// If Q ={EQ}each sublearner will immediately complete
else
Get counterexample x= (x1,...,x k);
ifx∈∏c∗
i\∏Sithen
// Only happens if Q = {EQ}
fori = 1 . . . k do
ifxi̸∈Sithen
Pass counterexample xitoAi;
UpdateSito new query from Ai;
else
fori = 1 . . . k do
Query p[i←xi]∈∏c∗
i;
ifp[i←xi]̸∈∏c∗
iandxi∈Sithen
Pass counterexample xitoAi;
UpdateSito new query from Ai;
EachAireturns some ci;
Return∏ci;
Algorithm 2: Algorithm for learning from Equivalence (or Subset) Queries, Membership Queries,
and One Positive Example
Finally, learning from only membership queries and one positive example if fairly easy.
Proposition 5. IfQ={Mem}and a single positive example p∈∏c∗
iis given, then∏c∗
iis
learnable in k·∑#Mem i(c∗
i)membership queries.
Proof. The algorithm learns by simulating each Aiin sequence, moving on to Ai+1onceAireturns
a hypothesis ci. For any membership query Mimade byAi,Mi∈c∗
iif and only if p[i←Mi]∈∏c∗
i
by Observation 2. Therefore the algorithm is successfully able to simulate the oracle for each Ai,
yielding a correct hypothesis ci.
6 Learning From Only Membership Queries
We have seen that learning with membership queries can be made signiﬁcantly easier if a single
positive example is given. In this section we describe a learning algorithm using membership queries
when no positive example is given. This algorithm makes O(max i{#Mem i(c∗
i)}k) queries, matching
the lower bound given in a previous section.
For this algorithm to work, we need to assume that ∅̸∈Cifor alli. If not, there is no way
to distinguish between an empty and non-empty concept. For example consider the classes C1=
{{1},∅}andC2={{j}|j∈N}. It is easy to know when we have learned the correct class in C1or
inC2using membership queries. However, learning from their cross-product is impossible. For any
8 B. Caulﬁeld & S. Seshia
ﬁnite number of membership queries, there is no way to distinguish between the sets ∅and{(1,j)}
for somejthat has yet to be queried.
The main idea behind this algorithm is that learning from membership queries is easy once a
single positive example is found. So the algorithm runs until a positive example is found from each
concept or until all concepts are learned. If a positive example is found, the learner can then run
the simple algorithm from Proposition 5 for learning from membership queries and a single positive
example.
Proposition 6. Algorithm 3 will terminate after making O(max i{#Mem i(c∗
i)}k)queries.
Proof. The algorithm works by constructing sets Siof elements and querying all possible elements of∏Si. We will get our bound of O(max i{#Mem i(c∗
i)}k) by showing the algorithm will ﬁnd a positive
example once|Si|>max i{#Mem i(c∗
i)}for alli. Since the algorithm queries all possible elements
of∏Si, it is suﬃcient to prove that Siwill contain an element of c∗
ionce|Si|>#Mem i(c∗
i). We
will now show this is true for each i.
Assume that the sublearner Aieventually terminates with the correct answer c∗
i. Let qi:=
(qi
1,qi
2,...)∈X∗
ibe the elements whose membership Aiwould query assuming it only received
negative answers from an oracle. If qiis ﬁnite, then there is some set Ni∈CithatAioutputs
after querying all elements in qi(and receiving negative answers). We will consider the cases when
c∗
i=Niandc∗
i̸=Ni
Assumec∗
i=Ni: Then by our assumption that∏c∗
i̸=∅,Nicontains some element ni. Note
that although sampling elements from a set might be expensive in general, this is only done for
Niand can therefore be hard-coded into the learning algorithm. The algorithm will start with
Si:={ni}, soSicontains an element of c∗
iat the start of the algorithm.
Assumec∗
i̸=Ni: By our assumption that Aieventually terminates, Aimust eventually query
someqi
j∈c∗
i(Otherwise, Aiwould only receive negative answers and would output Ni). So after
jsteps,Sicontains some element of c∗
i. Sincej <#Mem i(c∗
i), we have that Sicontains a positive
example once|Si|>#Mem i(c∗
i), completing the proof.
fori= 1...kdo
ifNiandniexist then
SetSi:={ni};
else
SetSi:={};
Setj:= 0;
while SomeAihas not terminated do
fori = 1, . . . , k do
ifAihas not terminated then
Get queryqi
jfromAi;
Pass answer ‘no’ to Ai;
SetSi:=Si∪{qi
j};
forx∈∏Sido
Query x∈∏c∗
i;
ifx∈∏c∗
ithen
Run Proposition 5 algorithm using xas a positive example;
j:=j+ 1;
Algorithm 3: Algorithm for Learning from Membership Queries Only
Modularity in Query-Based Concept Learning 9
7 Learning from Equivalence or Subset Queries is Hard
The previous section showed that learning cross products of membership queries requires at most
O(max i{#Mem i(ci)}k) membership queries. A natural next question is whether this can be done
for equivalence and subset queries. In this section, we answer that question in the negative. We will
construct a class Cthat can be learned from nequivalence or subset queries but which requires at
leastknqueries to learn Ck.
We deﬁne Cto be the set{c(s)|s∈N∗}, where c(s) is deﬁned as follows:
c(λ) :={λ}×N
c(s) := ({s}×N)∪csub(s)
csub(sa) := ({s}×(N\{a}))∪csub(s)
For example, c(12) = ({12}×N)∪({1}×(N\{2}))∪({λ}×(N\{1})).
An important part of this construction is that for any two strings s,s′∈N, we have that
c(s)⊆c(s′) if and only if s=s′. This implies that a subset query will return true if and only if the
true concept has been found. Moreover, an adversarial oracle can always give a negative example for
an equivalence query, meaning that oracle can give the same counterexample if a subset query were
posed. So we will show that Cis learnable from equivalence queries, implying that it is learnable
from subset queries.
We will prove a lower-bound on learning Ckfrom subset queries from an adversarial oracle. This
will imply that Ckis hard to learn from equivalence queries, since an adversarial equivalence query
oracle can give the exact same answers and counterexamples as a subset query oracle.
Proposition 7. There exist algorithms for learning from equivalence queries or subset queries such
that any concept c(s)∈Ccan be learned from |s|queries.
Proof. (sketch) Algorithm 4 shows the learning algorithm for equivalence queries, and Figure 2 show
the decision tree. When learning c(s) for anys∈N∗, the algorithm will construct sby learning at
least one new element of sper query. Each new query to the oracle is constructed from a string
that is a substring of sIf a positive counterexample is given, this can only yield a longer substring
ofs.
Result: Learns C
Sets=λ;
while True do
Query c(s) to Oracle ifOracle returns ‘yes’ then
return c(s)
ifOracle returns (s′,m)∈c∗\c(s)then
Sets=s′;
ifOracle returns (s,m)∈c(s)\c∗then
Sets=sm;
Algorithm 4: Learning Cfrom equivalence queries.
10 B. Caulﬁeld & S. Seshia
c(λ)
c(1) c(2) ...
c(11) c(12) ...(λ,1)
(λ,2)
(1,1)
(1,2)
Fig. 2: A tree representing Algorithm 4. Nodes are labelled with the queries made at each step, and
edges are labelled with the counterexample given by the oracle.
7.1 Showing Ckis Hard to Learn
It is easy to learn C, since each new counterexample gives one more element in the target string s.
When learning a concept,∏c(si), it is not clear which dimension a given counterexample applies
to. Speciﬁcally, a given counterexample xcould have the property that x[i]∈c(si) for alli̸=j,
but the learner cannot infer the value of this j. It must then proceed considering all possible values
ofj, requiring exponentially more queries for longer si. This subsection will formalize this notion
to prove an exponential lower bound on learning Ck. First, we need a couple deﬁnitions.
A concept∏c(si) isjustiﬁable if one of the following holds:
–For alli,si=λ
–There is an iand ana∈Nandw∈N∗such thatsi=wa, and thek-ary cross-product
c(s1)×···× c(w)×···× c(sk) was justiﬁably queried to the oracle and received a counterexample
xsuch that x[i] = (w,a).
A concept is justiﬁably queried if it was queried to the oracle when it was justiﬁable.
For any strings s,s′∈N∗, we writes≤s′ifsis a substring of s′, and we write s<s′ifs≤s′
ands̸=s′. We say that the sum of string lengths of a concept∏c(si) is of sizerif∑|si|=r
Proving that learning is hard in the worst-case can be thought of as a game between learner
and oracle. The oracle can answer queries without ﬁrst ﬁxing the target concept. It will answer
queries so that for any n, after less than knqueries, there is a concept consistent with all given
oracle answers that the learning algorithm will not have guessed. The speciﬁc behavior of the oracle
is deﬁned as follows:
–It will always answer the same query with the same counterexample.
–Given any query∏c(si)⊆c∗, the oracle will return a counterexample xsuch that for all i,
x[i] = (si,ai), andaihas not been in any query or counterexample yet seen.
–The oracle never returns ‘yes’ on any query.
The remainder of this section assumes that queries are answered by the above oracle. An example
of answers by the above oracle and the justiﬁable queries it yields is given below.
Modularity in Query-Based Concept Learning 11
Example 1. Consider the following example when k= 2. First, the learner queries ( c(λ),c(λ)) to the
oracle and receives a counter-example (( λ,1),(λ,2)). The justiﬁable concepts are now ( c(1),c(λ))
and (c(λ),c(2)). The learner queries ( c(1),c(λ)) and receives counterexample ((1 ,3),(λ,4)). The
learner queries ( c(λ),c(2)) and receives counterexample (( λ,5),(2,6)). The justiﬁable concepts are
now (c(1),c(4)), ( c(1·3),c(λ)), (c(5),c(2)) and ( c(λ),c(2·6)). At this point, these are the only possible
solutions whose sum of string lengths is 2. The graph of justiﬁable queries is given in Figure 3.
JQ:(c(λ),c(λ))
CE:((λ,1),(λ,2))
JQ:(c(1),c(λ))
CE:((1,3),(λ,4))JQ:(c(λ),c(2))
CE:((λ,5),(2,6))
JQ:(c(1·3),c(λ)) JQ:(c(1),c(4)) JQ:(c(5),c(2)) JQ:(c(λ),c(2·6))1≤s1 2≤s2
1,3≤s1 4≤s2 5≤s1 2,6≤s2
Fig. 3: The tree of justiﬁable queries used in Example 1. Each node lists the justiﬁable query (JQ)
and counterexample (CE) given for that query. The edges below each node are labelled with the
possible inferences about s1ands2that can be drawn from the counterexample.
The following simple proposition can be proven by induction on sum of string lengths.
Proposition 8. Let∏c(si)be a justiﬁable concept. Then for all w1,w2, . . . ,wkwhere for all i,
wi≤si,∏c(wi)has been queried to the oracle.
Proposition 9. If all justiﬁed concepts∏c(si)with sum of string lengths equal to rhave been
queried, then there are kr+1justiﬁed queries whose sum of string lengths equals r+ 1
Proof. This proof follows by induction on r. Whenr= 0, the concept∏c(λ) is justiﬁable.For
induction, assume that there are krjustiﬁable queries with sum of string lengths equal to r. By
construction, the oracle will always chose counterexamples with as-yet unseen values in N. So
querying each concept∏c(si) will yield a counterexample xwhere for all i,x[i] = (si,ai) for new
ai. Then for all i, this query creates the justiﬁable concept∏c(s′
j), wheres′
j=sjfor allj̸=iand
s′
i=c(si·ai). Thus there are kr+1justiﬁable concepts with sum of string lengths equal to r+ 1.
We are ﬁnally ready to prove the main theorem of this section.
Theorem 1. Any algorithm learning Ckfrom subset (or equivalence) queries requires at least kr
queries to learn a concept∏c(si), whose sum of string lengths is r. Equivalently, the algorithm
takesk∑#qisubset (or equivalence) queries."
1910.06067,D:\Database\arxiv\papers\1910.06067.pdf,"The paper describes a method for reconstructing MR images from undersampled k-space data using a generative adversarial network (GAN).  The authors mention that the model is robust to noise and achieves good performance on both noise-free and noisy images.  What specific design choices or training strategies contribute to the model's robustness to noise, and how do these choices differ from typical GAN training approaches?","The authors incorporate data augmentation with noisy images during training, which helps the model learn to reconstruct images even in the presence of noise.  This differs from typical GAN training, which often focuses solely on noise-free data.  Additionally, the use of a Wasserstein loss function, as opposed to a standard binary cross-entropy loss, further improves the training process and contributes to the model's robustness.","rated while training the generator. It is given by:
LMAE =E(∥G(x)−y∥1), (4)
where∥·∥ 1denotes theℓ1norm. Since the human vision
system is sensitive to structural distortions in images, it is
important to preserve the structural information in MR im-
ages, which is crucial for clinical analysis. Moreover, ℓ1
norm minimization of the pixel-wise difference does not
enforce textural and structural correctness, which may lead
to a reconstructed output of poor diagnostic quality. Su-
per resolution is another well-known inverse problem that
tries to interpolate both low frequency and high frequency
components from a low resolution image. Inspired by previ-
ous works on super resolution [37], a mean SSIM (mSSIM)
[35] based loss is incorporated in order to improve the re-
construction of ﬁne textural details in the images. It is for-
mulated as follows:
LmSSIM = 1−E
1
KK∑
j=1SSIM (Gj(x),yj)
,(5)
whereKis the number of patches in the image, and SSIM
is calculated as follows:
SSIM (u,v) =2µuµv+c1
µ2u+µ2v+c12σuv+c2
σ2u+σ2v+c2,(6)
whereuandvrepresent two patches, and µandσdenote
the mean and variance, respectively. c1andc2are small
constants to avoid division by zero.
The overall loss for training the generator is given by:
LGEN =α1LMAE +α2LmSSIM−α3E(D(G(x))),(7)
whereα1,α2, andα3are the weighting factors for various
loss terms.
3. Results and Discussion
3.1. Training settings
In this work, a 1-D Gaussian mask is used for undersam-
pling the k-space. Since the ZFR xis complex valued, the
real and imaginary components are concatenated and passed
to the generator in the form of a two channel real valued in-
put. The batch size is set as 32. The discriminator is updated
three times before every generator update. The threshold
for weight clipping is 0.05. The growth rate for the dense
blocks is set as 32, βis 0.2, and 12 RRDBs are used. The
number of ﬁlters in the last layer of each RRDB is 512.
Adam optimizer [16] is used for training with β1= 0.5and
β2= 0.999. The learning rate is set as 10−4for the genera-
tor and 2×10−4for the discriminator. The weighting fac-
tors areα1= 20 ,α2= 1, andα3= 0.01. The model1is
1The code is available at: https://puneesh00.github.io/
cs-mri/implemented using Keras framework [8] with TensorFlow
backend. For training, 4 NVIDIA GeForce GTX 1080 Ti
GPUs are used, each having 11 GB RAM.
3.2. Data details
For the purpose of training and testing, two different
datasets are used. We ﬁrst evaluate our model on T-1
weighted MR images of brain from the MICCAI 2013 grand
challenge dataset [18]. This is followed by another eval-
uation using MR images of knee (coronal view) from the
MRNet dataset [4]. The images in both the datasets are of
size256×256. In order to make the reconstructed out-
put robust to noise, data augmentation is carried out us-
ing images with 10% and 20% additive complex Gaussian
noise in the k-space. To make the set of training images
for the MICCAI dataset, 19 797 images are randomly taken
from the training set of the aforementioned dataset. Out
of these, noise is added to 6335 images, while the remain-
ing 13 462 images are used without any noise. In addition,
990 images are chosen from the 13 462 noise-free images,
and noise is added to them also, to get a total of 20 787
images for training. Among the noisy images, number of
images with 10% and 20% noise is equal. Thus, the set
contains 64.76% noise-free images, 30.48% noisy images
whose corresponding noise-free images are not present in
the training set, and 4.76% noisy images whose correspond-
ing noise-free images are present in the training set. To
make the set of training images for the MRNet dataset, a to-
tal of 12 500 images are taken from the training set, where
the aforementioned ratio of noise-free, overlapping noisy
images, and non-overlapping noisy images is maintained.
For testing, 2000 images are chosen randomly from the test
sets of the respective datasets. The tests are conducted in
three stages: using noise-free images, using images with
10% noise added to them, and using images with 20% noise.
3.3. Results
Table 1 summarizes the quantitative results to study the
effect of addition of various components to the model.
These results are reported for images taken from the MIC-
CAI dataset, in which 20% of the raw k-space samples are
retained. For all the cases, the generator is trained with
LGEN . In the ﬁrst case, the GAN model comprises of a
U-net generator (without RRDBs) and a patch-based dis-
criminator, with BN present throughout the network. It is
trained with Wasserstein loss. In the subsequent cases, the
use of RRDBs (without BN), followed by addition of BN
to RRDBs results in signiﬁcant improvement in PSNR of
the reconstructed outputs corresponding to noise-free im-
ages. As mentioned in Section 3.1, the loss function used in
training of all the networks takes the weight for LMAE as
20times the weight for LmSSIM . Such a ratio might cause
the model to focus more towards reducing the MAE. This
Table 1. Ablation study of the model.
Network Settings 1st2nd3rd4th5th
U-net G + patch-based D     
RRDBs     
BN in RRDBs     
Data augmentation     
Wasserstein loss     
Images PSNR (dB) / mSSIM
Noise-free 40.45 / 0.9865 41.39 / 0.9810 41.88 / 0.9829 41.80 / 0.9820 42.31 / 0.9841
10% noise added 38.25 / 0.9641 38.03 / 0.9624 38.03 / 0.9620 39.55 / 0.9728 39.80 / 0.9751
20% noise added 33.98 / 0.9217 34.01 / 0.9210 33.78 / 0.9180 37.21 / 0.9576 37.56 / 0.9619
Figure 3. Reconstruction results of the proposed method for 20% undersampled images, taken from the MICCAI dataset. (a) GT, recon-
struction results for (b) noise-free image, (c) image with 10% noise, and (d) image with 20% noise. The top right inset indicates the zoomed
in ROI corresponding to the red box. The bottom right inset indicates the absolute difference between the ROI and the corresponding GT.
The images are normalized between 0 and 1.
results in a more consistent performance of PSNR as the
training progresses. In the inference on noisy test images
(both 10% and 20%), the PSNR and mSSIM have relatively
less consistent performance as seen in the ablation studies.
One possible reason for this observation might be the large
number of nonlinearities present in the model, which give
the ability to learn a highly complex function. As mentioned
in [28], a highly complex function can have improved per-
formance for the noise-free case at the cost of slightly in-
creased sensitivity to noise as compared to its less complex
counterparts. The use of data augmentation with noisy im-
ages, in the ﬁfth case, results in signiﬁcantly better quantita-
tive results for the reconstruction of noisy images, as com-
pared to the ﬁrst three cases. This improves the robustness
of the model. In the fourth case, we train the network with
the conventional binary cross-entropy based adversarial loss
instead of Wasserstein loss. On comparing this case with
the ﬁfth case, it is evident that the use of Wasserstein loss
improves the training process. The settings of the ﬁfth case
are ﬁnalized and used for the subsequent results reported in
this work.
The qualitative results of the proposed method are shown
in Fig. 3 for 20% undersampled images taken from the
MICCAI dataset. It can be seen that the proposed method
is able to reconstruct the structural content in the image,
including many ﬁne details, successfully. This is also in-
dicated by the quantitative results shown in Table 1. Also,
the contrast of the reconstructed image looks very similar to
that of the GT. The reconstruction results for noisy inputs,as well as their differences with the corresponding GT, in-
dicate the robustness of the model.
Fig. 4 and Table 2 show the qualitative and quanti-
tative comparison of the proposed method, respectively,
with some state-of-the-art methods like DLMRI [26],
DeepADMM [34], BM3D [11], and DAGAN [33]. These
results are reported for images taken from the MICCAI
dataset, in which 30% of the k-space data is retained. The
comparison of the zoomed in ROI of the reconstructed out-
puts corresponding to the noise-free images, produced by
the aforementioned methods, as well as the difference with
the GT show that these methods are not able to fully pre-
serve the structural content present in the GT. It can be
seen that our method produces the least difference between
the ROI and the corresponding GT. Even in the case of
noisy images, our method is robust to the artifacts in the
image as it produces a smooth background, similar to the
GT, whereas other methods produce outputs with noisy ar-
tifacts as well as granularity. This can be seen in the re-
sults shown in the second and third row in Fig. 4. The
artifacts are more visible in the background of the zoomed
in ROI, whereas the granularity can be more easily seen in
the greyish boundary surrounding the brain structure as well
as in the black background. Moreover, the contrast is much
better preserved in our reconstructed outputs as seen in the
zoomed in ROI of all the three rows in Fig. 4.
The quantitative results also reinforce the effectiveness
of the proposed method. Table 2 shows that both the PSNR
and mSSIM for the proposed method are signiﬁcantly better
Figure 4. Qualitative results and comparison with previous methods for 30% undersampled images, taken from the MICCAI dataset. The
ﬁrst row shows reconstruction results for noise-free images, the second row shows reconstruction results for images with 10% noise, and
the third row shows reconstruction results for images with 20% noise. The top right inset indicates the zoomed in ROI corresponding to the
red box. The bottom right inset indicates the absolute difference between the ROI and the corresponding GT. The images are normalized
between 0 and 1.
Table 2. Quantitative comparison with previous methods using MICCAI dataset.
MethodNoise-free images 10% noise added 20% noise added Reconstruction/
PSNR (dB) mSSIM PSNR (dB) mSSIM PSNR (dB) mSSIM Test time (s)
DLMRI[26] 37.405 0.8732 34.144 0.6140 31.564 0.4346 25.7732
DeepADMM[34] 41.545 0.8946 39.078 0.8105 35.373 0.6000 0.3135
BM3D[11] 42.521 0.9764 37.836 0.7317 33.657 0.4947 6.8230
DAGAN[33] 43.329 0.9860 42.006 0.9814 39.160 0.9619 0.0063
Proposed 46.877 0.9943 42.338 0.9855 39.493 0.9740 0.0091
than the previous methods for noise-free as well as images
with 10% and 20% noise. All the previous methods, with
the exception of DAGAN, experience a signiﬁcant decline
in the PSNR and mSSIM values when their reconstruction
results for noise-free and noisy images are compared. This
proves that the reconstruction quality signiﬁcantly deteri-
orates on addition of noise as these methods lack robust-
ness. It is observed that the proposed method signiﬁcantly
outperforms the other methods in the noise-free setting, but
the improvement in the noisy setting is less signiﬁcant. As
mentioned before, this might be the result of the large num-
ber of nonlinearities present in the model, which allow the
learned function to be highly complex and obtain better per-
formance for the noise-free case at the cost of slightly more
sensitivity to noise [28]. However, the proposed augmen-
tation technique increases the robustness of the model, as
seen by the results presented in Table 2. Moreover, the re-
construction time of the proposed method is 9.06 ms per im-age, which can facilitate real-time reconstruction of MR im-
ages. DLMRI and BM3D have a much higher reconstruc-
tion time owing to the iterative fashion in which they obtain
the output. On the other hand, GAN based approaches have
a reconstruction time of the order of milliseconds as the test-
ing phase only involves a forward pass through the trained
generator. For the generator model of DAGAN and the pro-
posed approach, the FLOPs (total number of ﬂoating-point
operations) are 197M and 314M, respectively. The corre-
sponding FLOPS (ﬂoating-point operations per second) are
31.31G and 34.55G, which are calculated using the infer-
ence times listed in Table 2.
To demonstrate the generalization of the proposed ap-
proach, Table 3 and Fig. 5 show the quantitative and qual-
itative results for images taken from the MRNet dataset, in
which 30% of the k-space data is retained, as well as the
comparison with DAGAN [33], which obtained the closest
results on the MICCAI dataset. From Table 3, it is observed
that the proposed method outperforms DAGAN by a signif-
icant margin as it obtains better results for images with 20%
noise than those obtained by DAGAN on noise-free images.
It is also observed that the PSNR and mSSIM values ob-
tained for MRNet dataset are lower than those obtained for
MICCAI dataset. One possible reason for this might be the
larger black region present in the images in the MICCAI
dataset, which lacks any details or structural information.
Fig. 5 shows that the proposed method is able to obtain a
reconstructed output of high quality, as the difference be-
tween the GT and the reconstructed image is very low.
Table 3. Quantitative results and comparison using MRNet dataset.
MethodPSNR (dB) / mSSIM
Noise-free images 10% noise added 20% noise added
DAGAN[33] 31.529 / 0.8754 30.452 / 0.8182 28.267 / 0.7098
Proposed 34.823 /0.9412 33.522 /0.9167 32.034 /0.8884
Figure 5. Qualitative results and comparison for 30% undersam-
pled image, taken from the MRNet dataset. These are the recon-
struction results for noise-free images. The left inset indicates the
zoomed in ROI corresponding to the red box. The right inset indi-
cates the absolute difference between the ROI and the correspond-
ing GT. The images are normalized between 0 and 1.
Figure 6. Results of zero-shot inference. (a,d) GT, (b,e) ZFR, (c,f)
reconstruction results for noise-free image. The top right inset in-
dicates the zoomed in ROI corresponding to the red box. The bot-
tom right inset indicates the absolute difference between the ROI
and the corresponding GT. The images are normalized between 0
and 1.
We also tested the model trained on MR images of brainfrom the MICCAI dataset to reconstruct MR images of ca-
nine legs from the MICCAI 2013 challenge. Fig. 6 shows
the results of this zero-shot inference for images in which
20% of the k-space data is retained. Though no images of
canine legs were used for training, the model is able to faith-
fully reconstruct most of the structural content, and is able
to achieve average PSNR and mSSIM values of 41.28 dB
and 0.9788, respectively, for 2000 test images.
Further, we performed the zero-shot inference of the
model trained on 30% undersampled MR images of knee
from the MRNet dataset to reconstruct MR images of ca-
nine legs from the MICCAI 2013 challenge. It is able to
achieve average PSNR and mSSIM values of 43.79 dB and
0.9883, respectively, for 2000 test images.
Potential hallucination by GANs : Conventional GAN
training techniques may suffer from hallucination of de-
tails which could potentially be harmful for image diag-
nosis. The proposed scheme tries to control the hallucina-
tion of details by the use of pixel-wise MAE loss as well as
the mSSIM based loss in the image domain, both of which
try to ensure that the generated image is close to the GT.
LMAE tries to make sure that the low frequency details
are of the generated output as closely aligned to the ground
truth, whereas LmSSIM focuses on increasing the similar-
ity of generated and GT in terms of structural details. The
zero-shot inference is also helpful in pointing out that the
proposed GAN model has shown no sign of hallucination
on data samples taken from a distribution that is different
from the training distribution.
Additional Experiment for Super Resolution : To un-
derstand the usability of the proposed model in other com-
puter vision applications, we take super resolution (SR) as
a vision task, for which the model is not optimized, and
evaluate its performance on some commonly used datasets
for this task. All the hyperparameter values mentioned in
section 3.1 are maintained for this experiment, except α1,
which is set as 30. For training, patches of size 192×192
were used from the images present in the DIV2K [1] and the
Flickr2K datasets. These are the high resolution (HR) or the
GT images ( y). This variable image size is supported by the
fully convolutional architecture of the generator as well as
the discriminator, i.e. neither of the networks involve the
use of dense layers. The patch-based discriminator, which
classiﬁes sections of the image and not the entire image,
also allows the image size to be variable. For the task of
4×super resolution, the corresponding low resolution (LR)
images of size 48×48are used. These are obtained using
bicubic downsampling, which is a widely used degradation
model. As the size of the input and the output images is the
same in our framework, we use the images obtained using
bicubic interpolation on the low resolution images as the
input for the model ( x). For the purpose of testing, we use
Set5 [3] and Set14 [36] datasets. The quantitative results of
our model as well as the comparison with previous methods
for4×super resolution are presented in Table 4. The PSNR
and mSSIM values are calculated by considering only the Y
channel of the images, after converting them from RGB to
YCbCr colorspace, as mentioned in several previous works
on super resolution. The qualitative results obtained using
our approach are illustrated in Fig. 7 and Fig. 8. Although
the proposed framework is optimized for the task of CS-
MRI reconstruction, it gives satisfactory performance for
the super resolution task as well.
Table 4. Quantitative results and comparison with previous meth-
ods for the SR experiment.
MethodPSNR (dB) / mSSIM
Set5 Set14
Bicubic 28.42 / 0.8104 26.00 / 0.7027
SRCNN[7] 30.48 / 0.8628 27.50 / 0.7513
VDSR[15] 31.35 / 0.8830 28.02 / 0.7680
FSRCNN[9] 30.72 / 0.8660 27.61 / 0.7550
LapSRN[17] 31.54 / 0.8850 28.19 / 0.7720
EDSR[20] 32.46 / 0.8968 28.80 / 0.7876
Ours 31.63 / 0.8982 27.62 / 0.7784
Figure 7. Qualitative results of the SR experiment on an image
from Set5.
Figure 8. Qualitative results of the SR experiment on an image
from Set14.
4. Conclusion
In this paper, a novel GAN based framework has been
utilized for CS-MRI reconstruction. The use of RRDBs in
a U-net based generator architecture increases the amount
of information available. In order to preserve the high fre-
quency content as well as the structural details in the recon-
structed output, a patch-based discriminator and structural
similarity based loss have been incorporated. The use of
noisy images during training makes the reconstruction re-
sults highly robust to noise. The proposed method is able
to outperform the state-of-the-art methods, while maintain-
ing the feasibility of real-time reconstruction. In future, we
plan to analyze the performance of the proposed model fordifferent k-space sampling patterns. In order to improve the
reconstruction time, we plan to work on lightweight archi-
tectures. Further work may be carried out on devising regu-
larization terms that help to preserve the ﬁnest of details in
the reconstructed output.
Acknowledgement
We would like to thank the reviewers for their valuable
feedback and constructive comments.
References
[1] E. Agustsson and R. Timofte. NTIRE 2017 challenge on
single image super-resolution: Dataset and study. In The
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) Workshops , July 2017.
[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gener-
ative adversarial networks. In Proceedings of the 34th In-
ternational Conference on Machine Learning , volume 70,
pages 214–223, International Convention Centre, Sydney,
Australia, 06–11 Aug 2017.
[3] M. Bevilacqua, A. Roumy, C. Guillemot, and M. L. Alberi-
Morel. Low-complexity single-image super-resolution based
on nonnegative neighbor embedding. In BMVC , 2012.
[4] N. Bien, P. Rajpurkar, R. L. Ball, J. Irvin, A. Park, E. Jones,
et al. Deep-learning-assisted diagnosis for knee magnetic
resonance imaging: Development and retrospective valida-
tion of MRNet. PLoS Medicine , 15, 2018.
[5] A. Bora, A. Jalal, E. Price, and A. G. Dimakis. Compressed
sensing using generative models. In Proceedings of the 34th
International Conference on Machine Learning , volume 70,
pages 537–546, 2017.
[6] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Dis-
tributed optimization and statistical learning via the alternat-
ing direction method of multipliers. Foundations and Trends
in Machine Learning , 3(1):1–122, 2011.
[7] K. He C. Dong, C. C. Loy and X. Tang. Learning a deep con-
volutional network for image super-resolution. In The Euro-
pean Conference on Computer Vision (ECCV) , page 184199,
2014.
[8] F. Chollet et al. Keras. https://keras.io , 2015.
[9] C. Dong, C. C. Loy, and X. Tang. Accelerating the super-
resolution convolutional neural network. In The European
Conference on Computer Vision (ECCV) , 2016.
[10] D. L. Donoho. Compressed sensing. IEEE Transactions on
Information Theory , 52(4):1289–1306, April 2006.
[11] E. M. Eksioglu. Decoupled algorithm for MRI reconstruc-
tion using nonlocal block matching model: BM3D-MRI.
Journal of Mathematical Imaging and Vision , 56(3):430–
440, Nov 2016.
[12] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D.
Warde-Farley, S. Ozair, A. Courville, and Y . Bengio. Genera-
tive adversarial nets. In Advances in Neural Information Pro-
cessing Systems 27 , pages 2672–2680. Curran Associates,
Inc., 2014.
[13] E. Herrholz and G. Teschke. Compressive sensing principles
and iterative sparse recovery for inverse and ill-posed prob-
lems. Inverse Problems , 26(12):125012, nov 2010.
[14] P. Isola, J. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 5967–5976, July 2017.
[15] J. Kim, J. K. Lee, and K. M. Lee. Accurate image super-
resolution using very deep convolutional networks. In 2016
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , pages 1646–1654, 2016.
[16] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. In 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, Confer-
ence Track Proceedings , 2015.
[17] W. S. Lai, J. B. Huang, N. Ahuja, and M. H. Yang. Deep
laplacian pyramid networks for fast and accurate super-
resolution. In IEEE Conference on Computer Vision and
Pattern Recognition , 2017.
[18] B. Landman and S. Warﬁeld (Eds.). 2013 Diencephalon stan-
dard challenge.
[19] D. Lee, J. Yoo, and J. C. Ye. Deep residual learning for
compressed sensing MRI. In 2017 IEEE 14th International
Symposium on Biomedical Imaging (ISBI 2017) , pages 15–
18, April 2017.
[20] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee. Enhanced
deep residual networks for single image super-resolution.
In2017 IEEE Conference on Computer Vision and Pattern
Recognition Workshops (CVPRW) , pages 1132–1140, 2017.
[21] Y . Liu, J. F. Cai, Z. Zhan, D. Guo, J. Ye, Z. Chen, and X.
Qu. Balanced sparse model for tight frames in compressed
sensing magnetic resonance imaging. PLOS ONE , 10(4):1–
19, 2015.
[22] M. Lustig, D. Donoho, and J. M. Pauly. Sparse MRI: The
application of compressed sensing for rapid MR imaging.
Magnetic Resonance in Medicine , 58(6):1182–1195, 2007.
[23] M. Mardani, E. Gong, J. Y . Cheng, S. S. Vasanawala, G. Za-
harchuk, L. Xing, and J. M. Pauly. Deep generative adver-
sarial neural networks for compressive sensing MRI. IEEE
Transactions on Medical Imaging , 38(1):167–179, Jan 2019.
[24] M. Mirza and S. Osindero. Conditional generative adversar-
ial nets. ArXiv , abs/1411.1784, 2014.
[25] X. Qu, W. Zhang, D. Guo, C. Cai, S. Cai, and Z. Chen. It-
erative thresholding compressed sensing MRI based on con-
tourlet transform. Inverse Problems in Science and Engi-
neering , 18(6):737–758, 2010.
[26] S. Ravishankar and Y . Bresler. MR image reconstruction
from highly undersampled k-space data by dictionary learn-
ing. IEEE Transactions on Medical Imaging , 30(5):1028–
1041, May 2011.
[27] O. Ronneberger, P. Fischer, and T. Brox. U-net :Convolu-
tional networks for biomedical image segmentation. ArXiv ,
abs/1505.04597, 2015.
[28] P. Roy, S. Ghosh, S. Bhattacharya, and U. Pal. Effects of
degradations on deep neural network architectures. ArXiv ,
abs/1807.10108, 2018.[29] J. Schlemper, J. Caballero, J. V . Hajnal, A. N. Price, and D.
Rueckert. A deep cascade of convolutional neural networks
for dynamic MR image reconstruction. IEEE Transactions
on Medical Imaging , 37(2):491–503, Feb 2018.
[30] X. Wang, K. Yu, S. Wu, J. Gu, Y . Liu, C. Dong, Y . Qiao,
and C. C. Loy. Esrgan: Enhanced super-resolution genera-
tive adversarial networks. In The European Conference on
Computer Vision (ECCV) Workshops , pages 63–79, Septem-
ber 2018.
[31] Z. Wang and A. C. Bovik. Mean squared error: Love it or
leave it? A new look at signal ﬁdelity measures. IEEE Signal
Processing Magazine , 26(1):98–117, Jan 2009.
[32] S. Xu, S. Zeng, and J. Romberg. Fast compressive sens-
ing recovery using generative models with structured latent
variables. In IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pages 2967–2971,
May 2019.
[33] G. Yang, S. Yu, H. Dong, G. Slabaugh, P. L. Dragotti, X. Ye,
F. Liu, S. Arridge, J. Keegan, Y . Guo, and D. Firmin. DA-
GAN: Deep de-aliasing generative adversarial networks for
fast compressed sensing MRI reconstruction. IEEE Transac-
tions on Medical Imaging , 37(6):1310–1321, June 2018.
[34] Y . Yang, J. Sun, H. Li, and Z. Xu. Deep ADMM-Net for
compressive sensing MRI. In Advances in Neural Informa-
tion Processing Systems 29 , pages 10–18. Curran Associates,
Inc., 2016.
[35] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
Image quality assessment: From error visibility to struc-
tural similarity. IEEE Transactions on Image Processing ,
13(4):600–612, April 2004.
[36] R. Zeyde, M. Elad, and M. Protter. On single image scale-up
using sparse-representations. In Proceedings of the 7th Inter-
national Conference on Curves and Surfaces , page 711730,
Berlin, Heidelberg, 2010. Springer-Verlag.
[37] H. Zhao, O. Gallo, I. Frosio, and J. Kautz. Loss functions for
image restoration with neural networks. IEEE Transactions
on Computational Imaging , 3(1):47–57, March 2017."
2010.01526,D:\Database\arxiv\papers\2010.01526.pdf,"In a scenario where a machine learning model is used to analyze text data from various sources, what are the potential challenges and benefits of using a client-specific approach to adapt the model's predictions to the unique characteristics of each source?","A client-specific approach can address biases introduced by the unique characteristics of each data source, leading to improved accuracy. However, it requires careful consideration of the trade-off between model complexity and the potential for overfitting to specific clients.","Figure 2: Proportion of true and predicted entity labels
on OOD client NW/Xinhua. Similar trends observed
on other OOD domains (Figure 4 of Appendix).
Figure 3: Fraction Positive Predicted versus average
review length by baseline and KYC. Each dot/cross is
a domain and the dotted lines indicate the best ﬁt lines.
Statistical Signiﬁcance We verify the statistical
signiﬁcance of the gains obtained for the Sentiment
Analysis and Auto-complete tasks; the gains in
the case of NER are much larger than statistical
variation. Shown in Tables 4 and 5 are the sam-
ple estimate and standard deviation for three runs
along with the p value corresponding to the null
hypothesis of signiﬁcance testing. In both cases,
we see that the gains of KYC over the baseline are
statistically signiﬁcant.
Diagnostics We provide insights on why KYC’s
simple method of learning per-client label biases
from client sketches is so effective. One expla-
nation is that the baseline had large discrepancy
between the true and predicted class proportions
for several OOD clients. KYC corrects this dis-
OOD Clients Base KYC p-value
Electronics+Games 86.9(0.39) 87.7(0.33) 0.05
Industrial+Tools 87.6(0.19) 87.7(0.09) 0.14
Books+Kindle Store 83.4(0.03) 84.1(0.14) 0.01
CDs+Digital Music 82.4(0.24) 83.2(0.08) 0.02
Arts+Automotive 90.2(0.21) 90.4(0.31) 0.20
Average 86.1(0.16) 86.6(0.13) 0.02
Table 4: Statistical signiﬁcance of results on the OOD
clients by KYC for Sentiment Classiﬁcation. For every
entry contains the mean with the standard deviation in
parenthesisOOD Clients Base KYC p-value
sci.space 26.5(0.4) 26.4(0.2) 0.39
comp.hw 29.6(0.4) 29.0(0.3) 0.07
sci.crypt 29.7(0.4) 29.6(0.7) 0.46
atheism 28.3(0.2) 28.1(0.2) 0.14
autos 28.0(0.5) 27.9(0.4) 0.34
mideast 27.4(0.4) 27.3(0.4) 0.37
Average 28.2(0.2) 27.9(0.0) 0.04
Table 5: Statistical signiﬁcance of results on the OOD
clients by KYC for the Auto Complete task. For every
entry contains the mean with the standard deviation in
parenthesis
crepancy via computed per-client biases. Figure 2
shows true, baseline, and KYC predicted class pro-
portions for one OOD client on NER. Observe how
labels like date ,GPE,money andorg are under-
predicted by baseline and corrected by KYC. Since
KYC only corrects label biases, instances most
impacted are those close to the shared decision
boundary, and exhibiting properties correlated with
labels but diverging across clients. We uncovered
two such properties:
Ambiguous Tokens In NER the label of sev-
eral tokens changes across clients, E.g. tokens
likemillion, billion in ﬁnance clients like
NW/Xinhua are money 92% of the times whereas
in general only 50% of the times. Based on client
sketches, it is easy to spot ﬁnance-related topics
and increase the bias of money label. This helps
KYC correct labels of borderline tokens.
Instance Length For sentiment labeling, review
length is another such property. Figure 3 is a scat-
ter plot of the average review length of a client
versus the fraction predicted as positive by the base-
line. For most clients, review length is clustered
around the mean of 61, but four clients have length
>90. Length of review is correlated with label: on
average, negative reviews contain 20 words more
than positive ones. This causes baseline to under-
predict positives on the few clients with longer
reviews. The topics of the four outlying clients
(video games, CDs, Toys&Games) are related so
that the client sketch is able to shift the decision
boundary to correct for this bias. Using only nor-
malized average sentence length as the client sketch
bridges part of the improvement of KYC over the
baseline (details in Appendix C) implying that aver-
age instance length should be part of client sketch
for sentiment classiﬁcation tasks.
Salience TF Binary Sum- Architecture
Concat IDF BOW mary Deep Decomp MoE- g
OD 80.1 80.0 81.0 75.4 80.9 76.0 74.9
ID 86.0 85.9 77.8 81.8 85.9 85.0 79.8
Table 6: Comparing variant client sketches ( Sc) and
network architectures (⨁andYθ) of KYC in Fig 1.
Ablation Studies We explored a number of alter-
native client sketches and models for harnessing
them. We present a summary here; details are in
the Appendix C and D. Table 6 shows average F1
on NER for three other sketches: TF-IDF, Binary
bag of words, and a 768-dim pooled BERT em-
bedding of ten summary sentences extracted from
client corpus (Barrios et al., 2016). KYC’s de-
fault term saliency features provides best accuracy
with TF-IDF a close second, and embedding-based
sketches the worst. Next, we compare three other
architectures for harnessing gin Table 6: Deep ,
where module⨁after concatenating gandM
adds an additional non-linear layer so that now the
whole decision boundary, and not just bias, is client-
speciﬁc. KYC’s OOD performance increases a bit
over plain concat. Decompose , which mixes two
softmax matrices with a client-speciﬁc weight α
learned from g.MoE- g, which is like MoE but
uses the client sketch for expert gating. We observe
that the last two options are worse than KYC.
4 Conclusion
We introduced the problem of lightweight client
adaption in NLP service settings. This is a promis-
ing area, ripe for further research on more complex
tasks like translation. We proposed client sketches
and KYC: an early prototype server network for
on-the-ﬂy adaptation. Three NLP tasks showed
considerable beneﬁts from simple, per-label bias
correction. Alternative architectures and ablations
provide additional insights.
References
Ankur Bapna, Gokhan T ¨ur, Dilek Hakkani-T ¨ur, and
Larry Heck. 2017. Towards zero-shot frame seman-
tic parsing for domain scaling.
Federico Barrios, Federico L ´opez, Luis Argerich, and
Rosa Wachenchauzer. 2016. Variations of the simi-
larity function of textrank for automated summariza-
tion. Proc. Argentine Symposium on Artiﬁcial Intel-
ligence, ASAI .
S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira.
2006. Analysis of representations for domain adap-tation. In Proceedings of the 19th International Con-
ference on Neural Information Processing Systems ,
NIPS’06.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 confer-
ence on empirical methods in natural language pro-
cessing , pages 120–128. Association for Computa-
tional Linguistics.
Yitao Cai and Xiaojun Wan. 2019. Multi-domain sen-
timent classiﬁcation based on domain-aware embed-
ding and attention. In Proceedings of the 28th Inter-
national Joint Conference on Artiﬁcial Intelligence ,
pages 4904–4910. AAAI Press.
Fabio M Carlucci, Antonio D’Innocente, Silvia Bucci,
Barbara Caputo, and Tatiana Tommasi. 2019. Do-
main generalization by solving jigsaw puzzles. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 2229–2238.
Xilun Chen and Claire Cardie. 2018. Multinomial ad-
versarial networks for multi-domain text classiﬁca-
tion. In Proceedings of the 2018 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, Volume 1 (Long Papers) , pages 1226–1240.
H. Daum ´eIII. 2007. Frustratingly easy domain adapta-
tion. pages 256–263.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Qi Dou, Daniel Coelho de Castro, Konstantinos Kam-
nitsas, and Ben Glocker. 2019. Domain general-
ization via model-agnostic learning of semantic fea-
tures. In Advances in Neural Information Process-
ing Systems , pages 6447–6458.
Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao,
and Rui Yan. 2018. Style transfer in text: Explo-
ration and evaluation. In Thirty-Second AAAI Con-
ference on Artiﬁcial Intelligence .
Hongyu Gong, Suma Bhat, Lingfei Wu, Jinjun Xiong,
and Wen-mei Hwu. 2019. Reinforcement learning
based text style transfer without parallel training cor-
pus. arXiv preprint arXiv:1903.10671 .
Jiang Guo, Darsh J. Shah, and Regina Barzilay. 2018.
Multi-source domain adaptation with mixture of ex-
perts. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
Brussels, Belgium, October 31 - November 4, 2018 ,
pages 4694–4703.
Chen Jia, Xiaobo Liang, and Yue Zhang. 2019. Cross-
domain ner using cross-domain language modeling.
InProceedings of the 57th Annual Meeting of the
Association for Computational Linguistics , pages
2464–2474.
Guillaume Lample, Sandeep Subramanian, Eric Smith,
Ludovic Denoyer, Marc’Aurelio Ranzato, and Y-
Lan Boureau. 2019. Multiple-attribute text rewrit-
ing. In International Conference on Learning Rep-
resentations .
Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and
Alex Chichung Kot. 2018a. Domain generalization
with adversarial feature learning. CVPR .
Jing Li, Shuo Shang, and Ling Shao. 2020. Metaner:
Named entity recognition with meta-learning. In
Proceedings of The Web Conference 2020 , pages
429–440.
Juncen Li, Robin Jia, He He, and Percy Liang. 2018b.
Delete, retrieve, generate: A simple approach
to sentiment and style transfer. arXiv preprint
arXiv:1804.06437 .
Bill Yuchen Lin and Wei Lu. 2018. Neural adaptation
layers for cross-domain named entity recognition.
InProceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing , pages
2012–2022.
Zihan Liu, Genta Indra Winata, and Pascale Fung.
2020. Zero-resource cross-domain named entity
recognition. arXiv preprint arXiv:2002.05923 .
G´abor Melis, Tom ´aˇs Koˇcisk´y, and Phil Blunsom. 2020.
Mogriﬁer lstm. In International Conference on
Learning Representations .
Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019.
Justifying recommendations using distantly-labeled
reviews and ﬁne-grained aspects. In Proceedings
of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP) , pages 188–197, Hong
Kong, China. Association for Computational Lin-
guistics.
Jiaul H Paik. 2013. A novel tf-idf weighting scheme
for effective ranking. In SIGIR Conference , pages
343–352.
Vihari Piratla, Praneeth Netrapalli, and Sunita
Sarawagi. 2020. Efﬁcient domain generalization via
common-speciﬁc low-rank decomposition. arXiv
preprint arXiv:2003.12815 .
Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhut-
dinov, and Alan W Black. 2018. Style trans-
fer through back-translation. arXiv preprint
arXiv:1804.09000 .
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. Ontonotes: A uniﬁed relational
semantic representation. In Proceedings of the Inter-
national Conference on Semantic Computing , ICSC
’07, page 517–526, USA. IEEE Computer Society.
Alan Ramponi and Barbara Plank. 2020. Neural unsu-
pervised domain adaptation in nlp—a survey.Sebastian Ruder. 2019. Neural Transfer Learning for
Natural Language Processing . Ph.D. thesis, Na-
tional University of Ireland, Galway.
Darsh J Shah, Raghav Gupta, Amir A Fayazi, and Dilek
Hakkani-Tur. 2019. Robust zero-shot cross-domain
slot ﬁlling with example values. arXiv preprint
arXiv:1906.06870 .
Shiv Shankar, Vihari Piratla, Soumen Chakrabarti,
Siddhartha Chaudhuri, Preethi Jyothi, and Sunita
Sarawagi. 2018. Generalizing across domains via
cross-gradient training. In International Conference
on Learning Representations .
Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2017. Style transfer from non-parallel text
by cross-alignment. In Advances in neural informa-
tion processing systems , pages 6830–6841.
Emma Strubell, Ananya Ganesh, and Andrew Mc-
Callum. 2019. Energy and policy considera-
tions for deep learning in nlp. arXiv preprint
arXiv:1906.02243 .
Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.
2019. How to ﬁne-tune bert for text classiﬁcation?
InChinese Computational Linguistics , pages 194–
206, Cham. Springer International Publishing.
Haohan Wang, Zexue He, Zachary C Lipton, and Eric P
Xing. 2019. Learning robust representations by
projecting superﬁcial statistics out. arXiv preprint
arXiv:1903.06256 .
Zichao Yang, Zhiting Hu, Chris Dyer, Eric P Xing, and
Taylor Berg-Kirkpatrick. 2018. Unsupervised text
style transfer using language models as discrimina-
tors. In Advances in Neural Information Processing
Systems , pages 7287–7298.
NLP Service APIs and Models for Efﬁcient Registration of New Clients
(Appendix)
A Reproducibility/Implementation
Details
In this section we provide details about the dataset,
architecture and training procedures used for each
of the three tasks. We provide the datasets used,
code, hyperparameters for all the tasks in the code
submitted along with the submission.
A.1 NER
We use the standard splits provided in the
Ontonotes dataset (Pradhan et al., 2007). Our code-
base builds on the ofﬁcial PyTorch implementation
released by (Devlin et al., 2018). We ﬁnetune a
cased BERT base model with a maximum sequence
length of 128 tokens for 3 epochs which takes 3
hours on a Titan X GPU.
A.2 Sentiment Classiﬁcation
As described previously, we use the Amazon
dataset (Ni et al., 2019). For each review, we use
the standard protocol to convert the rating to a bi-
nary class label by marking reviews with 4 or 5
stars as positive, reviews with 1 or 2 stars as neg-
ative and leaving out reviews with 3 stars. We
randomly sample data points from each domain
to select 1000, 200 and 500 positive and nega-
tive reviews each for the train, validation and test
splits, respectively. We leave out the domains that
have insufﬁcient examples, leaving us with 22 do-
mains. We use the ﬁnetuning protocol provided
by the authors of (Sun et al., 2019) and use the un-
cased BERT base model with a maximum sequence
length of 256 for this task. We train for 5 epochs
(which takes 4 hours on a Titan X GPU) and use the
validation set accuracy after every epoch to select
the best model.
A.3 Auto Complete Task
We use 20NewsGraoup dataset while regarding
each content class label as a client. We remove
header, footer from the content of the documents
and truncate the size of each client to around 1MB.
We use word based tokenizer with a vocabulary
restricted to top 10,000 tokens and demarcate sen-
tence after 50 tokens. The reported numbers in
Table 3 are when using TF-IDF vector for domain
sketch. We diIn this section, we reportd not evalu-
ate other kinds of domain sketch on this task. Wetrain all the methods for 40 epochs with per epoch
train time of 4 minutes on a Titan X GPU.
We adopt the tuned hyperparameters correspond-
ing to PTB dataset to conﬁgure the baseline Melis
et al. (2020). Since the salience information from
the client sketch can be trivially exploited in per-
plexity reduction and thereby impede learning
desired hypothesis beyond trivially copying the
salience information, we project the sketch vector
to a very small dimension of 32 before fanning it
out to the size of vocabulary. We did not use any
non-linearity in Gφand also employ dropout on
the sketches.
B Details of MoE method (Guo et al.,
2018)
MoE employs a shared encoder and a client spe-
ciﬁc classiﬁer. We implemented their proposal to
work with our latest encoder networks. Our im-
plementation of their method is to the best of our
efforts faithful to their scheme. The only digres-
sion we made is in the design of discriminator: we
use a learnable discriminator module that the en-
coder fools while they adopt MMD based metric to
quantify and minimize divergence between clients.
This should, in our opinion, only work towards
their advantage since MMD is not sample efﬁcient
especially given the small size of our clients.
OOD ID
OOD Clients Base KYC Base KYC
BC/CCTV + BC/Phoenix 63.8 70.1 86.00 86.7
BN/PRI + BN/VOA 88.7 91.6 84.5 86.2
NW/WSJ + NW/Xinhua 73.9 79.2 80.8 82.2
BC/CNN + TC/CH 78.3 80.4 85.6 87.1
WB/Eng + WB/a2e 76.2 78.9 86.4 87.5
Average 76.2 80.0 84.7 85.9
Table 7: Performance on the NER task on the
Ontonotes dataset when using TF-IDF as the client
sketch.
C Results with Different Client Sketches
In this section we provide results on every OOD
split for the different client sketches described in
Section 3 along with more details.
•TF-IDF : This is a standard vectorizer used in
Information Retrieval community for document
similarity. We regard all the data of the client as
Figure 4: Proportion of true and predicted entity labels for different OOD clients (left) BC/Phoenix (right)
BC/CCTV .
OOD ID
OOD Clients Base KYC Base KYC
BC/CCTV + BC/Phoenix 63.8 75.3 86.0 79.3
BN/PRI + BN/VOA 88.7 90.5 84.5 78.7
NW/WSJ + NW/Xinhua 73.9 82.7 80.8 71.4
BC/CNN + TC/CH 78.3 80.3 85.6 79.9
WB/Eng + WB/a2e 76.2 76.4 86.4 79.6
Average 76.2 81.0 84.7 77.8
Table 8: Performance on the NER task on the
Ontonotes dataset when using Binary Bag of Words as
the client sketch.
OOD ID
OOD Clients Base KYC Base KYC
BC/CCTV + BC/Phoenix 63.8 61.5 86.0 83.0
BN/PRI + BN/VOA 88.7 82.3 84.5 85.2
NW/WSJ + NW/Xinhua 73.9 82.3 80.8 75.0
BC/CNN + TC/CH 78.3 72.5 85.6 83.2
WB/Eng + WB/a2e 76.2 78.3 86.4 82.5
Average 76.2 75.4 84.7 81.8
Table 9: Performance on the NER task on the
Ontonotes dataset when using sentence embddings av-
eraged over an extracted summary.
OOD ID
OOD Clients Base Sali- Avg Base Sali- Avg
ence Len ence Len
Electronics+Games 86.4 88.1 86.9 88.5 89.0 88.6
Industrial+Tools 87.4 87.6 88.3 88.2 88.9 88.8
Books+Kindle Store 83.5 84.6 84.5 88.0 88.9 89.0
CDs+Digital Music 82.5 83.0 83.1 89.0 89.0 89.0
Arts+Automotive 89.9 90.6 90.2 88.2 88.6 88.5
Average 86.0 86.8 86.6 88.4 88.8 88.8
Table 10: Accuracy on the Sentiment Analysis task
when using average review length as the client sketch.
Columns “Saliency” and “Avg Len” refer to using KYC
with the default saliency features and normalized re-
view lengths as client sketches, respectively.a document when computing this vector. The cor-
responding numbers using this sketch are shown
in Table 7 and are only slightly worse than the
salience features.
•Binary Bag of Words (BBoW) : A binary vec-
tor of the same size as vocabulary is assigned to
each client while setting the bit corresponding to
a word on if the word has occurred in the client’s
data. We notice an improvement on the OOD set
but a signiﬁcant drop in ID numbers as seen in
Table 8, 6. We attribute this to the strictly low rep-
resentative power of BBoW sketches compared
to the other sketches. The available train data
for NER is laced with rogue clients which are
not labeled and are instead assigned the default
tag: “O”. Proportion of KYC’s improvement on
this task comes from the ability to distinguish
bad clients and keeping their parameters from
not affecting other clients. This, however, is not
possible when the representative capacity of the
sketch is compromised. Thereby we do worse on
ID using this sketch but not on OOD meaning
the model does worse on the bad clients (which
are only part of ID, and not OOD).
•Contextualized Embedding of Summary : We
also experiment with using deep-learning based
techniques to extract the topic and style of a
client by using the “pooled” BERT embeddings
averaged over sentences from the client. Since
the large number of sentences from every client
would lead to most useful signals being killed
upon averaging, we ﬁrst use a Summary Extrac-
tor (Barrios et al., 2016) to extract roughly 10
sentences per client and average the sentence em-
beddings over these sentences only. This method
turns out to be ineffective in comparison to the
other client sketches, indicating that sentence em-
beddings do not capture all the word-distribution
information needed to extract useful correction.
•Average Instance Length: For the task of Senti-
ment Analysis, we also experiment with passing
a single scalar indicating average review length
as the client sketch in order to better understand
and quantify the importance of average review
length on the performance of KYC. We linearly
scale the average lengths so that all train clients
have values in the range [−1,1]. As can be seen
in Table 10, this leads to a signiﬁcant improve-
ment over the baseline. In particular, the OOD
splits CDs + Digital Music and Books + Kindle
Store have reviews that are longer than the av-
erage and consequently result in improvements
when augmented with average length informa-
tion. The gains from review length alone are not
higher than our default term-saliency sketch indi-
cating that term frequency captures other mean-
ingful properties as well.
D Results with Different Model
Architectures
In this section we provide results for the different
network architecture choices described in Section 3
•Deep : The architecture used is identical to that
shown in Figure 1 barring⨁, which now con-
sists of an additional 128-dimensional non-linear
layer before the ﬁnal softmax transform Yθ.
•Decompose : The ﬁnal softmax layers is decom-
posed in to two. A scalar αis predicted from the
client sketch using Gφsimilar to KYC. The ﬁnal
softmax layer then is obtained through convex
combination of the two softmax layers using α.
Figure 5 shows the overview of the architecture.
•MoE- g: We use the client sketch as the drop-in
replacement for encoded instance representation
employed in Guo et al. (2018). The architecture
is sketched in Figure 6. As shown in Table 13,
this method works better than the standard MoE
model, but worse than KYC.
OOD ID
OOD Clients Base KYC Base KYC
BC/CCTV + BC/Phoenix 64.8 74.5 85.6 86.8
BN/PRI + BN/VOA 89.5 90.0 84.1 85.6
NW/WSJ + NW/Xinhua 74.4 80.6 80.2 92.8
BC/CNN + TC/CH 78.0 79.6 86.1 87.5
WB/Eng + WB/a2e 75.6 79.9 85.8 87.1
Average 76.5 80.9 84.4 86.0
Table 11: Performance on the NER task on the
Ontonotes dataset using KYC-Deep.lossy
Yθ +
Mθ
xα
GφSM 1
SM 2
Sc
Figure 5: Decompose overview:⨁indicates a
weighted linear combination. SM i,i∈{1,2}repre-
sent the softmax matrices which are combined using
weightsα.
OOD ID
OOD Clients Base KYC Base KYC
BC/CCTV + BC/Phoenix 64.1 56.0 85.6 86.3
BN/PRI + BN/VOA 89.6 89.9 84.6 85.5
NW/WSJ + NW/Xinhua 72.3 68.2 81.2 80.0
BC/CNN + TC/CH 78.5 77.5 85.9 86.6
WB/Eng + WB/a2e 75.5 71.0 86.1 86.7
Average 76.0 72.5 84.7 85.2
Table 12: Performance on the NER task on the
Ontonotes dataset using Decompose.
lossy
+
p1(y|x)p2(y|x)... pn(y|x)
Yθ1Yθ2 Yθn
Mθα
Gφ
Scx
Figure 6: MoE- goverview:⨁indicates a weighted
linear combination. p i(y|x)represents the ithexpert’s
predictions and αrepresents weights for expert gating.
OOD ID
OOD Clients Base KYC Base KYC
BC/CCTV + BC/Phoenix 64.8 74.7 85.6 84.0
BN/PRI + BN/VOA 89.5 88.3 84.1 83.6
NW/WSJ + NW/Xinhua 74.4 61.6 80.2 64.8
BC/CNN + TC/CH 78.0 73.7 86.1 82.1
WB/Eng + WB/a2e 75.6 76.3 85.8 84.4
Average 76.5 74.9 84.4 79.8
Table 13: Performance on the NER task on the
Ontonotes dataset using MoE- g."
2403.18192,D:\Database\arxiv\papers\2403.18192.pdf,"In the context of deep learning, how do techniques that select mini-batches based on sample difficulty affect the training process and model performance, particularly in multi-label classification settings?","These techniques enhance network precision and hasten training convergence by prioritizing samples that are more challenging for the model to classify, leading to more effective and robust decision boundaries. However, in multi-label settings, the complexity of measuring uncertainty for each label and aggregating these measures significantly increases computational demands.","4 Ao Zhou et al.
[15]implementsaperturbation-basedtechniqueforlabel-specificfeaturestability
within a probabilistically relaxed expected risk minimization framework.
The imbalanced approaches proposed for MLC can be divided into three cat-
egories: sampling methods, classifier adaptation, and ensemble approaches [29].
Sampling methods aim to balance the label distribution by either oversampling
[7,19,30] minority labels or undersampling [8,23] majority ones, thus aiming to
produce the more balanced versions of the training set before training. Classi-
fier adaptation [11] techniques modify existing algorithms to make them more
sensitivetolabelimbalance,oftenbyadjustingdecisionthresholdsorincorporat-
ing imbalance-aware loss functions. Ensemble approaches [20] combine multiple
models or algorithms to leverage their collective strength, often incorporating
mechanisms to specifically address label imbalance, such as weighted voting or
selective ensemble training focused on underrepresented labels.
In the field of multi-label learning, the existing batch selection algorithms
are only applicable to active multi-label learning tasks [5,10], which are different
from the scope of our multi-label classification. Active multi-label learning aims
to progressively select unlabeled samples with the maximum information for
manual annotation.
2.2 Hard Sample and Batch Selection in Single Label Data
Hard samples are crucial for deep learning as they drive the model to refine
its decision boundaries and improve generalization, making the learning process
more effectiveand robust [26,21,17]. Forexample, OHEM(Online HardExample
Mining)[26]focusesontrainingwithhardsamples,identifiedbytheirhighlosses,
and exclusively using these instances to update the model’s gradients.
Recentstudieshavepointedoutthattheperformanceofdeepneuralnetworks
depends heavily on how well the mini-batch samples are selected. Meanwhile, it
has been verified that neural networks converge faster with the help of intelligent
batch selection strategies [6,18,27]. Techniques that select mini-batches based on
sample difficulty have enhanced network precision and hastened training con-
vergence. For instance, Online Batch Selection [22] boosts training efficiency by
rankingsamplesaccordingtotheirrecentlossvaluesandadjustingtheirselection
probabilitywithanexponentialdecaybasedonrank.Thisapproachstrategically
prioritizes higher-loss samples for upcoming mini-batches. Further, Recency Bias
[27] targets samples with fluctuating predictions, using recent history to gauge
uncertainty and increase their sampling probability for upcoming batches. Ada-
Boundary [28] prioritizes samples near the decision boundary where the model
is uncertain, accelerating convergence by focusing on these critical samples. In
multi-label settings, the complexity of measuring uncertainty for each label and
aggregatingthesemeasuressignificantlyincreasescomputationaldemands.Addi-
tionally, the inherent sparsity and inter-label correlations in multi-label datasets
can render traditional uncertainty-based selection methods less effective, result-
ing in suboptimal batch choices.
Title Suppressed Due to Excessive Length 5
3 Proposed Method
3.1 Preliminaries
LetX=Rddenote the input space and Y={l1, l2, . . . , l q}denote the label
space with q labels. A multi-label instance is denoted as (xi, Yi), with xi∈ X
as the feature vector and Yi⊆ Yindicating the relevant labels set. A binary
vector yi= [yi1, yi2, . . . , y iq]of dimension q, with elements in {0,1}, represents
the set of labels Yi. Here, yij= 1implies that lj∈Yi, and yij= 0indicates
thatlj/∈Yi. The objective of multi-label classification is to learn a prediction
function f:X → 2Ybased on a dataset D={(xi, Yi)|1≤i≤n}. For any new
example xu∈ X, the function predicts a subset of labels f(xu)⊆ Yas relevant.
In multi-label data, the metric IRLbl[7] is commonly used to evaluate the
level of imbalance for a particular label. Let Cb
jbe the count of samples for which
the value of the j-th label is b∈ {0,1}. Then the metric IRLblis defined as:
IRLbl j=C1
max/C1
j (1)
where C1
maxdenotes the greatest number of instances for any label that is as-
signed a value of 1. MeanIR =1
qPq
j=1IRLbl jmeasures the average imbalance
across all labels. Let Lmrepresent the set of minority labels, defined by those
labels whose IRLbl jexceeds the MeanIR . A label ljis classified as a minority
ifIRLbl j> MeanIR and as a majority otherwise.
LetB∈Rn×qdenote a local imbalance matrix, where Bijsignifies the lo-
cal imbalance for instance xiregarding label lj, defined by the proportion of
neighboring instances that belong to a different class.
Bij=1
kP
(xm,ym)∈KxiJyij̸=ymjK, y ij= 1
0, otherwise(2)
where JπKrepresents the indicator function, which yields 1 if JπKis true and 0
otherwise, and Kxirepresents the k-nearest neighbors ( kNNs) of xibased on the
Euclidean distance.
3.2 Rank-Based Batch Selection
The rank-based batch selection [22] sorts samples by loss, assigns sample selec-
tion probabilities based on rank, and selects mini-batches accordingly. Given a
dataset of nsamples, the probability of selecting the i-th sample is:
p(i) =exp (log( se)/n)r(ℓi)
Pn
i=1exp (log( se)/n)r(ℓi)(3)
where r(ℓi)denotes the rank of the elements in ascending order within ℓ. Here
ℓ∈Rnis defined as the vector with its i-th element representing the model loss
of the i-th sample. Furthermore, serepresents the selection pressure, influencing
the disparity in probabilities between the most and least significant samples.
6 Ao Zhou et al.
Inmulti-labelclassificationneuralnetworks,thelossfunctionusuallyinvolves
several components. For example, the KL divergence is used by many models
[1,15,40], to quantify the difference between the distribution of input and latent
space. However, this loss function component is not directly related to the classi-
fication target. We instead utilize multi-label binary cross-entropy as the loss for
rank-based batch selection. The rationale behind this choice and its effectiveness
are thoroughly discussed and validated in Section 4.5.
3.3 Class Imbalance Aware Weighting
To increase the likelihood of minority label instances being included in batches.
we introduce a weight for each instance and incorporate it into ℓ. LetS∈Rn×q
represent the matrix that captures local imbalance, defined by:
Sij=

BijJBij<1KPn
i′=1Bi′jJBi′j<1Kif JBi′j<1K
−1, otherwise(4)
where normalization is applied to the local class imbalance of label jfor all
instances to emphasize labels with fewer samples. Meanwhile, JBi′j<1Kis
used to exclude the influence of outliers. Further, let ϵ∈Rnbe the imbalance
vector, with ϵi=P
lj∈LmSijJSij̸=−1Kfor each instance, accumulating local
imbalance from non-noisy minority labels. Given a vector α= [1,1,···,1]∈Rn,
the instance weight vector w∈Rnis defined as: w=α+ϵ. Then, upon
combining with w, the loss vector ℓcan be rewritten as:
ℓ=w⊙ℓ. (5)
where ⊙represents the Adam product. It’s important to highlight that ℓis solely
utilized for determining sample probability in the batch selection and does not
contribute to the model’s gradient update process. All data has a chance to be
selected during the training process to avoid introducing system bias
3.4 Incorporate Quantization Index
In Eq.3, the ranking-based method for sample selection probability tends to
magnify small differences, resulting in significant (disproportionate) shifts in
rankings and selection probabilities.
A feasible way to circumvent this is to exploit the quantization index [9]
to smooth the rank value and limit it within the range of n. The quantization
function Q(ℓi)3is given by:
Q(ℓi) =ℓi
∆
(6)
3⌈π⌉is upward rounding function
Title Suppressed Due to Excessive Length 7
where ∆is defined as the adaptive quantization step size, equivalent to ℓmax/n.
Here, ℓmaxrepresents the maximum element of ℓ. Combining Q(ℓi), the proba-
bility of sample selection in multi-label adaptive batch can be rewritten as:
p(i) =exp (log( se)/n)Q(ℓi)
Pn
j=1exp (log( se)/n)Q(ℓj)(7)
Fig. 2.The work flow of multi-label deep learning model training with adaptive batch
selection
Figure 2 shows the workflow of the model training with adaptive batch se-
lection, the red line indicates adaptability on a batch basis.
Before the training, the set Pis initialized to track each sample’s p(i). Mean-
while,themodelrequiresacontinuouswarm-upphaseof γepochstoalleviatethe
instability caused by initial random initialization. In adaptive batch selection,
each sample’s selection probability is linked to its quantization index q, calcu-
lated by ℓusing an adaptive step size ∆. Here, ℓmaxdenotes the highest loss
among all samples after the previous batch of training ends. After each batch,
we adjust the quantization index based on ℓand then update Pby recalculating
each sample’s p(i).
3.5 Variant of Adaptive Batch Selection Exploiting Label
Correlations
It is well-known that label correlations are significant in multi-label learning. La-
bel correlations can provide additional information, especially when some labels
have insufficient training samples. By considering label correlation, we propose
a chain-based adaptive batch selection method. First, we select the seed sample
based on the selection probability set P. Given that ljis the seed sample’s as-
sociated label with the maximum IRLbl, we employ the adjacency matrix Ato
identify the ⌈Card⌉(Cardrepresents the label cardinality) labels most closely
8 Ao Zhou et al.
related to lj4, constituting the set Lc. We then define Dcas the collection of
instances linked to the labels within Lc. From the second sample onward, selec-
tion relies on a new probability set Pc, defined as Pcor={p(i)|(xi,yi)∈Dc}.
The selected sample then becomes the new seed sample for the next round of
selection. This process is repeated until it reaches the batch size.
3.6 Convergence Guarantee
Adaptive batch selection methods typically meet Adam’s convergence standards,
assuming their sampling distributions are strictly positive andgradient esti-
mates unbiased . We theoretically demonstrate that our approach meets these
key convergence conditions and the Proofcan be found in the appendix of
supplementary materials.
4 Experiments and Analysis
4.1 Experiment Setup
Datasets We analyze thirteen multi-label datasets spanning text, image, and
bioinformatics domains, sourced from the MULAN repository [32].
name n d q Card Dens MeanIR domain name n d q Card Dens MeanIR domain
Corel5ka5000 499 374 3.52 0.01 189.57 images tmc2007 28596 490 22 2.15 0.10 15.16 text
rcv1subset3 6000 944 101 2.61 0.03 68.33 text bibtex 7395 1836 159 2.40 0.02 12.50 text
rcv1subset1 6000 944 101 2.88 0.03 54.49 text enron 1702 1001 53 3.38 0.06 9.93 text
rcv1subset2 6000 944 101 2.63 0.03 45.51 text yeast 2417 103 14 4.24 0.30 7.20 biology
yahoo-Arts 7484 2314 25 1.67 0.07 26.00 text LLOG-F 1460 1004 75 15.93 0.21 5.39 text
cal500 502 68 174 26.04 0.15 20.58 music scene 2407 294 6 1.07 0.18 1.25 images
yahoo-Business 11214 2192 28 1.47 0.06 16.90 text
Table 1. The multi-label datasets used in the experiments.
Characteristics and imbalance levels of these datasets are detailed in Table
1, including Card, the mean labels per instance associated, and Dens, the ratio
of Card to the overall label count.
Evaluation Metrics To assess the efficacy of the batch method in multi-label
classification, six commonly utilized evaluation metrics are adopted, comprising
Macro-F, Micro-F, Macro-AUC, Ranking Loss, Hamming Loss, and One Error.
Please refer to [31,38] for detailed definitions of these metrics.
4A∈Rq×qis the symmetric conditional probability matrix, which measures the
co-occurrence relationship between labels, and the definition can be found in the
appendix of supplementary materials.
Title Suppressed Due to Excessive Length 9
Base Classifier and Implementation Details We used five multi-label deep
models, namely C2AE [34], MPVAE [1], PACA [16], CLIF [14], and DELA [15].
We configure each model according to the parameter settings provided in the
original paper and its source code, including layer dimensions, activation func-
tions, and other specifics. For any hyperparameters not detailed, we standardize
configurations across batch selection methods to maintain consistency. For opti-
mization, Adam, with a batch size of 128, weight decay of 1e-4, and momentums
of 0.999 and 0.9, is employed. As for the adaptive batch selection parameters,
we used the best selection pressure se, obtained from se={2,8,16,64}, and set
the warm-up threshold γto 3. Technically, a small γis enough to warm up, but
to reduce the performance variance caused by randomly initialized, we use the
first three epochs as the warm-up period and share the model parameters for all
strategies during the warm-up period [28]. We employ five-fold cross-validation
to evaluate the above approaches on the 13 data sets. In each fold, we record
the results on the test set at the best epoch of the validation set. Our code can
be found in Anonymous GitHub 1.
4.2 Experimental Results
Table 2 shows the comparative average performance metrics of two different
batch selection techniques, adaptive batch selection, and random batch selec-
tion, in different models. ""Random"" denotes the use of random batch selection
with ""shuffle=True"", which refers to the batch selection used by these models.
Adaptive batch selection consistently outperformed random selection, achieving
the highest rankings across various evaluation metrics in most datasets. This
advantage was particularly pronounced in datasets with high imbalance, such as
Corel5k, rcv1subset1, rcv1subset2, rcv1subset3, as well as in those with a large
number of labels, such as Corel5k, bibtex, and cal500. Adaptive batch selection
significantly enhances the performance of embedding-based models like C2AE
and models employing sophisticated neural networks, such as the CLIF. These
results highlight our method’s broad effectiveness in deep learning models.
The results presented in Table 3, derived from the Wilcoxon signed-ranks test
[2] at a 0.05 significance level, conclusively indicate that our adaptive batch se-
lection method outperforms random batch selection with statistical significance.
4.3 Convergence Analysis
In this section, we add a ranking batch selection based on BCE loss (named
""Hard"") as a comparison from the ablation perspective. Compared with the
Adaptive method, Hard ignores imbalance weight wand does not employ the
quantization index. Figure 3 illustrates the convergence curves of three distinct
batch selection strategies, viewed from the perspectives of epochs, batches, and
time, along with the validation set performance curves evaluated using Macro-
AUC.
Epoch Perspective : As shown in Figure 3(a), the adaptive batch selection
outperforms the other strategies in the scene and bibtex dataset, converging to
10 Ao Zhou et al.
C2AEMacro-F ↑ Micro-F ↑ Macro-AUC ↑Ranking Loss ↓Hamming Loss ↓One Error ↓
Dataset Random Adaptive Random Adaptive Random Adaptive Random Adaptive Random Adaptive Random Adaptive
Corel5k 0.0702 0.0726 0.19430.1985 0.61650.6361 0.29220.2832 0.05330.0393 0.82020.8002
rcv1subset3 0.2659 0.2768 0.43870.4481 0.81360.8298 0.12870.1187 0.03620.0344 0.54230.5321
rcv1subset1 0.2611 0.2723 0.4565 0.4664 0.83960.8589 0.09300.0917 0.03990.0380 0.50810.4918
rcv1subset2 0.2505 0.2609 0.44630.4544 0.81570.8277 0.11140.1067 0.03690.0346 0.55050.5355
yahoo-Arts1 0.2007 0.2146 0.3973 0.4064 0.7108 0.7287 0.11880.1129 0.0528 0.0528 0.51920.5147
cal500 0.2473 0.2634 0.46410.4880 0.57810.5972 0.26100.2413 0.26160.2417 0.16150.1391
yahoo-Business1 0.2224 0.2243 0.41520.4176 0.7478 0.74550.0373 0.0379 0.0452 0.0446 0.82390.8199
tmc2007 0.4332 0.4405 0.56430.5895 0.86970.8711 0.07850.0764 0.07000.0685 0.27100.2691
bibtex 0.2683 0.2778 0.37360.3788 0.85940.8674 0.13180.1216 0.01940.0147 0.48780.4443
enron 0.2587 0.2692 0.5510 0.5622 0.6690 0.6698 0.16100.1496 0.09010.0840 0.37710.3354
yeast 0.4040 0.4275 0.65360.6567 0.70450.7100 0.1719 0.1723 0.2011 0.1981 0.25270.2362
LLOG-F 0.3021 0.3066 0.55560.5694 0.63750.6419 0.20110.1961 0.20750.1976 0.2340 0.2066
scene 0.7249 0.7368 0.71630.7277 0.93030.9361 0.09200.0892 0.09680.0942 0.26050.2508
MPVAEMacro-F ↑ Micro-F ↑ Macro-AUC ↑Ranking Loss ↓Hamming Loss ↓One Error ↓
Corel5k 0.1131 0.1155 0.21960.2311 0.70250.7054 0.2316 0.2322 0.0254 0.0245 0.6903 0.7009
rcv1subset3 0.3190 0.3262 0.47330.4751 0.89710.9051 0.07020.0697 0.03010.0296 0.42070.4182
rcv1subset1 0.3514 0.3605 0.46640.4780 0.89970.9071 0.06930.0681 0.03580.0349 0.42750.4148
rcv1subset2 0.3532 0.3595 0.48200.4854 0.90320.9082 0.06590.0628 0.0302 0.0300 0.4173 0.4112
yahoo-Arts1 0.3364 0.3528 0.46840.4780 0.74560.7492 0.13840.1341 0.05720.0565 0.50950.5039
cal500 0.2026 0.2084 0.36640.3783 0.53650.5379 0.3183 0.3205 0.2394 0.2381 0.36620.3648
yahoo-Business1 0.3409 0.3580 0.46700.4774 0.78650.7923 0.04520.0421 0.01790.0161 0.82350.8202
tmc2007 0.5154 0.5242 0.62750.6326 0.89180.8947 0.06120.0606 0.0700 0.0696 0.2583 0.2528
bibtex 0.3395 0.3385 0.4522 0.4543 0.87240.8740 0.10450.1026 0.01450.0144 0.41010.4041
enron 0.3007 0.2964 0.5531 0.5548 0.7251 0.7241 0.1418 0.1390 0.08380.0832 0.30410.2967
yeast 0.4434 0.4486 0.62350.6261 0.69490.7013 0.19330.1907 0.2178 0.2182 0.2708 0.2710
LLOG-F 0.4179 0.4297 0.55310.5635 0.76670.7692 0.23870.2360 0.19150.1908 0.30700.2796
scene 0.7702 0.7833 0.76080.7762 0.94620.9484 0.06930.0655 0.08350.0782 0.21060.1965
PACAMacro-F ↑ Micro-F ↑ Macro-AUC ↑Ranking Loss ↓Hamming Loss ↓One Error ↓
Corel5k 0.1447 0.14310.2372 0.2359 0.7468 0.7543 0.16180.1587 0.02820.0277 0.6982 0.7047
rcv1subset3 0.3500 0.3576 0.48700.4899 0.90910.9165 0.05260.0507 0.03140.0306 0.41250.4077
rcv1subset1 0.3540 0.3586 0.45870.4618 0.9066 0.9099 0.0528 0.0515 0.03670.0364 0.43010.4243
rcv1subset2 0.3512 0.3547 0.4893 0.4856 0.9067 0.9158 0.0520 0.0512 0.03100.0307 0.41380.4098
yahoo-Arts1 0.2273 0.2331 0.39810.4032 0.75230.7612 0.13650.1314 0.05240.0515 0.54730.5408
cal500 0.1215 0.1286 0.37050.3721 0.58210.5881 0.23920.2345 0.19570.1930 0.11790.1157
yahoo-Business1 0.3219 0.3207 0.4940 0.5041 0.82360.8279 0.0349 0.0368 0.0155 0.0134 0.81190.8091
tmc2007 0.4890 0.4902 0.60400.6132 0.88370.8849 0.06450.0642 0.07490.0743 0.27780.2726
bibtex 0.2995 0.3008 0.41010.4128 0.84760.8528 0.0923 0.0926 0.0157 0.0128 0.45240.4497
enron 0.3051 0.3071 0.57620.5781 0.73510.7391 0.11350.1095 0.0845 0.0845 0.28010.2761
yeast 0.4180 0.4232 0.64350.6476 0.70960.7097 0.17370.1721 0.20510.2010 0.23910.2382
LLOG-F 0.3837 0.3996 0.56120.5643 0.71040.7131 0.20220.1968 0.17910.1783 0.23650.2318
scene 0.7712 0.7727 0.76400.7697 0.94420.9464 0.06630.0667 0.08430.0827 0.20150.2011
CLIFMacro-F ↑ Micro-F ↑ Macro-AUC ↑Ranking Loss ↓Hamming Loss ↓One Error ↓
Corel5k 0.1246 0.1269 0.23750.2379 0.75340.7683 0.1677 0.1762 0.0241 0.0240 0.6628 0.6660
rcv1subset3 0.3057 0.3079 0.47290.4738 0.92680.9279 0.06060.0602 0.03410.0294 0.41290.4113
rcv1subset1 0.3498 0.3506 0.47110.4746 0.92210.9307 0.06170.0576 0.03550.0338 0.41920.4072
rcv1subset2 0.3265 0.3284 0.47560.4824 0.92790.9329 0.06110.0610 0.03020.0299 0.42720.4166
yahoo-Arts1 0.3384 0.3412 0.47720.4797 0.7580 0.7535 0.1293 0.1207 0.05560.0561 0.49360.4928
cal500 0.1007 0.1066 0.35470.3620 0.58010.5982 0.23000.2259 0.19100.1895 0.12150.1196
yahoo-Business1 0.3765 0.37610.5097 0.5082 0.7801 0.7940 0.04300.0374 0.0163 0.0165 0.8099 0.8087
tmc2007 0.5337 0.53060.6451 0.6355 0.9048 0.9059 0.05520.0550 0.07000.0697 0.25400.2521
bibtex 0.3423 0.3467 0.46870.4696 0.89830.9011 0.08840.0841 0.01330.0132 0.3767 0.3846
enron 0.2903 0.2909 0.57280.5770 0.77000.7763 0.12320.1198 0.07580.0745 0.25040.2492
yeast 0.4141 0.4157 0.64820.6561 0.71070.7191 0.16790.1607 0.19950.1930 0.24290.2221
LLOG-F 0.3962 0.3958 0.5671 0.5661 0.7659 0.7703 0.21690.2138 0.20000.1691 0.23400.2326
scene 0.7606 0.7709 0.74730.7589 0.94180.9454 0.07250.0676 0.08910.0850 0.22840.2093
DELAMacro-F ↑ Micro-F ↑ Macro-AUC ↑Ranking Loss ↓Hamming Loss ↓One Error ↓
Corel5k 0.0755 0.0963 0.18120.2081 0.75560.7621 0.16010.1532 0.02160.0223 0.65260.6520
rcv1subset3 0.2673 0.2995 0.46400.4704 0.91750.9183 0.05690.0581 0.03050.0300 0.43070.4080
rcv1subset1 0.2842 0.3184 0.44240.4544 0.91790.9194 0.05760.0571 0.03620.0356 0.43550.4305
rcv1subset2 0.2868 0.3125 0.46800.4748 0.92030.9220 0.05400.0538 0.03120.0303 0.43580.4118
yahoo-Arts1 0.2735 0.2688 0.4617 0.4628 0.73870.7444 0.12270.1202 0.05470.0537 0.5131 0.5136
cal500 0.0666 0.0833 0.32830.3473 0.56330.5927 0.23220.2273 0.19040.1894 0.12150.1156
yahoo-Business1 0.3295 0.3196 0.4942 0.4913 0.7979 0.8062 0.03580.0343 0.01730.0167 0.8119 0.8153
tmc2007 0.5417 0.5495 0.65460.6550 0.91210.9171 0.05240.0517 0.06730.0667 0.25440.2504
bibtex 0.2956 0.3151 0.43990.4517 0.90420.9053 0.07670.0746 0.01330.0137 0.40240.4012
enron 0.2743 0.2688 0.5800 0.5931 0.77270.7748 0.11540.1108 0.07350.0722 0.2421 0.2426
yeast 0.3813 0.3859 0.64820.6551 0.70060.7121 0.16770.1631 0.19750.1933 0.23280.2160
LLOG-F 0.3749 0.3798 0.52080.5859 0.79090.7930 0.18650.1849 0.16030.1601 0.1752 0.1806
scene 0.7496 0.7684 0.73960.7585 0.94050.9449 0.0722 0.0726 0.0898 0.0834 0.23090.2183
Table 2. Results of batch selection strategies using five MLC Deep classifier with
Adam optimizer on different datasets
Title Suppressed Due to Excessive Length 11
C2AE MPVAE PACA CLIF DELA
Macro-F win(0.0002)win(0.0017)win(0.0046)win(0.0215)win(0.0171)
Micro-F win(0.0002)win(0.0002)win(0.0081) tie (0.0942) win(0.0012)
Macro-AUC win(0.0012)win(0.0005)win(0.0002)win(0.0034)win(0.0002)
Ranking Loss win(0.0012)win(0.0061)win(0.0171)win(0.0171)win(0.0046)
Hamming Loss win(0.0022)win(0.0012)win(0.0022)win(0.0081)win(0.0105)
One Error win(0.0022)win(0.0134)win(0.0171)win(0.0479) tie (0.1465)
Table 3. Summary of the Wilcoxon Signed-Ranks Test for adaptive against random
in terms of each evaluation metric at 0.05 significance level. p-values are shown in the
brackets
lower losses with fewer epochs. Hard batch selection method shows improved
convergence over random batch methods on the first three datasets but under-
performs on the bibtex datatset. The latter may be due to the small difference
in training sample loss, failing the loss ranking strategy between samples. The
convergence curve of the adaptive batch selection proves the effectiveness of the
quantization index. Specifically, we observe that training with random batch, the
loss on the bibtex dataset plateaued after only a few epochs. In contrast, adopt-
ing adaptive batching allowed the model to break through this ""bottleneck"",
leading to a continued decline in the loss convergence curve.
Batch Perspective : As shown in Figure 3(b), the adaptive batch shows
less volatility and a steadier loss reduction during training. In contrast, the hard
batch method is more volatile. The random batch method displays moderate
volatility. The stability offered by adaptive batch selection is advantageous since
large fluctuations, particularly upward ones in the last batch of an epoch, could
disrupt the entire training epoch and adversely affect validation performance.
Time Perspective : As shown in Figure 3(c), to evaluate performance gains
in training time, we measure the time required to achieve the same training loss
levels (based on the minimum training loss achieved by a random batch). For
example,onthescenedataset,randombatchselectionrequiresapproximately7.8
seconds to reach the minimum loss, while the adaptive batch selection only needs
5.7 seconds. On the scene, rcv1subset1, and bibtex datasets, the adaptive batch
reduces training time by 14.1 %, 10.8 %, and 84.6 %, respectively. Although the
adaptive batch increases training time by 37.2 %on the yahoo-Business dataset,
the overall convergence curve and evaluation metric results suggest that the
adaptive batch method offers notable time performance benefits. Despite the
extra time needed to compute and adjust the selection probability set during
batch selection, this benefit persists.
Validation Perspective : As shown in Figure 4, adaptive batch selection
notably enhances model performance. For the scene and rcv1subset1 datasets,
all approaches achieve similar Macro-AUC by training’s end. Yet, the adaptive
batch method attains peak validation epochs more swiftly than random selec-
tion strategies. More importantly, the adaptive batch significantly improved the
validation set’s Macro-AUC on both the yahoo-Business1 and bibtex datasets."
2211.14312,D:\Database\arxiv\papers\2211.14312.pdf,"How does the use of topological masking in the TopViT model contribute to the model's ability to handle low-quality input data, such as chromosomes with poor banding or morphology?","The topological masking strategy in TopViT preserves the 2D spatial relationships between patches in the original image, allowing the model to implicitly learn and leverage the original image's topology. This helps the model to better understand the context of individual patches and make more robust predictions even when dealing with low-quality input data.","Figure
1:
Illustration
of
the
algorithm
for
chromosome
identification
and
anomaly
detection.
For
both
tasks,
the
inputs
are
chromosome
images.
Those
images
are
partitioned
into
patches,
flattened
and
fed
to
Transformers-networks.
The
chromosome
identification
task
serves
as
a
pre-training
phase
for
learning
good
initial
weights
of
the
Transformer-based
image
encoder
that
are
then
fine-tuned
for
the
anomaly
detection
task.
We
tested
three
different
Vision
Transformers
(ViTs)
architectures:
(a)
regular
ViT(s),
(b)
Performers
applying
ReLU-kernels
from
the
XViT
library
of
scalable
ViTs
and
finally
(c)
Topological
Transformers
(TopViTs).
The
attention
matrix
is
never
explicitly
materialized
in
Performers.
Instead
the
components
Q’
and
K’
defining
its
factorization
are
disentangled
(the
order
of
computations
for
Performers’
attention
is
identified
by
the
red
block
in
the
XViT-Performer
module
in
the
figure).
TopViTs
modulate
regular
attention
matrix
A
(
using
ReLU-kernel)
via
element
wise
(Hadamard)
multiplication
with
the
topological
mask
M
.
The
entries
of
the
latter
encode
learnable
functions
of
the
distances
between
corresponding
patches
in
the
original
(unflattened)
image
rather
than
in
its
flattened
representation.
Therefore,
crucially
the
original
2D-topology
of
the
image
canvas
is
being
preserved
and
used
implicitly
to
enrich
the
flattened
representation
of
the
image.
Figure
2.
Chromosome
Identification.
A)
Accuracy
of
Inception,
Performer
(xViT-P),
ViT,
and
TopViT 
hyperparameter
model
sweeps
on
the
chromosome
identification
test
set.
B)
Confusion
matrix
of
best
TopVit
model 
on
the
test
set.
C)
Umap
projection
of
the
last
intermediate
layer
(prior
to
the
logits
layer)
for
the
TopViT
model
for 
chromosomes
from
the
test
set.
Each
point
is
colored
by
its
ground
truth
label
with
predictions
that
disagree
with
the 
training
label
enlarged
10-fold
and
marked
with
an
X.
D)
Accuracy
(left
y-axis)
and
percentage
of
remaining
data 
(right
y-axis)
after
removing
high
entropy
predictions
as
a
function
of
entropy
cutof f.
As
an
example,
removing 
predictions
with
higher
than
3.6x10
-06
entropy
increases
the
accuracy
to
99.9
by
removing
5%
of
the
data.

Results
An
overview
of
our
selected
modeling
approach
is
shown
in
Figure
1
.
Individual
chromosome
images
were
curated
from
42,049
karyograms
(~70%
normal,
split
into
an
80%
training,
10%
validation,
and
10%
test
sets)
prepared
from
9,207
bone
marrow
and
peripheral
blood
specimens
(94.5%
and
5.5%
respectively)
of
4,711
patients
with
hematological
malignancies.
These
chromosome
images
were
first
partitioned
into
“patches”
and
flattened
into
a
linear
vector
before
being
passed
into
a
Transformer
network
for
the
pretraining
task
of
predicting
individual
chromosomes.
Multiple
Transformer-based
variants
were
tested
(
Figure
2A
),
primarily
differing
in
the
nature
of
the
self-attention
encoder
blocks.
The
first
variant
utilized
the
standard
ViT
attention
as
described
previously
10
.
The
second
variant
used
a
memory
and
time
efficient
Performer-based
attention
11,12
.
The
final
ViT
variant,
TopViT,
uses
a
recently
published
topological
masking
strategy
to
encode
distances
between
patches
in
the
original
unflattened
image,
thereby
preserving
the
2D-topology
of
the
original
chromosome
image
13
.
The
highest
accuracy
model
for
chromosome
identification
was
then
fine-tuned
to
produce
individual
models
for
each
aberration-specific,
“normal”/”abnormal”
classification
task.
Chromosome
Identification
The
pretraining
task
for
normal
chromosome
identification
contained
roughly
equal
counts
for
all
chromosomes
(with
the
exceptions
of
X
and
Y).
Figure
2A
shows
performance
of
the
three
ViT
variants
described
in
Figure
1
and
a
CNN-based
(InceptionV3
14
)
model.
In
each
case,
a
variety
of
hyper-parameters
were
explored
with
the
models
ranked
by
their
best
performance
during
training
on
a
validation
set
(Figure
2A,
individual
points).
All
ViT-based
strategies
substantially
outperformed
the
Inception-baseline,
and
only
a
small
reduction
in
performance
was
observed
when
transitioning
from
standard
ViT
to
the
more
efficient
Performer-based
models
(Figure
2A,
blue
and
orange).
The
TopViT
model
(Figure
2A,
green)
yielded
far
more
performant
models
(a
nearly
50%
reduction
in
errors
on
average
compared
to
the
Inception
baseline)
and
was
selected
for
all
subsequent
analyses.
This
model
successfully
classified
normal
chromosomes
with
an
accuracy
of
99.3%.
We
next
examined
the
distribution
and
nature
of
errors
for
the
optimal
parameter
TopViT
model.
Figure
2B
shows
the
confusion
matrix
of
predicted
vs.
original
labels.
Most
discordant
predictions
were
between
chromosomes
of
similarly-sized
chromosomes
and
balanced
(roughly
symmetric
around
the
diagonal
in
Figure
2B).
While
some
of
the
more
frequently
swapped
predictions
are
consistent
with
those
one
might
expect
of
a
human
practitioner
due
to
similarities
in
appearance
(such
as
chr4/chr5
and
chr21/chr22),
others
(such
as
chr16/chr19
and
chr20/chr22)
seem
to
be
specific
to
the
model.
To
better
understand
the
relationship
between
discordant
and
concordant
predictions,
we
created
a
UMAP
embedding
of
the
last
layer
of
the
TopViT
model
for
all
test
examples
(Figure
2C).
Clustering
of
certain
discordancies
suggested
the
same
underlying
mechanism
for
systematic
swaps
seen
in
Figure
2B.
Generally,
chromosomes
were
fairly
well
separated
and
discordancies
were
enriched
at
the
periphery
(or
outside)
of
chromosome
clusters,
suggesting
that
the
model
considers
these
less
canonical
representations
of
the
predicted
chromosome.
To
explore
this,
we
computed
the
entropy
of
the
24-class
probability
vector
for
all
predictions
(Figure
2D).
Concordant
predictions
had
much
lower
entropy
than
the
discordant
predictions
indicating
that
the
model
had
significantly
higher
uncertainty
for
discordancies.
When
introducing
an
entropy-based
filter
(<
.00001)
we
were
able
to
substantially
improve
the
calling
accuracy
to
99.9%
while
only
dropping
3%
of
the
data
(Supplemental
Figure
1).
We
then
performed
expert,
manual
re-examination
of
all
discordancies
(both
low
and
high-entropy).
Most
(72%)
of
low-entropy
discordancies
were
actually
mislabels
in
the
training
set,
while
most
(90%)
of
high-entropy
discordancies
were
due
to
low-quality
data
(cross-overs,
folded
or
bent
chromosomes,
poor
banding,
poor
chromosome
morphology,
and
cellular
debris
(Supplemental
Figure
2).
Dropping
all
low-quality
chromosomes
and
correcting
mislabels
in
the
test
set
led
to
a
revised
accuracy
of
99.95%.
Chromosomal
Aberration
Detection
We
next
employed
the
method
for
the
more
challenging
task
of
aberration
detection.
To
evaluate
the
utility
of
the
model
in
this
revised
context,
we
first
employed
two
of
the
most
common,
medically
relevant
aberrations
in
our
cohort:
del(5q)
and
t(9;22)
(Table
1).
Deletion
of
the
long
arm
of
chromosome
5
[del(5q)]
is
associated
with
MDS
and
other
myeloid
neoplasms.
Translocation
between
the
long
arms
of
chromosomes
9
and
22
[t(9;22)]
is
associated
with
CML
and
acute
lymphoblastic
leukemia
(ALL),
and
more
rarely
with
other
acute
leukemias.
Figure
3.
Aberration
visualization
and
performance
of
t(9;22)
and
del(5q).
A)
Umap
projection
of
normal
and 
abnormal
chromosomes
5,
9,
and
22
using
the
chromosome
identification,
TopViT
model.
Abnormality
of 
chromosome
5
is
del(5q)
and
chromosomes
9
and
22
is
t(9;22).
B)
AUC
PR
and
ROC
AUC
performances
of
t(9;22) 
and
del(5q)
under
different
levels
of
downsampling
(x-axis).
Fine-tuned
models
are
shown
in
purple
and
randomly 
initialized
models
in
green.
To
start,
we
used
the
optimal
model
architecture
(TopViT)
with
randomly
initialized
weights
and
retrained
a
two-class
(normal/abnormal)
model
for
both
aberrations
(Figure
3B).
While
we
observed
reasonable
performance
for
del(5q)
and
the
derivative
chromosome
22
of
t(9;22),
i.e.
the
Philadelphia
(Ph)-chromosome,
the
model
exhibited
weaker
performance
for
the
derivative
9
chromosome
from
t(9;22)
[der(9)]
(Figure
3B,
rightmost
green
points
in
each
subplot).
Next,
we
reran
the
experiment
but
initialized
the
model
with
weights
from
the
chromosome
identification
model
and
then
fine-tuned
the
model
for
aberration
detection
(as
described
in
Figure
1A
and
Methods).
In
all
cases
performance
improved,
with
der(9)
AUC’s
becoming
comparable
to
that

of
del(5q)
and
der(22)
(Figure
3B,
rightmost
purple
points
in
each
subplot).
To
better
understand
the
intuition
for
this,
we
visualized
embeddings
of
healthy
and
abnormal
chromosomes
5,
9,
and
22
before
fine-tuning
(Figure
3A).
As
expected,
while
der(9)
was
more
mixed,
del(5q)
exhibited
some
separation
prior
to
fine-tuning,
indicating
that
the
latent
embedding
space
directly
captured
features
sensitive
to
the
missing
portion
of
chromosome
5.
To
model
low-frequency
aberrations,
we
downsampled
the
number
of
abnormal
examples
for
these
three
aberrant
chromosome
sets
(Figure
3B).
Performance
of
all
points
was
evaluated
on
the
same
held-out
test
set
as
that
used
for
the
complete
training
set.
Fine-tuned
models
consistently
maintained
high
AUC’s
even
with
10s
of
examples,
while
models
initialized
with
random
weights
required
hundreds
of
training
examples
(or
more).
Notably,
the
Ph-chromosome
[der(22)]
aberration
set
maintained
robust
fine-tuning
performance
with
only
three
examples,
with
performance
comparable
to
the
randomly
initialized
model
on
the
full
dataset,
Figure
3B.
Given
the
strong
performance
in
the
downsampling
task,
we
examined
other
infrequent
aberrations
from
our
patient
cohort.
In
addition
to
having
sufficient
examples
(at
least
10
patients)
to
evaluate
performance,
the
selected
aberrations
were
critical
for
diagnosis
and
prognosis
and
represented
diverse
rearrangements,
prevalence,
and
difficulty
levels
for
microscopic
assessment
(Table
1)
15–17
.
Figure
4,
shows
performance
across
the
four
additional
aberrations
tested.
Given
the
limited
sample
sizes,
the
points
here
represent
the
average
performance
across
a
10-fold
cross-validation
(split
by
specimen
--
all
cells
collected
from
a
patient
hospital
visit).
While
individual
performance
was
often
lower
than
for
the
more
frequent
del(5q)
and
t(9;22)
aberrations,
ROC
AUC
was
high
across
all
aberrations
and
median
and
mean
AUC
PR
exceeded
0.8
and
0.7,
respectively,
across
all
cases
(Figure
4A,B).
We
extended
the
analysis
to
simultaneously
test
for
their
partner
chromosomes
(i.e.,
the
other
member
of
the
pair
involved
in
a
translocation)
as
well
as
aggregating
within
cells
(purple)
and
across
all
cells
(blue/green)
from
a
patient
specimen
(Figure
4C
and
Supplemental
Figure
3A).
We
employed
simple
aggregation
techniques
(See
Methods)
with
either
the
max
probability
score
across
examples
or
the
second
best
probability
score
to
better
model
clinical
practice
of
requiring
two
aberrant
cells
to
define
a
clone
18
.
Substantial
improvement
was
seen
in
both
cases,
with
the
resulting
specimen
level
predictions
showing
zero
false
positives
with
full
or
near
full
recall
(Figure
4C).
We
then
applied
this
same
strategy
back
to
the
original
del(5q)
and
t(9;22)
patient
cohorts
(Figure
4C).
As
before,
a
notable
improvement
was
observed
in
both
aberrations.
In
particular,
when
accounting
for
both
chromosomes
in
the
t(9;22)
translocation
we
were
again
able
to
achieve
100%
precision
and
recall
(Figure
4C,
right
plot,
green
line).
As
a
final
step
we
attempted
to
identify
aberrant
chromosomes
de
novo.
For
each
chromosome
we
calculated
the
Euclidean
distance
between
its
embedding
vector
and
all
embedding
vectors
from
normal
chromosomes
in
the
training
set,
and
then
selected
the
distance
to
the
nearest
neighbor
in
the
training
set.
PR-curves
generated
from
these
distances
(Figure
4D,
Supplemental
Figure
4A)
had
performances
comparable
to
fine-tuning
across
all
aberrations
(Figure
4C,
Supplemental
Figure
3A).
In
particular,
del(5q)
and
t(9;22)
returned
perfect
accuracy,
while
two
of
the
other
abnormalities
(inv(3)
and
t(11;19))
showed
100%
precision
with
>90%
recall.
Figure
4:
Performance
of
rare
aberrations
and
precision-recall
curves
when
aggregating
predictions
across 
cells
and
specimens.
10-fold
cross-validation
performance
for
AUC
PR
(
A
)
and
ROC
AUC
(
B
).
Each
boxplot 
corresponds
to
a
distinct
cross-validation
set
for
each
chromosome
involved
in
an
aberration
and
is
colored
by 
aberration.
Individual
points
for
folds
and
averages
(black
diamond)
are
overlaid
on
each
boxplot.
(
C
)
Precision-recall 
curves
for
t(9;11),
t(11;19),
del(5q),
and
t(9;22),
at
the
individual
chromosome
image
level
(orange)
or
aggregated
at 
the
cell
(purple)
or
specimen
levels.
For
specimen
level
>=
1
abnormal
(blue)
the
single
highest
probability
abnormal 
chromosome
was
used,
for
specimen
level
>=
2
abnormals
(green)
the
second
highest
probability
abnormal 
chromosome
was
used.
Similarly ,
(
D
)
shows
precision-recall
for
de
novo
aberration
detection
based
on
distance
to 
N-nearest
chromosomes
(here
50th)
for
t(9;11),
t(11;19),
del(5q),
and
t(9;22),
respectively .

Table
1:
Summary
of
Chromosome
Aberrations
Evaluated
*
Aberration
Gene
fusion
or
rearrangement
Prevalence
15,17
Clinical
significance
16,19 
Difficulty
level
Aberration
type
†
Number
of
abnormal
Number
of
normal
Prognosis
Therapy
Specimens
Chr
§§
Specimens
Chr
§§
t(9;22)(q34;q11.2) 
BCR
::
ABL1
100%
25%
2-4%
<1%
CML
Adult
B-ALL
Pediatric
B-ALL
AML
and
MPAL
Good
Poor
Poor
Poor
TKI
TKI
TKI
TKI
suggested,
along
with
chemotherapy
Easy
Inter-chromosomal
160
1094
1242
9144
del(5q)
NA
100%
10-40%
MDS
w/
isolated
del(5q)
Myeloid
neoplasm
Good
Variable
††
Lenalidomide
Variable
Easy
to
moderate
Intra-chromosomal
unbalanced
251
1247
1482
7613
t(9;11)(p21.3;q23.3)
KMT2A
::
MLLT3
9-12%
2%
<1%
Pediatric
AML
Adult
AML
Other
myeloid
neoplasm
Intermediate
Chemotherapy;
HSCT
Moderate
Inter-chromosomal
11
39
1092
3513
inv(16)**
CBFB
::
MYH11
5-8%
AML
(frequency
decreases
with
age)
Good
Chemotherapy
Hard
Intra-chromosomal
balanced:
Pericentric
inversion
10
19
1091
3449
inv(3)(q21q26)
EVI1
rearrangement
1-2%
<1%
AML
MDS
Poor
Chemotherapy;
HSCT
Moderate
Intra-chromosomal
balanced:
Paracentric
inversion
16
23
1097
3469
t(11;19)(q23.3;p13.1)
t(11;19)(q23;p13.3)
KMT2A
::
ELL
KMT2A
::
MLLT1
<1%
Myeloid
neoplasm
Poor
Chemotherapy;
HSCT
Moderate
Inter-chromosomal
14
47
1095
3497
t(15;17)(q24;q21)***
PML
::
RARA
5-8%
AML
§
Good
ATRA-based
therapy
Easy
Inter-chromosomal
3
3
1081
3418
t(8;21)(q21;q22)***
RUNX1
::
RUNX1T1
1-5%
AML
Good
Chemotherapy
Easy
Inter-chromosomal
6
15
1081
3441
t(6;9)(p23;q34.1)***
DEK
::
NUP214
<2%
<1%
AML
Other
myeloid
neoplasm
Poor
Chemotherapy;
HSCT
Hard
Inter-chromosomal
2
2
1081
3409
*
Abbreviations:
AML,
acute
myeloid
leukemia;
APL,
acute
promyelocytic
leukemia;
ATRA,
all-trans
retinoic
acid; 
B-ALL,
B-lymphoblastic
leukemia/lymphoma;
CML,
chronic
myeloid
leukemia;
HSCT ,
hematopoietic
stem
cell 
transplantation;
MDS,
myelodysplastic
syndromes;
MPAL,
mixed
phenotype
acute
leukemia;
TKI,
Tyrosine
kinase 
inhibitors.
**
One
patient
with
t(16;16)
was
not
included
in
the
analysis
data
set
for
inv(16).
***
These
translocations
were
each
seen
in
fewer
than
10
patients
and
therefore
not
formally
analyzed
in
this
study .
†
“Balanced”
refers
to
aberration
being
apparently
balanced
during
microscopic
assessment
††
Depending
on
concurrent
abnormalities
§
Also
known
as
APL
§  §
Chromosomes
Figure
5.
Aberration
visualization
and
aberration
detection
performance
on
2021-2022
validation
dataset.
A) 
UMAP
visualization
of
normal
and
abnormal
chromosomes
5,
9,
and
22
from
2021-2022
dataset,
using
chromosome 
identification
TopViT
model
trained
on
2016-2020
dataset.
B)
ROC
AUC
for
rare
abnormalities
in
the
2021-2022 
dataset,
evaluated
on
10
models
fine-tuned
on
training
sets
from
2016-2020
data.
(
C
)
Precision-recall
curves
for 
t(9;11),
t(11;19),
del(5q),
and
t(9;22),
at
the
individual
chromosome
image
level
(orange)
and
aggregated
at
the
cell 
(purple)
or
specimen
levels.
For
specimen
level
>=
1
abnormal
(blue)
the
single
highest
probability
abnormal 
chromosome
was
used,
for
specimen
level
>=
2
abnormal
(green)
the
second
highest
probability
abnormal 
chromosome
was
used.
All
the
evaluations
are
on
2021-2022
dataset,
and
models
trained
on
training
sets
from 
2016-2020.
Similarly ,
(
D
)
shows
precision-recall
for
de
novo
aberration
detection
based
on
distance
to
N-nearest 
point
(here
50th)
from
chromosome
identification
model
.
To
evaluate
the
generalizability
of
our
aberration
detection
models,
we
used
an
entirely
independent
validation
set
derived
from
patient
samples
clinically
tested
between
2021
and
2022
.
For
these
evaluations
we
did
not
train
any
new
models
and
solely
assessed
the
generalizability
of
models
previously
trained
on
2016-2020
patient
data
(Figure
5).
Across
all
models
and
aberrations
we
had
high
precision
and
recall,
when
considering
specimen-level
detection
(100%
precision
and
recall
in
most
instances,
Figure
5CD,
Supplemental
Figure
3B,
Supplemental
Figure
4B).
The
de
novo
performance
on
the
2021-2022
dataset
(Figure
5D,
Supplemental
Figure
4B)
matched
that
of
the
2016-2020
dataset
(Figure
4D)
across
all
aberrations.
This
is
reinforced
by
the
clear
separation
between
normal
and
aberrant
chromosomes
seen
in
UMAP
projections
(Figure
5A)
for
our
most
frequent
aberrations
(t(9;22)
and
del5q).
For
rare
abnormalities,
where
we
had
smaller
training
sets
with
only
10s
of
examples,
the
fine-tuned
models
did
not
generalize
as
well
on
the
2021-2022
dataset,
likely
due
to
overfitting.

However,
the
de
novo
approach
was
able
to
identify
the
abnormal
chromosomes
on
more
recent
data
even
better
than
the
2016-2020
test
set.
This
suggests
that
the
de
novo
approach
is
more
robust
to
overfitting
and
can
generalize
better
to
new
data
for
rare
abnormalities.
At
the
specimen
level
considering
the
definition
of
“clonal”
per
standard
clinical
practice,
both
the
de
novo
and
fine-tuned
models
were
able
to
separate
normal
and
abnormal
chromosomes
for
all
patients
and
for
all
abnormalities
on
the
validation
dataset.
Discussion
Here,
we
present
a
novel
deep
learning
model
utilizing
Topological
transformers
that
yield
high-quality
models
for
chromosome
aberration
detection.
This
work
not
only
provides
highly
accurate
models
that
further
improve
the
accuracy
for
chromosome
identification
(with
99.3%
accuracy)
but
also
demonstrates
a
practical
approach
for
the
detection
of
chromosomal
aberrations
with
accuracies
between
97.8%
and
99.4%
at
the
individual
chromosome
level.
At
the
patient
sample
level,
taken
into
consideration
the
ISCN
definition
of
clonality
-
a
structural
aberration
has
to
be
seen
in
at
least
two
cells
to
be
called
clonal
-
our
models
demonstrated
100%
accuracy.
This
study
provides
functionality
clinicians
can
immediately
utilize
and
a
framework
to
potentially
broaden
karyotype
usage.
As
one
example,
karyotype
analysis
is
required
for
the
selection
of
appropriate
therapy
for
patients
with
AML,
meaning
that
with
the
current
labor-intensive
approach,
patients
with
an
explosive
disease
like
AML
often
need
to
wait
a
week
or
more
for
cytogenetic
results
before
they
can
start
therapy.
The
potential
rapidity
of
the
approach
described
here
would
be
of
major
clinical
benefit.
Further,
because
of
the
labor
involved,
currently
karyotyping
is
generally
limited
to
20
cells
per
sample.
The
ability
to
analyze
hundreds
of
metaphases
per
specimen
would
increase
the
sensitivity
of
the
assay
and
reveal
further
details
about
the
clonal
architecture
of
these
diseases.
Specifically,
rare
abnormalities
at
a
lower,
subclonal,
level
are
important
driving
forces
for
cancer
recurrence
after
initial
chemotherapy-induced
remission
20
.
These
data,
in
turn,
should
lead
to
improved
risk-stratification,
better
treatment
selection,
and
the
development
of
novel
methods
for
following
the
course
of
treatment.
The
quantitative
accuracy
improvements
(Figure
2)
are
substantial
but
only
tell
part
of
the
story.
Qualitatively,
the
models
handled
low-quality
input;
i.e.,
metaphase
chromosomes
at
400-band
level
or
less
from
bone
marrow
and
leukemic
blood
specimens.
Of
the
9207
total
specimens
used
in
this
study,
94.5%
were
bone
marrow
aspirate
and/or
bone
marrow
biopsy
specimens
while
5.5%
were
peripheral
blood
from
patients
with
hematological
malignancies.
In
contrast,
previously
published
studies
used
phytohemaglutinnin-stimulated
peripheral
blood
lymphocytes,
amniotic
fluid
samples,
or
normal
cells
from
bone
marrow
specimens,
which
generally
yield
chromosomes
with
better
morphology
and
higher
band
resolution.
We
are
encouraged
that
our
models
were
able
to
handle
more
challenging
samples
with
variable
sample
quality
.
Moreover,
expert
clinicians
evaluating
the
most
performant,
TopViT,
models
reported
a
substantially
better
experience.
While
the
InceptionV3,
ViT
and
XViT-P
models
missed
a
number
of
“easy”
cases,
discordancies
in
the
TopViT
model
were
primarily
due
to
low-quality
input
or
mislabeling
in
the"
2109.11301,D:\Database\arxiv\papers\2109.11301.pdf,"How do real-world active learning strategies address the challenge of annotator error-proneness, and what are the different types of annotator performance models used to address this challenge?","Real-world active learning strategies address annotator error-proneness by incorporating annotator performance models that estimate the quality of annotations provided by different annotators. These models can be categorized based on the type of annotator performance they assume, such as uniform, annotation-dependent, or query-dependent performance.","4
of them. An exemplary representation of the query given above
would beq={xn,xm}. The mathematical representations
of all possible queries are summarized in a set QX, which
depends on the underlying classiﬁcation problem and the set
of observed instances X[47]. Due to this dependency, we
can interpret the queries as random events, and Qdenotes the
associated random variable. In most cases, a query asks for
the class label of a speciﬁc instance such that we can deﬁne
QX=Xas query set.
The task of a real-world AL strategy is to generate a
sequence for the execution of the annotation process, which is
assumed to consist of countable distinct (time) steps. In other
words, a sequence answers the question: “Which annotator
has to answer which query at which time step?”. Accordingly,
we deﬁne a sequence as a function S∶N→P(QX×A),
such that (ql,am)∈S(t)induces an annotation of query ql
by annotator amat time step t∈N. The annotation be-
havior of an annotator can be modeled through a condi-
tional distribution Pr(Zm/divides.alt0Q=ql,t)from which z(t)
lm∈ΩZ
is drawn as annotation of annotator amfor queryql, i.e.,
z(t)
lm∼Pr(Zm/divides.alt0Q=ql,t). As a result, annotators are not com-
pulsorily deterministic in their decisions. Still, decisions might
also change throughout the annotation process, e.g., if an
annotator gets tired during the annotation process [48]. An
annotation process executed until the beginning of the time
steptaccording to a sequence Sleads to a data set
D(t)=/braceleft.alt2(ql,am,z(t′)
lm) /divides.alt2t′∈N∧∃t′<t∶(ql,am)∈S(t′)
∧z(t′)
lm∼Pr(Zm/divides.alt0Q=q,t′)/braceright.alt2 (2)
consisting of triplets of a query, an annotator, and an annota-
tion. We deﬁne the end of a sequence Sas the last time step
at which an annotation has been performed, i.e., where the
selection is empty:
tS=max({t/divides.alt0S(t)≠/uni2205∧t∈N}). (3)
On a data set D(t), a classiﬁcation model described by its
parametersθcan be trained. We denote the resulting param-
eters of the classiﬁcation model by θD(t). For example, these
parameters would correspond to weights in the case of a neural
network [49] taken as a classiﬁcation model. The trained
classiﬁcation model predicts class labels for given instances,
where the prediction for an instance x∈ΩXis denoted by
ˆy(x/divides.alt0θD(t))∈ΩY. In many cases, the classiﬁcation model
can predict the class label of an instance and estimate the
probabilities of class memberships. In this case, we denote the
estimated class membership probability that a given instance x
belongs to class ybyPr(Y=y/divides.alt0X=x,θD(t)).
C. Objective
Given the formalized problem setting and generalizing the
objective deﬁnitions in [38, 50] toward all query and anno-
tation types including complex cost schemes, we formulate
the objective of real-world AL as determining the optimal
annotation sequence for a cost-sensitive classiﬁcation problem:
S∗=arg min
S∈ΩS/bracketleft.alt1MC(θD(tS+1)/divides.alt0κ)+AC(D(tS+1) /divides.alt0ν)/bracketright.alt(4)
subject to the constraints C,where ΩSdenotes the set of all potential sequences. MC and
AC are the misclassiﬁcation and annotation cost, respectively.
We expect them to be on the same scale. Otherwise, extra
normalization might be necessary. The optimal annotation
sequence S∗minimizes the total cost while satisfying all
constraints C. A common constraint is a maximum annotation
budgetB∈R>0, i.e.,C={AC(D(tS+1) /divides.alt0ν)≤B}. The total
cost is decomposed into MC and AC, where the vector κ
encodes given hyperparameters for computing the MC, e.g, a
cost matrix, and the vector νrepresents the hyperparameters
for computing AC, e.g., wages of the annotators. We provide
a more detailed discussion on different cost schemes in the
setting of real-world AL in Section III.
Since it is difﬁcult to ﬁnd the optimal annotation se-
quence S∗given by Eq. 4 in advance [38], an AL strategy
aims to approximate the optimal solution through a greedy
approach. Therefore, the annotation sequence Sis deﬁned
iteratively at run time by executing a cycle where one iteration
corresponds to a single time step. We start with the description
of such a cycle for traditional AL. Subsequently, we restructure
it to ﬁt the setting of real-world AL.
D. Traditional Active Learning Cycle
In traditional AL, an omniscient and omnipresent annota-
torA={a1}is assumed to be available [19]. Moreover, a
query expects the class label of an instance such that the set
of queries can be represented by QX=Xand the set of
annotations is given by the set of classes, i.e., ΩZ=ΩY. Tra-
ditional AL strategies differ between the labeled (annotated)
setL(t)={(xn,yn) /divides.alt0 (xn,a1,yn)∈D(t)}and the unlabeled
(non-annotated) set U(t)={xn/divides.alt0xn∈X∧(xn,yn)∉L(t)}
obtained after executing the (t−1)-th iteration cycle. The
main idea is to develop a strategy intelligently selecting
instances from the unlabeled pool U(t)to which the anno-
tatora1assigns true class labels. Due to the omniscience
of this annotator a1, the annotation distribution satisﬁes
Pr(Z1=yn/divides.alt0X=xn,t)=1for all iteration cycles t∈Nand
observed instances xn∈X. Fig. 3 summarizes the entire
selection procedure as a cycle.
The selection of an instance is based on a so-called utility
measureφ∶X→R[51] estimating the utilities of the observed
instances Xregarding the classiﬁcation model to be trained.
In general, the unlabeled instance with the maximum utility is
selected in iteration cycle t:
xn∗=arg max
xn∈U(t)/bracketleft.alt1φ(xn/divides.alt0θL(t))/bracketright.alt. (5)
There are many approaches computing instances’ utilities. In
the following, we brieﬂy describe two fundamental concepts:
●The simplest concept of utility measures is uncertainty
sampling (US) [52], which usually requires an instance’s
class membership probabilities estimated by the classi-
ﬁcation model to be trained. Alternatively, distances to
decision boundaries [53] are used as proxies of them.
US ranks all instances in the unlabeled pool U(t)based
on an uncertainty measure and queries the label for
the instance with the maximum uncertainty regarding its
5
class information. A common uncertainty measure is the
entropyH[54] of the class distribution such that an
instance’s utility estimated is computed as
φUS(xn/divides.alt0θL(t))=H[Pr(Y/divides.alt0X=xn,θL(t))]. (6)
●The decision-theoretic framework expected error reduc-
tion (EER) [55] estimates the performance of the classi-
ﬁcation model. Therefor, EER assumes that the instances
in the unlabeled pool U(t)form a validation set. For each
unlabeled instance, the classiﬁcation model’s expected
error is computed on this validation set by retraining the
classiﬁcation model with each combination of the given
unlabeled instance and its possible class label. The multi-
ple retraining procedures of the classiﬁcation model lead
to high computational complexity. The resulting estimate
of the negative expected error deﬁnes the utility measure.
Correspondingly, EER selects the instance leading to the
minimum estimated error.
One of the main challenges regarding the design of utility
measures is the exploration-exploitation trade-off. On the
one hand, we aim to select instances near the classiﬁcation
model’s decision boundary to reﬁne it (exploitation). On the
other hand, we aim to select instances in unknown regions
(exploration) [56]. More advanced AL strategies balance this
trade-off by considering distances to the decision boundaries,
density, class distribution estimates [57, 58, 59], or using a
Bayesian approach [60].
In batch mode AL [23], we must consider the diversity of
instances since a batch of instances is selected in each learning
iteration cycle. However, a detailed analysis of instance utility
measures in the traditional AL setting is beyond the scope of
this survey, and a more detailed discussion on them is given
in [19, 20, 51, 61].
Selected Unlabeled
     Instance       Labeled
Instance
Labeled Pool Unlabeled Pool
RetrainStopContinueClassiﬁcation ModelOmniscient Annotator
Fig. 3. Traditional AL cycle according to [19]: (1) At the start of the iteration
cyclet, the traditional AL strategy selects an unlabeled instance xn∗from
the unlabeled pool: U(t+1)=U(t)/uni2216{xn∗}. (2) Subsequently, the instance
is presented to the omniscient annotator A={a1}who provides its true class
labelyn∗. The resulting instance-label pair is inserted into the labeled pool:
L(t+1)=L(t)∪{(xn∗,yn∗)}, (3) on which the classiﬁcation model is
retrained by updating its parameters θL(t)→θL(t+1). (4) At the end of
the cycle, the traditional AL strategy decides whether to continue or to stop
learning. This decision is made by a so-called stopping criterion [62, 63, 64],
which is part of ongoing research and not within this survey’s scope.E. Real-world Active Learning Cycle
The traditional AL cycle depicted in Fig. 3 has to be
adjusted to ﬁt the setting of real-world AL. Our resulting cycle,
including the real-world AL strategy’s elements (i.e., query
utility measure, annotator performance measure, and selection
algorithm), is shown in Fig. 4.
The query utility measure φ∶QX→Rφis an element
being already part of the traditional AL setting. However, in
the real-world AL setting, not only can the class labels of non-
annotated (unlabeled) instances be queried, but more general
queries can be selected for annotation. This also includes a
re-annotation of instances, known as repeated labeling [65],
re-labeling [66], or backward instance labeling [67]. Hence,
the strict distinction into a non-annotated (unlabeled) set U(t)
and an annotated (labeled) set L(t)is often not adequate
anymore. As a result, the utility measure φneeds to be
adapted to quantify the utility of more general queries. Another
adaption concerns the form of the output of the utility measure.
Instead of computing a single score per query, i.e., Rφ⊆R,
a utility measure may provide a more general description for
each query, e.g., a distribution, which can then be combined
with annotator performance estimates [68, 69]. We provide
an overview of query utility measures for different query and
annotation types in Section IV.
The annotator performance measure ψ∶QX×A→Rψ
represents a novel element compared to traditional AL strate-
gies and is deﬁned through an annotator model. Similar to a
classiﬁcation model, an annotator model has parameters ωD(t)
learned from a data set D(t). Its main task concerns the
estimation of the performance ψ(ql,am/divides.alt0ωD(t))∈Rψof
an annotator amregarding a query ql[68], e.g., the prob-
ability for providing a correct annotation. In most cases,
ψ(ql,am/divides.alt0ωD(t))is a point estimate, i.e., Rψ⊆R, but there
are also annotator models estimating probability distributions
over annotator performances [68, 69]. Moreover, an annotator
model may account for improvements and deteriorations of an-
notators’ performances, e.g., when an annotator learns or gets
exhausted. The annotator performance may also be affected
by collaboration mechanisms between the annotators, e.g., the
best annotator is asked to teach the worst annotator [70]. We
provide an overview of annotator performance measures in
Section V.
A real-world AL strategy is completed by the selection
algorithm as the ﬁnal element. It updates the annotation
sequence Sby selecting query-annotator pairs in each iteration
cyclet. This selection is speciﬁed by choosing a subset of
query-annotator pairs S(t)⊆QX×A. Therefor, it assesses
potential query-annotator pairs through the query utility and
annotator performance measure. If the set S(t)contains mul-
tiple queries, we face similar challenges as in batch mode
AL, e.g., selecting diverse queries. We provide an overview
of selection algorithms in Section VI.
AC and MC are modeled in AL literature by designing
cost-sensitive variants of query utility measures, annotator
performance measures, or selection algorithms. We provide
an overview in Section III.
6
Data Set
Classiﬁcation Model
Retrain
Annotator ModelSelection AlgorithmAnnotators
Queries
StopContinueSelected Query-Annotator
          PairsSelect Update Data Set
Annotator
Performance
Measure
Query
Utility
Measure
Fig. 4. Proposed real-world AL cycle: (1) At the start of the iteration
cyclet, the classiﬁcation and annotator model, both trained on the current data
setD(t), provide information regarding the query utility and the annotator
performance measure of the real-world AL strategy. (2) Based on both
measures, the real-world AL strategy’s selection algorithm speciﬁes a set of
query-annotator pairs S(t)⊆QX×A. Each pair (ql,am)∈S(t)initiates
an annotation of query qlby annotator am. (3) The annotations are inserted
into the data set: D(t+1)=D(t)∪{(ql,am,z(t)
lm) /divides.alt0 (ql,am)∈S(t)}.
(4) Then, the classiﬁcation and annotator model are retrained on the updated
data set (θD(t)→θD(t+1)andωD(t)→ωD(t+1)). (5) At the end of the
iteration, the real-world AL strategy decides whether to stop or to continue
learning.
III. C OST TYPES
MC and AC are the most crucial cost types in the real-world
setting, and we will summarize typical schemes of them in
this section. There exist several additional types of cost when
solving a classiﬁcation problem, e.g., cost of computation
(e.g., renting a graphics processing unit) and cost of test (e.g.,
getting the results of a blood test). They are described as
a taxonomy in [71]. At the end of this section, we present
a literature overview of real-world AL strategies explicitly
modeling MC and/or AC.
A. Misclassiﬁcation Cost
Mistakes of the classiﬁcation model induce MC (the ﬁrst
summand in Eq. 4). In the literature, we identiﬁed three cost
schemes and describe them in increasing order complexity in
the following:
Uniform MC: Each classiﬁcation error is charged at an
equal cost. The classiﬁcation model’s performance is inversely
proportional to the misclassiﬁcation rate [72], i.e., the propor-
tion of misclassiﬁed instances. This cost scheme is the simplest
one and is assumed by traditional AL strategies.
Class-dependent MC: This cost scheme is probably the
most common one in cost-sensitive classiﬁcation [73, 74].
The cost of a classiﬁcation error is deﬁned by means of
a cost matrix/table C∈RC×C
≥0, where an entry C[y,y′]in
rowyand column y′denotes the cost of predicting the class
label ˆy(xn/divides.alt0θD(t))=y′, when the instance xnactually
belongs to class yn=y. Our grid classiﬁcation example could
use the mean absolute error on class numbers as a typical cost
measure for ordinal classes [75]. It would be implemented
through C[y,y′]=/divides.alt0y−y′/divides.alt0. In some applications, the costmatrix is extended by adding an extra column representing
cases where the classiﬁcation model is too uncertain and
rejects predicting a class label (known as reject option [76]).
Instance-dependent MC: Costs of classiﬁcation errors
depend on speciﬁc characteristics of instances. An example
is fraud detection, where the amount of money involved in
a particular case has an essential impact on MC [77]. For
our grid classiﬁcation example, it would be more expensive if
many households were affected by overloading a low-voltage
grid. Consequently, the feature # house connections in
Eq. 1 is to play a central role when computing the cost of
misclassifying a grid.
MC can be computed as the expectation regarding the
true (but unknown) joint distribution Pr(X,Y)of instances
and class labels [76]. For example, class-dependent MC is
computed according to
MC(θD(t)/divides.alt0C)= E
Pr(X=x,Y=y)/bracketleft.alt1C[y,ˆy(x/divides.alt0θD(t)]/bracketright.alt. (7)
In practice, the exact computation of MC is often infeasible
due to the limited size of test data. Furthermore, in the real-
world AL setting, its estimation based on a separate set of
instances is challenging because of a sampling bias (arising
from the active data acquisition) [78] and the lack of known
ground truth class labels (arising from the error-proneness of
the annotators). Nevertheless, some real-world AL strategies
take imbalanced, i.e., class- or instance-dependent, MC into
account.
B. Annotation Cost
AC (the second summand in Eq. 4) arises from the work
effort of the annotators who have to invest time to decide
on appropriate annotations for the posed queries. The exact
speciﬁcation of AC depends on the underlying cost scheme. In
the literature, we identiﬁed four different schemes and describe
them in increasing order of complexity in the following:
Uniform AC : The cost of obtaining an annotation is con-
stant for each query and independent of the queried annotator.
Correspondingly, the AC is proportional to the number of
acquired annotations. This cost scheme is the simplest one
and is frequently used. In particular, it is often employed
in crowdsourcing environments, where the requester sets a
constant pay rate per query. This means the qualiﬁcation of
an annotator and the time spent on annotating a query have
no impact on the AC.
Annotator-dependent AC : In this cost scheme, the AC
explicitly depends on the queried annotator. This setting is
typical when annotators with different qualiﬁcations receive
different earnings per query, e.g., annotators with different lev-
els of expertise. Of course, there is typically no guarantee that
expensive annotators provide more accurate annotations [14].
Query-dependent AC : Since there may be more or less
difﬁcult queries, the cost of annotating a query may depend on
the query itself. For example, assessing the hosting capacity of
a large and complex low-voltage grid may require more time
than assessing a small and simple grid. Another example is
the annotation of voice mails, where the duration of a voice
mail is used as a proxy of the AC, e.g., 0.01 US dollar per
7
second [50]. For the classiﬁcation of documents, the number
of words or characters in a document is often correlated to
the AC [34]. Additionally, the query type affects the AC. For
example, comparing two instances and deciding whether both
belong to the same class is often easier than assigning an
instance to one of many classes [79, 80].
Query- and annotator-dependent AC : If the query and
annotator-dependent cost schemes are considered, the AC
varies across the pairs of query and annotator [81]. This cost
scheme ﬁts scenarios in which annotators are paid according to
their individual hourly wages and the annotation time depends
on the query [11].
The exact computation of AC depends on the underlying
scheme. If we exemplary assume annotator-dependent AC with
ν={ν1,...,νM}andνm>0representing the payment per
query for annotator am, we would obtain
AC(D(t) /divides.alt0ν)=M
/summation.disp
m=1νm⋅N(t)
m, (8)
N(t)
m=/summation.disp
(q,a,z)∈D(t)δ(a/uni2250am), (9)
where/uni2250denotes a Boolean comparison and the indicator
functionδ∶{false,true}→{0,1}returns one if the argument
is true and zero otherwise. Correspondingly, N(t)
m∈Nis the
number of annotations provided by annotator amuntil the start
of stept. In certain scenarios, such an exact speciﬁcation of
the AC is infeasible. This is when the annotation time is the
major cost factor and is not known in advance. Therefore, an
AL strategy is required to estimate the AC before querying an
annotator.
C. Literature Overview
Table I gives a literature overview of real-world AL strate-
gies, explicitly modeling imbalanced MC or AC. The ﬁrst part
of this table lists strategies being MC-sensitive, i.e., class-
dependent or instance-dependent. The second part summarizes
strategies taking imbalanced AC into account, i.e., annotator-
and/or query-dependent. Each strategy is categorized accord-
ing to its cost scheme, the type of classiﬁcation problem
(binary vs. multi-class), and its predeﬁned or estimated re-
quired cost information (cost matrix, annotation time, etc.).
Additionally, we provide a brief description of each strategy’s
main idea. A more in-depth analysis of them is provided in
the appendices of this survey.
IV. I NTERACTION SCHEMES
Interaction with human annotators forms an essential part
of AL. In this survey, we focus on the AL typical query-
annotation-based interaction. For this purpose, we provide
an overview of different query types and annotation types
based on the literature. At the end of this section, we present
a literature overview of existing real-world AL strategies
using different combinations of queries and annotations as
interaction schemes.A. Query Types
The set of possible queries QXspeciﬁes how a real-world
AL strategy can interact with the available annotators A.
Depending on the underlying classiﬁcation problem, there are
different possibilities to design these queries. In the literature,
we identiﬁed the following three most common query types:
Instance queries ask for information on a speciﬁc in-
stance xnas illustrated in Fig. 5(a) and is the most common
query type. Next to class labels, a query may request additional
information. Concrete examples are presented in [44, 101],
where annotators are asked for conﬁdence scores interpreted
as proxies of an instance’s class membership probabilities.
Region queries do not query information regarding a
speciﬁc instance, but ask annotators to provide information
about an entire region in the feature space [102]. For this
purpose, the query is to be formulated in an appropriate
and human-readable representation [103]. A common way
to achieve this requirement involves formulating premises of
sharp or possibilistic classiﬁcation rules by deﬁning conditions
on the value ranges of features [104]. An example of such a
region query is depicted in Fig. 5(b). Although a region query
provides class information about many instances, this type of
query differs from batch mode AL, where each instance of a
selected batch is annotated individually [23].
Comparison queries enhance the learning process by ob-
taining relative information between instances [47]. For exam-
ple, the comparison query, illustrated in Fig. 5(c), compares
two instances xnandxmby requesting whether they belong
to the same class or not [46, 105]. Regarding the ordinal grid
classiﬁcation example, another conceivable comparison query
may ask which of the two grid instances xnandxmhas a
superior hosting capacity.
Going beyond these three query types, we will present our
own proposals for query types as future research directions in
Section VII.
B. Annotation Types
Usually, the type of an annotation depends on the query
itself. In this survey, we differentiate between the following
three annotation types:
Distinct annotations are the simplest form of annotations.
They represent categorical information without the scope of
interpretation. Most AL strategies use them to encode class
labels. Other AL strategies expect a simple yes ornoas a
distinct annotation [46, 47]. Furthermore, they can encode a
sorting of instances in case of a comparison query [106].
Soft annotations allow for the representation of continuous
information. They are often inaccurate and subjective. Many
AL strategies use them to obtain information on the conﬁdence
of a provided class label by requesting a numerical value in a
continuous conﬁdence interval [43, 101]. Another example is
the use of probabilistic labels as gradual annotations [44, 107],
which enhanced the classiﬁcation performance for certain
tasks, e.g., in the medical domain [108].
Explanatory annotations are the most informative type of
annotations. Instead of only communicating a distinct or soft
decision, an explanatory annotation also explains why a certain
8
TABLE I
LITERATURE OVERVIEW OF COST -SENSITIVE REAL -WORLD AL STRATEGIES .
Strategy Cost Scheme Classiﬁcation Problem Cost Information
Misclassiﬁcation Cost (MC)
Margineantu [82],
Joshi et al. [79, 80]class-dependent MC multi-class cost matrix (predeﬁned)
These strategies compute the expected MC on the annotated set. Therefor, they simulate the annotation of an instance and
its addition to the classiﬁcation model’s training set. We can interpret this approach as a cost-sensitive variant of EER.
Liu et al. [83]class-dependent MC multi-class cost matrix (predeﬁned)
This strategy extends traditional US by making use of self-supervised training. Therefor, a cost-sensitive classiﬁcation model
is trained on instances annotated by annotators and instances annotated by a cost-insensitive classiﬁcation model.
Chen and Lin [84]class-dependent MC multi-class cost matrix (predeﬁned)
The ﬁrst variant of this strategy computes the maximum expected MC of an instance. In contrast, the second variant computes
the cost-weighted minimum margin between the two predictions with the lowest estimated MCs.
Krempl et al. [85]class-dependent MC binary cost ratio of false negative vs. false positive (predeﬁned)
This strategy computes the density-weighted expected MC reduction in an instance’s neighborhood within the feature space.
Therefor, it simulates the annotation of an instance and its addition to the classiﬁcation model’s training set.
K¨ading et al. [86]class-dependent MC multi-class cost function (predeﬁned)
This strategy computes the classiﬁcation model’s expected change by simulating the annotation of an instance.
Nguyen et al. [87]class-dependent MC binary cost matrix (predeﬁned)
This strategy computes the expected MC reduction when obtaining an annotation from an error-prone annotator and from an
infallible expert. Therefore, it employs a cost-sensitive variant of EER.
Huang and Lin [88]class-dependent MC multi-class cost matrix (predeﬁned)
This strategy is based on a cost embedding approach, which transfers the MC information into a distance measure of a latent
space. Utilities are deﬁned as expected MCs that are represented through distances in the latent space.
Min et al. [89]class-dependent MC multi-class cost matrix (predeﬁned)
This strategy queries only annotations for instances with MCs being higher than their respective ACs. The MCs are estimated
through a cost-sensitive k-nearest neighbor model.
Wu et al. [90],
Wang et al. [91]class-dependent MC binary cost matrix (predeﬁned)
These strategies employ a density-based clustering technique to construct a master tree of instances. In an iterative process,
this master tree is subdivided into blocks and for each block an estimated MC-optimal number of instances are annotated.
Krishnamurthy et al.
[92, 93]instance-dependent MC multi-class cost of predicting a class label for an instance (estimated)
These strategies query MC information per instance and class from an annotator. Their idea is to query the actual MC
information for the class label, for which an instance has the largest estimated MC range.
Annotation Cost (AC)
Zheng et al. [14],
Chakraborty [94]annotator-dependent AC mutli-class cost of querying an annotator (predeﬁned)
These strategies solve an optimization problem to specify a subset of annotators with low ACs and high performances.
Moon and Carbonell
[95], Huang et al. [96]annotator-dependent AC multi-class cost of querying an annotator (predeﬁned)
These strategies compute the annotator performance per AC unit to prefer annotators with high performances and low ACs.
Nguyen et al. [87]annotator-dependent AC multi-class cost ratio of querying crowd worker vs. expert (predeﬁned)
This strategy normalizes the utility of querying an expert or crowd worker by their respective ACs to ﬁnd a trade-off between
expensive but correct expert annotations and cheap but error-prone crowd worker annotations.
Margineantu [82],
Donmez and Carbonell
[38, 39]query-dependent AC multi-class cost of annotating a query (predeﬁned)
These strategies subtract the query’s individual AC from its utility to prefer highly useful queries with low ACs. These ACs
are assumed to be known in advance for each query or to follow a predeﬁned model.
Joshi et al. [79, 80]query-dependent AC multi-class number of comparisons per query (estimated)
These strategies use multiple comparison queries to reveal the class label of a non-annotated instance. The expected number
of comparisons required to reveal an instance’s class label is used as an AC proxy and subtracted from an instance’s utility.
Tsou and Lin [97]query-dependent AC multi-class cost of annotating a query (predeﬁned)
This strategy builds a decision tree throughout the AL process. It computes the average AC of the already annotated instances
in each leaf of this tree. These AC estimates are used to normalize the query utilities of the instances in the respective leaves.
Settles et al. [81],
Haertel et al. [98],
Tomanek and Hahn [99],
Wallace et al. [100]query-dependent AC multi-class annotation time per query (estimated)
These strategies estimate the annotation time per query. Therefore, they use either historical data in form of logged annotation
times or employ prior knowledge regarding a domain, e.g., the number of words when annotating text. The estimated annotation
times are considered by normalizing query utility or employing a linear rank combination of utilities and annotation times.
Wallace et al. [11]annotator-, query-dependent AC multi-classannotation time per query (estimated) + cost per time unit
for each annotator (predeﬁned)
This strategy computes ACs by multiplying the annotation times (estimated through the number of words in a document)
with the respective salaries (predeﬁned) of the annotators. Annotators with low estimated ACs are more often queried.
Arora et al. [34]annotator-, query-dependent AC multi-class annotation time per query-annotator pair (estimated)
This strategy estimates the annotation time as a function of the annotator and the query. Therefor, it uses features to describe
the query and the annotator in combination with historical data in form of logged annotation times.
9
decision has been made. An exemplary explanation would be:
“The instance xndoes not belong to the positive class because
its feature value xndis too low.” [109].
C. Literature Overview
A query mostly requests information of a speciﬁc kind. Ac-
cordingly, the query and annotation types are closely coupled.
Table II gives a literature overview of existing combinations
of queries and annotations as interaction schemes. The query
“To which class does instance xnbelong?” known already
from the traditional AL setting is excluded. A more in-depth
analysis of the real-world AL strategies in Table II with a focus
on their query utility measures is provided in the appendices
of this survey.
V. A NNOTATOR PERFORMANCE MODELS
The error-proneness of annotators poses a major challenge
in real-world AL [36]. In this section, we discuss the typical
factors inﬂuencing the performance of error-prone annotators.
Moreover, we identify three different types of annotator per-
formance. At the end of this section, we present a literature
overview of existing annotator performance models.
A. Inﬂuence Factors
We refer to “annotator performance” as a general term for
the quality of the annotations obtained from an annotator.
There is no clear deﬁnition of this term, but there exist
several concrete interpretations, e.g., label accuracy [123],
conﬁdence [101], uncertainty [70], reliability [124], etc. Such
an interpretation is closely coupled to the annotation type and
the expected optimal annotation of a query.
The annotator performance may be affected by various
factors [125, 126], and the most prominent ones identiﬁed in
the AL literature are given in the following:
The domain knowledge of annotators has an essential im-
pact on their performances [127]. Insufﬁcient knowledge leads
to a deterioration of the annotator performance. In complex
tasks, such as assessing the hosting capacity of a low-voltage
grid, a certain level of domain knowledge is indispensable.
Thequery difﬁculty affects the probability of obtaining an
optimal annotation [12, 128, 129]. For example, in recognition
of hand-written digits, it is often more challenging to differen-
tiate between the digits 1 and 7 than discriminating between
the digits 1 and 8 [44]. Next to the subject of a query, also its
type can be crucial for the performance of an annotator [80].
The ability for a reliable self-assessment of annotators
plays a central role, particularly in scenarios where queries ask
for conﬁdence scores as annotations [101]. Although empirical
studies [11, 130] have shown that annotators can reliably
estimate their performances in some domains, the Dunning-
Kruger-effect [131] states that, in particular, unskilled an-
notators provide not only erroneous annotations, but they
also cannot realize their mistakes. This effect has also been
conﬁrmed in a large-scale crowd-sourcing study [132].
Motivation or level of interest of an annotator may
inﬂuence the elaborateness during the annotation process. Forexample, in a crowdsourcing study analyzed in [127], more
interested annotators performed superiorly.
Thepayment of an annotator may have a signiﬁcant impact
on the annotator performance, such that well-paid annotators
provide more high-quality annotations. In a crowdsourcing
environment, the improvement of the annotation quality has
been conﬁrmed by increasing the pay from 0.10$ to 0.25$ per
query [127].
The annotator has to be concentrated when annotating a
query [133]. Otherwise, annotation mistakes arise because of
missing mindfulness or tiredness.
A constant stream of queries of the same type may be
annoying for the annotator [134]. Therefore, the way of
interaction between the AL strategy and an annotator may
inﬂuence the annotation results and needs to be designed
appropriately. For example, different interaction schemes can
lead to different degrees of an annotator’s enjoyability, as
experimentally shown in [135].
The learning aptitudes of annotators are also crucial for
their performances. For example, one could teach the annota-
tors to provide high-quality annotations [125].
The collaboration between annotators is also interlinked
with their performances. Incorporating corresponding mecha-
nisms for collaboration can strongly improve the annotation
quality [136].
B. Annotator Performance Types
Modeling and quantifying the inﬂuence of each of the
previously listed factors on annotator performance is infea-
sible. Instead, existing annotator models abstract from these
factors to estimate annotator performance. In the literature,
we identiﬁed three different types of annotator performances.
Therefor, we generalize the class label noise taxonomy, pre-
sented by Fr ´enay and Verleysen [137], to the setting of real-
world AL by including queries and annotations instead of
instances and classes. The resulting statistical taxonomy of
annotator performance types is presented in Fig. 6, and we
provide more details in the following:
Uniform annotator performance: The annotator perfor-
mance depends only on the characteristics of the annotator.
As a result, the query itself or the query’s optimal annotation
has no inﬂuence. An example is given in Fig. 7, where an
annotator has the constant probability of 90% to recognize a
hand-written digit correctly.
Annotation-dependent annotator performance: The an-
notator performance depends next to the annotator’s charac-
teristics on the optimal annotation for a query. An example
is given in Fig. 7, where an annotator is better at identifying
the digit 1 (constant correctness probability of 90%) than the
digit 7 (constant correctness probability of 70%) in images of
hand-written digits.
Query-dependent annotator performance: The annotator
performance depends on the annotator’s characteristics, the
query, and the optimal annotation. An example is given in
Fig. 7, where an annotator has a low probability to correctly
identify the third digit as 7 because it can be misinterpreted
as the digit 2.
10
Before Annotation After Annotation
Feature FeatureFeature
(a) Example of an instance query.
Before Annotation After Annotation
Feature FeatureFeature (b) Example of a region query.
Before Annotation After Annotation
Feature FeatureFeature (c) Example of a comparison query.
Class 1 Class 2 Non-annotated Diﬀerent Classes Same Class
Fig. 5. Illustration of query types within the feature space: For a binary classiﬁcation problem, the two-dimensional instances of an artiﬁcially generated
setX⊂R2are plotted according to their feature values. Probabilistic annotations are depicted by using the corresponding proportions of the red and blue
colors. Figure 5(a) illustrates an instance query by marking the selected instance xn∗for which the class membership probability for the blue class is expected
as an annotation. A region query deﬁning the region X1∈[−3,−1]∧X2∈[1,3]is depicted by the gray rectangle in Fig. 5(b). Again, the class membership
probability for the blue class represents the annotation. Figure 5(c) illustrates a comparison query requesting whether the instances xn∗andxm∗belong
to the same class (solid gray line). A solid black line connects instances belonging to the same class, and a dashed black line indicates that instances are
assigned to different classes.
As an additional dimension, possible temporal dependencies
regarding annotator performance can be taken into account.
Therefore, we differ between persistent and time-varying
annotator performance. In the ﬁrst case, the annotator per-
formance is constant during the entire annotation process. In
the latter case, the annotator performance may increase due to
the learning progress of an annotator [70, 81] or may decrease
because of exhaustion or emerging boredom [135].
C. Literature Overview
During the AL process, the performances of the annotators
are estimated by annotator models. Table III provides a lit-
erature overview, including a categorization of those models.
Next to the assumptions regarding the type of annotator per-
formance, we use several other factors to categorize different
annotator models. In particular, the query and annotation
types described in Section IV are essential properties of an
annotator model. However, to the best of our knowledge,
existing annotator models focus on instance queries such that
no column for the query type is present in Table III. As a
further category, we differentiate between the assumed relation
of the annotators. In the case of multiple annotators, they are
either independent or collaborative. If a model can work with a
single annotator, the term single is denoted for this category.
Furthermore, we indicate in Table III whether an annotator
model allows for the integration of prior knowledge regarding
the performances of annotators. Additionally, we provide a
brief description of each annotator model’s main idea. A more
in-depth analysis of these annotator models is provided in the
appendices of this survey.
VI. S ELECTION ALGORITHMS
The selection of query-annotator pairs is based on a se-
lection algorithm. It uses the query utility measure φand
the annotator performance measure ψas basis to spec-
ifyS(t)⊆QX×Aas the set of query-annotator pairs in each
AL iteration cycle t∈N. In this context, we differentiate
between two types of selection algorithms, explained in the
following. At the end of this section, we present a literature
overview of existing selection algorithms.QZPm
Zmt
UniformQZPm
Zmt
Annotation-dependentQZPm
Zmt
Query-dependent
Fig. 6. Statistical models of annotator performance types: Following the
idea of Fr ´enay and Verleysen [137], we present three different annotator
performance types as graphical models. There are four random variables
depicted as nodes: Qis the query, Zis the optimal annotation, Zmis the
annotation provided by annotator am, andPmis the variable indicating the
performance of the annotator am. In the simplest case, Pmis a binary variable
to represent whether an annotator provides the optimal annotation ( Pm=1)
or not (Pm=0). We denote observed variables by shading the corresponding
nodes, whereas the other nodes represent latent variables. The variable t
is a deterministic parameter denoting the time. Arrows represent statistical
dependencies, e.g., the optimal annotation always depends on the underlying
query. The dashed arrow between the annotator performance variable Pmand
the timetindicates an optional dependency. If this dependency is considered,
the annotator performance is time-varying [48]. Otherwise, it is assumed to
be persistent.
Optimal Annotation:
Query:
Annotator Performance:Which digit is 
shown in the image?
Uniform:
Annotator-dependent:
Query-dependent:90% 90% 90%
90%
90%70%
95%70%
40%
Fig. 7. Illustration of annotator performance types: There are three images of
hand-written digits. Assuming a uniform annotator performance, an annotator
has an equal chance of correct digit recognition for each of the three
images. In the case of annotation-dependent performance values, the chance
of recognizing a digit correctly depends on its true class as optimal annotation,
e.g., the annotator is better at recognizing digit 1than digit 7. The assumption
of query-dependent performance values is more general and realistic. For
example, the annotator has a low chance of recognizing the right digit due
to its unclear writing. In the case of time-varying annotator performances,
the chance of correct digit recognition can change over time, e.g., the chance
may increase due to the learning progress of an annotator [70, 81] or may
decrease because of exhaustion or emerging boredom [135].
11
TABLE II
LITERATURE OVERVIEW OF COMBINATIONS OF QUERIES AND ANNOTATIONS EMPLOYED BY REAL -WORLD AL STRATEGIES .
Strategy Query Annotation
Instance Queries
Hu et al. [110] Does instance xn∈Xbelong to concept K⊂ΩY? distinct annotations: ΩZ={yes,no}
Bhattacharya and Chakraborty [111]To which class in /braceleft.alt1y(1),...,y(n)/braceright.alt1⊂ΩYdoes
instance xn∈Xbelong?distinct annotations: ΩZ=ΩY
Cebron et al. [112] To which class does instance xn∈Xnot belong? distinct annotations: ΩZ=P(ΩY)
Donmez and Carbonell [38, 39],
Wallace et al. [11], Fang and Zhu
[113], Zhong et al. [45], K ¨ading
et al. [86]Provided that you are conﬁdent: What is the class
label of instance xn∈X?distinct annotations: ΩZ=ΩY∪{uncertain }
Donmez and Carbonell [38, 39],
Ni and Ling [101], Calma et al. [44]What is the class label of instance xn∈Xand
how conﬁdent are you?soft annotations: ΩZ=ΩY×ΩCwhere ΩCdenotes the
set of possible conﬁdence scores
Song et al. [43]How conﬁdent are you that instance xn∈X
belongs to the positive class?soft annotations: ΩZ=[−1,1]withz∈Ωzindicating the
conﬁdence that xnbelongs to the positive class
Biswas and Parikh [109]Does instance xn∈Xbelong to class y∈ΩY? If
this is not the case, can you explain the reason?explanatory annotations: ΩZ={yes}∪ΩEwithΩE
representing the set of explanations
Teso and Kersting [114]Does instance xn∈Xbelong to class y∈ΩY
because of explanation e∈ΩE?explanatory annotations: ΩZ={yes}∪ΩY∪ΩEwith
ΩErepresenting the set of explanations
Region Queries
Druck et al. [115], Settles [116]For which classes is a positive feature value
Xd>0highly indicative?distinct annotations: ΩZ=P(ΩY)withz⊆ΩY
indicating the set of possible classes
Du and Ling [102]What is the proportion of positive instances in the
region described by the constellation of categorical
features, e.g., X1/uni22501∧X4/uni22500∧X8/uni22500? soft annotations: ΩZ=[0,1]withz∈ΩZindicating the
proportion of positive instances Du and Ling [10], Luo and
Hauskrecht [117, 118, 103], Rashidi
and Cook [104], Haque et al. [119]What is the proportion of positive instances
in the region described by the feature constellation,
e.g.,X1∈[0,2]∧X2≤10∧X3/uni22503?
Comparison Queries
Fu et al. [46, 105],
Joshi et al. [79, 80]Do instance xn∈Xand instance xm∈Xbelong
to the same class?distinct annotations: ΩZ={yes,no}
Xiong et al. [120]Is instance xn∈Xmore similar to instance
xm∈Xthan instance xo∈X?distinct annotations: ΩZ={yes,no,uncertain }
Kane et al. [47], Xu et al. [121],
Hopkins et al. [122]Is instance xn∈Xmore likely to belong to the
positive class than instance xm∈X?distinct annotations: ΩZ={yes,no}
Qian et al. [106]What is the decreasing order of the instances
{xn,xm,xo}⊂Xregarding their similarities to
instance xp∈X?distinct annotations: ΩZconsists of all possible ordering
of the available instances, e.g., (xm,xo,xn)∈ΩZ
A. Sequential Selection of Queries and Annotators
Sequential selection of queries and annotators is made in
two steps. In the ﬁrst step, one or multiple (in the case of
batch mode AL) queries with the highest utilities are selected.
In a second step, corresponding annotators are selected and
assigned to the respective queries, e.g., a predeﬁned number
of the annotators with the highest estimated performances per
query [139]. Ideally, the selected annotators lead to low AC
while providing high accuracy annotations. The main motiva-
tion for a sequential selection is to emphasize useful queries
by selecting them in advance of the annotators. Moreover, the
issue of annotator selection reduces to determining a ranking
of the annotators regarding a selected query. As a result,
not the exact but only the relative differences between the
performances of the annotators are crucial for the annotator
selection.
B. Joint Selection of Queries and Annotators
Selecting queries without considering the annotator’s per-
formances can result in low-quality annotations because thereis no guarantee that at least one annotator has a sufﬁcient
performance regarding a selected query [45]. This problem can
be resolved by applying a selection algorithm jointly selecting
queries and annotators. For this purpose, the query utility
and the annotator performance measure are to be combined
appropriately, e.g., by taking their product [96]. Compared to
the sequential selection of queries and annotators, the joint
selection comes with higher computational complexity. Instead
of computing the annotator performance estimates only for
the selected queries, the annotator performance estimates are
required for each possible query. Moreover, exact estimates
regarding the annotator performance are more crucial since the
annotator performance estimates are directly integrated into the
selection criterion. If these estimates are unreliable, not only
the annotator selection will be negatively affected but also the
combination with the query selection.
C. Literature Overview
Table IV provides an overview of selection algorithms
employed by existing real-world AL strategies, which select"
2302.14624,D:\Database\arxiv\papers\2302.14624.pdf,"What factors, beyond the inherent difficulty of a language, can significantly influence the performance of a language recognition system, and how do these factors interact with each other?","Factors such as the type of speech data used for training and testing, the duration of the speech segments, and the presence of specific metadata can all impact system performance. These factors can interact in complex ways, making it challenging to isolate the influence of any single factor.","Figure 7: Language confusability of the leading systems
Figure 8: A data source type distribution and effect on systems
(a)
 (b)
Figure 9: SAD duration effect on system performance (a) SAD
duration distribution for the dev and test set (b) T1 system per-
formance vs. SAD duration.
upper-left to bottom-right are PMiss (false reject rates) and the
off-diagonal values are PFA(false alarm rates). A higher false
alarm probability implies a potential confusability for that lan-
guage pair. For simplicity, results of PTarget = 0.5for the
four leading systems are demonstrated using heatmap confu-
sion matrices. Given the testset and systems, a higher con-
fusability is observed for three clusters of language pairs as
follows: 1) among Arabic languages (ara-aeb, ara-arq, ara-
ayl), 2) between South African English (eng-ens) and Indian-
accented South African English (eng-iaf), and 3) Ndebele (nbl-
nbl), Tsonga (tso-tso), Venda (ven-ven), Xhosa (xho-xho) and
Zulu (zul-zul).
To gain insight on how metadata variables (i.e., factors)
affect system performance, we conducted experiments giventhe metadata listed in Section 3.3. For simplicity, the fol-
lowing analyses are demonstrated using data source type and
speech duration only. The LRE22 data was collected in
two primary genres, namely, conversational telephone speech
(CTS) and broadcast narrowband speech (BNBS) which we
call data source type . Figure 8 shows system performance
(actC Primary ) partitioned by data source type (CTS vs BNBS)
for all the primary submissions under the ﬁxed training condi-
tion. The top-left pie chart is a distribution of CTS and BNBS
on the testset, which is imbalanced. The bar plot shows a per-
formance comparison between CTS (blue) and BNBS (orange)
across all the teams. The results indicates that, given the im-
balanced distribution, CTS is more challenging and that data
source type has a strong effect on system performance; a simi-
lar trend is observed across the systems.
Durations of testset segments varied between 3s and 35s
of speech that have been randomly sampled and extracted from
longer recordings as determined by an automatic Speech Ac-
tivity Detector (SAD) which we call SAD duration . Figure 9a
shows a distribution of SAD duration for the testset and Fig-
ures 9b shows the performance of a top-performing system by
SAD duration . Given the testset and systems, it is seen that
when SAD duration increases, actC Primary signiﬁcantly de-
creases up to a certain duration (between 15s and 20s). After
that, a diminishing return on system performance improvement
is observed across the systems.
6. Conclusions
We presented a summary of the 2022 NIST Language Recog-
nition Evaluation with an emphasis on low resource languages
and random duration of speech segments.
The results showed that almost no calibration error was ob-
served for the top-performing systems for both the ﬁxed and
open training condition. Overall, the submissions under the
open training condition had better performance compared to the
ﬁxed condition submissions, with only one exception. Given the
testset and primary systems under the ﬁxed training condition,
we found that Oromo and Tigrinya were easier to detect while
Xhosa and Zulu were harder to detect. A greater confusabil-
ity was observed for the language pairs 1) among Zulu, Xhosa,
Ndebele, Tsonga, and Venda, 2) between South African and
Indian-accent South African English, and 3) among Tunisian,
Algerian, and Libyan Arabic languages. Some of the metadata,
such as data source type andSAD duration , had a signiﬁcant
effect on system performance for all systems. In terms of SAD
duration , when speech duration increased, system performance
signiﬁcantly increased up to a certain duration, and then we ob-
served a diminishing return on system performance afterward.
7. Disclaimer
These results presented in this paper are not to be construed or
represented as endorsements of any participant’s system, meth-
ods, or commercial product, or as ofﬁcial ﬁndings on the part of
NIST or the U.S. Government.
The work of MIT Lincoln Laboratory (MITLL) is spon-
sored by the Department of Defense under Air Force Contract
No. FA8702-15-D-0001. Any opinions, ﬁndings, conclusions
or recommendations expressed in this material are those of the
author(s) and do not necessarily reﬂect the views of the U.S. Air
Force.
8. References
[1] NIST, “NIST language recognition evaluation overview,” 1996-
2022, [Online; accessed 17-February-2023].
[2] H. Zhao, D. Bans ´e, G. Doddington, C. Greenberg, J. Hern ´andez-
Cordero, J. Howard, L. Mason, A. Martin, D. Reynolds, E. Singer,
and A. Tong, “Results of the 2015 NIST language recognition
evaluation,” in Interspeech 2016 , San Francisco, USA, September
2016, pp. 3206–3210.
[3] S. O. Sadjadi, T. Kheyrkhah, A. Tong, C. S. Greenberg, D. A.
Reynolds, E. Singer, L. P. Mason, and Hernandez-Cordero, “The
2017 nist language recognition evaluation.” in Odyssey , 2018, pp.
82–89.
[4] S. O. Sadjadi, T. Kheyrkhah, C. Greenberg, E. Singer,
D. Reynolds, L. Mason, and J. Hernandez-Cordero, “Performance
Analysis of the 2017 NIST Language Recognition Evaluation,” in
Proc. Interspeech 2018 , 2018, pp. 1798–1802.
[5] A. F. Martin, C. S. Greenberg, J. M. Howard, G. R. Doddington,
and J. J. Godfrey, “NIST language recognition evaluation - pastand future,” in Odyssey 2014 , Joensuu, Finland, June 2014, pp.
145–151.
[6] F. Ahmad and G. Wid ´en, “Language clustering and knowledge
sharing in multilingual organizations: A social perspective on lan-
guage,” Journal of Information Science , vol. 41, no. 4, pp. 430–
443, 2015.
[7] J. Valk and T. Alum ¨ae, “V oxlingua107: A dataset for spoken lan-
guage recognition,” in 2021 IEEE Spoken Language Technology
Workshop (SLT) , 2021, pp. 652–658.
[8] M. P. Harper, “Data resources to support the Babel program,”
https://goo.gl/9aq958, [Online; accessed 17-February-2023].
[9] NIST, “Speech ﬁle manipulation software (SPHERE) pack-
age version 2.7,” ftp://jaguar.ncsl.nist.gov/pub/sphere-2.
7-20120312-1513.tar.bz2, 2012, [Online; accessed 01-March-
2018].
[10] NIST, “The 2022 NIST Language Recognition Evaluation Plan,”
https://tsapps.nist.gov/publication/get pdf.cfm?pub id=935161,
2022, [Online; accessed 01-Feb-2023]."
1807.10018,D:\Database\arxiv\papers\1807.10018.pdf,"How does the proposed framework address the issue of redundancy in video descriptions, which is a common problem in video dense captioning?","The framework tackles redundancy by progressively selecting key events and generating sentences conditioned on what has been said before, ensuring that each sentence conveys a distinctive message and avoids repetition.","4 Yilei Xiong, Bo Dai, Dahua Lin
select spatial regions to form region sequences. Finally, it employs a sequence-
to-sequence submodule to transfer region sequences into captions.
Although closely related, video dense captioning is diﬀerent from video cap-
tioning. Particularly, a model for video dense captioning could generate multiple
captions, each covers a small period of the input video, where the periods can be
overlapped with each other, leading to a lot of redundancy in the corresponding
captions. On the contrary, a model for video captioning should generate a single
description consisting of several coherent sentences for the entire input video.
Video Captioning Our method targets the topic of video captioning. Related
works can be roughly divided into two categories based on whether a single
sentence or a paragraph is generated for each input video. In the ﬁrst category,
a single sentence is generated. Among all the works in this category, Rohrbach
et al [18] detected a set of visual concepts at ﬁrst, including verbs, objects and
places, and then applied an LSTM net to fuse these concepts into a caption.
Yuet al [30] and Pan et al [13] followed a similar way, but respectively using
a semantic attention model and a transfer unit to select detected concepts and
generate a caption. Instead of relying on visual concepts, Hori et al [8] and
Venugopalan et al [23] use features from multiple sources including appearance
and motion to improve quality of the generated caption. There are also eﬀorts
devoted to improving the decoder side. Wang et al [25] added a memory network
before the LSTM net during the decoding process to share features at diﬀerent
timestamps. Baraldi et al [1] applied a boundary detecting module to share
features hierarchically. While they are able to produce great captions, a single
sentence is diﬃcult to capture all semantic information in a video, as one video
usually contains several distintive events.
The second category is to generate a paragraph to describe a video. Our
method belongs to this category. In this category, Yu et al [29] applied hierar-
chical recurrent neural networks, where a sentence generator is used to generate
a single sentence according to a speciﬁc topic , and a paragraph generator is used
to capture inter-sentence statistics and feed the sentence generator with a series
of topics. The most similar work to our method is the one presented in [19]. This
method ﬁrst select a subset of clips from the input video, and then use a decoder
to generate sentence from these clips to form a paragraph summary of the entire
video. Our method is diﬀerent from these existing works from two aspects. (1)
When generating each sentence of the paragraph, the method in [29] requires
the features from the entire video, which is expensive for very long videos, while
our method only requires features in selected proposals. (2) In [19], the clips are
selected according to frame quality in advance as a preprocessing step, without
taking into account the coherence of narration. This way will lead to redundancy
in the resulting paragraph. On the contrary, our method selects key events along
with the generation of captions in a progressive way. The selection of the next
key event depends on what has been said before in preceding captions. Also,
this process takes into account temporal and semantic relationships among the
selected events in order to ensure the coherence of the resultant paragraph.
Move Forward and Tell: A Progressive Generator of Video Descriptions 5
Semantic eventLSTMKeepLSTMIgnoreLSTMKeepCaptioning ModuleA gymnast is seen standing before a beam and begins performing a gymnastics routineA man is seen speaking to the camera and leads into her laying down on a boardCaptioning Module
Visual featureRange featureLSTM
Captioning ModuleKeepWe see an ending screenCaptionfeatureCaptionfeature
Fig. 2. An overview of our framework, which at ﬁrst localize important events from
the entire video. It then generates a coherent and concise descriptive paragraph upon
these localized events. Speciﬁcally, an LSTM net, serves as a selection module, will pick
out a sequence of coherent and semantically independent events, based on appearances,
temporal locations of events, as well as their semantic relationships. And based on this
selected sequence, another LSTM net, serves as a captioning module, will generate
a single sentence for each event in the sequence conditioned on previous generated
sentences, which are then concatenated sequentially as the output of our framework.
3 Generation Framework
Our task is to develop a framework that can generate coherent paragraphs to
describe given videos. Speciﬁcally, a good description should possess three prop-
erties: (1) Relevant : the narration aligns well with the events in their temporal
order. (2) Coherent : the sentences are organized into a logical and ﬂuent nar-
rative ﬂow. (3) Concise : each sentence conveys a distinctive message, without
repeating what has been said.
3.1 Overview
A natural video often comprises multiple events that are located sparsely along
the temporal range. Here, events refer to those video segments that contain
distinctive semantics that need to be conveyed. Taking the entire video as input
for generating the description is ineﬃcient, and is likely to obscure key messages
when facing numerous noisy clips. Therefore, we propose a framework as shown
in Figure 2, which generates a descriptive paragraph in two stages, namely event
localization andparagraph generation . In event localization, we localize candidate
events in the video with a high recall. In paragraph generation, we ﬁrst ﬁlter
out redundant or trivial candidates, so as to get a sequence of important and
distinctive events. We then use this sequence to generate a single descriptive
6 Yilei Xiong, Bo Dai, Dahua Lin
paragraph for the entire video in a progressive manner, taking into account the
coherence among sentences.
3.2 Event Localization
To localize event candidates, we adopted the clip proposal generation scheme
presented in [31], using the released codes. This scheme was shown to be ef-
fective in locating important clips from untrimmed videos with high accuracy.
Speciﬁcally, following [31], we calculate frame-wise importance scores, and then
group the frames into clips of via a watershed procedure. This method outputs
a collection of clips as event candidates . These clips have varying durations and
can overlap with each other. As our focus is on paragraph generation, we refer
readers to [31] for more details of event localization.
Note that not all the candidates derived in this stage is worthy of descrip-
tion. The paragraph generation will select a subset of candidates that contain
important and distinctive messages, along with the generation process.
3.3 Progressive Event Selection and Captioning
Given a sequence of events, there are various ways to generate a descriptive
paragraph. While the most straightforward way is to generate a single sentence
for each event in the sequence, it is very likely to introduce a large amount of
redundancy. To generate coherent and concise description, we can select a subset
of distinctive events and generate sentences thereon. The key challenge here is
to stride a good balance between suﬃcient coverage and conciseness.
In this work, we develop a progressive generation framework that couples two
recurrent networks, one for event selection and the other for caption generation.
Event Selection With all event candidates arranged in the chronological or-
der, denoted as ( e1,...,eT), the event selection network begins with the ﬁrst
candidate in the sequence and moves forward gradually as follows:
h0=0,ht= LSTM( ht−1,vt,rt,ckt), (1)
pt= sigmoid( wT
pht), yt= 1[pt>δ]. (2)
Speciﬁcally, it initializes the latent state vector h0to be zeros. At each step t, it
updates the latent state htwith an LSTM cell and computes pt, the probability
ofetcontaining relevant and distinctive information, by applying a sigmoid
function to wT
pht. Ifptis above a threshold δ,ytwould be set to 1, indicating
that the candidate etwill be selected for sentence generation.
The updating of htdepends on four diﬀerent inputs: (1) ht−1: the latent state
at the preceding step. (2) vt: the visual feature of et, extracted using a Temporal
Segmental Network (TSN) [26]. (3) rt: the range feature, similar as the image
mask in [4], represented by a binary mask that indicates the normalized time
span ofetrelative to the entire duration. (4) ckt: the caption feature of ekt,
Move Forward and Tell: A Progressive Generator of Video Descriptions 7
wherektis the index of the lastselected event candidate (before t). Here, the
caption feature is from the caption generation network , which we will present
next. Particularly, cktis chosen to be the latent state of the caption generation
network at the ﬁnal decoding step when generating the description for ekt.
With the previous caption feature cktincorporated, the event selection net-
work is aware of what have been said in the past when making a selection. This
allows it to avoid selecting the candidates that are semantically redundant.
Caption Generation On top of the selected events, the caption generation
network will produce a sequence of sentences, one for each event, as follows:
g(k)
0={
0, k = 1
g(k−1)
∗, k> 1,g(k)
l= LSTM( g(k)
l−1,u(k)
l,w(k)
l−1), (3)
s(k)
l=Wsg(k)
l, w(k)
l∼softmax( s(k)
l). (4)
Here, g(k)
ldenotes the latent state at the l-th step of the caption generation
network when describing the k-th selected event. u(k)
ldenotes a visual feature
of a subregion of the event. Here, the computation of u(k)
lfollows the scheme
presented in [16], which allows the network to dynamically attend to diﬀerent
subregions as it proceeds1.w(k)
lis the word produced at the l-th step, which is
sampled from softmax( s(k)
l).
This network is similar to a standard LSTM for image captioning except for
an important diﬀerence: When k > 1, the latent state is initialized to g(k−1)
∗ ,
the latent state at the last decoding step while generating the previous sentence.
This means that the generation of each sentence (except the ﬁrst one) is condi-
tioned on the preceding one, which allows the generation to take into account
the coherence among sentences.
Discussions The event selection network and the caption generation network
work hand in hand with each other when generating a description for a given
video. On one hand, the selection of next event candidate depends on what has
been said. Particularly, one input to the event selection network is ckt, which is
set to the g(kt)
∗, the last latent state of the caption generation network in generat-
ing the previous sentence. On the other hand, the caption generation network is
invoked only when the event selection network outputs yt= 1, and the generation
of the current sentence depends on those that come before. The collaboration
between both networks allows the framework to produce paragraphs that can
cover major messages, while being coherent and concise.
Note that one may also use Non-Maximum Suppression (NMS) to directly
remove temporal overlapped events. This simple way is limited compared to
ours, as it only considers temporal overlap while ignoring the semantic relevance.
1We provide more details of the computation of u(k)
lin the supplemental materials
8 Yilei Xiong, Bo Dai, Dahua Lin
Another way is to ﬁrst generate sentences for all events and then select a subset of
important ones based on text summarization [19]. This approach, however, does
not provide a mechanism to encourage linguistic coherence among sentences,
which is crucial for generating high-quality descriptions.
4 Training
Three modules in our framework need to be trained, namely event localization ,
caption generation , and event selection . In particular, we train the event lo-
calization module simply following the procedure presented in [31]. The other
two modules, the caption generation network and the event selection network,
are trained separately. We ﬁrst train the caption generation network using the
ground-truth event captions. Thereon, we then train the event selection network,
which requires the caption generation states as input.
4.1 Training Caption Generation Network
The caption generation network models the distribution of each word condi-
tioned on previous ones and other inputs, including the visual features of the
corresponding event u(k)and the ﬁnal latent state of for the preceding sentence
g(k−1)
∗ . Hence, this distribution can be expressed as pθ(wl|w1:l−1;u(k),g(k−1)
∗ ),
where θdenotes the network parameters. We train this network through two
stages: (1) initial supervised training, and (2) reinforcement learning.
The initial supervised training is performed based on pairs of events and
their corresponding ground-truth descriptions, with the standard cross entropy
loss. Note that this network requires g(k−1)
∗ as input, which is provided on the
ﬂyduring training. In particular, we feed the ground-truth sentences for each
video one by one. At each iteration, we cache the ﬁnal latent state for the current
sentence and use it as an input for the next one.
Supervised training encourages the caption generation network to emulate
the training sentences word by word. To further improve the quality of the
resultant sentences, we resort to reinforcement learning. In this work, we em-
ploy the Self-Critical Sequence Training (SCST) [16] technique. Particularly, we
consider the caption generation network as an “agent” , and choosing a word an
“action” . Following the practice in [16], we update the network parameters using
approximated policy gradient in the reinforcement learning stage.
The key to reinforcement learning is the design of the rewards. In our design,
we provide rewards at two levels, namely the sentence-level and the paragraph-
level. As mentioned, the network takes in the ground-truth events of a video
sequentially, producing one sentence for each event (conditioned on the previous
state). These sentences together form a paragraph. When a sentence is generated,
it receives a sentence-level reward. When the entire paragraph is completed, it
receives a paragraph-level reward. The reward is deﬁned to be the CIDEr [22]
metric between the generated sentence/paragraph and the ground truth.
Move Forward and Tell: A Progressive Generator of Video Descriptions 9
4.2 Training Event Selection Network
The event selection network is a recurrent network that takes a sequence of can-
didate events as input and produces a sequence of binary indicators (for selecting
a subset of candidates to retain). We train this network in a supervised manner.
Here, the key question is how to obtain the training samples. We accomplish
this in two steps: (1) labeling and (2) generating training sequences.
First, for each video, we use the event localization module to produce a
series of event candidates as ( e1,...,eT). At the same time, we also have a set of
ground-truth events provided by the training set, denoted as ( e∗
1,...,e∗
T∗). For
each ground-truth event e∗
j, we ﬁnd the candidate eithat has the highest overlap
with it, in terms of the temporal IoU, and label it as positive, i.e.settingyi= 1.
All the other event candidates are labeled as negative.
Second, to generate training sequences, we consider three diﬀerent ways:
–(S1) Complete sequences , which simply uses the whole sequence of candidates
for every video, i.e.(e1,e2,···,em).
–(S2) Subsampling at intervals , which samples event candidates at varying
intervals, e.g.(e2,e4,···,em), to obtain a larger set of sequences.
–(S3) Subsampling negatives , which keeps all positive candidates, while ran-
domly sampling the same number of negative candidates in between.
Note that the positive and negative candidates are highly imbalanced. For
each video, positive candidates are sparsely located, while negative ones are
abundant. The scheme (S3) explicitly rebalances their numbers. Our experiment
shows that (S3) often yields the best performance.
The event selection network is trained with the help of the caption generation
network. To be more speciﬁc, whenever the event selection network yields a
positive prediction, the caption feature cktwill be updated based on the caption
generation network, which will be fed as an input to the next recurrent step.
5 Experiment
We report our experiments on ActivityNet Captions [10], where we compared
the proposed framework to various baselines, and conducted ablation studies to
investigate its characteristics.
5.1 Experiment Settings
The ActivityNet Captions dataset [10] is the largest publicly available dataset
for video captioning. This dataset contains 10 ,009 videos for training, and 4 ,917
for validation. Compared to previous datasets [7, 15, 17], it has two orders of
magnitude more videos. The videos in this dataset are 3 minutes long on average.
Each video in the training set has one set of human labeled annotations, and
each video in the validation set has two such sets. Here, a set of annotations
is a series of sentences, each aligned with a long or short segment in the video.
10 Yilei Xiong, Bo Dai, Dahua Lin
About 10% of all segments have overlaps with each other. On average, each set
of annotations contains 3 .65 sentences.
While ActivityNet Captions was originally designed for the task of video
dense captioning, we adapt it to our task with the ground-truth paragraphs
derived by sequentially concatenating the sentences within each set of segment-
based annotations. As a result, for each video in the training set, there is one
ground-truth paragraph, and for each video in the validation set, there are two
ground-truth paragraphs. Since the annotations for testing videos are not pub-
licly accessible, we randomly split the validation set in half, resulting in 2 ,458
videos for tuning hyperparameters, and 2 ,459 videos for performance evaluation.
We setδ= 0.3,L= 100 in the event selection module, and N= 10 in the
captioning module. We separately train the three modules in our framework. In
particular, the event localization module is trained according to [31], where the
localized events have a recall of 63.77% at 0.7 tIoU threshold. For captioning
module, the LSTM hidden size is ﬁxed to 512. As discussed in section 4, we
ﬁrst train the model under the cross-entropy objective using ADAM [9] with an
initial learning rate of 4 ×10−4. We choose the model with best CIDEr score on
the validation set. We then run SCST training initialized with this model. The
reward metric is CIDEr, for both sentence and paragraph reward. SCST train-
ing uses ADAM with a learning rate 5 ×10−5. One batch contains at least 80
events, since events in the same video are fed into a batch simultaneously. For the
event selection module, we train it on a collection of labeled training sequences
prepared as described in Section 4, where each training sequence contains 64
candidate events. We use cross-entropy as the loss function and SGD with mo-
mentum as the optimizer. The learning rate is initialized to 0 .1 and scaled down
by a factor of 0 .1 every 10,000 iterations. We set the SGD momentum to 0 .9,
weight decay to 0 .0005, and batch size to 80.2
5.2 Evaluation
We evaluate the performance using multiple metrics, including BLEU [14], ME-
TEOR [5], CIDEr [22] and Rouge-L [12]. Besides, we notice there exists a general
problem for video captioning results, i.e., repetition or redundancy. This may be
due to that captioning module can’t distinguish detailed events. For example,
the captioner may take both ironing collar and ironing sleeve as ironing. Describ-
ing repeated things deﬁnitely hurts the coherence of the descriptions. However
this can not reﬂect in the above metrics.
To measure this eﬀect, a recent work proposes Self-BLEU [32] by evaluating
how one sentence resembles the rest a generated paragraph. We also propose
another metric, called Repetition Evaluation (RE). Given a generating descrip-
tionci, the number of time an n-gramwkoccurs in it is denoted as hk(ci). The
Repetition Evaluation computes a redundancy score for every description ci:
RE(ci) =∑
kmax (hk(ci)−1,0)∑
khk(ci), (5)
2Code will be made publicly available soon
Move Forward and Tell: A Progressive Generator of Video Descriptions 11
where the gram length, n, takes a large number, like 4 in our experiments. The
corpus-level score is the mean score across all descriptions. Ideally, a description
withnrepetitions would get a score around ( n−1)/n.
5.3 Comparison with Other Methods
We compared our framework with various baselines, which we describe as below.
(1)Sentence-Concat : a simple baseline that equally splits a video into four
disjoint parts, and describes each part with a single sentence using the caption-
ing model as ours. The ﬁnal paragraph is derived by concatenating these four
sentences. Using this baseline, we are able to study the eﬀect of localizing events
in the input video.
(2)Hierarchical-RNN [29]: a more sophisticated way to generate paragraphs
from a video, where a topic RNN generates a sequence of topics to control the
generation of each individual sentence. With the topic embeddings as input, a
sentence RNN generates a sentence for each topic in the sequence.
(3)Dense-Caption [10]: one of the state-of-the-art methods for video dense
captioning, which generates a single sentence for each candidate event and then
concatenate them all into a paragraph, regardless of their similarities. This base-
line is used to demonstrate the diﬀerences between video dense captioning and
video captioning .
(4)Dense-Caption-NMS : a method based on the above Dense-Caption. It se-
lects events from candidate events of Dense-Caption using Non-Maximum Sup-
pression (NMS), removing those that are highly overlapped with others on tem-
poral range.
(5)Semantic-Sum [19]: a recent method that also identiﬁes the video seg-
ments as ours. We ﬁnd that this method gets best performance when setting
sentence length as 3 and using Latent Semantic Analysis [21] in summarization
module.
(6)Move Forward and Tell (MFT) : our proposed framework which progres-
sively select events and produce sentences conditioned on what have been said
before.
(7)GT-Event : this baseline directly applies our captioning module on ground-
truth events . In principle, this should serve as a performance upper bound, as it
has access to the ground-truth event locations.
Table 1 presents the results for diﬀerent methods on ActivityNet Captions,
from which we have the following observations: (1) Dense-Caption performs
very poorly since there are many redundant proposals and therefore repeated
sentences indicating by RE and Self-BLEU metrics. The RE score is terribly
high, meaning two thirds of the descriptions are likely redundant. This clearly
shows the diﬀerences between video dense captioning and video captioning. (2)
Sentence-Concat andDense-Caption-NMS are at a comparable level, much bet-
ter than Dense-Caption . These two methods may beneﬁt from a common as-
pect, i.e.their events are almost not overlapped. But some important events
may not be localized here. (3) Hierarchical-RNN improves the result to the next
level, achieving 25 .53% Rouge-L. This suggests that it produces more coherent"
1709.05324,D:\Database\arxiv\papers\1709.05324.pdf,"What are the limitations of using convolutional neural networks for segmenting small regions in medical images, and how can these limitations be addressed?","Convolutional neural networks can struggle to accurately segment small regions due to the down-sampling operations in pooling layers, which can smooth out fine details. This can be mitigated by modifying the network architecture to strengthen the end-to-end training or by incorporating post-processing techniques like super-voxel or recurrent neural networks.","Figure 2.3: The general architecture of a fully convolutional neural network[28]. The convolutional layers with di ﬀerent ﬁlter size carry out the convolution
operations with the output of the previous layer or the initial image input. The convolutional layers are followed by the pooling layer that down-samples the
dimension of data while mitigating the computational overhead. The fusion link between the network output and that of previous convolutional layers fuses the
information of both deep and shallow layers, which reﬁnes the spatial segmentation of the image.
where iandjrange from 1 to N. The unary potential ψu(xi) is
calculated for each pixel by the FCN model, producing a heat
map with label assignment xi. The pairwise potential is de-
scribed by:
ψp(xi,xj)=µ(xi,xj)K∑
m=1ω(m)k(m)(fi,fj) (2)
where each k(m)is a set of Gaussian kernel functions for the fea-
ture vector fof pixel iandjseparately,ω(m)describes the lin-
ear combination weights and µis a label compatibility function
which introduces a penalty for neighboring similar pixels which
were assigned di ﬀerent labels. The pairwise function classi-
ﬁes pixels with similar feature vectors into the same class so
that di ﬀerent classes are prone to be divided at content bound-
aries. Then the fully connected CRFs model infers the ﬁnal
label of each pixel by using mean ﬁeld approximation, which is
described in detail in Koltun [35].
3. Method
3.1. Problem statement
Given a retinal OCT image IM×N, the goal is to assign each
pixel{xi,j,i∈M,j∈N}to a class cwhere cis in the range of
the class space C={c}={1,···,K}forKclasses. We consider
a 2-class problem that treats the CME regions as the target class
and others as background class.
3.2. Pre-processing
The Pre-processing of retinal OCT images involves 3D de-
noising and region cropping. Due to the speckle noise present
in the images, the BM3D algorithm [36] is used on each 3DOCT volume. This algorithm is a block-matching de-noising
method using sparse 3D transform-domain collaborative ﬁlter-
ing, with the aim of generating high contrast OCT images. The
de-noising process suppresses OCT speckle noise while sharp-
ening the image, ultimately improving the presentation of CME
region. After the de-noising step, the retinal region is cropped
from the OCT volume to eliminate the unrelated vitreous ob-
jects and the tissues beneath the Choroidal layer.
3.3. Network architecture of FCN model
The FCN-8 model adapted the architecture from the VGG-
16 network [29]. This architecture fuses the ﬁnal convolutional
layer with the output from the last two pooling layers to make
an 8 pixel stride prediction net. We modiﬁed the network by
adding an additional score layer to reduce the prediction to 2
classes. We also used a dice layer to compute the overlap with
a dice similarity coe ﬃcient [37] between predicted results and
ground truth labels. By using the pre-trained weights of the con-
volutional kernel, we can transfer pre-trained weights to learn
new features of retinal OCT images.
3.4. Data augmentation
The original data set only contained 110 OCT images. We
enriched the data by applying spatial translations to each im-
age, which involved random translation, rotation, ﬂipping and
cropping. The total number of training images was increased
to 2800 in this manner, and 20% of these images were used for
validation.
3.5. Loss function and optimization
The network is jointly trained with logistic loss and Dice loss.
The multinomial logistic loss, used in original FCN model [28],
4
provides a probabilistic similarity at the pixel level, comparing
the ground truth label and the prediction results. In our model,
the loss function was implemented using softmax with loss
layer, built in Ca ﬀe [33]. We also use Dice loss [37] to com-
pute the spatial overlap between prediction and ground truth.
During the training process, we have used the stochastic gradi-
ent descent (SGD) as an optimizer to reduce the loss function.
4. Experiments
4.1. Experimental datasets
To validate the e ﬀectiveness of our approach, the method was
tested on the same dataset used in previous work by Chiu et al.
[19]. The dataset comprises 110 SD-OCT B-scan images which
are collected from 10 anonymous patients with DME. Selected
B-scans are located at the fovea and at two opposite sides of
the foveal region (1 B-scan at central fovea and 5 B-scans at 2,
5, 10, 15 and 20 slices away from the central region on each
side). These B-scan images are annotated for 7 retinal layers
and intra-layer ﬂuid sections. The image size is 512 ×740. For
each patient, 11 B-scans of the 3D volume data are annotated by
two professional graders using DOCTRAP semi-automatic seg-
mentation software (Duke University, Durham, NC, USA) [38]
or manually annotated. Speciﬁcally, the ﬂuid CME section was
ﬁrstly annotated using DOCTRAP software package, then re-
viewed and manually corrected to reﬁne the segmentation. For
the concrete regions, the grader performed the annotation from
experience by estimating the region in relation to the thickness
of the retinal layer.
4.2. Experimental setting
In our experiments, we adapted the pre-trained FCN-8 model
from Hariharan et al. [39]. The architecture of the network is
shown in Table 1. The model weights were pre-tuned rather
than randomly initialized. A further ﬁne-tuning was performed
to ﬁt the OCT images. The hyperparameter settings for the
training are as follows: the number of training epochs is 30;
the batch size is set to 1; the base learning rate is 1 .0e10−4and
is reduced by an order of magnitude after every 10 epochs; and
the validation interval was done after each epoch. For the in-
put batch, we keep the original pixel values rather than those
subtracted by the mean value. The experiments were run on a
desktop computer with an Intel CPU, 16 GB of RAM, and a
NVIDIA 1080Ti GPU with 16GB VRAM.
The grayscale OCT image data was reformatted to RGB
with channel conversion as required by the network architec-
ture. The training data was stored using an lightning memory-
mapped database backend with PNG compression format for
feature encoding and label encoding. 20% of data was used for
validation process.
We also modiﬁed the parameter setting of the fully connected
CRFs model. The original model [35] was devised for RGB
image inference. In order to adapt the model for grayscale im-
ages, the parameters of the kernel were changed in the pairwise
function. The standard deviation is 2 for the color independent
function, and 0.01 for the color dependent term. We set the
ground truth certainty to 0.6 in order to obtain the best results.4.3. Evaluation metrics
In order to compare our method with Chiu’s work, the eval-
uation of the framework is done based on same the two metrics
reported in [19]. With a manual segmentation by a grader as
ground truth, we measured the Dice overlap coe ﬃcient [40] for
the ﬂuid segmentation. The Dice coe ﬃcient, deﬁned in Equa-
tion 3, calculates the index of overlay of auto-segmented results
Xautoand manual annotation Xmanual . Furthermore, we calcu-
lated the mean Dice coe ﬃcient for each patient and performed
the Wilcoxon matched-pairs test across all patients to evaluate
the di ﬀerence between framework results and the correspond-
ing manual results.
Dice =2|Xauto∩Xmanual|
|Xauto|+|Xmanual|(3)
4.4. Segmentation results
After training the FCN network, we tested the framework
with a separate test data set which was not previously seen by
the network. The selected data set comprises a set of OCT im-
ages exhibiting various morphologies of DME symptoms. In
this section we show the segmentation results of the framework
compared to manual segmentation results for di ﬀerent DME
conditions. We also present the comparison between manual
annotations and the framework results where small edema re-
gions failed to be segmented. Throughout the whole study, our
results were compared with those from [19] in both a quali-
tative and quantitative way using the same metric to show an
improvement in terms of segmentation accuracy.
The appearance of ﬂuid edema in OCT images is often shown
with a higher contrast against surrounding tissue. Figure 4.1 (A)
depicts the foveal region a ﬀected with a ﬂuid edema. The dom-
inant edema detaches retinal layers and small edema regions
surround it sporadically. It can be seen on the framework re-
sults that the boundaries of the dominant edema are accurately
segmented out, and the smaller edema on the right-hand side
is also segmented individually with clear boundaries. Although
two graders annotated the same ﬂuid regions, the framework
shows an improvement in terms of boundary recall. A signiﬁ-
cant inconsistency among manual segmentations on the left side
of the fovea has also been observed. Because our network was
trained using labels previously provided by the ﬁrst grader, it is
prone to be more similar to their results.
Figure 4.2 (A) shows a concrete edema embedded within
retinal layers on the right-hand side of the fovea region. The
distinctive features of this edema are not visually recognizable
in this OCT image due to low contrast within the retinal lay-
ers. However the presence of the edema causes a deformation
of the retinal layers on the right-hand side, with a correspond-
ing change of the layer interval, making them noticeably thicker
than those on left-hand side. It is observed in Fig. 4.2 (D) that
the segmented results obtained with our approach presents a
high correspondence to the manually segmented results. How-
ever, it is worth noting that our approach has segmented several
regions which had been represented separately under manual
annotation as a single large one.
5
Table 1: The architecture of the FCN-8 model used in the experiment.
Type Filter size Stride Filter number Padding Number of blocks Fusion Link
Block 1Convolution 3 1 64 100×2ReLU - - - -
Block 2 Max Pool 2 2 - 0 ×1
Block 3 Convolution 3 1 128 1×2ReLU - - - -
Block 4 Max Pool 2 2 - 0 ×1
Block 5 Convolution 3 1 256 1×3ReLU - - - -
Block 6 Max Pool 2 2 - 0 ×1
Block 7 Convolution 3 1 512 1×3ReLU - - - -
Block 8 Max Pool 2 2 - 0 ×1
Block 9 Convolution 3 1 512 1×3ReLU - - - -
Block 10 Max Pool 2 2 - 0 ×1
Block 11 Convolution 7 1 4096 0×2ReLU - - - -
Block 12 Convolution 1 1 21 0 ×1
Block 13 Deconvolution 4 2 21 -
Block 14 Convolution 1 1 21 0
×1×2 fusionCrop - - - -
Element-wise Fuse - - - -
Deconvolution 4 2 21 -
Block 15 Convolution 1 1 21 0
×1×4 fusionCrop - - - -
Element-wise Fuse - - - -
Deconvolution 16 8 21 -
Block 16 Crop - - - - ×1
Block 17 Convolution 1 1 2 0 ×1
Block 17 Dice - - - - ×1
Block 19 SoftmaxWithLoss - - - - ×1
In Fig. 4.3 (D) we demonstrate results of our framework op-
erating on images depicting more severe morphological con-
ditions. The retinal layer in Fig. 4.3 (A) was destroyed by
a large section of edema which mixed various shapes of ﬂuid
and concrete edemas. The central ﬂuid section is fused with
the concrete part on both the left and the right-hand side of the
images, where both sections were successfully segmented by
our approach. It can be seen that our approach shows that the
segmentation achieved is consistent with the appearance of the
pathology and accurately covers the entire region. Several small
sections were also well segmented but some slimmer region is
not detected since it is too slim to be perceived by the network.
In Fig. 4.5, we present additional results from severe conditions
that present complex structure appearances.
Apart from the segmentation of large areas, there are still
some cases where our approach failed to segment the target re-
gion accurately. One such event occurred when small target
regions were not segmented out, as shown in Fig. 4.1 (D). In
this case, the small regions were a ﬀected by speckle noise and
smoothing e ﬀects caused by the pre-processing resulting in the
small regions being undetectable by the framework, due to theirsmall initial size leading to them being smoothed out after sev-
eral pooling layers. In another case, some target regions were
only partially segmented, as shown in Fig. 4.4.
Figure 4.5 depicts the comparison of our method with the one
reported in Chiu et al. [19] for three segmentation attempts car-
ried out complex retinopathy conditions. It is found that Chiu’s
method performs a slight over-segmentation near the bottom
area. The segmentation results carried out with our method
are closer to those which have been manually segmented. In
Fig. 4.6, three images are shown which depict segmentation
results on concrete and non-obvious ﬂuid edema. The over-
segmentation noted earlier can also be observed , as well as
a false segmentation happening in the region on the left-hand
side. Again, the results obtained with our method have a better
agreement with those obtained via manual segmentation. Fur-
thermore, our results are more adherent to target based on im-
age content. This is also found in Fig. 4.7 where additional re-
sults are presented from the segmentation of an example of ﬂuid
edema. Quantitatively, we compared the Dice overlap coe ﬃ-
cient and p-value of Wilcoxon matched-pairs test for the DME
region. Dice coe ﬃcients were calculated for all 10 patients and
6
Figure 4.1: The segmentation results of ﬂuid edema by graders and the frame-
work. (A): original image of the ﬂuid edema region. (B) and (C): segmented
results by two separate graders. The segmentation of the ﬂuid region is in good
agreement, however the region on the left-hand side was not segmented by the
ﬁrst grader. (D): segmented results obtained with the framework. The ﬂuid sec-
tions are segmented from the boundary. On the left-hand side, a small region of
concrete edema is also detected. However unlike the image obtained from the
second grader, the entire region is not covered.
the process was repeated as per[19]. Speciﬁcally, the Dice coef-
ﬁcient was calculated based on all test images and the Wilcoxon
matched-pairs test was calculated based on the mean Dice coef-
ﬁcient across all patients for our automated method and for the
corresponding results from two graders. It was found that the
Dice coe ﬃcient of our approach is 0 .61±0.21 standing for mean
and standard deviation which outperforms 0 .51±0.34 reported
in [19] (the higher, the better). Both methods are comparable
with the Dice coe ﬃcient between manual graders ( 0 .58±0.32).
The p-value for our approach is 0 .53, which is also better than
0.43 reported in Chiu et al. [19] (where a coe ﬃcient value of 1
indicates perfect agreement).
5. Conclusions and future work
In this paper, we presented a framework for segmenting CME
regions in OCT retinal images by training fully convolutional
Figure 4.2: The segmentation results of concrete edema by graders and the
framework. (A): original image. (B) and (C): segmented results by two separate
graders. The segmentation results by two graders are similar. (D): segmented
results obtained with our framework. On the right-hand side, most of the re-
gion is segmented continuously. The segmented region just under the foveal pit
partially covers the small region of concrete edema at the fovea.
neural networks (FCN) followed by post-precessing involving
the fully connected CRFs (dense CRFs) method. The FCN
model is an end-to-end network that takes advantage of the con-
volutional layers with skipping links to predict tissue classiﬁca-
tion at the pixel level. The skipping link allows the output from
deeper convolutional layers to be fused with that from shallower
convolutional layers. This feature compensates for the coarse
prediction caused by down-sampling in the pooling layer and
produces more reﬁned results. We added the Dice layer to train
the network where the overlap of predicted results and ground
truth was taken into account during training process. We further
ﬁne-tuned the network using the pre-trained model and utilized
data augmentation to compensate for the requirement of large
amounts of training data. The dense CRFs are integrated to
further reﬁne the segmentation results. With dense CRFs, the
recall rate of the boundaries was improved.
7
Figure 4.3: Segmentation results for more severe edema as carried out by
graders and the framework. (A): original image. (B) and (C): segmented results
by two graders. (D): segmented results by the framework. It is seen that both
the concrete an ﬂuid regions exhibiting complex appearances are segmented.
The segmentation results obtained with this framework
demonstrates the potential abilities of end-to-end convolutional
neural networks for OCT retinal image segmentation tasks. We
emphasize that a key advantage of this method it’s potential to
cope with a variety of symptoms. The morphological proper-
ties of a certain retinopathy can be diverse and di ﬀerently rep-
resented depending on di ﬀerent conditions. Conventional seg-
mentation methods for OCT retinal image segmentation were
often tailored for a speciﬁc task and their performance may vary
depending on retinopathy condition. Conversely, convolutional
neural networks can be trained to identify various retinopathy
conditions, thus avoiding the requirement for a multitude of ad
hoc rules. We intend to extend the application of this frame-
work for retinal layer segmentation and segmentation of other
diseases by training the network with larger training datasets
via manual annotations of the symptoms of interest. It is ex-
pected that this learning based approach be adapted for more
complex cases and can be more ﬂexible than ﬁxed mathemati-
Figure 4.4: The failed case of segmentation result. (A): original image. (B) and
(C): segmented results by two graders. (D): segmented results by the frame-
work. It can be seen that part of the concrete edema is not completely seg-
mented where it appears close to the surrounding tissue.
cal model-based approaches. Therefore, this study is valuable
for the ﬁnal goal of developing a universal OCT segmentation
approach.
The results from our experiments also identiﬁed some dis-
advantages of the framework. Although the network architec-
ture involves the skip link to fuse the appearance information
of shallow layers with the deeper layer outputs to make a more
precise prediction, the structural character still presents some
limitations in extracting accurate locations of small target re-
gions. However, this disadvantage may be mitigated by modi-
fying the network structure, namely by strengthening the end-
to-end training.
In this paper, our experiment is focused on segmentation of
cystoid macular edema. As future work, we intend to extend the
method to several ophthalmic conditions, including deformed
retinal layer segmentation caused by age-related macular de-
generation, epiretinal membrane, macular hole and retinal de-
tachment. We also intend to investigate the beneﬁts of using
more e ﬃcient network architectures to improve the segmenta-
tion accuracy. At the same time, we plan to incorporate super-
voxel and recurrent neural networks as a post-processing step
to enhance the inference e ﬃciency with the aim of 3D segmen-
tation which is more practical for quantitative analysis and clin-
ical diagnosis.
Acknowledgements
We appreciate the discussion and suggestion on frame-
work architecture given by Jinchao Liu from Visionmetric Ltd,
and the technical support during the development for system
compatibility and hardware deployment from Nicholas French
(SPS, Kent). We also appreciate the suggestions and comments
about the manuscript from Sally Makin (SPS, Kent).
References
[1] D. Huang, E. A. Swanson, C. P. Lin, J. S. Schuman, W. G. Stinson,
W. Chang, M. R. Hee, T. Flotte, K. Gregory, C. A. Puliaﬁto, et al., Optical
coherence tomography, Science (New York, NY) 254 (1991) 1178.
[2] A. Bradu, K. Kapinchev, F. Barnes, A. Podoleanu, Master slave en-face
oct/slo, Biomedical optics express 6 (2015) 3655–3669.
[3] M. J. Marquesa, S. Rivetb, A. Bradua, A. Podoleanua, Polarization-
sensitive plug-in optical module for a fourier-domain optical coherence
tomography system, in: Proc. of SPIE V ol, volume 10053, pp. 100531B–
1.
[4] J. F. De Boer, B. Cense, B. H. Park, M. C. Pierce, G. J. Tearney, B. E.
Bouma, Improved signal-to-noise ratio in spectral-domain compared
with time-domain optical coherence tomography, Optics letters 28 (2003)
2067–2069.
8
Figure 4.5: A comparison of Chiu’s work and our results. (A) and (B): annotations by two graders. (C): Segmentation results from Chiu’s work. (D): our results.
From the comparison, it can be seen that the results at third row is slightly over-segmented and not coherent to the boundaries. This incoherence is not present in
our results.
Figure 4.6: A comparison between Chiu’s work and the results obtained with our method. (A) and (B): annotations by two graders. (C): Segmentation results from
Chiu’s work. (D): our results. From the comparison, it can be seen that both sub-segmentation and over-segmentation are corrected in our results which have a
higher agreement with manual annotation
[5] B. Potsaid, I. Gorczynska, V . J. Srinivasan, Y . Chen, J. Jiang, A. Cable,
J. G. Fujimoto, Ultrahigh speed spectral /fourier domain oct ophthalmic
imaging at 70,000 to 312,500 axial scans per second, Opt. Express 16
(2008) 15149–15169.
[6] M. Adhi, J. S. Duker, Optical coherence tomography–current and futureapplications, Current opinion in ophthalmology 24 (2013) 213.
[7] V . Manjunath, M. Taha, J. G. Fujimoto, J. S. Duker, Choroidal thickness
in normal eyes measured using cirrus hd optical coherence tomography,
American journal of ophthalmology 150 (2010) 325–329.
[8] R. Margolis, R. F. Spaide, A pilot study of enhanced depth imaging op-
9
Figure 4.7: A comparison between Chiu’s work and the results obtained with
our method. (A) and (B): annotations by two graders. (C): Segmentation results
from Chiu’s work. (D): our results. It is clearly shown that our results are
adhering more consistently to the edema boundary of the manual segmentation.
tical coherence tomography of the choroid in normal eyes, American
journal of ophthalmology 147 (2009) 811–815.
[9] A. E. Fung, G. A. Lalwani, P. J. Rosenfeld, S. R. Dubovy, S. Michels,
W. J. Feuer, C. A. Puliaﬁto, J. L. Davis, H. W. Flynn, M. Esquiabro, An
optical coherence tomography-guided, variable dosing regimen with in-
travitreal ranibizumab (lucentis) for neovascular age-related macular de-
generation, American journal of ophthalmology 143 (2007) 566–583.
[10] B. Haouchine, P. Massin, R. Tadayoni, A. Erginay, A. Gaudric, Diagnosis
of macular pseudoholes and lamellar macular holes by optical coherence
tomography, American journal of ophthalmology 138 (2004) 732–739.
[11] L. Yang, J. B. Jonas, W. Wei, Optical coherence tomography–assisted en-
hanced depth imaging of central serous chorioretinopathycentral serous
chorioretinopathy, Investigative ophthalmology & visual science 54
(2013) 4659–4665.
[12] R. Klein, B. E. Klein, S. E. Moss, K. J. Cruickshanks, The wiscon-
sin epidemiologic study of diabetic retinopathy: Xvii: The 14-year in-
cidence and progression of diabetic retinopathy and associated risk fac-
tors in type 1 diabetes11proprietary interest: none., Ophthalmology 105
(1998) 1801–1815.
[13] L. P. Aiello, R. L. Avery, P. G. Arrigg, B. A. Keyt, H. D. Jampel, S. T.
Shah, L. R. Pasquale, H. Thieme, M. A. Iwamoto, J. E. Park, et al., Vas-
cular endothelial growth factor in ocular ﬂuid of patients with diabetic
retinopathy and other retinal disorders, New England Journal of Medicine
331 (1994) 1480–1487.
[14] T. Otani, S. Kishi, Y . Maruyama, Patterns of diabetic macular edema with
optical coherence tomography, American journal of ophthalmology 127
(1999) 688–693.
[15] M. W. Johnson, Etiology and treatment of macular edema, American
journal of ophthalmology 147 (2009) 11–21.
[16] I. Kozak, V . L. Morrison, T. M. Clark, D.-U. Bartsch, B. R. Lee, I. Falken-
stein, A. M. Tammewar, F. Mojana, W. R. Freeman, Discrepancy between
ﬂuorescein angiography and optical coherence tomography in detection
of macular disease, Retina (Philadelphia, Pa.) 28 (2008) 538.
[17] M. R. Hee, C. A. Puliaﬁto, C. Wong, J. S. Duker, E. Reichel, B. Rutledge,
J. S. Schuman, E. A. Swanson, J. G. Fujimoto, Quantitative assessment
of macular edema with optical coherence tomography, Archives of oph-
thalmology 113 (1995) 1019–1029.
[18] P. A. Dufour, L. Ceklic, H. Abdillahi, S. Schroder, S. De Dzanet, U. Wolf-
Schnurrbusch, J. Kowal, Graph-based multi-surface segmentation of oct
data using trained hard and soft constraints, IEEE transactions on medical
imaging 32 (2013) 531–543.
[19] S. J. Chiu, M. J. Allingham, P. S. Mettu, S. W. Cousins, J. A. Izatt, S. Far-
siu, Kernel regression based segmentation of optical coherence tomog-
raphy images with diabetic macular edema, Biomedical optics express 6
(2015) 1172–1194.
[20] P. P. Srinivasan, S. J. Heﬂin, J. A. Izatt, V . Y . Arshavsky, S. Farsiu, Auto-
matic segmentation of up to ten layer boundaries in sd-oct images of the
mouse retina with and without missing layers due to pathology, Biomed-
ical optics express 5 (2014) 348–365.
[21] S. J. Chiu, X. T. Li, P. Nicholas, C. A. Toth, J. A. Izatt, S. Farsiu, Auto-
matic segmentation of seven retinal layers in sdoct images congruent with
expert manual segmentation, Optics express 18 (2010) 19413–19428.
[22] R. Kaﬁeh, H. Rabbani, M. D. Abramo ﬀ, M. Sonka, Intra-retinal layer
segmentation of 3d optical coherence tomography using coarse grained
diﬀusion map, Medical image analysis 17 (2013) 907–928.
[23] D. Martin, C. Fowlkes, D. Tal, J. Malik, A database of human segmented
natural images and its application to evaluating segmentation algorithmsand measuring ecological statistics, in: Proc. 8th Int’l Conf. Computer
Vision, volume 2, pp. 416–423.
[24] Y . Boykov, G. Funka-Lea, Graph cuts and e ﬃcient nd image segmenta-
tion, International journal of computer vision 70 (2006) 109–131.
[25] V . Kolmogorov, R. Zabin, What energy functions can be minimized via
graph cuts?, IEEE transactions on pattern analysis and machine intelli-
gence 26 (2004) 147–159.
[26] L. R. Ford, D. R. Fulkerson, Maximal ﬂow through a network, Canadian
journal of Mathematics 8 (1956) 399–404.
[27] D. C. Fernandez, Delineating ﬂuid-ﬁlled region boundaries in optical
coherence tomography images of the retina, IEEE transactions on medical
imaging 24 (2005) 929–945.
[28] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for se-
mantic segmentation, in: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 3431–3440.
[29] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-
scale image recognition, arXiv preprint arXiv:1409.1556 (2014).
[30] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V . Vanhoucke, A. Rabinovich, Going deeper with convolutions, in: Pro-
ceedings of the IEEE conference on computer vision and pattern recogni-
tion, pp. 1–9.
[31] C. M. Bishop, Pattern recognition and machine learning, springer, 2006.
[32] P. Liskowski, K. Krawiec, Segmenting retinal blood vessels with¡? pub
newline?¿ deep neural networks, IEEE transactions on medical imaging
35 (2016) 2369–2380.
[33] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, T. Darrell, Ca ﬀe: Convolutional architecture for fast fea-
ture embedding, arXiv preprint arXiv:1408.5093 (2014).
[34] Y . LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. E. Hubbard, L. D. Jackel, Handwritten digit recognition with a back-
propagation network, in: Advances in neural information processing sys-
tems, pp. 396–404.
[35] V . Koltun, E ﬃcient inference in fully connected crfs with gaussian edge
potentials, Adv. Neural Inf. Process. Syst 4 (2011).
[36] K. Dabov, A. Foi, V . Katkovnik, K. Egiazarian, Bm3d image denoising
with shape-adaptive principal component analysis, in: SPARS’09-Signal
Processing with Adaptive Sparse Structured Representations.
[37] F. Milletari, N. Navab, S.-A. Ahmadi, V-net: Fully convolutional neu-
ral networks for volumetric medical image segmentation, in: 3D Vision
(3DV), 2016 Fourth International Conference on, IEEE, pp. 565–571.
[38] J. Y . Lee, S. J. Chiu, P. P. Srinivasan, J. A. Izatt, C. A. Toth, S. Farsiu, G. J.
Jaﬀe, Fully automatic software for retinal thickness in eyes with diabetic
macular edema from images acquired by cirrus and spectralis systems-
fully automatic sd-oct retinal thickness software in dme, Investigative
ophthalmology & visual science 54 (2013) 7595–7602.
[39] B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, J. Malik, Semantic con-
tours from inverse detectors, in: International Conference on Computer
Vision (ICCV).
[40] T. Sørensen, A method of establishing groups of equal amplitude in plant
sociology based on similarity of species and its application to analyses of
the vegetation on danish commons, Biol. Skr. 5 (1948) 1–34.
10"
2404.17511,D:\Database\arxiv\papers\2404.17511.pdf,"While the paper focuses on achieving fairness in graph learning, it acknowledges that existing definitions of fairness, particularly those related to group fairness, can sometimes conflict with individual fairness. How does the paper address this potential conflict and propose a solution that balances both group and individual fairness?","The paper addresses this conflict by introducing a novel definition of ""individual fairness within groups,"" which focuses on ensuring fairness among individuals within the same group, rather than across all groups. This allows for a more nuanced approach to fairness, where individuals within a group are treated equitably while potentially allowing for differences in treatment between groups.","ual fairness within groups by utilizing a node similarity matrix. FairGI also incorporates adversarial
learning to achieve group fairness with respect to EO and SP. This innovative framework represents
a more refined and comprehensive method for achieving both group and individual fairness in graph
learning, encompassing a broader spectrum of fairness considerations.
3 P RELIMINARY
3.1 P RELIMINARIES FOR FAIRNESS LEARNING IN GRAPHS
3.1.1 I NDIVIDUAL FAIRNESS
Individual fairness emphasizes fairness at the individual level, ensuring that individuals with similar
inputs are treated consistently and fairly. We present the definition of individual fairness in graph
learning based on the Lipschitz continuity below (Kang et al., 2020).
Definition 1. (Individual Fairness.) Let G= (V,E)be a graph with node set Vand edge set E.fG
is the graph learning model. Z∈Rn×nzis the output matrix of fG, where nzis the embedding
dimension for nodes, and n=|V|.M∈Rn×nis the similarity matrix of nodes. The model fGis
individual fair if its output matrix Zsatisfies
LIf(Z) =P
vi∈VP
vj∈V∥zi−zj∥2
FM[i, j]
2=Tr(ZTLZ)≤mϵ, (1)
where L∈Rn×nis the Laplacian matrix of M,ϵ∈R+is a constant and mis the number of
nonzero values in M.ziis the ith row of matrix ZandM[i, j]is the element in the ith row and jth
column of matrix M.LIfcan be viewed as the population individual bias of model fG.
3.1.2 G ROUP FAIRNESS
In this paper, we consider two key definitions of group fairness, which are Statistical Parity (SP)
(Dwork et al., 2012) and Equal Opportunity (EO) (Hardt et al., 2016).
Definition 2. (Statistical Parity.)
∆SP=P(ˆy= 1|s= 0)−P(ˆy= 1|s= 1), (2)
where ˆyis the predicted label, yis the ground truth of the label, and sis the sensitive attribute.
Definition 3. (Equal Opportunity.)
∆EO=P(ˆy= 1|y= 1, s= 0)−P(ˆy= 1|y= 1, s= 1), (3)
where ˆyis the predicted label, yis the ground truth of the label, and sis the sensitive attribute.
3.2 P ROBLEM FORMULATION AND NOTATIONS
3.2.1 N OTATIONS
LetG= (V,E,X)be an undirected graph, where Vis the set of nodes and Eis the set of edges. We
have|V|=n. LetX∈Rn×dbe the input matrix of node features with ddimensions. In this paper,
we assume the dataset contains a single sensitive feature characterized by binary values. Let sibe
the sensitive attribute of the ith node in GandS={s1, s2, ..., s n}. Letyibe the target label of the
ith node in G. The sensitive attribute divides the nodes into two groups. Without loss of generation,
we name the group with s= 1, s∈ S as protected group and s= 0, s∈ S as unprotected group.
Still, our methods can be easily extended to sensitive attributes with multiple values.
In this work, we address the unfairness issues in the node classification task on graphs. Our method
ensures both group fairness and individual fairness within groups while preserving comparable ac-
curacy performance. The fair graph learning problem is defined below.
3.2.2 P ROBLEM
LetG= (V,E,X)be an undirected graph with sensitive attribute s∈ S. Denote X∈Rn×das the
matrix of node features. Let Pbe the set of groups of nodes in Gdivided by S, i.e.V=Vp2SVp2,
4
where Vpi={vi|vi∈ V, si= 0, si∈ S} andVp2={vi|vi∈V, si= 1, si∈ S} . A function
fG:G→Rn×dhlearns the node embeddings of G, i.e.,
fG(G,S) =H, (4)
where H∈Rn×dhis the node embedding matrix, |V|=nanddhis the dimension of node em-
beddings. fsatisfies group fairness and individual fairness within groups if and only if Hdoes not
contain sensitive information and for ∀pk∈ P,
∥hi−hj∥2≤ck∥xi−xj∥2,∀i, j∈ Vpk, (5)
where hiisith node embedding learned from f,xiis the ith node feature from Xandck∈R+is
the Lipschitz constant for group pk.
4 M ETHODOLOGY
4.1 F RAMEWORK
We propose a novel framework that balances group fairness and individual fairness in the groups to
address the problem shown in figure 1 and provide a more precise measurement for group fairness.
Both group fairness and individual fairness have limitations in real-world application. While group
fairness focuses on fairness at the group level, it ignores individual fairness within these groups.
Current individual fairness approaches measure individual fairness by Lipchitz Continous (Kang
et al., 2020). However, strictly adhering to individual fairness may lead to conflicts with group fair-
ness (Dwork et al., 2012). Our method provides a novel framework to address the above challenges
by proposing a novel definition of individual fairness within groups. Combining this new definition
with group fairness, we develop a more precise and reliable approach to guarantee fairness in graph
learning.
The detailed algorithm of FairGI is shown in Algorithm 1. Since our method allows for model train-
ing even when sensitive labels are partly missing, we initially train a sensitive attribute estimator
utilized GCN (Kipf & Welling, 2016) to predict the unlabeled sensitive attributes. For the node clas-
sification task, we employ GAT (Wang et al., 2019) to generalize node embedding and predict target
labels. We design a novel loss function to achieve individual fairness within groups in our frame-
work. To guarantee group fairness, we employ an adversarial learning layer that hinders adversaries
from precisely predicting sensitive attributes, thereby reducing the bias from sensitive information.
Unlike FairGNN (Dai & Wang, 2021), which solely optimizes SP, we theoretically demonstrate that
our adversarial loss function can enhance group fairness in terms of both EO and SP. In addition, we
devise a conditional covariance constraint loss function to increase the stability of the adversarial
learning process and prove that optimizing the loss function leads to the minimum value of EO.
Fig. 2 shows the framework of FairGI. In this framework, we employ a GNN classifier for the node
classification task. To ensure individual fairness within groups, we propose a loss function LIfgto
minimize bias among individuals within the same group. To mitigate group bias, we incorporate
adversarial learning and develop covariance loss functions to optimize EO and SP. Additionally, we
enhance the adversarial learning process by integrating a sensitive attribute classifier fS.
The comprehensive loss function of our approach is:
L=LC+LG+αLIfg, (6)
where LCis the loss for node label prediction, LGrepresents the loss for debiasing group unfairness,
andLIfgrepresents the loss for mitigating individual unfairness within groups.
4.2 O PTIMIZATION OF INDIVIDUAL FAIRNESS WITHIN GROUPS
4.2.1 C HALLENGES OF BALANCING INDIVIDUAL FAIRNESS AND GROUP FAIRNESS
We can observe that the definitions of group fairness, especially SP and EO, may contradict indi-
vidual fairness in specific circumstances. As noted by (Dwork et al., 2012), potential discrepancies
can arise between group and individual fairness when the distance between groups is significant.
Assume Γrepresents the protected group and Γ′denotes the unprotected group. If there is a con-
siderable distance between individuals in Γand those in Γ′, strictly adhering to individual fairness
5
Input graphGNN Classifier𝑓&GCN sensitive attribute classifier𝑓,""𝑦̂𝑠𝑠𝐿,-./̂𝑠′𝐿&𝐿%𝐿&'(
Similarity matrix of input nodes𝐿)*+Distance of outputsbetween nodesAdversary𝑓%𝑦
Individual fairness within groupsGroup fairness𝑣0𝑣1𝑣2𝑣3𝑣4Figure 2: Overview of FairGI. Our method comprises three main parts, i.e., an individual fairness
module, an group fairness module and a GNN classifier for node prediction.
Algorithm 1 Algorithm of our framework
1:Input: G(V,E), X,S
2:Output: Sensitive attribute classifier fS, node classifier fC, node label prediction ˆy
3:Train sensitive attribute classifier fSby given sensitive attribute labels using loss function LSens
in Eq.(11).
4:repeat
5: Obtain estimated sensitive attribute ˆs,ˆs∈˜SbyfS.
6: Optimize fGto predict the node label by loss function LCin Eq.(19)
7: Optimize fGto debias group unfairness by loss function LGin Eq.(18)
8: Optimize fGto debias individual unfairness within groups by loss function LIfgin Eq.(10)
9: Optimize adversary fAbyLAin Eq.(23)
10:until Converge
11:return fS,fGandˆy
may not guarantee similar outcomes for both groups. Thus, this can lead to a conflict with the goal
of group fairness, which seeks to maintain equal treatment and opportunity for all groups. Inspired
by (Dwork et al., 2012), we alleviate the conflicts between SP and individual fairness by loosening
the Lipschitz restriction between different groups.
4.2.2 L OSS FUNCTION FOR INDIVIDUAL FAIRNESS WITHIN GROUPS
To balance group and individual fairness and address the limitation of group fairness, we introduce
a novel definition of individual fairness within groups. Based on the proposed definition, we design
a loss function that ensures individual fairness within the known groups. Note that our approach is
also applicable to groups that are not mutually exclusive.
Definition 4. (Individual Fairness within Groups.) The measurement of individual fairness within
group pis:
Lp(Z) =P
vi∈VpP
vj∈Vp∥zi−zj∥2
2M[i, j]
np, (7)
where Vprepresents the set of nodes in group pandZdenotes the node embedding matrix. ziis
theith row of Z.Mis the similarity matrix of nodes. Lis the laplacian matrix of Mandnpis the
number of pairwise nodes in group pwith nonzero similarities in M.
Our objective focuses on optimizing individual fairness within each group, guaranteeing equitable
treatment for individuals in the same groups. To achieve this, our loss function is designed to mini-
mize the maximum individual unfairness over all groups in our loss function.
6
Firstly, we consider the loss function that minimizes the maximum unfairness over all the groups as
follows:
f∗=argminf∈H{max p∈PLp(Z)}, (8)
where Z=f(·),His a class of graph learning models, Pis the set of groups and Lpis the
individual loss for group p. Motivated by the concept of guaranteeing the optimal situation for the
most disadvantaged group, as presented in (Diana et al., 2021), we employ the minimax loss function
from Eq. (8). This approach prioritizes minimizing the maximum unfairness across groups, rather
than simply aggregating individual fairness within each group to ensure a more equitable outcome.
The optimal solution in Eq.(8) is hard to obtain, thus we can relax the loss function as expressed
in Eq. (9). Given an error bound γfor each group, the extension of the minimax problem can be
formulated as follows:
minmize f∈HX
p∈PLp(Z),subject to Lp(f)≤γ, p∈ P.(9)
In our framework, we not only focus on the loss of individual fairness within groups in Eq.(9), but
also consider the loss of label prediction and group fairness. Thus, we further convert Eq.(9) into
the unconstrained loss function shown in Eq.(10) by introducing Lagrange multiplier λpto the loss
function. We can achieve individual fairness within groups by minimizing the loss function below:
LIfg=X
p∈PLp(Z) +X
p∈Pλp(Lp(Z)−γ), (10)
where λpandγare hyperparameters in our model.
4.3 E NHANCING GROUP FAIRNESS THROUGH ENSURING EQUAL OPPORTUNITY AND
STATISTICAL PARITY
In this section, we enhance group fairness by introducing a novel loss function tailored to opti-
mize EO. This approach marks a significant difference from the covariance constraint proposed by
FairGNN (Dai & Wang, 2021), which only optimizes SP. Additionally, our methodology integrates
the loss functions for EO and SP, enabling the simultaneous optimization of both EO and SP, thus
presenting a comprehensive framework for enhancing group fairness in model predictions.
4.3.1 A DVERSERIAL LEARNING
As we address the circumstance where certain sensitive labels are absent, we utilized GCN (Kipf
& Welling, 2016) to train the sensitive estimator fS, and the loss function for the sensitive label
prediction is:
LSens=−1
|V|X
i∈V((si)log(ˆsi) + (1 −si)log(1−ˆsi)), (11)
where siis the sensitive attribute for the ith node, ˆsiis the predicted senstive labels.
To optimize SP, the min-max loss function of adversarial learning is (Dai & Wang, 2021):
min
ΘCmax
ΘALA1=Eh∼p(h|ˆs=1)[log(fA(h))] +Eh∼p(h|ˆs=0)[log(1−fA(h))], (12)
where ΘCis the parameters for graph classifier fC,ΘAis the parameters for adversary fAandh
is the node presentation of the last layer of GNN classifier fC.h∈p(h|ˆs= 1) denotes sampling
nodes from the protected group within the graph G.
While FairGNN demonstrates that optimizing Eq. (12) can achieve the minimum SP (Dai & Wang,
2021) in the GNN classifier, it does not guarantee the attainment of the minimum EO. Both EO
and SP are significant metrics for group fairness. Optimizing solely for SP can adversely affect the
performance of EO, leading to model bias. To address the shortage of Eq. (12), we propose a novel
min-max loss function designed for adversarial learning to achieve the minimum EO in Eq. (13).
min
ΘCmax
ΘALA2=Eh∼p(h|ˆs=1,y=1)[log(fA(h))] +Eh∼p(h|ˆs=0,y=1)[log(1−fA(h))]. (13)
7
The Theorem 6 in the Appendix demonstrates that the optimal solution of Eq. (13) ensures the
GNN classifier satisfies ∆EO= 0, given two easily attainable assumptions. In addition, we can
also mitigate sensitive information by letting fApredict the sensitive attribute closer to a uniform
distribution, as inspired by Gong et al. (2020).
To simultaneously optimize EO and SP, we combining Eq.(12) and Eq.(13) to obtain the loss func-
tion for adversarial learning as:
LA=LA1+LA2. (14)
4.3.2 C OVARIANCE CONSTRAINT
The limitation of adversarial debiasing is instability. Similar to adversarial learning, FairGNN fo-
cuses on optimizing Statistical Parity (SP) through the covariance constraint loss as described by
(Dai & Wang, 2021) but ignores the EO. Eq. (15) shows the
LR1=|Cov(ˆs,ˆy)|=|E[(ˆs−E[ˆs](ˆy−E[ˆy])]|, (15)
Note that ∆EO= 0is not the prerequisite of LR1= 0. Eq. (15) does not enhance the model fairness
in terms of EO.
Thus, we propose a covariance constraint loss function to optimize EO as follows:
LR2=|Cov(ˆs,ˆy|y= 1)|=|E[(ˆs−E[ˆs|y= 1](ˆ y−E[ˆy)|y= 1]|y= 1]|. (16)
Theorem 7 in the Appendix shows that under the mild assumption, LR2= 0 is the prerequisite of
∆EO= 0. Consequently, Eq. (16) effectively optimizes EO. We can further enhance group fairness
in our model by optimizing EO and SP using Eq. (17).
LCov=LR1+LR2. (17)
In conclusion, the loss function that we utilize to mitigate group fairness is:
LG=βLA+γLCov, (18)
where βandγare hyperparameters.
4.4 N ODE PREDICTION
For the node prediction task, we employ GAT (Wang et al., 2019) to predict node labels. The loss
function for GNN classifier fCis:
LC=−1
|V|X
i∈V((yi)log( ˆyi) + (1 −yi)log(1−ˆyi)). (19)
In the conclusion, we highlight the distinct contributions of our work in comparison to existing
methodologies. Our research introduces a novel problem statement that aims to address both group
fairness and individual fairness. This dual focus sets our work apart from others like FairGNN, which
only considers group fairness. Moreover, when considering group fairness, our approach not only
optimizes for SP but also for EO, providing a more comprehensive group fairness optimization. In
contrast, FairGNN’s loss functions are solely directed at SP. This broader and more comprehensive
fairness optimization strategy underscores the innovative contributions of our framework.
5 E XPERIMENTS
In this section, we conduct a comprehensive comparison between our proposed method and other
cutting-edge models, evaluating their performance on real-world datasets to demonstrate the effec-
tiveness of our approach.
5.1 D ATASETS AND BASELINES
In this experiment, we utilize three public datasets, Pokec_n Dai & Wang (2021) , NBA Dai & Wang
(2021), and Credit Yeh & Lien (2009).
8
Table 1: Comparisons of our method and baselines on three datasets. ↑denotes the larger value is
the better and ↓indicates the smaller value is the better. Best performances are in bold.
Dataset Method Acc↑ AUC↑ ∆SP↓ ∆EO↓ MaxIG ↓ IF↓
GCN 68.82±0.17 73.98±0.07 2.21±0.61 3.17±1.10 5.69±0.08 899.54 ±13.10
GAT 69.14±0.68 74.24±0.90 1.40±0.64 2.86±0.49 6.10±0.62 880.89 ±89.97
PRF 55.39±0.08 53.83±0.02 1.08±0.09 1.82±0.18 0.64±0.01 101.26 ±1.27
InFoRM 68.77±0.39 73.69±0.10 1.84±0.69 3.58±1.15 1.52±0.05 238.41 ±7.97
Pokec-n NIFTY 65.97±0.57 69.87±0.64 4.62±0.52 7.32±0.94 1.87±0.15 310.84 ±26.25
GUIDE 69.46±0.04 74.67±0.01 2.95±0.11 0.80±0.18 0.61±0.00 101.77 ±0.28
FairGNN 69.86±0.30 75.58±0.52 0.87±0.38 2.00±1.08 1.26±0.96 192.29 ±142.06
Ours 68.86±0.58 75.07±0.1 0.63±0.37 0.75±0.30 0.47±0.09 67.41±13.68
GCN 69.08±2.02 74.20±1.69 17.12±7.10 10.03±4.92 25.99±2.12 17.87±1.74
GAT 70.80±3.70 72.48±4.32 11.90±8.94 16.70±10.57 21.14±10.86 20.79±9.95
PRF 55.58±0.93 58.26±4.45 1.99±0.99 2.22±1.65 4.47±2.25 3.06±1.55
InFoRM 68.71±2.78 74.19±1.85 16.64±5.64 12.75±6.80 26.52±8.25 18.82±5.17
NBA NIFTY 70.55±2.30 76.18±0.83 11.82±4.28 5.69±3.48 17.14±5.01 11.93±3.43
GUIDE 63.31±2.86 67.46±3.44 13.89±5.11 10.50±4.76 29.54±16.34 19.84±10.75
FairGNN 72.95±2.10 77.37±1.11 1.19±0.43 0.62±0.43 10.91±12.59 18.51±23.72
Ours 73.13±1.75 79.28±0.46 0.43±0.28 0.62±0.32 0.12±0.11 0.08±0.07
GCN 70.35±0.99 65.18±6.58 14.55±6.13 13.92±6.00 5.18±1.34 39.11±6.69
GAT 70.89±1.84 71.30±1.64 15.95±2.40 15.96±2.77 5.88±3.34 35.28±15.41
PRF 69.87±0.09 69.90±0.04 14.63±0.78 13.96±0.79 5.80±0.08 39.79±0.63
InFoRM 69.91±3.70 65.55±5.6 14.80±3.84 14.82±4.18 4.09±1.68 33.62±13.86
Credit NIFTY 68.74±2.34 68.84±0.41 9.91±0.30 9.07±0.63 2.92±1.18 24.73±9.75
GUIDE 62.01±0.01 67.44±0.01 13.88±0.10 13.54±0.06 0.22±0.01 1.90±0.01
FairGNN 73.40±0.15 70.18±0.03 3.91±0.11 3.49±0.25 1.88±0.09 13.84±0.70
Ours 74.09±0.13 68.81±0.11 3.84±0.22 2.60±0.20 0.22±0.01 1.84±0.10
We compare our method with other state-of-art fairness models for graph learning. In our compar-
ison, we include basic GNNs like GCN (Kipf & Welling, 2016) and GAT (Velickovic et al., 2017),
which don’t fix bias. We also include GNNs like PFR (Lahoti et al., 2019) and InFoRM (Kang et al.,
2020), aiming at individual fairness. To compare group fairness methods, we include FairGNN (Dai
& Wang, 2021). Additionally, we include NIFTY (Agarwal et al., 2021), a method based on causal
inference for fairness, as a baseline in our study. Further descriptions of datasets and baselines can
be found in the Appendix.
5.1.1 E VALUATION METRICS
In this experiment, we primarily focus on analyzing and comparing individual fairness within groups
as well as group fairness. Furthermore, we assess the performance of the prediction task by employ-
ing metrics such as Area Under the Curve (AUC) and Accuracy (ACC). We include MaxIG, IF, SP,
and EO in the experiments for the fairness evaluation metrics. MaxIG is defined as:
MaxIG = max( Lp(Z)), p∈ P, (20)
where Lp(·)can be computed in Eq. (7) and Zis the output of GNN Classifier.
5.2 R ESULTS AND ANALYSIS
5.2.1 I NDIVIDUAL UNFAIRNESS AND GROUP UNFAIRNESS IN GRAPH NEURAL NETWORKS
Based on the experimental results presented in Table 1, several key findings emerge regarding the
performance and biases of various GNNs.
Traditional GNNs, such as GCN and GAT, exhibit both individual and group biases. This suggests
that while these models may have good performance, they do not adequately handle fairness issues.
Models that address group fairness, such as FairGNN, demonstrate good performance in group fair-
ness metrics like SP and EO, but struggle with individual fairness metrics, such as IF and MaxIG.
This underscores the challenge of simultaneously optimizing for both group and individual fairness.
On the contrary, models like PRF, InFoRM, NIFTY , and GUIDE, which primarily target individ-
ual fairness, perform well in mitigating individual biases. However, they have poor performance in
group fairness. This dichotomy indicates a potential trade-off regarding group-level fairness while
promoting individual fairness. These findings emphasize the need for more comprehensive solutions
that simultaneously address individual and group biases.
9
Ours Ours w/o Ifg Ours w/o EO0.200.250.300.350.400.450.50MaxIG(%)
Ours Ours w/o Ifg Ours w/o EO3.503.754.004.254.504.755.00SP(%)
Ours Ours w/o Ifg Ours w/o EO2.42.62.83.03.23.43.6EO(%)Figure 3: Comparison of our method, our method without loss function of individual fairness within
groups, our method without the optimization for EO.
5.2.2 E FFECTIVENESS OF FAIRGIIN MITIGATING BOTH INDIVIDUAL FAIRNESS AND GROUP
FAIRNESS
The results in Table 1 highlight the efficacy of our approach, leading to two primary observations:
(1) Our method outperforms competing methods by ensuring superior group fairness and intra-
group individual fairness while retaining comparable prediction accuracy and AUC of the ROC
curve; (2) While our technique is constrained only to fairness within groups, it remarkably achieves
superior population individual fairness compared to baselines. This suggests that we can attain the
pinnacle of population individual fairness by concentrating solely on intra-group individual fairness
and overlooking inter-group individual fairness. This outcome is intuitively reasonable given the
potential substantial variances among individuals from different groups.
5.3 A BLATION STUDIES
In the ablation study, we examine the impact of two modules, individual fairness within groups and
optimization of equal opportunity, on the performance of our method. We conduct a comparison
between our method and two of its variants using the Credit dataset. The first variant, ""ours w/o Ifg,""
omits the individual fairness within groups loss LIfgfrom our method. The second variant, ""ours
w/o EO,"" eliminates the optimizations for equal opportunity, specifically the loss functions LA2and
LR2, from our method.
Figure 3 illustrates the comparative results. We can observe that upon the removal of LIfg, there
is a noticeable increase in MaxIG, EO, and SP, with MaxIG experiencing the most significant rise.
This strongly attests to the efficacy of the loss function LIfgin enhancing individual fairness. When
we disregard the optimizations for EO, MaxIG remains relatively unchanged while both EO and SP
increase. This highlights the crucial role of LA2andLCov2in optimizing group fairness.
6 C ONCLUSION
In this paper, we present an innovative problem that considers both group fairness and individual
fairness within groups. In this particular context, we propose a novel definition named MaxIG for
individual fairness within groups. Furthermore, we propose a novel framework named FairGI to
achieve both group fairness and individual fairness within groups in graph learning. FairGI leverages
the similarity matrix to mitigate individual unfairness within groups. Additionally, it exploits the
principles of adversarial learning to mitigate group unfairness. Extensive experiments demonstrate
that FairGI achieves the best results in fairness and maintains comparable prediction performance.
REFERENCES
Chirag Agarwal, Himabindu Lakkaraju, and Marinka Zitnik. Towards a unified framework for fair
and stable graph representation learning. In Uncertainty in Artificial Intelligence , pp. 2114–2124.
PMLR, 2021.
Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and Kush R
Varshney. Optimized pre-processing for discrimination prevention. Advances in neural informa-
tion processing systems , 30, 2017.
10
Aaron Chalfin, Oren Danieli, Andrew Hillis, Zubin Jelveh, Michael Luca, Jens Ludwig, and Sendhil
Mullainathan. Productivity and selection of human capital with machine learning. American
Economic Review , 106(5):124–127, 2016.
Zicun Cong, Baoxu Shi, Shan Li, Jaewon Yang, Qi He, and Jian Pei. Fairsample: Training fair and
accurate graph convolutional neural networks efficiently. IEEE Transactions on Knowledge and
Data Engineering , 2023.
Enyan Dai and Suhang Wang. Say no to the discrimination: Learning fair graph neural networks with
limited sensitive attribute information. In Proceedings of the 14th ACM International Conference
on Web Search and Data Mining , pp. 680–688, 2021.
Enyan Dai and Suhang Wang. Learning fair graph neural networks with limited and private sensitive
attribute information. IEEE Transactions on Knowledge and Data Engineering , 2022.
Emily Diana, Wesley Gill, Michael Kearns, Krishnaram Kenthapadi, and Aaron Roth. Minimax
group fairness: Algorithms and experiments. In Proceedings of the 2021 AAAI/ACM Conference
on AI, Ethics, and Society , pp. 66–76, 2021.
Julia Dressel and Hany Farid. The accuracy, fairness, and limits of predicting recidivism. Science
advances , 4(1):eaao5580, 2018.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd innovations in theoretical computer science confer-
ence, pp. 214–226, 2012.
Sixue Gong, Xiaoming Liu, and Anil K Jain. Jointly de-biasing face recognition and demographic
attribute estimation. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,
August 23–28, 2020, Proceedings, Part XXIX 16 , pp. 330–347. Springer, 2020.
Dongliang Guo, Zhixuan Chu, and Sheng Li. Fair attribute completion on graph with missing
attributes. arXiv preprint arXiv:2302.12977 , 2023a.
Zhimeng Guo, Jialiang Li, Teng Xiao, Yao Ma, and Suhang Wang. Towards fair graph neural
networks via graph counterfactual. In Proceedings of the 32nd ACM International Conference
on Information and Knowledge Management , pp. 669–678, 2023b.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
Advances in neural information processing systems , 30, 2017.
Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances
in neural information processing systems , 29, 2016.
Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrim-
ination. Knowledge and information systems , 33(1):1–33, 2012.
Faisal Kamiran, Asim Karim, and Xiangliang Zhang. Decision theory for discrimination-aware
classification. In 2012 IEEE 12th international conference on data mining , pp. 924–929. IEEE,
2012.
Jian Kang, Jingrui He, Ross Maciejewski, and Hanghang Tong. Inform: Individual fairness on
graph mining. In Proceedings of the 26th ACM SIGKDD international conference on knowledge
discovery & data mining , pp. 379–389, 2020.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907 , 2016.
Preethi Lahoti, Krishna P Gummadi, and Gerhard Weikum. Operationalizing individual fairness
with pairwise fair representations. arXiv preprint arXiv:1907.01439 , 2019.
Martin Leo, Suneel Sharma, and Koilakuntla Maddulety. Machine learning in banking risk manage-
ment: A literature review. Risks , 7(1):29, 2019.
Yibo Li, Xiao Wang, Yujie Xing, Shaohua Fan, Ruijia Wang, Yaoqi Liu, and Chuan Shi. Graph
fairness learning under distribution shifts. arXiv preprint arXiv:2401.16784 , 2024.
11"
2311.01468,D:\Database\arxiv\papers\2311.01468.pdf,"While the paper highlights the success of GPT-J in ScienceWorld, it also mentions limitations of LLMs in general.  What are some of the potential risks associated with using LLMs to control real-world systems, and how do these risks relate to the specific limitations of LLMs discussed in the paper?","The paper warns that LLMs, despite their impressive capabilities, have unpredictable behavior and knowledge gaps.  This unpredictability poses a significant risk when using LLMs to control real-world systems, as their actions could lead to unintended consequences, potentially causing damage or harm.  The paper specifically mentions the risk of LLMs interacting with computer APIs or physical machinery in ways that could have negative real-world effects.","Games Actions (% of total)
Train ScoreStd.
Dev.Improv. Won Lost Valid A Vs IOs IS RAs Other
1DRRN N/A 17.95 1 .0x
GPT-J
2 All train Markov 7 359 24 .74 1 .05 1 .4x 117 74 61.13 3 .51 31 .14 2 .43 1 .77 0 .02
3 All train 7 359 62 .57 4 .32 3.5x 1 012 383 90.51 0.06 4 .68 2 .51 2 .07 0 .17
4 No variations 3 589 63.35 6.94 3.5x 1 037 372 90.39 0 .07 4 .28 3 .08 2 .00 0 .19
5 Up to 18 games 480 39 .78 2 .35 2 .2x 479 682 83.59 0 .39 11 .46 2 .81 1 .46 0 .30
Table 1: ScienceWorld scores over the 1 819 games that comprise the testset. The train column shows the number
oftrain games available during training. Improv. = relative improvement over DRRN. A Vs = Affordance Violations;
IOs = Invalid Objects; IS = Invalid Syntax; RAs = Redundant Actions. Note that the sum of won and lost games
does not equal the total number of testgames; as in many games, agents obtain a score that is neither 0 (lose) nor
100 (win).
games end if an agent fails, wins, or reaches the
action/step limit. We discuss the effect of the 100
action limit on evaluation in Appendix E.
Out of the box, ScienceWorld only reports a
player’s score, no other performance metrics. To
better understand a model’s behavior, we analyzed
our game transcripts to count the numbers of games
lost and won (0 or 100 points) and to classify each
action emitted by GPT-J as either valid or one of
five categories of invalid actions: Affordance Vio-
lations (A Vs, e.g., the agent tried to pour a chair),
Invalid Objects (IOs, i.e., the agent tries to interact
with a non-existent, hallucinated object), Invalid
Syntax (IS, i.e., the action is valid English, but is
not a valid ScienceWorld command), Redundant
Actions (RAs, e.g., the agent tries to open an al-
ready open door), and Other. We present our re-
sults in Table 1, where each score is the mean over
five training runs with different random seeds. All
our GPT-J-based agents outperform the DRRN, the
best-performing agent in the original ScienceWorld
evaluation (Wang et al., 2022).
All train Markov , the GPT-J model trained on
the entire training data using the Markov assump-
tion (i.e., conditioning actions only on the prior
action and the game feedback) outperforms DRRN
by a factor of 1.4 (row 2). Only 61.13% of the
actions emitted by this model are valid, and almost
a third (31.14%) involve non-existing objects. Our
result starkly contrasts those of Wang et al. (2022)
who evaluated an LLM trained with the Markov
assumption (a T5 architecture (Raffel et al., 2020)
initialized with the Maccaw weights (Tafjord and
Clark, 2021)) and found it performed poorly. De-
spite T5 being twice the size of the GPT-J in our
experiments (11B parameters vs. 6B), it only ob-tained 8 points on the entire test set, 3.1 times less
than our Markov-instructed GPT-J (our weakest
model). We postulate that our Markov-instructed
GPT-J agent outperforms T5 due to our more nat-
ural formulation of the task and GPT-J’s longer
maximum input length (2 048 word pieces vs. 512).
All train , the model trained on the entire set of
7 359 games, and which conditioned its actions on
as much history as possible (on average the previ-
ous 43.42 actions, see Appendix A), outperformed
DRRN by a factor of 3.5 (row 3). The model won
1 012 games (55% of the 1 819 test games) and
rarely emitted invalid actions (90.51% action valid-
ity).
Adjusting the training data such that each vari-
ation only appears with a single solution reduces
the training data to slightly less than half. However,
the resulting model ( No variations , row 4) obtains
a score almost one point higher than using all train-
ing (row 3). The model still wins, on average, 1 037
games (57%) with an action validity of 90.39%.
If we evaluate these two non-Markov models
(All train training and No variations using the eval-
uation methodology of Lin et al. (2023), the scores
change little ( 62.59vs.62.57forAll train ;61.24
vs.63.35forNo variations ). While these scores
are far from the top performing SwiftSage system
(T5 + GPT-4 ), they are similar to the 62.22score
obtained by the T5 + GPT-3.5-turbo SwiftSage sys-
tem (Lin et al., 2023, Table 3). Thus, the single
model 6B-parameter GPT-J model closes the gap
to the approximately 29 times larger dual model
which incorporated GPT-3.5-turbo.
Interestingly, even when only 480 games are
available for training (a mere 6.5% of the train-
ing data), GPT-J learns to play ScienceWorld and
RAs (%) +P. RAs (%) Score
All train Markov 1.77 0 .95 +0 .03
All train 2.07 0 .99−0.06
No variations 2.00 0 .74−0.14
Up to 18 games 1.46 0 .67 +0 .07
Table 2: Percentage of Redundant Actions (RAs) emit-
ted by our models before and after adding the precondi-
tions system (+P) and the change in SW scores.
still achieves a 2.2x improvement over DRRN ( Up
to 18 games , row 5). This model wins far fewer
games (26.3%), emits fewer valid actions (83.59%),
and has a tendency to try to interact with halluci-
nated objects (11.46% of its actions involve non-
existing objects). Despite the lower improvement
over DRRN, this result indicates that GPT-J can
generalize from even small amounts of training
data. We hypothesize that the background knowl-
edge GPT-J gained during its pre-training drives its
ability to generalize from such little training data.
All our results have a standard deviation of only
a few points, indicating good stability over the five
training runs with different seeds. However, like
Wang et al., we find that our models’ performance
varies greatly across the 30 tasks. We discuss the
per-task performance in Appendix D.
We show the effect of the preconditions system
in Table 2. The preconditions system almost halves
the percentage of redundant actions passed from
our models to ScienceWorld. However, the Sci-
enceWorld score changes only a fraction of a point,
inconsistently, up or down. The lack of change in
the ScienceWorld score is not surprising. We eval-
uate all models with an environment step/actions
limit of 100, meaning we can interpret the percent-
ages in the RAs column as the average count of
redundant actions per game. Even if the precondi-
tions system prevented all redundant actions, that
would only give each agent about two extra ac-
tions per game, not enough to influence the final
score meaningfully. Despite the lack of increase
in the ScienceWorld score, the preconditions sys-
tem achieves the goal of removing non-common
sensical actions from the LLM-game interaction.
5 Discussion and Conclusion
LLMs still exhibit non-common-sensical behav-
iors, such as emitting redundant actions or trying
to interact with nonexistent objects. External com-
ponents such as the precondition system shown inthis work can help alleviate some of these behav-
iors. Other external components could assist LLMs
with structured knowledge. Such knowledge could
be quickly inspected and updated by humans. Our
team has yet to succeed in significantly assisting
the LLM using external knowledge sources. While
we have managed to teach the LLM to interact with
a Q&A system that could answer Is-A questions for
the four classification tasks, such knowledge was
already present in the LLM, as evidenced by the
high scores on classification tasks in Appendix D.
Despite Wang et al. (2022)’s findings that LLMs
perform poorly in ScienceWorld, we find that with
a careful formulation of the prompt and access to
prior interactions with the game, even a single rea-
sonably sized LLM achieves a 3.5x improvement
over DRRN. The 6B-parameter GPT-J models can
match the performance of SwiftSage T5 + GPT-
3.5-turbo , a more complex architecture that uses a
model 29 times larger than GPT-J.
Even when learning from just a few hundred
games (6.5% of the training data), GPT-J achieves
a 2.2x improvement over DRRN. Despite the size-
able overall score improvement, the LLM performs
poorly on some ScienceWorld tasks. The strong
correlation between the performance of our GPT-
J-based models suggests that some tasks are gen-
uinely more difficult for the LLM.
Limitations
This paper shows that an agent based on GPT-J, a
Large Language Model (LLM), can perform ele-
mentary science experiments in a simulated text
environment. Our GPT-J agent outperforms the
state-of-the-art reinforcement learning models by
a factor of 3.5 using a single model rather than
DRRN’s 30 task-specific models. While GPT-J’s
performance is impressive, readers should remem-
ber that Large Language Models like GPT-J have
unforeseen gaps in knowledge and their behavior
is often unpredictable. It is appealing to look at our
experience with GPT-J and assume that one can
fine-tune LLMs and then task them with operating
machinery or robots in the real world. However,
due to LLMs’ unpredictable behavior, such an ap-
proach could result in material damage or even
injure humans, animals, or the environment. This
warning also applies to cases where LLMs are al-
lowed to operate computer APIs which, despite
their virtual nature, can have undesired effects in
the real world. Should LLMs be allowed to oper-
ate APIs or real-world machinery, they should not
be given complete control over the tools but oper-
ate within carefully chosen boundaries strictly en-
forced through software or through physical means.
When considering using LLMs to plan and exe-
cute in new domains, one should remember that the
LLM used in this paper benefited not just from the
opportunity to learn from example experiments but
also from its background knowledge. By its nature,
knowledge of elementary science is widely spread
in texts on the web, such as those that comprised
GPT-J’s pre-training data. For a detailed presen-
tation of GPT-J’s pre-training corpus, we direct
readers to Gao et al. (2020).
Finally, when interacting in ScienceWorld,
LLMs have a text-only view of the environment.
The environment is described via text with just the
detail necessary for accomplishing the tasks, akin
to having access to perfect Computer Vision and a
system that can filter out trifling information, allow-
ing the LLM to focus on planning. Access to per-
fect Computer Vision and relevance filters is a sub-
stantial limitation common to all approaches that
operate in text-only environments. More research
is necessary to understand how to teach LLMs to in-
corporate information from other modalities, such
as sight and sound. Approaches such as PaLM-
E (Driess et al., 2023) have shown that it is possi-
ble to integrate multiple modalities into Extremely
Large Language Models. But, at 526B parameters,
these models’ compute and energy requirements
seem to make them impractical for onboard pro-
cessing of mobile robotics platforms.
Ethics Statement
Environmental impact questions arise for any sci-
entific work that involves Large Language Mod-
els (LLMs). Most of the energy consumption of
LLMs occurs during pre-training (Patterson et al.,
2021). This work relies on already pre-trained lan-
guage models, which we only fine-tune. GPT-J,
an LLM with 6B parameters, is small enough to
be fine-tuned on a single modern compute node,
contrasted with LLMs using hundreds of billions of
parameters. For fine-tuning, we use nodes with 4x
NVIDIA A6000 GPUs, and we further increase the
training efficiency by offloading the optimizer from
GPU memory via DeepSpeed (Ren et al., 2021; Ra-
jbhandari et al., 2020), thus eliminating the need
for the newest, most power-hungry GPUs.Acknowledgements
This material is based on research supported by
DARPA under agreement number N66001-19-
24032. The U.S. Government is authorized to
reproduce and distribute reprints for Governmen-
tal purposes, notwithstanding any copyright nota-
tion thereon. The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of DARPA or the U.S. Government.
References
Michael Ahn, Anthony Brohan, Noah Brown, Yev-
gen Chebotar, Omar Cortes, Byron David, Chelsea
Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol
Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu,
Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang,
Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jes-
month, Nikhil Joshi, Ryan Julian, Dmitry Kalash-
nikov, Yuheng Kuang, Kuang-Huei Lee, Sergey
Levine, Yao Lu, Linda Luu, Carolina Parada, Pe-
ter Pastor, Jornell Quiambao, Kanishka Rao, Jarek
Rettinghouse, Diego Reyes, Pierre Sermanet, Nico-
las Sievers, Clayton Tan, Alexander Toshev, Vincent
Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,
Mengyuan Yan, and Andy Zeng. 2022. Do As I Can
and Not As I Say: Grounding Language in Robotic
Affordances. arXiv:2204.01691 [cs] .
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee,
Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind
Srinivas, and Igor Mordatch. 2021. Decision trans-
former: Reinforcement learning via sequence model-
ing. In Advances in Neural Information Processing
Systems , volume 34, pages 15084–15097. Curran As-
sociates, Inc.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. PaLM: Scaling Language
Modeling with Pathways. arXiv:2204.02311 [cs] .
Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey
Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan
Wahid, Jonathan Tompson, Quan Vuong, Tianhe
Yu, Wenlong Huang, Yevgen Chebotar, Pierre Ser-
manet, Daniel Duckworth, Sergey Levine, Vincent
Vanhoucke, Karol Hausman, Marc Toussaint, Klaus
Greff, Andy Zeng, Igor Mordatch, and Pete Florence.
2023. PaLM-E: An Embodied Multimodal Language
Model. arXiv:2303.03378 [cs] .
Leo Gao, Stella Rose Biderman, Sid Black, Laurence
Golding, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, Shawn
Presser, and Connor Leahy. 2020. The Pile: An
800GB Dataset of Diverse Text for Language Model-
ing. arXiv:2101.00027 [cs] .
Malik Ghallab, Adele Howe, Craig Knoblock, Drew
McDermott, Ashwin Ram, Manuela Veloso, Daniel
Weld, and David Wilkins. 1998. PDDL—the plan-
ning domain definition language. Technical Report
CVC TR-98-003/DCS TR-1165, Yale Center for Com-
putational Vision and Control .
Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Li-
hong Li, Li Deng, and Mari Ostendorf. 2016. Deep
reinforcement learning with a natural language action
space. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (ACL) .
ACL - Association for Computational Linguistics.
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and
Igor Mordatch. 2022. Language models as zero-shot
planners: Extracting actionable knowledge for em-
bodied agents. In Proceedings of the 39th Interna-
tional Conference on Machine Learning , volume 162
ofProceedings of Machine Learning Research , pages
9118–9147. PMLR.
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky
Liang, Pete Florence, Andy Zeng, Jonathan Tompson,
Igor Mordatch, Yevgen Chebotar, Pierre Sermanet,
Tomas Jackson, Noah Brown, Linda Luu, Sergey
Levine, Karol Hausman, and brian ichter. 2023. In-
ner monologue: Embodied reasoning through plan-
ning with language models. In Proceedings of The
6th Conference on Robot Learning , volume 205 of
Proceedings of Machine Learning Research , pages
1769–1782. PMLR.
Michael Janner, Qiyang Li, and Sergey Levine. 2021.
Offline reinforcement learning as one big sequence
modeling problem. In Advances in Neural Informa-
tion Processing Systems , volume 34, pages 1273–
1286. Curran Associates, Inc.
Peter Jansen. 2022. A systematic survey of text worlds
as embodied natural language environments. In Pro-
ceedings of the 3rd Wordplay: When Language Meets
Games Workshop (Wordplay 2022) , pages 1–15, Seat-
tle, United States. Association for Computational
Linguistics.Minsoo Kim, Yeonjoon Jung, Dohyeon Lee, and Seung-
won Hwang. 2022. PLM-based world models for
text-based games. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language
Processing , pages 1324–1341, Abu Dhabi, United
Arab Emirates. Association for Computational Lin-
guistics.
Sergey Levine, Aviral Kumar, George Tucker, and Justin
Fu. 2020. Offline Reinforcement Learning: Tuto-
rial, Review, and Perspectives on Open Problems.
arXiv:2005.01643 [cs] .
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020.
BART: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and com-
prehension. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 7871–7880, Online. Association for Computa-
tional Linguistics.
Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj
Ammanabrolu, Faeze Brahman, Shiyu Huang, Chan-
dra Bhagavatula, Yejin Choi, and Xiang Ren. 2023.
SwiftSage: A Generative Agent with Fast and Slow
Thinking for Complex Interactive Tasks.
OpenAI. 2023. Gpt-4 technical report.
David Patterson, Joseph Gonzalez, Quoc Le, Chen
Liang, Lluis-Miquel Munguia, Daniel Rothchild,
David So, Maud Texier, and Jeff Dean. 2021. Car-
bon emissions and large neural network training.
arXiv:2104.10350 [cs] .
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research ,
21(140):1–67.
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
and Yuxiong He. 2020. Zero: Memory optimizations
toward training trillion parameter models. In SC20:
International Conference for High Performance Com-
puting, Networking, Storage and Analysis , pages 1–
16.
Jie Ren, Samyam Rajbhandari, Reza Yazdani Am-
inabadi, Olatunji Ruwase, Shuangyan Yang, Minjia
Zhang, Dong Li, and Yuxiong He. 2021. ZeRO-
Offload: Democratizing Billion-Scale model train-
ing. In 2021 USENIX Annual Technical Conference
(USENIX ATC 21) , pages 551–564. USENIX Associ-
ation.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2020. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter.
Ishika Singh, Gargi Singh, and Ashutosh Modi. 2021.
Pre-trained language models as prior knowledge for
playing text-based games.
Oyvind Tafjord and Peter Clark. 2021. General-
Purpose Question-Answering with Macaw.
arXiv:2109.02593 [cs] .
Ben Wang. 2021. Mesh-Transformer-JAX: Model-
Parallel Implementation of Transformer Lan-
guage Model with JAX. https://github.com/
kingoflolz/mesh-transformer-jax .
Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and
Prithviraj Ammanabrolu. 2022. ScienceWorld: Is
your agent smarter than a 5th grader? In Proceedings
of the 2022 Conference on Empirical Methods in
Natural Language Processing , pages 11279–11298,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Shunyu Yao, Rohan Rao, Matthew Hausknecht, and
Karthik Narasimhan. 2020. Keep CALM and ex-
plore: Language models for action generation in text-
based games. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 8736–8754, Online. Association
for Computational Linguistics.
01020304050607080
Number of Actions0500100015002000250030003500Number of input buffersNumber of actions in GPT-J’s prompt during training (AVG: 43.42)Figure 1: Histogram illustrating the number of actions
that fit in GPT-J input for the All train training set.
A Sample ScienceWorld dialog
We fine-tune GPT-J on a formulation of Science-
World games transcribed as a natural language di-
alog between an agent (A) and the game (G). The
agent issues actions, and the game replies with the
current observation. Unlike prior work that mod-
eled RL algorithms using language models, we
provide no information about the current score or
score yet to be achieved. Listing 1 illustrates the
dialog format. Training GPT-J becomes an autore-
gressive text modeling task over the dialog format.
During inference, we fill GPT-J’s input buffer with
as much history as possible in the form of prior
(action, observation )tuples in the dialog format
and expect GPT-J to generate an action row (i.e.,
the text after A:). GPT-J’s input buffer, limited to a
maximum of 2048 word pieces, can accommodate,
on average, 43 prior actions using the dialog format
(Figure 1).
Our plain text, natural language sequence model-
ing format differs from prior approaches to framing
Reinforcement Learning as a sequence modeling
task. Instead of natural language input, Janner et al.
(2021) use special tokens to represent a discretized
version of the previous states and actions as input to
a transformer decoder that predicts the next action.
Notably, the transformer is a bespoke model, not
a pre-trained Language Model. For The Decision
Transformer (Chen et al., 2021), raw inputs are
projected into a continuous space using a learned
linear projection, and the projections become The
Decision Transformer’s input. Another difference
between our simplified dialog modeling technique
and The Decision Transformer is that our input
does not contain information about the game score
of the yet unrealized score. This frees up space
in the LLMs input buffer to allow for more prior
history.
B Training details
We trained GPT-J with bfloat16 representations us-
ing DeepSpeed stage 2 model parallelism over 4
NVIDIA A6000 GPUs. We used the AdamW opti-
mizer with a weight decay of 0.01 and offloaded it
to system memory via DeepSpeed (Ren et al., 2021;
Rajbhandari et al., 2020). We train each model for
two epochs over its corresponding train data and
a batch of 16 (one input per batch, four gradient
accumulation steps, 4 GPUs) and a learning rate of
1e-4.
C Evaluation setup
After training on each model’s respective training
data, we evaluated each agent on the task variations
provided in ScienceWorld’s test set. The evaluation
required agents to play a total of 1 819 games split
unevenly over the 30 tasks. The distribution of
games to tasks appears in the Games column of
Table 5.
By contrast, Wang et al. (2022) measured
DRRN’s performance against a random sample of
thedevset every 500 steps during DRRN’s train-
ing and reported the average score for the last 10%
of training steps. For a fair comparison between
DRRN and our models, we trained DRRN using
the published code and parameters, evaluated it
against the entire ScienceWorld testset, and re-
ported DRRN’s performance in Table 1.
Before we tested our trained DRRN model on
thetestset, we confirmed that our training was
successful by performing the same evaluation as in
the original ScienceWorld paper. In this evaluation,
our trained DRRN obtained 18.92 points, similar
to the published 17 points. Since these results are
close, we conclude that our retrained DRRN model
matches the capabilities of the DRRN model from
the original ScienceWorld paper. For completeness,
we also evaluated our trained DRRN model on the
entire devset, where it obtained 18.11 points com-
pared to the 17.95 that it obtained when evaluated
on the entire testset (i.e., the result we include in
Table 1). The difference in performance between
DRRN on devandtestis similar to what we ob-
served for our GPT-J-based models.
One final thing to note is that there are severalways to compute mean performance in Science-
World. In Table 1, we report scores averaged over
the 1 819 games in the testset. This mean gives
equal weight to each game in the set. But, since
variations are nonuniformly distributed over tasks,
the contribution of each task to the final score is
determined by the number task of variations in the
testset (e.g., the Changes of State (Boiling) task
only has 9 variations in test, while Friction (known
surfaces) has 348 variations). Wang et al. (2022)
computed the mean score by calculating the mean
score per task and then the mean score over the 30
tasks. This method gives equal weight to each task
while potentially obscuring failures in tasks with
many variations. The difference between the two is
similar to that between micro- andmacro-precision ,
and both methods are valid. In Table 3, we com-
pare the micro- andmacro- averaged scores. The
macro score is lower than the micro score for GPT-
J-based models. Nonetheless, the best-performing
GPT-J-based model still outperforms DRNN by a
factor of 2.7.
Micro
ScoreImprov. Macro
ScoreImprov.
DRRN 17.95 1 .0x 18.75 1 .0x
All train Markov 25.19 1 .4x 22.73 1 .2x
All train 62.57 3.5x 50.60 2.7x
No variations 63.35 3.5x 50.61 2.7x
Up to 18 games 39.78 2 .2x 34.29 1 .8x
Table 3: Comparison of micro- andmacro- scores.
D Scores per Task
Table 1 shows that our GPT-J-based agent can
achieve up to 3.5x the score of DRRN. Looking
into the average scores separated by task provides
further insight into the successes and shortcomings
of the LLM approach to playing ScienceWorld.
Table 5 shows results split by task and by task cate-
gory. Each row in the table is based on five training
and evaluation runs with different seeds. In each
cell, we present the mean score and the standard
deviation.
Like Wang et al. (2022), we find that perfor-
mance varies considerably from task to task. Even
for our best model, All train , the scores range from
0 points (on Mixing paints (tertiary colors) ) to 100
points on several tasks, e.g., Identify longest-lived
animal . However, it is not just the score that varies
from task to task. We observe many variations in
Listing 1: An example transcript as used to train GPT-J.
Your task is to measure the melting point of chocolate , which is located around
the kitchen . First , focus on the thermometer . Next , focus on the chocolate .
If the melting point of chocolate is above -10.0 degrees celsius , focus on
the blue box . If the melting point of chocolate is below -10.0 degrees
celsius , focus on the orange box . The boxes are located around the kitchen .
A: look around
G: This room is called the hallway . In it , you see : the agent ; a substance
called air ; a picture . You also see : A door to the art studio ( that is
closed ); A door to the bedroom ( that is closed ); A door to the greenhouse (
that is closed ); A door to the kitchen ( that is closed ); A door to the
living room ( that is closed ); A door to the workshop ( that is closed )
A: inventory
G: In your inventory , you see : an orange
A: open door to kitchen
G: The door is now open .
A: go to kitchen
G: You move to the kitchen .
A: look around
G: This room is called the kitchen . In it , you see : the agent ; a substance
called air ; a blue box ( containing nothing ); a chair . On the chair is:
nothing .; a counter . On the counter is: a bowl ( containing a red apple , a
banana , an orange , a potato ), a drawer .; a cupboard . The cupboard door is
closed . ; a freezer . The freezer door is closed . ; a fridge . The fridge door
is closed . ; a glass jar ( containing a substance called sodium chloride ); a
lighter ; a orange box ( containing nothing ); a oven , which is turned off .
The oven door is closed . ; a painting ; a sink , which is turned off . In the
sink is: nothing .; a substance called soap ; a stopwatch , which is
deactivated . ; a stove , which is turned off . On the stove is: nothing .; a
table . On the table is: a glass cup ( containing nothing ).; a thermometer ,
currently reading a temperature of 10 degrees celsius . You also see : A door
to the bathroom ( that is closed ); A door to the hallway ( that is open ); A
door to the outside ( that is closed )
A: pick up thermometer
G: You move the thermometer to the inventory .
A: focus on thermometer in inventory
G: You focus on the thermometer .
A: look around
G: This room is called the kitchen . In it , you see : the agent ; a substance
called air ; a blue box ( containing nothing ); a chair . On the chair is:
nothing .; a counter . On the counter is: a bowl ( containing a red apple , a
banana , an orange , a potato ), a drawer .; a cupboard . The cupboard door is
closed . ; a freezer . The freezer door is closed . ; a fridge . The fridge door
is closed . ; a glass jar ( containing a substance called sodium chloride ); a
lighter ; a orange box ( containing nothing ); a oven , which is turned off .
The oven door is closed . ; a painting ; a sink , which is turned off . In the
sink is: nothing .; a substance called soap ; a stopwatch , which is
deactivated . ; a stove , which is turned off . On the stove is: nothing .; a
table . On the table is: a glass cup ( containing nothing ).. You also see : A
door to the bathroom ( that is closed ); A door to the hallway ( that is open );
A door to the outside ( that is closed )
A: open freezer
G: The freezer is now open .
A: look in freezer
G: Inside the freezer is: nothing
...
the values for standard deviation (shown in paren-
theses next to each score), with many tasks having
a standard deviation multiple times larger than the
ones in Table 1 and included at the bottom of Ta-
ble 5.
All four models’ performance varies a lot with
the task. A strong Pearson correlation between the
per-task results of all four models (Table 4) hints
that some tasks are genuinely more difficult for our
GPT-J models.
All
trainAll
train
MarkovNo vari-
ationsUp
to 18
games
All train 1
All train Markov 0.83 1
No variations 0.99 0.84 1
Up to 18 games 0.89 0.73 0.91 1
Table 4: Pearson correlation between the results in the
corresponding columns of Table 5.
E Environment step limits
Evaluating the performance of players in a turn-
based game raises the question of after how many
steps should one report and compare performance.
Wang et al. (2022) and Lin et al. (2023) report the
performance of models allowed to play up to 100
actions. This paper also reports performance up to
a maximum of 100actions.
However, the training games that ScienceWorld
generates suggest that evaluations with more than
100 steps might be necessary. Figure 2 shows that,
while the average length of training games is 57
actions and the great majority of games in the train-
ing set can be completed in less than 100 steps, a
considerable number of games require more than
100 actions to complete. (Lin et al., 2023) cited
computational costs as an argument for evaluating
up to only 100 actions
Computational costs are less important for the
GPT-J models we evaluate since the models are
small enough to run on a modern workstation.
However, we still report results after a maximum of
100 actions because (1) it makes our results easier
to compare to prior literature and (2) because we
see diminishing returns from running evaluation
past 100 actions. Figure 3 shows our models’ Sci-
enceWorld testscore as a function of the number of
game turns/actions. While the Markov assumption
GPT-J (All train Markov) flat-lines after about 50
020406080100120140160180200
Number of Actions0100200300400500600700Number of GamesNumber of actions in training games (AVG: 56.90)Figure 2: Histogram of the number of actions in games
from the No variations train set.
turns/actions, the other GPT-J-based models con-
tinue to meaningfully accumulate points up to the
evaluation limit of 100turns/actions. Past the 100
actions limit, the increase in ScienceWorld scores
is marginal for all our models."
2102.11585,D:\Database\arxiv\papers\2102.11585.pdf,"Given the increasing complexity of autonomous driving datasets, what are the key challenges and opportunities presented by the use of multi-modal data, particularly in the context of action detection and event understanding?","The use of multi-modal data, such as LiDAR and camera data, presents challenges in terms of data fusion and computational complexity, but also offers opportunities for richer and more accurate action detection and event understanding, leading to more robust and reliable autonomous driving systems.","IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. Y , 20XX 4
rain) and annotations (RGB, LiDAR/radar, 3D boxes). For
instance, Argovers [43] doubles the number of sensors in
comparison to KITTI [37] and nuScenes [49], providing 3D
bounding boxes with tracking information for 15 objects of
interest. Similarly, Lyft [44] provides 3D bounding boxes
for cars and location annotation including lane segments,
pedestrian crosswalks, stop signs, parking zones, speed
bumps, and speed humps. In a setup similar to KITTI’s [37],
in KITTI-360 [48] two ﬁsheye cameras and a pushbroom
laser scanner are added to have a full 360oﬁeld of view.
KITTI-360 contains semantic and instance annotations for
both 3D point clouds and 2D images, which include 19 ob-
jects. IMU/GPS sensors are added for localisation purposes.
Both 3D bounding boxes based on LiDAR data and 2D
annotation on camera data for 4 objects classes are provided
in Waymo [45]. In [46], using similar 3D annotation for 5 ob-
jects classes, the authors provide a more challenging dataset
by adding more night-time scenarios using a faster-moving
car. Amongst large-scale multimodal datasets, nuScenes
[49], Lyft L5 [44], Waymo Open [45] and A*3D [46] are
the most dominant ones in terms of number of instances,
the use of high-quality sensors with different types of data
(e.g., point clouds or 360◦RGB videos), and richness of
the annotation providing both semantic information and
3D bounding boxes. Furthermore, nuScenes [49], Argoverse
[43] Lyft L5 [44] and KITTI-360 [48] provide contextual
knowledge through human-annotated rich semantic maps,
an important prior for scene understanding.
Trajectory prediction . Another line of work considers
the problem of pedestrian trajectory prediction in the au-
tonomous driving setting, and rests on several inﬂuential
RGB-based datasets. To compile these datasets, RGB data
were captured using either stationary surveillance cameras
[50], [51], [52] or drone-mounted ones [53] for aerial view.
[54], [55] use RGB images capturing an egocentric view from
a moving car for future trajectory forecasting. Recently, the
multimodal 3D point cloud-based datasets [37], [38], [43],
[44], [45], [49], initially introduced for the benchmarking of
3D object detection and tracking, have been taken up for
trajectory prediction as well. A host of interesting recent
papers [56], [57], [58], [59] do propose datasets to study the
intentions and actions of agents using cameras mounted on
vehicles. However, they encompass a limited set of action
labels (e.g. walking, standing, looking or crossing), wholly
insufﬁcient for a thorough study of road agent behaviour.
Among them, TITAN [59] is arguably the most promising.
Our ROAD dataset is similar to TITAN in the sense that
both consider actions performed by humans present in
the road scene and provide spatiotemporal localisation for
each person using multiple action labels. However, TITAN’s
action labels are restricted to humans (pedestrians), rather
than extending to all road agents (with the exception of
vehicles with ‘stopped’ and ‘moving’ actions). The dataset
is a collection of much shorter videos which only last 10-
20 seconds, and does not not contemplate agent location
(a crucial source of information). Finally, the size of its
vocabulary in terms of number of agents and actions is
much smaller (see Table 1).
As mentioned, our ROAD dataset is built upon the
multimodal Oxford RobotCar dataset, which contains both
visual and 3D point cloud data. Here, however, we onlyTABLE 1
Comparison of ROAD with similar datasets for perception in
autonomous driving in terms of diversity of labels. The comparison is
based on the number of classes portrayed and the availability of action
annotations and tube tracks for both pedestrians and vehicles, as well
as location information. Most competitor datasets do not provide action
annotation for either pedestrians or vehicles.
Dataset Class Num. Location labelAction Ann Tube Ann
Ped. Veh. Ped. Veh.
SYNTHIA [60] 13 pixelwise ann. - - - -
SemKITTI [61] 28 3D sem. seg. - - - -
Cityscapes [24] 30 pixel level sem. - - - -
A2D2 [47] 14 3D sem. seg. - - - -
Waymo [45] 4 - - - ✓ ✓
Apolloscape [27] 25 pixel level sem. - - ✓ ✓
PIE [58] 6 - ✓ -✓ -
TITAN [59] 50 - ✓ ✓ ✓ ✓
KITTI360 [48] 19 sem. ann. - - - -
A*3D [46] 7 - - - - -
H3D [38] 8 - - - ✓ ✓
Argoverse [43] 15 - - - ✓ ✓
NuScense [49] 23 3D sem. seg. - - ✓ ✓
DriveSeg [36] 12 sem. ann. - - - -
Spatiotemporal action detection datasets
UCF24 [62] 24 - ✓ -✓ -
AVA [63] 80 - ✓ -✓ -
Multisports [64] 66 - ✓ -✓ -
ROAD (ours) 43 ✓ ✓ ✓ ✓ ✓
Ped. Pedestrian, Veh. Vehicle, ann. annotation, sem. seg. semantic segmentation
process a number of its videos to describe and annotate road
events. Note that it is indeed possible to map the 3D point
clouds from RobotCar’s LiDAR data onto the 2D images
to enable true multi-modal action detection. However, a
considerable amount would be required to do this, and will
be considered in future extensions.
ROAD departs substantially from all previous efforts,
as: (1) it is designed to formally introduce the notion of
road event as a combination of three semantically-meaningful
labels such as agent, action and location; (2) it provides both
bounding-box-level and tube-level annotation (to validate
methods that exploit the dynamics of motion patterns) on
long-duration videos (thus laying the foundations for future
work on event anticipation and continual learning); (3) it
provides temporally dense annotation; (4) it labels the ac-
tions not only of physical humans but also of other relevant
road agents such as vehicles of different kinds.
Table 1 compares our ROAD dataset with the other state-
of-the-art datasets in perception for autonomous driving, in
terms of the number and type of labels. As it can be noted
in the table, the unique feature of ROAD is its diversity in
terms of the types of actions and events portrayed, for all
types of road agents in the scene. With 12 agent classes,
30 action classes and 15 location classes ROAD provides
(through a combination of these three elements) a much
more reﬁned description of road scenes.
2.2 Action detection datasets
Providing annotation for action detection datasets is a
painstaking process. Speciﬁcally, the requirement to track
actors through the temporal domain makes the manual
labelling of a dataset an extremely time consuming exercise,
requiring frame-by-frame annotation. As a result, action de-
tection benchmarks are fewer and smaller than, say, image
classiﬁcation, action recognition or object detection datasets.
Action recognition research can aim for robustness
thanks to the availability of truly large scale datasets such
IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. Y , 20XX 5
as Kinetics [65], Moments [66] and others, which are the
de-facto benchmarks in this area. The recent ’something-
something’ video database focuses on more complex ac-
tions performed by humans using everyday objects [67],
exploring a ﬁne-grained list of 174 actions. More recently,
temporal activity detection datasets like ActivityNet [68]
and Charades [69] have come to the fore. Whereas the latter
still do not address the spatiotemporal nature of the action
detection problem, however, datasets such as J-HMDB-21
[70], UCF24 [71], LIRIS-HARL [72], DALY [73] or the more
recent AVA [63] have been designed to provide spatial and
temporal annotations for human action detection.
In fact, most action detection papers are validated on the
rather dated and small LIRIS-HARL [72], J-HMDB-21 [70],
and UCF24 [71], whose level of challenge in terms of pres-
ence of different source domains and nuisance factors is
quite limited. Although recent additions such as DALY [73]
and AVA [63] have somewhat improved the situation in
terms of variability and number of instances labelled, the
realistic validation of action detection methods is still an
outstanding issue. AVA is currently the biggest action de-
tection dataset with 1.6Mlabel instances, but it is annotated
rather sparsely (at a rate of one frame per second).
Overall, the main objective of these datasets is to vali-
date the localisation of human actions in short, untrimmed
videos. ROAD, in opposition, goes beyond the detection of
actions performed by physical humans to extend the notion
of other forms of intelligent agents (e.g., human- or AI-
driven vehicles on the road). Furthermore, in contrast with
the short clips considered in, e.g., J-HMDB-21 and UCF24,
our new dataset is composed of 22 very long videos (around
8 minutes each), thus stressing the dynamical aspect of
events and the relationship between distinct but correlated
events. Crucially, it is geared towards online detection
rather than traditional ofﬂine detection, as these videos are
streamed in using a vehicle-mounted camera.
2.3 Online action detection
We believe advances in the ﬁeld of human action recognition
[22], [74], [75], [76] can be useful when devising a gen-
eral approach to the situation awareness problem. We are
particularly interested in the action detection problem [21],
[63], [77], [78], in particular online action detection [19],
given the incremental processing needs of an autonomous
vehicle. Recent work in this area [19], [79], [80], [81], [82],
[83] demonstrates very competitive performance compared
to (generally more accurate) ofﬂine action detection meth-
ods [20], [63], [75], [84], [85], [86], [87], [88] on UCF-101-
24 [71]. As mentioned, UCF-101-24 is the main benchmark
for online action detection research, as it provides annota-
tion in the form of action tubes and every single frame of
the untrimmed videos in it is annotated (unlike AVA [63], in
which videos are only annotated at one frame per second).
A short review of the state-of-the-art in online action
detection is in place. Singh et al. [19]’s method was perhaps
the ﬁrst to propose an online, real-time solution to action
detection in untrimmed videos, validated on UCF-101-24,
and based on an innovative incremental tube construction
method. Since then, many other papers [81], [82], [87] have
made use of the online tube-construction method in [19].A common trait of many recent online action detection
methods is the reliance on ’tubelet’ [81], [82], [84] predictions
from a stack of frames. This, however, leads to processing
delays proportional to the number of frames in the stack,
making these methods not quite applicable in pure online
settings. In the case of [81], [82], [84] the frame stack is
usually 6-8 frames long, leading to a latency of more than
half a second.
For these reasons, inspired by the frame-wise (2D) nature
of [19] and the success of the latest single-stage object
detectors (such as RetinaNet [89]), here we propose a sim-
ple extension of [19] termed ’3D-RetinaNet’ as a baseline
algorithm for ROAD tasks. The latter is completely online
when using a 2D backbone network. One, however, can
also insert a 3D backbone to make it even more accurate,
while keeping the prediction heads online. We benchmark
our proposed 3D-RetinaNet architecture against the above-
mentioned online and ofﬂine action detection methods on
the UCF-101-24 dataset to show its effectiveness, twinned
with its simplicity and efﬁciency. We also compare it on
our new ROAD dataset against the state-of-the-art action
detection Slowfast [22] network. We omit, however, to re-
produce other state-of-the-art action detectors such as [90]
and [91], for [90] is affected by instability at training time
which makes it difﬁcult to reproduce its results, whereas [91]
is too complicated to be suitable as a baseline because of its
sparse tracking and memory banks features. Nevertheless,
both methods rely on the Slowfast detector as a backbone
and baseline action detector.
3 THE DATASET
3.1 A multi-label benchmark
The ROAD dataset is specially designed from the perspec-
tive of self-driving cars, and thus includes actions per-
formed not just by humans but by all road agents in speciﬁc
locations, to form road events (REs). REs are annotated by
drawing a bounding box around each active road agent
present in the scene, and linking these bounding boxes
over time to form ’tubes’. As explained, to this purpose
three different types of labels are introduced, namely: (i)
the category of road agent involved (e.g. Pedestrian ,Car,Bus,
Cyclist ); (ii) the type of action being performed by the agent
(e.g. Moving away ,Moving towards ,Crossing and so on), and
(iii) the location of the road user relative the autonomous
vehicle perceiving the scene (e.g. In vehicle lane ,On right
pavement ,In incoming lane ). In addition, ROAD labels the
actions performed by the vehicle itself. Multiple agents
might be present at any given time, and each of them may
perform multiple actions simultaneously (e.g. a Carmay be
Indicating right while Turning right ). Each agent is always
associated with at least one action label.
The full lists of agent, action and location labels are given
in the Supplementary material, Tables 1, 2, 3 and 4.
Agent labels . Within a road scene, the objects or people
able to perform actions which can inﬂuence the decision
made by the autonomous vehicle are termed agents . We only
annotate active agents (i.e., a parked vehicle or a bike or a
person visible to the AV but located away from the road are
not considered to be ’active’ agents). Three types of agent are
considered to be of interest, in the sense deﬁned above, to
IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. Y , 20XX 6
the autonomous vehicle: people, vehicles and trafﬁc lights.
For simplicity, the AV itself is considered just like another
agent: this is done by labelling the vehicle’s bonnet. People
are further subdivided into two sub-classes: pedestrians
and cyclists. The vehicle category is subdivided into six
sub–classes: car, small–size motorised vehicle, medium–size
motorised vehicle, large–size motorised vehicle, bus, motor-
bike, emergency vehicle. Finally, the ‘trafﬁc lights’ category
is divided into two sub–classes: Vehicle trafﬁc light (if they
apply to the AV) and Other trafﬁc light (if they apply to other
road users). Only one agent label can be assigned to each
active agent present in the scene at any given time.
Action labels . Each agent can perform one or more actions
at any given time instant. For example, a trafﬁc light can
only carry out a single action: it can be either red, amber,
green or ‘black’. A car, instead, can be associated with
two action labels simultaneously, e.g., Turning right and
Indicating right . Although some road agents are inherently
multitasking, some action combinations can be suitably de-
scribed by a single label: for example, pushing an object (e.g.
a pushchair or a trolley-bag) while walking can be simply
labelled as Pushing object . The latter was our choice.
AV own actions . Each video frame is also labelled with
the action label associated with what the AV is doing. To
this end, a bounding box is drawn on the bonnet of the
AV . The AV can be assigned one of the following seven
action labels: AV-move ,AV-stop ,AV-turn-left ,AV-turn-right ,
AV-overtake ,AV-move-left and AV-move-right . The full list
of AV own action classes is given in the Supplementary
material, Table 4. Note that these are separate classes only
applicable to the AV , with a different semantics than the
similar-sounding classes. For instance, the regular Moving
action label means ’moving in the perpendicular direction
to the AV’, whereas AV-move means that the AV is on
the move along its normal direction of travel. These labels
mirror those used for the autonomous vehicle in the Honda
Research Institute Driving Dataset (HDD) [92].
Location labels . Agent location is crucial for deciding what
action the AV should take next. As the ﬁnal, long-term
objective of this project is to assist autonomous decision
making, we propose to label the location of each agent from
the perspective of the autonomous vehicle. For example, a
pedestrian can be found on the right or the left pavement, in
the vehicle’s own lane, while crossing or at a bus stop. The
same applies to other agents and vehicles as well. There is
no location label for the trafﬁc lights as they are not mov-
able objects, but agents of a static nature and well-deﬁned
location. To understand this concept, Fig. 1 illustrates two
scenarios in which the location of the other vehicles sharing
the road is depicted from the point of view of the AV .
Trafﬁc light is the only agent type missing location labels,
all the other agent classes are associated with at least one
location label. A complete table with location classes and
their description is provided in Supplementary material.
3.2 Data collection
ROAD is composed of 22 videos from the publicly available
Oxford RobotCar Dataset [18] (OxRD) released in 2017 by
the Oxford Robotics Institute2, covering diverse road scenes
2. http://robotcar-dataset.robots.ox.ac.uk/under various weather conditions. The OxRD dataset, col-
lected from the narrow streets of the historic city of Oxford,
was selected because it presents challenging scenarios for
an autonomous vehicle due to the diversity and density of
various road users and road events. The OxRD dataset was
gathered using 6 cameras, as well as LIDAR (Light Detection
and Ranging), GPS (Global Positioning System) and INS
(Inertial Navigation System) sensors mounted on a Nissan
LEAF vehicle [18]. To construct ROAD we only annotated
videos from the frontal camera view.
Note, however, that our labelling process (described be-
low) is not limited to OxRD. In principle, other autonomous
vehicle datasets (e.g. [26], [93]) may be labelled in the same
manner to further enrich the ROAD benchmark,: we plan to
do exactly so in the near future.
Video selection . Within OxRD, videos were selected with
the objective of ensuring diversity in terms of weather
conditions, times of the day and types of scenes recorded.
Speciﬁcally, the 22 videos have been recorded both during
the day (in strong sunshine, rain or overcast conditions,
sometimes with snow present on the surface) and at night.
Only a subset of the large number of videos available in
OxRD was selected. The presence of semantically meaning-
ful content was the main selection criterion. This was done
by manually inspecting the videos in order to cover all types
of labels and label classes and to avoid ’deserted’ scenarios
as much as possible. Each of the 22 videos is 8 minutes and
20 seconds long, barring three videos whose duration is 6:34,
4:10 and 1:37, respectively. In total, ROAD comprises 170
minutes of video content.
Preprocessing . Some preprocessing was conducted. First,
the original sets of video frames were downloaded and
demosaiced, in order to convert them to red, green, and
blue (RGB) image sequences. Then, they were encoded into
proper video sequences using ffmpeg3at the rate of 12
frames per second (fps). Although the original frame rate in
the considered frame sequences varies from 11 fps to 16 fps,
we uniformised it to keep the annotation process consistent.
As we retained the original time stamps, however, the
videos in ROAD can still be synchronised with the LiDAR
and GPS data associated with them in the OxRD dataset,
allowing future work on multi-modal approaches.
3.3 Annotation process
Annotation tool . Annotating tens of thousands of frames
rich in content is a very intensive process; therefore, a
tool is required which can make this process both fast and
intuitive. For this work, we adopted Microsoft’s VoTT4. The
most useful feature of this annotation tool is that it can copy
annotations (bounding boxes and their labels) from one
frame to the next, while maintaining a unique identiﬁcation
for each box, so that boxes across frames are automatically
linked together. Moreover, VoTT also allows for multiple
labels, thus lending itself well to ROAD’s multi-label anno-
tation concept. A number of examples of annotated frames
from the two videos using the VOTT tool is provided in
supplementary material.
3. https://www.ffmpeg.org/
4. https://github.com/Microsoft/VoTT/
IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. Y , 20XX 7
Fig. 2. Number of instances of each class of individual label-types, in logarithmic scale.
Annotation protocol . All salient objects and actors within
the frame were labelled, with the exception of inactive par-
ticipants (mostly parked cars) and objects / actors at large
distances from the ego vehicle, as the latter were judged to
be irrelevant to the AV’s decision making. This can be seen
in the attached 30-minute video5portraying ground truth
and predictions. As a result, pedestrians, cyclists and trafﬁc
lights were always labelled. Vehicles, on the other hand,
were only labelled when active (i.e., moving, indicating,
being stopped at lights or stopping with hazard lights on on
the side of road). As mentioned, only parked vehicles were
not considered active (as they do not arguably inﬂuence the
AV’s decision making), and were thus not labelled.
Event label generation . Using the annotations manually
generated for actions and agents in the multi-label scenario
as discussed above it is possible to generate event-level
labels about agents, e.g. Pedestrian / Moving towards the AV
On right pavement orCyclist / Overtaking / In vehicle lane .
Any combinations of location, action and agent labels are
admissible. If location labels are ignored, the resulting event
labels become location-invariant.
In addition to event tubes, in this work we do explore agent-
action pair instances (see Sec. 5). Namely, given an agent
tube and the continuous temporal sequence of action labels
attached to its constituent bounding box detections, we can
generate action tubes by looking for changes in the action
label series associated with each agent tube. For instance, a
Carappearing in a video might be ﬁrst Moving away before
Turning left . The agent tube for the car will then be formed
by two contiguous agent-action tubes: a ﬁrst tube with label
pair Car / Moving away and a second one with pair Car /
Turning left .
3.4 Tasks
ROAD is designed as a sandbox for validating the six tasks
relevant to situation awareness in autonomous driving out-
lined in Sec. 1.1. Five of these tasks are detection tasks, while
5. https://www.youtube.com/watch?v=CmxPjHhiarA.TABLE 2
ROAD tasks and attributes.
Task type Problem type Output Multiple labels
Active agent Detection Box&Tube No
Action Detection Box&Tube Yes
Location Detection Box&Tube Yes
Duplex Detection Box&Tube Yes
Event Detection Box&Tube Yes
AV-action Temp segmentation Start/End No
the last one is a frame-level action recognition task some-
times referred to as ’temporal action segmentation’ [69],
Table 2 shows the main attributes of these tasks.
All detection tasks are evaluated both at frame-level
and at video- (tube-)level. Frame-level detection refers to the
problem of identifying in each video frame the bounding
box(es) of the instances there present, together with the rel-
evant class labels. Video-level detection consists in regressing
a whole series of temporally-linked bounding boxes (i.e.,
in current terminology, a ’tube’) together with the relevant
class label. In our case, the bounding boxes will mark a
speciﬁc active agent in the road scene. The labels may issue
(depending on the speciﬁc task) either from one of the
individual label types described above (i.e., agent, action
or location) or from one of the meaningful combinations
described in 3.3 (i.e., either agent-action pairs or events).
Below we list all the tasks for which we currently pro-
vide a baseline, with a short description.
1) Active agent detection (oragent detection ) aims at local-
ising an active agent using a bounding box (frame-
level) or a tube (video-level) and assigning a class
label to it.
2) Action detection seeks to localise an active agent
occupied in performing a speciﬁc action from the
list of action classes.
3) In agent location detection (orlocation detection ) a label
from the relevant list of locations (as seen from the
AV) is sought and attached to the relevant bounding
box or tube.
IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. Y , 20XX 8
4) In agent-action detection the bounding box or tube is
assigned a pair agent-action as explained in 3.3. We
sometimes refer to this task as ’duplex detection’.
5) Road event detection (orevent detection ) consist in
assigning to each box or tube a triplet of class labels.
6) Autonomous vehicle temporal action segmentation is a
frame-level action classiﬁcation task in which each
video frame is assigned a label from the list of
possible AV own actions. We refer to this task as
’AV-action segmentation’, similarly to [69].
3.5 Quantitative summary
Overall, 122Kframes extracted from 22 videos were la-
belled, in terms of both AV own actions (attached to the
entire frame) and bounding boxes with attached one or
more labels of each of the three types: agent, action, location.
In total, ROAD includes 560Kbounding boxes with 1.7M
instances of individual labels. The latter ﬁgure can be broken
down into 560Kinstances of agent labels, 640Kinstances
of action labels, and 499Kinstances of location labels.
Based on the manually assigned individual labels, we could
identify 603Kinstances of duplex (agent-action) labels and
454Kinstances of triplets (event labels).
The number of instances for each individual class from
the three lists is shown in Fig. 2 (frame-level, in orange). The
560Kbounding boxes make up 7,029,9,815,8,040,9,335
and 8,394 tubes for the label types agent, action, location,
agent-action and event, respectively. Figure 2 also shows the
number of tube instances for each class of individual label
types as number of video-level instances (in blue).
4 BASELINE AND CHALLENGE
Inspired by the success of recent 3D CNN architectures [74]
for video recognition and of feature-pyramid networks
(FPN) [94] with focal loss [89], we propose a simple yet
effective 3D feature pyramid network (3D-FPN) with focal
loss as a baseline method for ROAD’s detection tasks. We
call this architecture 3D-RetinaNet .
4.1 3D-RetinaNet architecture
The data ﬂow of 3D-RetinaNet is shown in Figure 3. The
input is a sequence of Tvideo frames. As in classical
FPNs [94], the initial block of 3D-RetinaNet consists of a
backbone network outputting a series of forward feature
pyramid maps, and of lateral layers producing the ﬁnal
feature pyramid composed by Tfeature maps. The second
block is composed by two sub-networks which process
these features maps to produce both bounding boxes ( 4
coordinates) and Cclassiﬁcation scores for each anchor
location (over Apossible locations). In the case of ROAD,
the integer Cis the sum of the numbers of agent, action,
location, action-agent (duplex) and agent-action-location
(event) classes, plus one reserved for an agentness score.
The extra class agentness is used to describe the presence
or absence of an active agent. As in FPN [94], we adopt
ResNet50 [95] as the backbone network.
2D versus 3D backbones . In our experiments we show re-
sults obtained using three different backbones: frame-based
ResNet50 (2D), inﬂated 3D (I3D) [74] and Slowfast [22], inthe manner also explained in [22], [75]. Choosing a 2D
backbone makes the detector completely online [19], with
a delay of a single frame. Choosing an I3D or a Slowfast
backbone, instead, causes a 4-frame delay at detection time.
Note that, as Slowfast and I3D networks makes use of a
max-pool layer with stride 2, the initial feature pyramid in
the second case contains T/2feature maps. Nevertheless,
in this case we can simply linearly upscale the output to T
feature maps.
AV action prediction heads . In order for the method to also
address the prediction of the AV’s own actions (e.g. whether
the AV is stopping, moving, turning left etc.), we branch
out the last feature map of the pyramid (see Fig. 3, bottom)
and apply spatial average pooling, followed by a temporal
convolution layer. The output is a score for each of the Ca
classes of AV actions, for each of the Tinput frames.
Loss function . As for the choice of the loss function,
we adopt a binary cross-entropy-based focal loss [89]. We
choose a binary cross entropy because our dataset is multi-
label in nature. The choice of a focal-type loss is motivated
by the expectation that it may help the network deal with
long tail and class imbalance (see Figure 2).
4.2 Online tube generation via agentness score
The autonomous driving scenario requires any suitable
method for agent, action or event tube generation to work
in an online fashion, by incrementally updating the existing
tubes as soon as a new video frame is captured. For this
reason, this work adopts a recent algorithm proposed by
Singh et al. [19], which incrementally builds action tubes
in an online fashion and at real-time speed. To be best of
our knowledge, [19] was the ﬁrst online multiple action
detection approach to appear in the literature, and was later
adopted by almost all subsequent works [81], [82], [87] on
action tube detection.
Linking of detections . We now brieﬂy review the tube-
linking method of Singh et al. [19], and show how it can
be adapted to build agent tubes based on an ’agentness’
score, rather than build a tube separately for each class
as proposed in the original paper. This makes the whole
detection process faster, since the total number of classes is
much larger than in the original work [19]. The proposed
3D-RetinaNet is used to regress and classify detection boxes
in each video frame potentially containing an active agent of
interest. Subsequently, detections whose score is lower than
0.025are removed and non-maximal suppression is applied
based on the agentness score.
At video start, each detection initialises an agentness
tube. From that moment on, at any time instance tthe
highest scoring tubes in terms of mean agentness score
up tot−1are linked to the detections with the highest
agentness score in frame twhich display an Intersection-
over-Union (IoU) overlap with the latest detection in the
tube above a minimum threshold λ. The chosen detection
is then removed from the pool of frame- tdetections. This
continue until the tubes are either assigned or not assigned
a detection from current frame. Remaining detections at
timetare used to initiate new tubes. A tube is terminated
after no suitable detection is found for nconsecutive frames.
As the linking process takes place, each tube carries scores
IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. Y , 20XX 9
backbone
CNNclass+box
subnetsTxWxH
x256
TxWxH
x256Conv
1x3x3Conv
1x3x3
Conv
3x3x3
Conv
3x3x3class+box
subnetsclass
subnet
box
subnetclass+box
subnetsTxWxH
xCxA
TxWxH
x4xA
3xTxWxH - input sequence of framesspatial
avg poolTx1x1x256Conv
3x1x1TxCa
aC - number of all classes
A - number of anchors
C  - number of AV action classes
Fig. 3. Proposed 3D-RetinaNet architecture for online video processing.
for all the classes of interest for the task at hand (e.g.,
action detection rather than event detection), as produced
by the classiﬁcation subnet of 3D-RetinaNet. We can then
label each agentness tube using the kclasses that show the
highest mean score over the duration of the tube.
Temporal trimming . Most tubelet based methods [81], [82],
[96] do not perform any temporal trimming of the action
tubes generated in such a way (i.e., they avoid deciding
when they should start or end). Singh et al. [19] proposed to
pose the problem in a label consistency formulation solved
via dynamic programming. However, as it turns out, tem-
poral trimming [19] does not actually improve performance,
as shown in [87], except in some settings, for instance in the
DALY [73] dataset.
The situation is similar for our ROAD dataset as opposed
to what happens on UCF-101-24, for which temporal trim-
ming based on solving the label consistency formulation in
terms of the actionness score, rather than the class score,
does help improve localisation performance. Therefore, in
our experiments we only use temporal trimming on the
UCF-101-24 dataset but not on ROAD.
4.3 The ROAD challenge
To introduce the concept of road event, our new approach to
situation awareness and the ROAD dataset to the computer
vision and AV communities, some of us have organised in
October 2021 the workshop ”The ROAD challenge: Event
Detection for Situation Awareness in Autonomous Driv-
ing”6. For the challenge, we selected (among the tasks
described in Sec. 3.4) only three tasks: agent detection, action
detection and event detection, which we identiﬁed as the
most relevant to autonomous driving.
As standard in action detection, evaluation was done in
terms of video mean average precision (video-mAP). 3D-
6. https://sites.google.com/view/roadchallangeiccv2021/.RetinaNet was proposed as the baseline for all three tasks.
Challenge participants had 18 videos available for training
and validation. The remaining 4 videos were to be used to
test the ﬁnal performance of their model. This split was
applied to all the three challenges (split 3 of the ROAD
evaluation protocol, see Section 5.3).
The challenge opened for registration on April 1 2021,
with the training and validation folds released on April
30, the test fold released on July 20 and the deadline for
submission of results set to September 25. For each stage
and each Task the maximum number of submissions was
capped at 50, with an additional constraint of 5 submissions
per day. The workshop, co-located with ICCV 2021, took
place on October 16 2021.
In the validation phase we had between three and
ﬁve teams submit between 15 and 17 entries to each of
three challenges. In the test phase, which took place after
the summer, we noticed a much higher participation with
138 submissions from 9 teams to the agent challenge, 98
submissions from 8 teams to the action challenge, and 93
submission from 6 teams to the event detection challenge.
The methods proposed by the winners of each challenge
are brieﬂy recalled in Section 5.4.
Benchmark maintenance . After the conclusion of the
ROAD @ ICCV 2021 workshop, the challenge has been re-
activated to allow for submissions indeﬁnitely. The ROAD
benchmark will be maintained by withholding the test set
from the public on the eval.ai platform7, where teams can
submit their predictions for evaluation. Training and vali-
dation sets can be downloaded from https://github.com/
gurkirt/road-dataset.
7. https://eval.ai/web/challenges/challenge-page/1059/overview
IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. Y , 20XX 10
5 EXPERIMENTS
In this section we present results on the various task the
ROAD dataset is designed to benchmark (see Sec. 3.4), as
well as the action detection results delivered by our 3D-
RetinaNet model on UCF-101-24 [62], [97].
We ﬁrst present the evaluation metrics and implementa-
tion details speciﬁc to ROAD in Section 5.1. In Section 5.2 we
benchmark our 3D-RetinaNet model for the action detection
problem on UCF-101-24. The purpose is to show that this
baseline model is competitive with the current state of the
art in action tube detection while only using RGB frames as
input, and to provide a sense of how challenging ROAD
is when compared to standard action detection bench-
marks. Indeed, the complex nature of the real-world, non-
choreographed road events, often involving large numbers
of actors simultaneously responding to a range of scenarios
in a variety of weather conditions makes ROAD a dataset
which poses signiﬁcant challenges when compared to other,
simpler action recognition benchmarks.
In Section 5.3 we illustrate and discuss the baseline
results on ROAD for the different tasks (Sec. 5.3.2), using
a 2D ResNet50, an I3D and a Slowfast backbone, as well as
the agent detection performance of the standard YOLOv5
model. Different training/testing splits encoding different
weather conditions are examined using the I3D backbone
(Sec. 5.3.3). In particular, in Sec. 5.3.4 we show the results
one can obtain when predicting composite labels as prod-
ucts of single-label predictions as opposed to training a
speciﬁc model for them, as this can provide a crucial ad-
vantage in terms of efﬁciency, as well as give the system the
ﬂexibility to be extended to new composite labels without
retraining. Finally, in Sec. 5.3.5 we report our baseline results
on the temporal segmentation of AV actions.
5.1 Implementation details
The results are evaluated in terms of both frame-level
bounding box detection and of tube detection. In the ﬁrst
case, the evaluation measure of choice is frame mean average
precision (f-mAP). We set the Intersection over Union (IoU)
detection threshold to 0.5(signifying a 50% overlap between
predicted and true bounding box). For the second set of
results we use video mean average precision (video-mAP), as
information on how the ground-truth BBs are temporally
connected is available. These evaluation metrics are stan-
dard in action detection [19], [81], [98], [99], [100].
We also evaluate actions performed by AV , as described in
3.1. Since this is a temporal segmentation problem, we adopt
the mean average precision metric computed at frame-level,
as standard on the Charades [69] dataset.
We use sequences of T= 8 frames as input to 3D-
RetinaNet. Input image size is set to 512×682. This choice
ofTis the result of GPU memory constraints; however,
at test time, we unroll our convolutional 3D-RetinaNet for
sequences of 32 frames, showing that it can be deployed in a
streaming fashion. We initialise the backbone network with
weights pretrained on Kinetics [65]. For training we use an
SGD optimiser with step learning rate. The initial learning
rate is set to 0.01and drops by a factor of 10after 18and
25epochs, up to an overall 30epochs. For tests on the UCF-
101-24 dataset the learning rate schedule is shortened to aTABLE 3
Comparison of the action detection performance (frame-mAP@0.5
(f-mAP) and video-mAP at different IoU thresholds) of the proposed
3D-RetinaNet baseline model with the state-of-the-art on the
UCF-101-24 dataset.
Methods /δ= f-mAP 0.2 0.5 0.75 0.5:0.9
RGB + FLOW methods
MR-TS Peng et al. [85] – 73.7 32.1 00.9 07.3
FasterRCNN Saha et al. [98] – 66.6 36.4 07.9 14.4
SSD + OJLA Behl et al. [80]∗– 68.3 40.5 14.3 18.6
SSD Singh et al. [19]∗– 76.4 45.2 14.4 20.1
AMTnet Saha et al. [84]∗– 78.5 49.7 22.2 24.0
ACT Kalogeiton et al. [81]∗– 76.5 49.2 19.7 23.4
TraMNet Singh et al. [87]∗– 79.0 50.9 20.1 23.9
Song et al. [101] 72.1 77.5 52.9 21.8 24.1
Zhao et al. [86] – 78.5 50.3 22.2 24.5
I3D Gu et al. [102] 76.3 – 59.9 – –
Liet al. [82]∗78.0 82.8 53.8 29.6 28.3
RGB only methods
RGB-SSD Singh et al. [19]∗65.0 72.1 40.6 14.1 18.5
RGB-AMTNet Saha et al. [84]∗– 75.8 45.3 19.9 22.0
3D-RetinaNet / 2D (ours)∗65.2 73.5 48.6 22.0 22.8
3D-RetinaNet / I3D (ours) 75.2 82.4 58.2 25.5 27.1
∗online methods
maximum 10epochs, and the learning rate drop steps are
set to 6and 8.
The parameters of the tube-building algorithm (Sec. 4.2)
are set by cross validation. For ROAD we obtain λ= 0.5
andk= 4. For UCF-101-24, we get λ= 0.25andk= 4.
Temporal trimming is only performed on UCF-101-24.
5.2 Baseline performance on UCF-101-24
Firstly, we benchmarked 3D-RetinaNet on UCF-101-
24 [62], [97], using the corrected annotations from [19]. We
evaluated both frame-mAP and video-mAP and provided
a comparison with state-of-the-art approaches in Table 3.
It can be seen that our baseline is competitive with the
current state-of-the-art [82], [102], even as those methods
use both RGB and optical ﬂow as input, as opposed to
ours. As shown in the bottom part of Table 3, 3D-RetinaNet
outperforms all the methods solely relying on appearance
(RGB) by large margins. The model retains the simplicity
of single-stage methods, while sporting, as we have seen,
the ﬂexibility of being able to be reconﬁgured by changing
the backbone architecture. Note that its performance could
be further boosted using the simple optimisation technique
proposed in [103].
5.3 Experimental results on ROAD
5.3.1 Three splits: modelling weather variability
For the benchmarking of the ROAD tasks, we divided
the dataset into two sets. The ﬁrst set contains 18 videos
for training and validation purposes, while the second set
contains 4 videos for testing, equally representing the four
types of weather conditions encountered.
The group of training and validation videos is further
subdivided into three different ways (’splits’). In each split,
15 videos are selected for training and 3 for validation.
Details on the number of videos for each set and split are
shown in Table 4. All 3 validation videos for Split-1 are
IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. Y , 20XX 11
TABLE 4
Splits of training, validation and test sets for the ROAD dataset with
respect to weather conditions. The table shows the number of videos in
each set or split. For splits, the ﬁrst ﬁgure is the number of training
videos, the second number that of validation videos.
Condition sunny overcast snow night
Training and validation 7 7 1 3
Split-1 7/0 4/3 1/0 3/0
Split-2 7/0 7/0 1/0 0/3
Split-3 4/3 7/0 1/0 3/0
Testing 1 1 1 1
overcast; 4 overcast videos are also present in the training
set. As such, Split-1 is designed to assess the effect of
different overcast conditions. Split-2 has all 3 night videos in
the validation subset, and none in the training set. It is thus
designed to test model robustness to day/night variations.
Finally, Split-3 contains 4 training and 3 validation videos
for sunny weather: it is thus designed to evaluate the effect
of different sunny conditions, as camera glare can be an
issue when the vehicle is turning or facing the sun directly.
Note that there is no split to simulate a bias towards
snowy conditions, as the dataset only contains one video of
that kind. The test set (bottom row) is more uniform, as it
contains one video from each environmental condition.
5.3.2 Results on the various tasks
Results are reported for the tasks discussed in Section 3.4.
Frame-level results across the ﬁve detection tasks are sum-
marised in Table 5 using the frame-mAP (f-mAp) metric, for
a detection threshold of δ= 0.5. The reported ﬁgures are
averaged across the three splits described above, in order
to assess the overall robustness of the detectors to domain
variations. Performance within each split is evaluated on
both the corresponding validation subset and test set. Each
row in the Table shows the result of a particular combination
of backbone network (2D, I3D, or Slowfast) and test-time
sequence length (in number of frames, 8 and 32). Frame-
level results vary between 16.8% (events) and 65.4% (agent-
ness) for I3D, and between 23.9% and 69.2% for Slowfast.
Clearly, for each detection task except agentnness (which
amounts to agent detection on ROAD) the performance is
quite lower than the 75.2% achieved by our I3D baseline
network on UCF-101-24 (Table 3, last row). This is again due
to the numerous nuisance factors present in ROAD, such
as signiﬁcant camera motion, weather conditions, etc. For a
fair comparison, note that there are only 11 agent classes, as
opposed to e.g. 23 action classes and 15 location classes.
Video-level results are reported in terms of video-mAP
in Table 6. As for the frame-level results, tube detection
performance (see Sec. 4.2) is averaged across the three splits.
One can appreciate the similarities between frame- and
video-level results, which follow a similar trend albeit at
a much lower absolute level. Again, results are reported
for different backbone networks and sequence lengths. Not
considering the YOLOv5 numbers, video-level results at de-
tection threshold δ= 0.2vary between a minimum of 20.5%
(actions) to a maximum of 33.0% (locations), compared to
the 82.4% achieved on UCF-101-24. For a detection threshold
δequal to 0.5, the video-level results lie between 4.7% (ac-
tions) and 11% (locations) compared to the 58.2% achievedTABLE 5
Frame-level results (mAP %) averaged across the three splits of ROAD.
The considered models differ in terms of backbone network (2D, I3D,
and Slowfast) and clip length (08 vs 32). The performance of YOLOv5
on agent detection is also reported. Detection threshold δ= 0.5. Both
validation and test performance are reported for each entry.
Model Agentness Agents Actions Locations Duplexes Events
2D-08 51.8/63.4 30.9/39.5 15.9/22.0 23.2/30.8 18.1/25.1 10.6/12.8
2D-32 52.4/64.2 31.5/39.8 16.3/22.6 23.6/31.4 18.7/25.8 10.8/13.0
I3D-08 52.3/65.1 32.2/39.5 19.3/25.4 24.5/34.9 21.5/30.8 12.3/16.5
I3D-32 52.7/65.4 32.3/39.2 19.7/25.9 24.7/35.3 21.9/31.0 12.6/16.8
Slowfast-08 68.8/ 69.2 41.9/47.5 26.9/31.1 34.6/ 37.3 31.6/36.0 18.1/23.7
Slowfast-32 69.3/68.7 42.6/43.7 27.3/31.7 34.8 /36.4 32.0/36.1 18.0/ 23.9
YOLOv5 - 57.9/56.9 - - - -
TABLE 6
Video-level results (mAP %) averaged across the three ROAD splits.
The models differ in terms of backbone network (2D, I3D and Slowfast)
and test time clip length (08 vs 32). The performance of YOLOv5 on
agent detection is also reported. Both validation and test performance
are reported for each entry.
Model Agents Actions Locations Duplexes Events
Detection threshold δ= 0.2
2D-08 22.2/25.1 10.3/13.9 18.2/24.8 16.1/21.9 12.8/14.7
2D-32 22.6/25.0 11.2/14.5 18.5/25.9 16.2/22.7 13.0/15.3
I3D-08 23.2/26.5 14.1/15.8 20.8/25.8 21.1/24.0 14.9/17.4
I3D-32 24.4/26.9 14.3/17.5 21.3/27.1 21.4/25.5 15.9/17.9
Slowfast-08 24.1/29.0 16.0/20.5 28.3/ 33.0 24.0/ 27.3 18.9/22.4
Slowfast-32 24.2/28.6 16.0/19.55 29.0/29.7 24.3/26.1 19.1/22.5
YOLOv5 38.8/43.3 - - - -
Detection threshold δ= 0.5
2D-08 8.9/7.5 2.3/3.0 5.2/6.1 6.5/6.1 5.1/5.3
2D-32 8.3/8.0 2.7/3.3 5.6/7.1 6.3/6.8 5.0/5.7
I3D-08 9.2/9.6 4.0/4.3 5.8/6.9 7.2/7.4 4.6/5.4
I3D-32 9.7/10.2 4.0/4.6 6.4/7.7 7.1/8.3 4.8/6.1
Slowfast-08 7.1/8.9 3.9/ 4.7 7.1/11.0 7.3 /7.7 6.5/6.6
Slowfast-32 8.3/9.8 3.7/4.4 8.4/10.0 7.1/ 9.0 5.3/7.3
YOLOv5 18.7/13.9 - - - -
on UCF-101-24 for the same IoU threshold. The difference is
quite dramatic, and highlights the order of magnitude of the
challenge involved by perception in autonomous driving
compared to a standard benchmark portraying only human
actions. Furthermore, we can notice a few important facts.
Streaming deployment . Increasing test sequence length
from 8 to 32 does not much impact performance. This in-
dicates that, even though the network is trained on 8-frame
clips, being fully convolutional (including the heads in the
temporal direction), it can be easily unrolled to process
longer sequences at test time, making it easy to deploy in
a streaming fashion. Being deployable in an incremental
fashion is a must for autonomous driving applications; this
is a quality that other tubelet-based online action detection
methods [81], [82], [87] fail to exhibit, as they can only be
deployed in a sliding window fashion. Interestingly, the
latest work on streaming object detection [104] proposes an
approach that integrates latency and accuracy into a single
metric for real-time online perception, termed ‘streaming
accuracy’. We will consider adopting this metric in the
future evolution of ROAD.
Impact of the backbone . Broadly speaking, the Slowfast [22]
and I3D [74] versions of the backbone perform as expected,
much better than the 2D version. A Slowfast backbone can
particularly help with tasks which require the system to
‘understand’ movement, e.g. when detecting actions, agent-"
2103.04147,D:\Database\arxiv\papers\2103.04147.pdf,"The paper describes a method for re-identifying occluded targets in a multi-object tracking system.  How does the proposed method address the challenge of re-identifying targets that have been occluded for an extended period, and what are the potential limitations of this approach?","The method addresses this challenge by extending the bounding box of occluded targets based on their uncertainty, which increases with each frame the target is not observed. This allows for a more robust matching process even when the target's actual location is unknown. However, this approach may be limited by the accuracy of the uncertainty estimation and could lead to false positives if the uncertainty is overestimated.","(a) (b) (c) (d)
Fig. 3 : Example of re-identifying an occluded target. Detections are depicted with thin black rectangles, Targets are depicted
with colored thick bounding boxes, extended bounding boxes are depicted with dashed rectangles. (a) A target is going to be
occluded behind another target. (b) A target is occluded by another target and an extended bounding box is assigned to it. (c)
The target is occluded behind another target for several frames and the size of its extended bounding box is increased. (d) The
extended bounding box helps to re-identify the occluded target.
targets, including occluded targets are calculated and the asso-
ciation is performed using the Hungarian algorithm. Because
the estimated bounding boxes of occluded targets do not cor-
rect in the update step of the Kalman ﬁlter, there may become
a considerable difference between their estimation and real
location. As a result, they may not match in the ﬁrst step.
In the second step, to help the re-identiﬁcation of un-
matched occluded targets, their bounding boxes are extended
according to their uncertainty. This uncertainty increases in
every next frame where the target is not observed. Then the
IoU between extended bounding boxes and remaining un-
matched detections should be calculated. Because the sizes
of occluded targets do not extend in reality and only their un-
certainty increase, the extended IoU is calculated as below:
IoUext=I(bbD,bbextT)
A(bbD) +A(bbT)−I(bbD,bbextT)(6)
In whichbbextTis the extended bounding box of the occluded
target. The extended bounding box is only used for comput-
ing intersection and the area of the estimated bounding box
is used in the denominator. To complete the second step, the
calculatedIoUextis passed to the Hungarian algorithm for
the association. In Fig. 3, the extended bounding box of an
occluded target is depicted by dashed lines.
3.4. Creating New Targets
During tracking, new targets may be entered into the scene. In
the ﬁrst few frames, all unmatched detections, represented by
¯Dtare considered as new targets. But after that, a new target
is created after the probability of its existence increases. This
is achieved when unmatched detections are presented near a
location for three successive frames. For detecting this situa-tion, the unmatched detection of the current frame, the previ-
ous frame, and the two frames before are used. To assign un-
matched detections from different frames to each other, a pro-
cess similar to associating detections to targets is performed.
At ﬁrst, an IoU matrix between ¯Dtand¯Dt−1and another IoU
matrix between ¯Dt−1and¯Dt−2are calculated. Then each
calculated IoU matrix is passed to the Hungarian algorithm.
If three unmatched detections from different frames are as-
signed to each other, a new target is created and its Kalman
ﬁlter is initialized using corresponding unmatched detections.
The process of creating a new target is shown in Fig. 4.
(a)
 (b)
 (c)
Fig. 4 : New target creation after three frames. The detections
are shown with thin black rectangles and the target is shown
with red thick rectangle. (a) A new target is seen for the ﬁrst
time. (b) The new target is seen for the second time. (c) The
new target is seen for the third time and a target is associated
with it.
MOTA↑MOTP↑ MT↑ ML↓ IDS↓FM↓ FP↓ FN↓ FPS↑
KDNT [30] BATCH 68.2 79.4 41.0% 19.0% 933 1093 11479 45605 0.7
LMP p [31] BATCH 71.0 80.2 46.9% 21.9% 434 587 7880 44564 0.5
MCMOT HDM [32] BATCH 62.4 78.3 31.5% 24.2% 1394 1318 9855 57257 35
NOMTwSDO16 [33] BATCH 62.2 79.6 32.5% 31.1% 406 642 5119 63352 3
EAMTT [34] ONLINE 52.5 78.8 19% 34.9% 910 1321 4407 81223 12
POI [30] ONLINE 66.1 79.5 34% 20.8% 805 3093 5061 55914 10
SORT [7] ONLINE 59.8 79.6 25.4% 22.7% 1423 1835 8698 63245 60
Deep SORT [13] ONLINE 61.4 79.1 32.8% 18.2% 781 2008 12852 56668 40
Proposed ONLINE 61.1 79.04 31.62% 21.34% 848 1331 12296 57738 162.7
Table 1 : The results of our proposed tracking algorithm compared to the results of state-of-the-arts on MOT16. The best are in
the bold format. The second best are in the blue format.
Detection MOTA ↑ MT↑ ML↓ IDS↓FM↓ FP↓ FN↓ FPS↑
Tractor ALL 53.5 19.5% 36.6% 2072 4611 12201 248047 1.5
GCNNMatch ALL 57.0 23.3% 34.6% 1957 2798 12283 228242 1.3
Proposed ALL 44.3 14.4% 45.6% 2191 5243 21796 290065 137.5
Tractor DPM 52.2 14.9% 37.5% 635 - 2908 86275 -
GCNNMatch DPM 55.5 21.5% 37.6% 564 782 2937 80242 -
Proposed DPM 29.7 5.5% 64.7% 530 1448 3048 128696 -
Tractor FRCNN 52.9 16.2% 34.7% 648 - 3918 83904 -
GCNNMatch FRCNN 56.1 22.3% 33.9% 647 934 4015 77950 -
Proposed FRCNN 44.9 14.7% 40.3% 795 1571 9102 93669 -
Tractor SDP 55.3 18.1% 32.9% 789 - 5375 77868 -
GCNNMatch SDP 59.5 26.0% 32.4% 746 1082 5331 70050 -
Proposed SDP 58.4 22.8% 32.0% 866 2224 9646 67700 -
Table 2 : The results of our proposed tracking algorithm compared to the results of state-of-the-art algorithms on MOT17. The
best are in the bold format. The second best are in the blue format.
3.5. Removing Targets
An unmatched target is removed, when its uncertainty in-
creases above a threshold. Like the occlusion handling sec-
tion, the uncertainty is proportional to the number of frames
in which the target is visible. Any unmatched target is re-
tained for at least kminframes. But more conﬁdent targets are
retained for more frames proportional to their age. To prevent
retaining targets for more than needed, targets are retained for
at mostkmax frames. An unmatched target is retained when:
tsu>min (kmin+AgeT
ck,kmax) (7)
in whichtsu, time since updated, is the number of successive
frames the estimated bounding box of a target is not corrected.
Because Kalman ﬁlters of occluded targets are updated, they
do not enter the deletion process.
4. EXPERIMENTS
To compare the proposed algorithm with SORT [7] and DEEP
SORT [13] algorithms, it is evaluated on MOT16 benchmark
using private detections from POI [30] paper. For a faircomparison, like POI and DEEP SORT papers, detections
have been thresholded at a conﬁdence score of 0.3. Also
for comparing the proposed algorithm with the most recent
algorithms, their results on MOT17 are presented.
4.1. Evaluation Metrics
For evaluating multiple object tracking algorithms, the fre-
quently used CLEAR MOT metrics [35] are reported, includ-
ing Multiple Object Tracking Accuracy (MOTA), and Mul-
tiple Object Tracking Precision (MOTP). Also, other popular
metrics are used including Mostly Tracked (MT), Mostly Lost
(ML), the number of False Negatives (FN), False Positives
(FP), ID-Switches (IDS), and track Fragmentation (FM).
4.2. Results
The result of running the proposed algorithm on the MOT16
benchmark alongside the result of baseline algorithms such
as SORT and DEEP SORT come in the table 1. In the MOTA
and MOTP metrics, the results of the proposed algorithm are
comparable with other online algorithms and it is slightly
lower than the Deep SORT algorithm. For the IDS metric,
Algorithm 1: SORT with occlusion handling
Data:Dt,Tt−1
Result: ¯Dt,Tt,¯Tt
1ˆTt=KFPredict (Tt−1)
2Aavg=AreaAverage (ˆTt)
3˜Tt,˜Dt,Ot,¯Tt,¯Dt=Associate (ˆTt,Dt,Aavg)
4˜Tt=KFCorrection (˜Tt,˜Dt)
5Ot=Correction (Ot)
6Tt←˜Tt+¯Tt+Ot
7
8Function Associate( ˆTt,Dt,Aavg)
9 Cascade Matching:
10P=IoU(Dt,ˆTt)
11 ˜Tt,˜Dt,¯Tt,¯Dt=Hungarian (P)
12 Target Re-identiﬁcation:
13Pd=IoU(¯Dt,¯Dt−1)
14Pext=IoUext(¯Tt,¯Dt)
15 ˜Tn
t,˜Dn
t= 2StepMatching (Pd,Pext)
16 ˜Tt←˜Tt+˜Tn
t
17 ¯Tt←¯Tt−˜Tn
t
18 ˜Dt←˜Dt+˜Dn
t
19 ¯Dt←¯Dt−˜Dn
t
20 Detecting Occlusion:
21Ot←∅
22P=CP(ˆTt)
23 foru∈¯Ttdo
24Cu=min(1,α∗Ageu
tsou∗Au
Aavg)
25Pu=max(P)
26 ifCu>COthen
27 Ot←Ot+u
28 ¯Tt←¯Tt−u
29 else ifCu>CTandPu>mincoverage
then
30 Ot←Ot+u
31 ¯Tt←¯Tt−u
in comparison to the SORT algorithm, the value from 1423
is decreased to 848 which is a 40% reduction in this metric
and it is only 8% above the Deep SORT algorithm. Also, in
fragmentation metric, the score of the proposed algorithm is
1331 which is 28% lower than the SORT score which is 1835,
and is even 34% lower than the Deep SORT algorithm with
the value of 2008. This is because the Deep SORT algorithm
does not detect occlusions and uses only the appearance to
re-identify targets after they appear again. All of these com-
parable or even better results are achieved with the only use
of geometric features and with a much lower computation
cost that leads to a much higher speed, even with ordinary
hardware.Algorithm 2: Target creation and removal
Data: ¯Dt,¯Dt−1,¯Dt−2,Tt,¯Tt
Result: ¯Dt,¯Dt−1,Tt
1Nt←∅
2Rt←∅
3ifframe num<minhitsthen
4Nt←¯Dt
5 ¯Dt←∅
6else
7Nt=FindNewTarget (¯Dt,¯Dt−1,¯Dt−2)
8Rt=RemoveTarget (¯Tt)
9Tt←Tt+Nt−Rt
10
11Function Find New Target( ¯Dt,¯Dt−1,¯Dt−2)
12Nt←∅
13Pd=IoU(¯Dt,¯Dt−1)
14Pdb=IoU(¯Dt−1,¯Dt−2)
15Nt,Nt−1= 2StepMatching (Pd,Pdb)
16 ¯Dt←¯Dt−Nt
17 ¯Dt−1←¯Dt−1−Nt−1
18
19Function Remove Target( ¯Tt)
20Rt←∅
21 foru∈¯Ttdo
22 iftsuu>min (kmin+Ageu
ck,kmax)then
23 Rt←Rt+u
The MOT17 benchmark includes three sets of detections
for every sequence. The detection algorithms are DPM, FR-
CNN, and SDP. The results of a tracking algorithm on this
benchmark are the aggregation of its results in each detection
set. In Table 2 the result of two state-of-the-art tracking al-
gorithms and the proposed algorithm are separated for each
detection set.
Between these three detection algorithms, the DPM has
the lowest performance. The performance of the FRCNN al-
gorithm is better than DPM, but lower than SDP and the SDP
algorithm has the best performance. For the DPM and FR-
CNN detection sets, there is a signiﬁcant difference between
the result of the proposed algorithm and these two algorithms.
But for the SDP detection set, the results of the proposed algo-
rithm are better than results of the algorithm proposed in [4]
and is comparable with results of algorithm discussed in [5].
These two tracking algorithms use appearance features and
their speed even with high-performance hardware is lower
than 2 FPS which is not appropriate for real-time applications.
But the proposed algorithm reaches comparable results with
the only use of geometric features while it is much faster than
them.
(a) (b) (c)
Fig. 5 : The changes of three evaluation metrics on conﬁdence thresholds are presented; (a) MOTA; (b) IDS; (c) FM
Fig. 6 : MOTA, IDS and FM versus COfor a ﬁxed value of CT
4.3. Sensitivity to parameters
The proposed algorithm relies on several parameters from
whichCOandCT, i.e. the conﬁdence thresholds for detect-
ing an occluded target, are the most important ones. Thus, the
sensitivity of the algorithm has been evaluated with respect to
these two parameters. The sensitivity evaluation is done on
the train data only since the test data was not accessible for
exhaustive analysis. The evaluation is done on both MOT16
and MOT17 datasets, separately. In Fig. 5, the changes in
MOTA, IDS, and FM metrics based on the changes in CO
andCTare plotted for MOTA17. Evidently, increasing CO
improves MOTA, and increasing CTslightly decreases it. In-
creasing both COandCTincreases IDS in general but there
are intermediate values in which IDS is near to its minimum.
Also, increasing both COandCTincreases the FM which
is not desired. To optimize the performance of the proposed
algorithm, a cost function proportional to the IDS and propor-
tional to the inverse of the MOTA is deﬁned. After minimiz-
ing this cost function, the best results for MOT17 are achieved
withCO= 0.75andCT= 0.35. The general behavior of
the algorithm is the same on MOT16 too and CO= 0.9and
CT= 0.55are chosen for it. Tuning these parameters can
indeed improve the performance of the algorithm, but the im-
portant point is that the sensitivity of the algorithm to these
parameters is low. For example in MOT17, the minimum and
maximum of MOTA for the whole range of COandCTis
46.5 and 47.3, respectively which is less than 1 percent. Alsothe minimum and maximum of IDS are 1448 and 1504 which
is less than 4 percent. In addition, for a ﬁxed value of CT,
MOTA, IDS and FM are plotted versus COin Fig 6. Nee-
dles to say, by increasing COMOTA and FM grow and IDS
changes non-monotonically. To balance between these met-
rics, 0.75 is chosen for COat which MOTA is maximum, IDS
is near its minimum and FM is not too high.
5. CONCLUSION
In this paper, a novel algorithm for tracking multiple objects
is proposed to handle occlusions and re-identiﬁcation of lost
targets efﬁciently. This algorithm only uses geometric cues
including the location and size of the bounding box of detec-
tions. As a result, it is very fast and is appropriate for real-
time applications. The performance of this algorithm is com-
parable to other algorithms that use appearance features and
could reach comparable results in terms of MOTA, MOTP,
and IDS for the MOT16 dataset in comparison to the Deep
SORT algorithm. For the FM metric, the results is better than
the Deep SORT algorithm. Moreover, in the MOT17 bench-
mark, its result is comparable with the state-of-the-art algo-
rithms for the SDP detection set. The results of the proposed
algorithm on the MOT17 dataset show that if the detection
algorithm has good performance, there is no need to use ap-
pearance features to achieve desired performance and only
geometric cues can be used to achieve a much higher speed.
6. REFERENCES
[1] Mohammadreza Babaee, Yue You, and Gerhard Rigoll,
“Combined segmentation, reconstruction, and track-
ing of multiple targets in multi-view video sequences,”
Computer Vision and Image Understanding , vol. 154,
pp. 166–181, 2017.
[2] Mohammadreza Babaee, Yue You, and Gerhard Rigoll,
“Pixel level tracking of multiple targets in crowded en-
vironments,” in European Conference on Computer Vi-
sion. Springer, 2016, pp. 692–708.
[3] Xinshuo Weng, Yongxin Wang, Yunze Man, and Kris
Kitani, “Gnn3dmot: Graph neural network for 3d
multi-object tracking with multi-feature learning,” arXiv
preprint arXiv:2006.07327 , 2020.
[4] Philipp Bergmann, Tim Meinhardt, and Laura Leal-
Taixe, “Tracking without bells and whistles,” in Pro-
ceedings of the IEEE international conference on com-
puter vision , 2019, pp. 941–951.
[5] Ioannis Papakis, Abhijit Sarkar, and Anuj Karpatne,
“Gcnnmatch: Graph convolutional neural networks for
multi-object tracking via sinkhorn normalization,” arXiv
preprint arXiv:2010.00067 , 2020.
[6] Yang Zhang, Hao Sheng, Yubin Wu, Shuai Wang, Wei
Ke, and Zhang Xiong, “Multiplex labeling graph for
near-online tracking in crowded scenes,” IEEE Internet
of Things Journal , vol. 7, no. 9, pp. 7892–7902, 2020.
[7] Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos,
and Ben Upcroft, “Simple online and realtime track-
ing,” in 2016 IEEE International Conference on Image
Processing (ICIP) . IEEE, 2016, pp. 3464–3468.
[8] Anton Milan, Laura Leal-Taix ´e, Ian Reid, Stefan Roth,
and Konrad Schindler, “Mot16: A benchmark for multi-
object tracking,” arXiv preprint arXiv:1603.00831 ,
2016.
[9] Pedro Felzenszwalb, David McAllester, and Deva Ra-
manan, “A discriminatively trained, multiscale, de-
formable part model,” in 2008 IEEE conference on com-
puter vision and pattern recognition . IEEE, 2008, pp.
1–8.
[10] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian
Sun, “Faster r-cnn: Towards real-time object detection
with region proposal networks,” in Advances in neural
information processing systems , 2015, pp. 91–99.
[11] Fan Yang, Wongun Choi, and Yuanqing Lin, “Exploit
all the layers: Fast and accurate cnn object detector withscale dependent pooling and cascaded rejection classi-
ﬁers,” in Proceedings of the IEEE conference on com-
puter vision and pattern recognition , 2016, pp. 2129–
2137.
[12] Rudolph Emil Kalman, “A new approach to linear ﬁlter-
ing and prediction problems,” 1960.
[13] Nicolai Wojke, Alex Bewley, and Dietrich Paulus,
“Simple online and realtime tracking with a deep associ-
ation metric,” in 2017 IEEE international conference on
image processing (ICIP) . IEEE, 2017, pp. 3645–3649.
[14] Lu Wang, Lisheng Xu, Min Young Kim, Luca Rigazico,
and Ming-Hsuan Yang, “Online multiple object track-
ing via ﬂow and convolutional features,” in 2017 IEEE
International Conference on Image Processing (ICIP) .
IEEE, 2017, pp. 3630–3634.
[15] Anton Milan, Seyed Hamid Rezatoﬁghi, Anthony Dick,
Ian Reid, and Konrad Schindler, “Online multi-
target tracking using recurrent neural networks,” arXiv
preprint arXiv:1604.03635 , 2016.
[16] Laura Leal-Taix ´e, Gerard Pons-Moll, and Bodo Rosen-
hahn, “Everybody needs somebody: Modeling social
and grouping behavior on a linear programming multi-
ple people tracker,” in 2011 IEEE international confer-
ence on computer vision workshops (ICCV workshops) .
IEEE, 2011, pp. 120–127.
[17] Boyu Chen, Dong Wang, Peixia Li, Shuang Wang, and
Huchuan Lu, “Real-time’actor-critic’tracking,” in Pro-
ceedings of the European Conference on Computer Vi-
sion (ECCV) , 2018, pp. 318–334.
[18] Li Zhang, Yuan Li, and Ramakant Nevatia, “Global
data association for multi-object tracking using network
ﬂows,” in 2008 IEEE Conference on Computer Vision
and Pattern Recognition . IEEE, 2008, pp. 1–8.
[19] Minyoung Kim, Stefano Alletto, and Luca Rigazio,
“Similarity mapping with enhanced siamese net-
work for multi-object tracking,” arXiv preprint
arXiv:1609.09156 , 2016.
[20] Kwang-Yong Kim, Jun-Seok Kwon, and Kee-Seong
Cho, “Multi-object tracker using kemelized correlation
ﬁlter based on appearance and motion model,” in 2017
19th International Conference on Advanced Communi-
cation Technology (ICACT) . IEEE, 2017, pp. 761–764.
[21] Haoyang Feng, Xiaofeng Li, Peixin Liu, and Ning Zhou,
“Using stacked auto-encoder to get feature with con-
tinuity and distinguishability in multi-object tracking,”
inInternational Conference on Image and Graphics .
Springer, 2017, pp. 351–361.
[22] Maryam Babaee, Zimu Li, and Gerhard Rigoll, “A dual
cnn–rnn for multiple people tracking,” Neurocomputing ,
vol. 368, pp. 69–83, 2019.
[23] Chanho Kim, Fuxin Li, and James M Rehg, “Multi-
object tracking with neural gating using bilinear lstm,”
inProceedings of the European Conference on Com-
puter Vision (ECCV) , 2018, pp. 200–215.
[24] Mohib Ullah and Faouzi Alaya Cheikh, “A directed
sparse graphical model for multi-target tracking,” in
Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition Workshops , 2018, pp.
1816–1823.
[25] Chanho Kim, Fuxin Li, Arridhana Ciptadi, and James M
Rehg, “Multiple hypothesis tracking revisited,” in Pro-
ceedings of the IEEE international conference on com-
puter vision , 2015, pp. 4696–4704.
[26] Liangliang Ren, Jiwen Lu, Zifeng Wang, Qi Tian, and
Jie Zhou, “Collaborative deep reinforcement learning
for multi-object tracking,” in Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV) , 2018,
pp. 586–602.
[27] Qiankun Liu, Qi Chu, Bin Liu, and Nenghai Yu, “Gsm:
Graph similarity model for multi-object tracking,” .
[28] Yongxin Wang, Kris Kitani, and Xinshuo Weng, “Joint
object detection and multi-object tracking with graph
neural networks,” .
[29] Xingyi Zhou, Vladlen Koltun, and Philipp Kr ¨ahenb ¨uhl,
“Tracking objects as points,” arXiv preprint
arXiv:2004.01177 , 2020.
[30] Fengwei Yu, Wenbo Li, Quanquan Li, Yu Liu, Xiao-
hua Shi, and Junjie Yan, “Poi: Multiple object tracking
with high performance detection and appearance fea-
ture,” in European Conference on Computer Vision .
Springer, 2016, pp. 36–42.
[31] Margret Keuper, Siyu Tang, Yu Zhongjie, Bjoern An-
dres, Thomas Brox, and Bernt Schiele, “A multi-cut
formulation for joint segmentation and tracking of mul-
tiple objects,” arXiv preprint arXiv:1607.06317 , 2016.
[32] Byungjae Lee, Enkhbayar Erdenee, Songguo Jin,
Mi Young Nam, Young Giu Jung, and Phill Kyu Rhee,
“Multi-class multi-object tracking using changing point
detection,” in European Conference on Computer Vi-
sion. Springer, 2016, pp. 68–83.
[33] Wongun Choi, “Near-online multi-target tracking with
aggregated local ﬂow descriptor,” in Proceedings of
the IEEE international conference on computer vision ,
2015, pp. 3029–3037.[34] Ricardo Sanchez-Matilla, Fabio Poiesi, and Andrea
Cavallaro, “Online multi-target tracking with strong and
weak detections,” in European Conference on Computer
Vision . Springer, 2016, pp. 84–99.
[35] K. Bernardin and R. Stiefelhagen, “Evaluating multi-
ple object tracking performance: the clear mot metrics,”
EURASIP Journal on Image and Video Processing , vol.
2008, no. 1, pp. 246309, 2008."
2308.11206,D:\Database\arxiv\papers\2308.11206.pdf,"How does the proposed method address the issue of attribute confusion in text-to-image generation, particularly in the context of generating garment images?","The method introduces a semantic-bundled cross-attention mechanism that encourages similar attention maps for attribute adjectives and the corresponding garment part subjects within an Attribute-Phrase, thereby preventing mismatched attention regions and reducing attribute confusion.","Figure 3. Overview of DiffCloth. During the diffusion step, we leverage constituency parsing to extract the text structure and obtain a tree
of all attribute- phrases (APs). Given this structure information, the structural semantic consensus partitions the garment images using a
segmentor into multiple visual parts, which are then matched with the APs using a bipartite matching to get structural semantic alignment.
This generates the LHungarian loss. Similarly, to preserve structure similarity between the attention maps of the attribute adjectives and
the corresponding garment part subjects we introduce semantic-bundled cross-attention, which addresses the attribute confusion issue via
theLbundle loss. More specifically, query Qis obtained from the visual representation Xt, while keys Kare computed for each word.
Lbundle then aims to encouraging similar attention maps for each AP. Finally, the losses are used to refine the feature representation of the
diffusion model at each step.
steps following the Gaussian transition q(zt|zt−1):
q(zt|zt−1) =N(zt;p
1−βtzt−1, βtI), (2)
where βdenotes the noise scale, Iis the identity matrix, and
ztis the latent of the timestep t.
By optimizing a noise estimator ϵθ, the model is trained
to reverse the diffusion process and generate images from
random noise by optimizing the loss LDM:
LDM=Et,z0,ϵ
∥ϵθ(zt)−ϵ∥2
. (3)
A synthesized image x∗
0is obtained by denosing noise
xTforTsteps and decoding it using the decoder x∗
0=
Dec(z∗
0).
DiffCloth is trained on the garment images by optimizing
Eq. (3) and sampled using the guidance from our proposed
structure semantic consensus and semantic-bundled losses,
which will be detailed in the following sections.
3.2. Structural Semantic Consensus Guidance
Our structural semantic consensus guidance is based on
the intuition that there are structural similarities between
visual and textual representations in cross-modal garment
synthesis. As shown in Fig. 3, a segmentor trained on noisy
inputs can be used to partition garment images into multiplevisual parts that adhere to the standard structural patterns
used by humans in garment design.1The visual structured
components can be denoted as:
V= [Vfull, V1, V2, ..., V m], (4)
where Vfulldenotes the full garment image and Viis the ith
part image of Vfullindicated by the mask Mi,e.g., sleeves,
body piece, hood.
Similarly, we can obtain the text structure by leverag-
ing constituency parsing to extract a tree of all Attribute-
Phrases (APs), which are crucial for depicting the semantic
components of a garment image:
W= [Wfull, W1, W2, ..., W m], (5)
where Wfulldenotes the full prompt and Wiis the ithlin-
guistic AP in the tree structure e.g., ‘blue sweater’, ‘classic
hood’, ‘long sleeves’, where meaningless conjunctions, e.g.,
‘and’, ‘with’ are omitted.
Bipartite Matching. In order to generate garment im-
ages with part-level consensus between these two collec-
tions of visual and linguistic components, we formulate
the cross-modal semantic alignment as a set-to-set bipartite
matching problem. Our objective is to find a permutation,
1More details are provided in Sec. 4.
Prompt: Navy blue jacket with straight-point red collar, front button and long sleeves.
Stable Diffusion DiffCloth
Attribute ConfusionBundled semantics 
in AP
navy blue jacket red collar navy blue jacket red collarFigure 4. Visualization of the cross-attention of Stable Diffusion
and our DiffCloth.
σ, of the set of msemantic components, which minimizes
the pair-wise semantic matching loss Lmatch (Vi, Wˆσ(i)):
ˆσ= arg min
σmX
iLmatch (Vi, Wˆσ(i)), (6)
where Lmatch (Vi, Wˆσ(i)) = CLIP (Vi, Wˆσ(i))and
CLIP (·,·)denotes the CLIP similarity [24]. The optimal
matching is obtained by using the Hungarian algo-
rithm [17]. Further, we define our Hungarian matching
loss to compute the hierarchical structure alignment score
onVandWby calculating the part-level and image-level
alignment scores:
LHungarian (V,W) =mX
iCLIP (Vi,Wˆσ(i))+CLIP (Vfull,Wfull).
(7)
The latent code ztin the tthdenoising step of the diffu-
sion model is then refined via the structural semantic con-
sensus guidance:
ˆzt←zt+α· ∇ztLHungarian (ϕ(zt),W), (8)
where ϕ(zt)denotes the collection of visual structured com-
ponents for the decoded image corresponding to zt.
3.3. Semantic-bundled Cross-attention
Current text-to-image diffusion models, such as Stable
Diffusion, have demonstrated that cross-attention between
prompt tokens and visual feature maps results in coarse se-
mantic alignment. However, for complex garment descrip-
tions, a phenomenon of ‘attribute confusion’ arises, which
can severely impact the reliability of fashion generators.
Specifically, attributes and garment parts may be wrongly
paired and some attributes may be ignored in the gener-
ated image, resulting in imprecise and unsatisfactory gen-
erated garments. An example of this is provided in Fig. 4,
where the output image is a ‘Red jacket with blue cuffs.’
while the input prompt is ‘Navy blue jacket with red col-
lar.’ To reveal the underlying reason for the incorrect attri-
bution of ‘red’, we visualize the cross-attention maps be-
tween the visual tokens and the linguistic tokens in Fig. 4.
Figure 5. Overview of the garment manipulation pipeline.
It can be observed from the results given by Stable Dif-
fusion that the attention map of ‘red’ is spatially similar
to that of ‘jacket’ rather than ‘collar’, which leads to the
unexpected mismatched attention regions for the Attribute-
Phrase pair ‘red collar’. To address this issue, we propose
a semantic-bundled cross-attention mechanism that lever-
ages a semantic-bundled loss Lbundle to preserve the spatial
structure similarity between the attention maps of the at-
tribute adjectives and the garment part subject. Formally,
given an input prompt Wfull, we first obtain the collection
of attribute-phrases {W1, W2, ..., W m}using the aforemen-
tioned linguistic parsing tree. Our goal is to make the at-
tention maps of the Niattribute adjectives for the AP Wi,
{Wj
i}Ni
j=1, and the part noun WNi+1
i ,i.e.,{Mj
i}Ni+1
j=1share
similar spatial structures. We therefore regard an attention
mapMj
ias a multi-dimensional probability distribution and
define the internal structural similarity for Wias:
dIS(Vfull, Wi) =X
(j,k)∈(Ni+1
2)dJS(Mj
i, Mk
i),(9)
where Ni+1
2
denotes the 2-combination set of the Ni+
1indexes, dJSis the Jensen-Shannon Divergence [8], and
the attention mask Mj
iis obtained from the cross-attention
between the text token Wj
iand image Vfull. We then define
the semantic-bundled loss for {Wi}m
i=1as
Lbundle (Vfull,W) =mX
i=1dIS(Vfull, Wi). (10)
Similarly to Eq. (8), we again shift the latent code ˆztto bun-
dle the semantics of attribute adjectives and the part noun in
the APs in the denosing stage:
z′
t←ˆzt−β· ∇ztLbundle (zt,W). (11)
3.4. Region Consistency for Garment Manipulation
DiffCloth is inspired by Prompt-to-Prompt [9] and al-
lows manipulation of the generated images by simply mod-
ifying the input text prompt. Formally speaking, given an
original prompt input and its W, we can locally manipulate
Black cotton women tops with 
jersey knit ,crew neck and short 
sleeves, featuring logo print at 
the chest.Green women dresses with 
classic collar, long sleeves, 
and drawstring fastening waist, 
featuring painterly print.Black cotton shirt with chest 
patch pocket, round neck and 
long sleeves, featuring logo 
patch at the chest.Navy blue cotton men sweaters 
knitwear with drawstring hood and 
long sleeves, featuring front zip 
fastening and single patch pocket.Stable 
DiffusionComposable 
DiffusionAttend-and-
ExicteStructure
Diffusion DiffCloth
Figure 6. Results of DiffCloth on the garment synthesis task for some difficult examples that require the precise generation of fine-grained
details. DiffCloth outperforms existing SOTA methods and is capable of generating semantically-correct results. The boxes are used to
highlight specific areas that should contain the elements highlighted in the text.
an output image Ithat is generated from Wby simply mod-
ifying WtoW∗which will result in the updated image I∗.
For example, we can change a text token Wj
itoWj,∗
iand
replace its attention map Mj
iwith a new one Mj,∗
iin each
diffusion step. However, we find that this simple applica-
tion of Prompt-to-Prompt [9] degrades our bundled seman-
tics for APs that were introduced in Sec. 3.3 and may lead
to attribute confusion problems in the editing phase.
To preserve the bundled semantics for attribute-phrases
during manipulation, as shown in Fig. 5, we propose to
replace the attention maps of all tokens {Wj
i}Ni+1
j=1 in an
Attribute-Phrase Wirather than solely handling the token
we need to change. For example, if we want to change
the attribute of the sleeves, e.g., “long sleeves” →“short
sleeves”, we need to inject the attention maps of both “long”
and “sleeves”. Following Prompt-to-Prompt [9], we need to
run the diffusion step again by merging the new attention
maps{Mj,∗
i}Ni+1
j=1 with the fixed ones. In the tthdenois-
ing step, we can then use the semantic-bundled guidance in
Eq. (11) again to preserve the internal structural similarity
for{Mj,∗
i}Ni+1
j=1.Another issue with garment manipulation is how to avoid
editing regions that are not relevant to the Attribute-Phrase
W∗
ithat is being modified. To address this, we select a dy-
namic threshold pas the first quartile of the pixel activations
in the attention map Mj
iand use it to binarize Mj
ito a mask
Bj
iby thresholding. In this way, we obtain binarized masks
{Bj
i}Ni+1
j=1 and{Bj,∗
i}Ni+1
j=1 according to WiandW∗
i, re-
spectively. The irrelevant region is then indicated by the
blended mask Bi:
Bi= (Ni+1M
j=1Bj
i)M
(Ni+1M
j=1Bj,∗
i), (12)
whereLdenotes boolean summation. Similarly, when
modifying multiple APs, e.g.,{Wi}i∈Γ, we can compute a
global mask Bacross {Wi}i∈ΓasB=L
i∈ΓBi, where Γ
denotes the indexes of the APs that are being manipulated.
The region consistency is encouraged in each denoising
step by blending the two latent representations ztandz∗
t
using B:
z∗
t−1←Denoise (B·(zt−z∗
t) +z∗
t) (13)
10%11%12%
9%20%21%26%24%32%35%
0%5%10%15%20%25%30%35%40%
Less Garment Part Leakage Less Attribute ConfusionStable Diffusion Composable DiffusionStructureDiffusion
Attend-and-Excite DiffCloth
4%
19%
30%47%Blended Diffusion
Prompt-to-Prompt
Null-text Inversion
DiffCloth(a) Garment Synthesis
(b) Garment ManipulationFigure 7. Human evaluation results for the garment synthesis and
garment manipulation tasks.
where Denoise (·)denotes a DiffCloth denoising process.
4. Experiments
Datasets:
Experiments are conducted on the CM-Fashion
dataset [41], which consists of garment images and their
mask at resolution 512×512. This high-resolution fashion
dataset contains 509,482 image-text pairs from various
garment categories and is split into 409,482/100,000
training/testing pairs. In addition, we used 100,000
image-mask pairs from the training set to train the seg-
mentor for segmenting noisy garment images into parts.
Implementation Details: The implementation closely
follows Stable Diffusion [29]. However, we finetuned the
model on the CM-Fashion dataset as the pre-trained Stable
Diffusion did not produce garments on a homogeneous
white background. Our models are trained on 8 Tesla V100
GPUs with a batch size of 32. During the generator training
phase, the model is trained for 80 epochs with learning rate
1e-6.
Our segmentor is Pointrend [16], which was trained us-
ing input images with added noise. The model was trained
for 150 epochs with a learning rate of 4e-5. Further details
are provided in the supplementary material.
Baselines and Evaluation Metrics. For the generation
step, we compare DiffCloth to the state-of-the-art methods
TediGAN [35], Cogview [5], VQGAN [4], ARMANI [41],
Stable Diffusion [29], Composable Diffusion [20], Struc-
tureDiffusion [7] and Attend-and-Excite [3]. To ensure fair
comparisons, all models use our generator that has beenMethod FID ↓ IS↑ CLIPScore ↑
TediGAN [35] 27.37 18.46 0.5587
Cogview [5] 12.198 23.99 0.6572
VQGAN [4] 13.249 20.33 0.6423
ARMANI [41] 12.336 24.32 0.6988
Stable Diffusion [29] 9.475 24.59 0.8169
Composable Diffusion [20] 9.499 25.91 0.8306
StructureDiffusion [7] 9.238 25.36 0.8459
Attend-and-Excite [3] 9.351 26.87 0.8241
DiffCloth(Ours) 9.201 26.95 0.8974
Table 1. Comparison of DiffCloth to prior state-of-the-art ap-
proaches on the CM-Fashion dataset.
trained on the CM-fashion dataset and we use the official
inference code provided by the authors. For the manip-
ulation step, we leverage Blended Diffusion [1], Prompt-
to-Prompt [9], and Null-text Inversion [22] as our primary
points of comparison, as this allows us to use the same diffu-
sion model as for DiffCloth.2We employ three widely used
metrics, namely the Fr ´ echet Inception Distance (FID) [11],
the Inception Score (IS) [2] and the CLIPScore [10] to eval-
uate the quality of the generation results. Furthermore, we
conduct an Human Evaluation to evaluate different methods
according to the text-image similarity of their results as well
as their overall generation and manipulation quality. More
specifically, for the garment synthesis task, we requested
that participants assess the generated images based on two
criteria: the extent of garment part leakage and the amount
of attribute confusion. For the garment manipulation task,
we instructed them to evaluate the performance based on
whether a model preserves the consistency of the content in
regions that are not relevant to the manipulation.
4.1. Comparison With State-Of-The-Art Methods
Qualitative Result We provide a qualitative comparison of
DiffCloth’s garment generation ability compared to state-
of-the-art approaches [4, 5, 26, 29]. DiffCloth is able to
synthesize realistic fashion images that comply with the tex-
tual description, while prior approaches generate garment
images that match the overall content of the textual de-
scription, but tends to neglect fine-grained information in
the input text (red box in Fig. 6). In contrast, DiffCloth is
capable of generating semantically bound parts by utiliz-
ing our proposed semantic-bundled cross-attention module.
Specifically, words located within an AP generate separate
attributes, which enhance DiffCloth’s ability to generate se-
mantically coherent images.
For the garment manipulation, the results in Fig. 8
demonstrate the superiority of our proposed approach. We
can locally manipulate an image and maintain the consis-
tency of the content of the manipulation-irrelevant regions
2Note, as Blended Diffusion [1] is not a mask-free approach, we pro-
vide it with a manually drawn mask that reflects the text description.
Input 
Image
Text Query
(simplified)
Prompt-to-
Prompt
DiffClothBlended
Diffusionchange cap sleeve to 
short sleevechange collar to red 
collarchange chest pocket to 
red chest pocketPromptBrown/blue cotton 
ladies' top with floral 
print, keyhole collar and 
cap sleeves.Navy blue jacket with 
straight-point collar , 
front button and belted 
waist. White long sleeve shirt 
with chest pocket.
Null-text 
Inversion
Figure 8. Results of DiffCloth for garment manipulation. The
boxes are used to highlight specific areas that should contain the
elements highlighted in the text.
by using our region consistency strategy.
Quantitative Result We apply FID [11] and IS [2] to
measure the quality of the synthesized images. Further,
we use the CLIPScore [10] to measure the relevance of
the text to a given image. A higher CLIPScore indicates
that the text is more relevant to the image. As reported in
Tab. 1, our proposed DiffCloth outperforms the baselines
Stable Diffusion [29], Composable diffusion [20], Struc-
tureDiffusion [7] and Attend-and-Excite [3] in all cases by a
large margin, obtaining the lowest FID score and the high-
est IS and CLIPScore for the garment synthesis. In addi-
tion, we designed two human evaluation studies to quan-
titatively compare the generation and manipulation results
with the baselines. For generation, we ask participants to
select the generated results that exhibit minimal attribute
confusion and Garment Part Leakage. For the manipulation
task, we evaluate the effectiveness of the method by ask-
ing participants to select the results that best preserves theMethod L1 L2 FID ↓ IS↑ CLIPScore ↑
DiffCloth † ✗ ✗ 9.475 24.59 0.8169
DiffCloth ⋆ ✓ ✗ 9.381 25.45 0.8821
DiffCloth ∗ ✗ ✓ 9.221 26.69 0.8423
DiffCloth ✓ ✓ 9.201 26.95 0.8974
Table 2. Quantitative results of our ablation studies. L1 and L2 de-
note the structural semantic consensus guidance and the semantic-
bundled cross-attention, respectively.
Prompt DiffCloth DiffCloth DiffCloth 
DiffCloth
 Women's black 
 sweater with 
 crew neck and 
 long sleeves, 
 featuring normal 
 cuff and flat hem.
change 
normal cuff to 
butterfly cuff DiffCloth 
 Light-blue cotton 
 denim shirt collar, 
 with front button 
 fastening,  chest 
 pocket long 
 sleeves, buttoned 
 cuffs.Garment Synthesis
Garment ManipulationDiffCloth DiffCloth * DiffCloth 
 A Blue logo print 
 T-shirt  featuring 
 shark print on the 
 chest, crew neck 
 and short sleeves.
 †
 †★
 £
Figure 9. Qualitative results of our ablation studies for garment
synthesis (top) and manipulation (bottom).
area that is irrelevant to the text modification. Aggregating
the scores per model in Fig. 7, we observe that DiffCloth’s
results are preferred for both the garment synthesis or ma-
nipulation tasks. Furthermore, it is also noticeable that the
human-based evaluation indicates a larger difference among
the models compared to the machine evaluation.
4.2. Ablation study
In the garment synthesis task, to validate the effective-
ness of the structural semantic consensus guidance and the
semantic-bundled cross-attention, we design three variants
of our proposed method and evaluate the performance of the
different variants according to their metric scores. We de-
note Stable Diffusion [29] as DiffCloth †, DiffCloth without
structural semantic consensus guidance as DiffCloth ⋆, and
denote DiffCloth without semantic-bundled cross-attention
as DiffCloth ∗. For the garment manipulation task, we con-
sider DiffCloth without region consistency as our ablated
model and denote it as DiffCloth £.
As reported in Tab. 2, incorporating either the struc-
tural semantic consensus guidance or the semantic-bundled
cross-attention (or both) leads to significant improvements
in FID, IS and CLIPScore. These results indicate that our
proposed mechanisms can produce more realistic and se-
mantically accurate results. Additionally, as illustrated in
Fig. 9, the incorporation of structural semantic consensus
guidance (as DiffCloth ⋆) leads to the generation of more ac-
curate parts, whereas the exclusion of the semantic-bundled
cross-attention increases attribute confusion. Finally, re-
moving the region consistency strategy in garment manip-
ulation causes the model to affect parts that should not be
modified, as demonstrated in Fig. 9.
5. Conlusion
In this work, we propose DiffCloth, a diffusion-based
pipeline for garment synthesis and manipulation, which
aligns the structural cross-modal semantics between input
prompts and garment images to address the problem of gar-
ment part leakage and attribute confusion. Moreover, Dif-
fCloth provides a convenient way to manipulate its gener-
ated garments by replacing the Attribute-Phrase in the text
prompt, while ensuring that the content in regions unrelated
to the modification is preserved using a consistency loss.
Experiments on the CM-Fashion demonstrate DiffCloth’s
superior effectiveness compared to existing methods.
Limitation and future work: A limitation of our approach
is the sensitivity to noisy text, which may make accurate
correspondance matching more challenging. To address this
limitation, we aim to explore how the text information can
be leveraged to further strengthen the model’s robustness.
6. Acknowledgement
This work was supported in part by National Key R &D
Program of China under Grant No.2020AAA0109700,
Guangdong Outstanding Youth Fund(Grant
No.2021B1515020061), Shenzhen Science and Tech-
nology Program(Grant No.RCYX20200714114642083),
Shenzhen Fundamental Research Program(Grant
No.JCYJ20190807154211365), Nansha Key RD Pro-
gram under Grant No.2022ZD014 and Sun Yat-sen
University under Grant No.22lgqb38 and 76160-12220011.
We thank MindSpore for the partial support of this work,
which is a new deep learning computing framwork.3.
References
[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended
diffusion for text-driven editing of natural images. In CVPR ,
pages 18187–18197. IEEE, 2022.
[2] Shane Barratt and Rishi Sharma. A note on the inception
score. arXiv preprint arXiv:1801.01973 , 2018.
[3] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and
Daniel Cohen-Or. Attend-and-excite: Attention-based se-
3https://www.mindspore.cnmantic guidance for text-to-image diffusion models. arXiv
preprint arXiv:2301.13826 , 2023.
[4] Katherine Crowson, Stella Biderman, Daniel Kornis,
Dashiell Stander, Eric Hallahan, Louis Castricato, and Ed-
ward Raff. VQGAN-CLIP: open domain image generation
and editing with natural language guidance. In ECCV , vol-
ume 13697 of Lecture Notes in Computer Science , pages 88–
105. Springer, 2022.
[5] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,
Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,
Hongxia Yang, et al. Cogview: Mastering text-to-image gen-
eration via transformers. Advances in Neural Information
Processing Systems , 34:19822–19835, 2021.
[6] Hao Dong, Jingqing Zhang, Douglas McIlwraith, and Yike
Guo. I2t2i: Learning text to image synthesis with textual
data augmentation. In ICIP , pages 2015–2019. IEEE, 2017.
[7] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun
Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang,
and William Yang Wang. Training-free structured diffusion
guidance for compositional text-to-image synthesis. arXiv
preprint arXiv:2212.05032 , 2022.
[8] Bent Fuglede and Flemming Topsøe. Jensen-shannon diver-
gence and hilbert space embedding. In ISIT, page 31. IEEE,
2004.
[9] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control. arXiv preprint
arXiv:2208.01626 , 2022.
[10] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
and Yejin Choi. Clipscore: A reference-free evaluation met-
ric for image captioning. arXiv preprint arXiv:2104.08718 ,
2021.
[11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems ,
30, 2017.
[12] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022.
[13] Seunghoon Hong, Dingdong Yang, Jongwook Choi, and
Honglak Lee. Inferring semantic layout for hierarchical text-
to-image synthesis. In CVPR , pages 7986–7994, 2018.
[14] He Huang, Philip S Yu, and Changhu Wang. An introduction
to image synthesis with generative adversarial nets. arXiv
preprint arXiv:1803.04469 , 2018.
[15] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen
Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:
Text-based real image editing with diffusion models. arXiv
preprint arXiv:2210.09276 , 2022.
[16] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross B.
Girshick. Pointrend: Image segmentation as rendering. In
CVPR , pages 9796–9805. Computer Vision Foundation /
IEEE, 2020.
[17] Harold W. Kuhn. The hungarian method for the assignment
problem. Naval Research Logistics Quarterly , 1955.
[18] Qicheng Lao, Mohammad Havaei, Ahmad Pesaranghader,
Francis Dutil, Lisa Di Jorio, and Thomas Fevens. Dual
adversarial inference for text-to-image synthesis. In ICCV ,
pages 7567–7576, 2019.
[19] Wenbo Li, Pengchuan Zhang, Lei Zhang, Qiuyuan Huang,
Xiaodong He, Siwei Lyu, and Jianfeng Gao. Object-driven
text-to-image synthesis via adversarial training. In CVPR ,
pages 12174–12182, 2019.
[20] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and
Joshua B. Tenenbaum. Compositional visual generation with
composable diffusion models. In ECCV , volume 13677,
pages 423–439. Springer, 2022.
[21] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-
Yan Zhu, and Stefano Ermon. Sdedit: Image synthesis and
editing with stochastic differential equations. arXiv: Com-
puter Vision and Pattern Recognition , 2021.
[22] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and
Daniel Cohen-Or. Null-text inversion for editing real images
using guided diffusion models. In CVPR , 2023.
[23] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,
Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya
Sutskever, and Mark Chen. GLIDE: towards photorealis-
tic image generation and editing with text-guided diffusion
models. In ICML , volume 162 of Proceedings of Machine
Learning Research , pages 16784–16804. PMLR, 2022.
[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In ICML , volume
139 of Proceedings of Machine Learning Research , pages
8748–8763. PMLR, 2021.
[25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research , 21(1):5485–5551, 2020.
[26] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022.
[27] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In ICML , volume 139
ofProceedings of Machine Learning Research , pages 8821–
8831. PMLR, 2021.
[28] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative ad-
versarial text to image synthesis. In ICML , pages 1060–1069.
PMLR, 2016.
[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , pages 10674–
10685. IEEE, 2022.
[30] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. arXiv preprint arXiv:2208.12242 , 2022.
[31] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar SeyedGhasemipour, Burcu Karagol Ayan, S Sara Mahdavi,
Rapha Gontijo Lopes, et al. Photorealistic text-to-image
diffusion models with deep language understanding. arXiv
preprint arXiv:2205.11487 , 2022.
[32] Ming Tao, Hao Tang, Fei Wu, Xiaoyuan Jing, Bing-Kun
Bao, and Changsheng Xu. DF-GAN: A simple and effec-
tive baseline for text-to-image synthesis. In CVPR , pages
16494–16504. IEEE, 2022.
[33] A ¨aron van den Oord, Oriol Vinyals, and Koray
Kavukcuoglu. Neural discrete representation learning.
InNIPS , pages 6306–6315, 2017.
[34] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-
Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah
Laszlo, David J Fleet, Radu Soricut, et al. Imagen editor
and editbench: Advancing and evaluating text-guided image
inpainting. arXiv preprint arXiv:2212.06909 , 2022.
[35] Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu.
Tedigan: Text-guided diverse face image generation and ma-
nipulation. In CVPR , pages 2256–2265. Computer Vision
Foundation / IEEE, 2021.
[36] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,
Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-
grained text to image generation with attentional generative
adversarial networks. In CVPR , pages 1316–1324. Computer
Vision Foundation / IEEE Computer Society, 2018.
[37] Hui Ye, Xiulong Yang, Martin Tak ´ac, Rajshekhar Sunderra-
man, and Shihao Ji. Improving text-to-image synthesis us-
ing contrastive learning. In BMVC , page 154. BMV A Press,
2021.
[38] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-
fei Yang, Burcu Karagol Ayan, et al. Scaling autoregres-
sive models for content-rich text-to-image generation. arXiv
preprint arXiv:2206.10789 , 2022.
[39] Mingkuan Yuan and Yuxin Peng. Text-to-image synthesis
via symmetrical distillation networks. In ACMMM , pages
1407–1415, 2018.
[40] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and
Yinfei Yang. Cross-modal contrastive learning for text-to-
image generation. In CVPR , pages 833–842. Computer Vi-
sion Foundation / IEEE, 2021.
[41] Xujie Zhang, Yu Sha, Michael C. Kampffmeyer, Zhenyu
Xie, Zequn Jie, Chengwen Huang, Jianqing Peng, and Xi-
aodan Liang. ARMANI: part-level garment-text alignment
for unified cross-modal fashion design. In ACMMM , pages
4525–4535. ACMMM, 2022.
[42] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. DM-
GAN: dynamic memory generative adversarial networks for
text-to-image synthesis. In CVPR , pages 5802–5810. Com-
puter Vision Foundation / IEEE, 2019."
2203.10996,D:\Database\arxiv\papers\2203.10996.pdf,"How does the application leverage user interactions and content features to provide personalized recommendations, and what are the potential limitations of this approach?","The application utilizes a combination of collaborative filtering (CF) and content-based filtering (CBF) to personalize recommendations, considering user interactions, content features, and demographic information. However, this approach might struggle to recommend truly novel or unexpected content, potentially limiting exploration and serendipity for users.","4 Jinseok Seol et al.
Fig. 2. The home screen of our AI-driven fashion SNS, iTOO . Users can browse and
interact with the OOTDs of other users just like in ordinary SNS. Most of the contents
are curated by the recommender system.
3.1 Share and Browse OOTD
One of the main purposes of the application is to share a picture of your OOTD.
A user can add a brief description and hashtags when uploading the picture. Im-
mediately, the fashion items that comprise the OOTD are detected and analyzed
automatically by AI. Therefore, users can easily share extensive information by
simply taking a picture and leaving a short description. Moreover, users can
browse through OOTDs posted by other users through curation or exploration.
As shown in Figure 2, the home screen recommends OOTDs that ﬁt the prefer-
ence, style, and body shape of the user.
3.2 Look into OOTD
By examining the OOTD detail view, a user can check out purchasable retail
products that are similar to the comprising items of the OOTD as illustrated in
Figure 1. This function beneﬁts users who want to buy fashion products through
OOTD curation in place, and retail shops can merchandise their product through
viral marketing. In addition, other OOTDs with a similar style are recommended.
Such curations help users to drill down OOTD pools that ﬁt the preference of
the user.
3.3 Get More Recommendations
Besides aforementioned OOTD recommendations, the “style leaders” who often
post trending and decent OOTDs are also recommended to a user as who-to-
follow. To get more accurate curations, a user can provide detailed information
of the fashion persona such as demographic information, body shape, and prefer-
ence style tags. All information including OOTD interactions, proﬁle, following
AI-Driven Fashion SNS with E-Commerce 5
user list is gathered to recommender system and provides a personalized recom-
mendation.
4 AI Components
This section introduces the AI technologies that enable fashion SNS focused
on OOTD images. We used the best performing deep learning models in our
knowledge, and they were ﬁne-tuned to be suited for the fashion domain and
the application. The core models of the application, visual search and OOTD
recommender system, are described in Sections 5 and 6 respectively. Note that
the part of the AI component is also used to construct datasets for training the
other AI components.
4.1 Fashion Object Detector
A fashion image may present a single product, but in many cases, it comes with
a person wearing several fashion garments. Therefore, localizing or detecting
where the fashion items are in the image is a process that must be preceded. We
considered pose estimation and human parsing methods, however, we adopted
a model that predicts region of interest (RoI) for practicality because human
information does not always come in. As a training dataset, we mixed and re-
organized Street2Shop [39], ModaNet [40], and DeepFashion [22,23] datasets.
In detail, we mapped category information to 6 super-categories (top, bottom,
outer, dress, shoes, bag) in order to combine datasets from diﬀerent sources. Due
to the throughput performance issue, YoloV4 [41] were selected as our detector
model. Note that in the application situation, we could assume that each OOTD
image has at least one fashion item. Moreover, top/bottom items and dresses are
mutually exclusive, so considering these properties, we added a post-processing
module and achieved performance gain in terms of recall. Furthermore, we also
included the fashion category classiﬁer module from Section 4.2 to increase the
precision. The predicted RoI is cropped and inferred by the category classiﬁer,
and ﬁltered out if the super-category is diﬀerent from the detector model. By
combining the YoloV4 with the post-processing module, we could build a fast
and accurate fashion object detector.
4.2 Fashion Category Classiﬁer
Classifying the category of a fashion item is another basic element of fashion item
recognition. We constructed an integrated dataset using ModaNet, DeepFashion,
iMaterialist [42] and crawled data from YOOX and Polyvore. To increase cate-
gory coverage, we crawled the data from the top popular 30 online fashion malls,
summing up to 1.7M images in total. Since the crawled data does not have RoI
labels, we used the fashion object detector model from Section 4.1 and ﬁltered
out RoIs with a super-category label parsed from metadata provided by the
malls. It is necessary to reorganize the category hierarchy to integrate multiple
6 Jinseok Seol et al.
datasets, thus we designed a category hierarchy consisting of 6 super-categories
and 32 sub-categories: 6 from outer, 6 from top, 6 from bottom, 2 from dress, 7
from shoes, and 5 from bag. As a classiﬁcation model backbone, EﬃcientNet [43]
was employed under consideration of inference speed and memory usage. We also
leverage the training techniques such as cosine annealing and label smoothing,
etc.
4.3 Fashion Attribute Tagger
We build a fashion attribute tagger model to ﬁnd detailed attributes of fashion
items such as color, style, length. Speciﬁcally, we deﬁned 18 attribute groups
where 11 are categorical, and 7 are multi-label. Since some attribute groups are
only limited according to the sub-category, outputs are ﬁltered out through the
post-processing module. Similar to other AI component models, we merged and
reorganized multiple datasets from diﬀerent sources: DeepFashion, iMaterialist,
Fashion550k [44], and MVC [45]. Adopted CNN backbone and training methods
are the same as fashion category classiﬁer.
5 Fashion Visual Search
To train the visual search model, the same-class labels denoting diﬀerent images
of the same item are necessary. In the fashion domain, the image variance espe-
cially the gap between the product image provided by the shopping malls and the
image uploaded by consumers is large. Therefore, it is important to construct a
model and datasets that can cover the cross-domain image retrieval task. To this
end, we collected multiple datasets and pre-processed them to build an in-house
dataset suitable for the fashion visual search model in the application. Note that
since it is common to learn through negative sampling rather than learning the
distribution of all items, dataset quality is sensitive to false-positives rather than
false-negatives.
5.1 Dataset Construction
Collecting Data Academic datasets ( e.g., DeepFashion) are often not complete
and cannot be directly applied to real-world applications due to the limitation
of category coverage. To ﬁll this gap, we selected and crawled the top popular
30 online fashion malls and collected a total of 0.3M items with 1.3M images,
including consumer photo review data. We conducted a small experiment to
conﬁrm that adding more data aﬀects search performance in terms of category
coverage.
Preprocessing Similar to the case of the fashion category classiﬁer from Section
4.2, crawled data cannot be used for training without preprocessing. We used
the fashion object detector model from Section 4.1 and acquired RoI crops for
AI-Driven Fashion SNS with E-Commerce 7
the localization, and ﬁltered out the crops that do not match the super-category
information parsed from target malls.
Meanwhile, when crawling the data from online malls, the abundant im-
age data is often located in the “descriptive image”, which consists of multiple
photos of a fashion item, description texts, and even irrelevant images like adver-
tisements. To gather meaningful data, we ﬁrst separated the descriptive image
with the connected components algorithm, then removed duplicate images using
perceptual hashing [46]. The detector model and post-process procedures are ap-
plied then after. Additionally, to reduce false-positive errors, we use the fashion
category classiﬁer and select images with the sub-category of the majority.
5.2 Color Separation
An easy-to-miss aspect when building a dataset for a fashion visual search model
is to separate fashion items that have multiple color variants. Many online fashion
malls, including DeepFashion dataset, treat item images that diﬀer only in color
as the same item. However, this scheme can lead CNN to neglect the color
information of the input image, and a “shortcut” by color information cannot
be used. In our settings, it is beneﬁcial to use this shortcut because the precision
of the search result is more important than the recall, and by conducting a
benchmark experiment, we conﬁrmed that separating the color variants into
diﬀerent items helps to improve precision. In the case of the DeepFashion dataset,
the color information labels are provided with ﬁne granularity, so we re-adjusted
the same-class label using the color tag of our fashion attribute tagger from
Section 4.3. Again, when it comes to precision, only the false-positives of the
dataset matter so the inaccuracy of the attribute tagging model does not aﬀect
critically.
5.3 Model
Most of the recent state-of-the-art methods on image retrieval tasks are based
on metric learning. When the model is trained, we can obtain a representation
vector from the input image by feeding it into the model, and similar items can
be retrieved through cosine similarity. We considered basic metric learning [20],
AP learning [47] and proxy-based methods [25]. However, methods that require
item embeddings are often diﬃcult to deal with numerous or variable item pool.
Moreover, we use an under/over-sampling scheme to balance the datasets from
diﬀerent sources, which means, the whole item pool is changed on every epoch.
Therefore, for the ﬂexibility of the training, we adopted simple N-pair contrastive
learning [48]. Although the basic metric learning cannot match the state-of-the-
art performance, it still serves as a decent baseline with advantages from other
aspects.
In concrete, to train the N-pair loss, we sample one positive (same-item)
image per input image and gather Nnegative image samples from the training
batch. The metric learning is performed using normalized-temperature cross-
entropy (NT-Xent) loss [49]. The rest of the training detail including backbone
8 Jinseok Seol et al.
CNN is similar to the category classiﬁer from Section 4.2. The dimension of the
representation vector was set to 128 for memory eﬃciency, and although a larger
dimension was under consideration, the performance improvement compared
to memory usage was not signiﬁcant. The under/over-sampling of datasets are
empirically adjusted considering the image types, characteristics, and category
distribution of each dataset.
Table 1. Performance comparison on DeepFashion In-shop dataset, using top- kaccu-
racy. Our N-pair based model may not be state-of-the-art, but it can be easily scaled
out to millions of items. The suggesting color separation training scheme (last row)
shows that even with simple label modiﬁcation, the top-1 accuracy can be improved
by a large margin.
Model k=1k=5k=10k=20
Liu et al. 2016b 53.0 - 73.0 76.0
Park et al. 2019 - 82.6 - 90.9
Cakir et al. 2019 90.9 - 97.7 98.5
Kim et al. 2020 92.6 - 98.3 98.9
Baseline 77.9 91.6 94.5 96.5
Color Separation 83.4 92.4 94.0 95.6
5.4 Experimental Results
Performance on Benchmark Dataset To show the precision gain on the
color separation scheme, we experimented on DeepFashion In-shop dataset, which
has 52,712 images with 7,982 items. Note that this dataset provides RoI crop
data, so we use the box coordinates with 20-pixel margin, then resized it into
256×256 images. Table 1 shows the results in top- kaccuracy, which checks
whether the positive image is within the top- kitems retrieved. Although the N-
pair baseline model cannot reach state-of-the-art performance, it is still a decent
baseline compared to older and complex models. On the other hand, we can see
the signiﬁcant performance gain in top-1 when the color separation scheme is
applied.
Dataset Inﬂuence To see the inﬂuence of the dataset constitutions, we trained
aN-pair model with three diﬀerent dataset settings: using only DeepFashion
dataset, adding more consumer photo review data from crawled online fashion
malls, and adding shoes and bags which DeepFashion does not have. The exam-
ples of visual search results are shown in Figure 3. In the results from the ﬁrst
query image, all three settings produced similar results. In the second case, since
the query image involves a partially human shape, the setting with an additional
consumer review image shows more robustness in terms of cross-domain image
AI-Driven Fashion SNS with E-Commerce 9
Fig. 3. Examples of results from visual search models, trained in diﬀerent dataset
compositions. Results from the second query show that by adding more review data,
robustness to cross-domain retrieval can be improved. The third query shows that the
model cannot accurately deal with unseen categories. We can conclude that in the
visual search model, dataset composition is critical as model architecture.
Fig. 4. Inference pipeline for OOTD. After the fashion object detector model ﬁnds
region of interest (RoI), the fashion category classiﬁer and the fashion attribute tagger
are applied to acquire more detailed information for each cropped RoI. After that,
the visual search model extracts the representation vectors and stores them to the
corresponding vector index.
retrieval. In the ﬁnal case, where the query image represents shoes, it can be seen
that settings without shoes and bags could not maintain the sub-category of the
query image. As a result, we argue that constructing a well-tempered dataset is
just as important as selecting the model.
5.5 Inference Pipeline
In the application, the visual search model has to be combined with other AI
components. As shown in Figure 4, when a user uploads an OOTD image, the
fashion object detector ﬁrst ﬁnds RoIs Then, the fashion category classiﬁer and
the fashion attribute tagger are applied to each cropped RoI in parallel. Finally,
the visual search model extracts representation vectors and store them into the
vector index, corresponding to the super-category.
10 Jinseok Seol et al.
6 Recommender System
With the recommender system, users receive personalized OOTD recommenda-
tions, similar styled OOTDs for each OOTD, and style leaders to follow.
6.1 Personalized OOTD Curation
On the ﬁrst screen of the service, users can get the recommendation of OOTDs
that suits their preferences. The recommendation basically leverage CF-CBF,
and the ﬁnal recommendation list is generated by mixing up with the weekly
best products and best products by demographic-based user segment. In the
case of CF-CBF, both user-based and item-based CF are used.
Style Vector A fashion item vector viof itemIiconsists of a concatenation
of representations of the category classiﬁer, the attribute tagger, and the visual
search model: for an item image xi,vi= concat(fC(xi),fA(xi),fS(xi)).For each
fashion item, the item style vector ˜viis obtained by subtracting the average of
item vectors of the sub-category which the given item belongs: for c(i) a sub-
category index of an item Ii,Sk={j|c(j) =k},¯vc(i)=|Sc(i)|−1∑
j∈Sc(i)vj,˜vi=
vi−¯vc(i).The OOTD style vector is deﬁned as the average of the item style
vector of the consisting items: ot=|o∗
t|−1∑
i∈o∗
t˜vi, whereo∗
tis a set of indices
of comprising items in the OOTD ot.
Semantic OOTD Similarity Given two OOTDs, we deﬁne semantic OOTD
similarity as a weighted sum of the cosine similarity between the OOTD style
vectors of each OOTD and the Jaccard similarity of the hashtags that are de-
pendent on the two OOTDs: for given OOTDs ot1andot2,
simo(ot1,ot2) =λo(ot1·ot2
∥ot1∥∥ot2∥)
+ (1−λo)|ao(t1)∪ao(t2)|
|ao(t1)∩ao(t2)|, (1)
whereao(t) denotes a set of hashtags of an OOTD ot. Note that we use this
similarity to make similar styled OOTD recommendations.
Semantic User Similarity Similar to semantic OOTD similarity, we deﬁne
a user style vector by aggregating style vectors of HOOTDs that the user has
recently viewed or liked. We use a weighted average to reﬂect the preferences of
recent interaction more strongly: for user un,
un=(H∑
mwm)−1H∑
m=1wmotm, (2)
wherewm=(H−m+1
H)α, andu∗
n={t1,t2,...,tH}is a set of indices of the
user’s recent OOTD views or likes, and 0 < α < 1 is a recency decay hyper-
parameter. Cosine similarity between two user style vectors and the Jaccard
AI-Driven Fashion SNS with E-Commerce 11
similarity between the preference tags in the user proﬁles are used to measure
the semantic user similarity: for user un1andun2,
simu(un1,un2) =λu(un1·un2
∥un1∥∥un2∥)
+ (1−λu)|au(un1)∪au(un2)|
|au(un1)∩au(un2)|,(3)
CF-CBF for OOTD We use Collaborative Filtering (CF) as the basis for
our recommendation algorithm. We ﬁrst calculate the TF-IDF values from user-
OOTD interactions. The value of TF-IDF is decayed to reﬂect the recency using
time decay coeﬃcient βd, wheredis days passed since the interaction has oc-
curred, and 0 <β < 1 is a decay rate hyper-parameter. We use both item-based
and user-based CF and combine it with other recommendation results. In the
case of item-based, the recommended OOTD list is obtained through similar-
ity of the OOTDs that the user has recently viewed. In the case of user-based,
the recommendation list is constructed by joining users obtained through user
similarity and the OOTD list that the user has recently viewed. In both cases,
CF-CBF can be implemented by considering TF-IDF as a CF part and semantic
similarity as a Content-Based Filtering (CBF) part. Let rnbe a TF-IDF vector
of a userun, treating user as a document when calculating the TF-IDF values.
Then, the ﬁnal similarity between two users un1,un2can be calculated as follows:
simCF-CBF (un1,un2) =λCF(rn1·rn2
∥rn1∥∥rn2∥+h)
+(1−λCF) sim u(un1,un2),(4)
wherehis a shrinkage term for the case with relatively small interactions [50].
Similar methodology is applied to item-based CF-CBF.
OOTD Curation With user-based and item-based CF-CBF, we mix up the
weekly best OOTD list with the best OOTD list by segment based on demo-
graphic information. Note that since a decay term is used, a result closer to
global taste is provided rather than personalized content to those who have not
used the application for a long time. This reﬂects the characteristics of the fash-
ion domain where trends change over time and provides exploration oppurtunity
and serendipity.
6.2 Style Leader Suggestion
A style leader means a person who can be subscribed, and a user can receive
better OOTD curation when they follow the style leader. For style leader recom-
mendation, both the latent method and the graph based method are used. In the
case of the latent method, recommendation candidates are determined using the
modiﬁed semantic user similarity. Here, we use cosine similarity between the user
style vectors of the recent view/like OOTD history of the follower and the user
style vectors of the recent upload OOTD history of the followee candidates. On
the other hand, when using the graph-based algorithm, recommendation candi-
dates are obtained by performing a random walk twice in the following/follower"
2311.12091,D:\Database\arxiv\papers\2311.12091.pdf,"While the paper focuses on improving image classification and object detection, what other computer vision tasks could benefit from the proposed attention mechanism and why?",The paper suggests that dense vision tasks like semantic segmentation and stereo matching could benefit from the proposed attention mechanism due to its ability to provide dense attention and consider the feature context holistically.,"MethodImageNet1K
Parameters (M) FLOPs (G) Top-1 (%) Top-5 (%)
ResNet-18 [12] 11.69 1.82 69.76 89.08
+ SENet [15] 11.78 1.82 70.59 89.78
+ BAM [27] 11.71 1.83 71.12 89.99
+ CBAM [34] 11.78 1.82 70.73 89.91
+ Triplet Attention [26] 11.69 1.83 71.09 89.99
+ EMCA [1] 11.19 1.70 71.00 90.00
+ DAS (ours) 11.82 1.86 72.03 90.70
ResNet-50 [12] 25.56 4.12 76.13 92.86
+ SENet [15] 28.07 4.13 76.71 93.38
+ BAM [27] 25.92 4.21 75.98 92.82
+ CBAM [34] 28.09 4.13 77.34 93.69
+ GSoP-Net [10] 28.29 6.41 77.68 93.98
+ A2-Nets [4] 33.00 6.50 77.00 93.50
+ GCNet [2] 28.10 4.13 77.70 93.66
+ GALA [23] 29.40 - 77.27 93.65
+ ABN [9] 43.59 7.66 76.90 -
+ SRM [21] 25.62 4.12 77.13 93.51
+ Triplet Attention [26] 25.56 4.17 77.48 93.68
+ EMCA [1] 25.04 3.83 77.33 93.52
+ ASR [37] 26.00 - 76.87 -
+ DAS (ours) 26.90 4.39 78.04 94.00
ResNet-101 [12] 44.46 7.85 77.35 93.56
+ SENet [15] 49.29 7.86 77.62 93.93
+ BAM [27] 44.91 7.93 77.56 93.71
+ CBAM [34] 49.33 7.86 78.49 94.31
+ SRM [21] 44.68 7.85 78.47 94.20
+ Triplet Attention [26] 44.56 7.95 78.03 93.85
+ ASR [37] 45.00 - 78.18 -
+ DAS (ours) 45.89 8.12 78.62 94.43
MobileNetV2 [29] 3.51 0.32 71.88 90.29
+ SENet [15] 3.53 0.32 72.42 90.67
+ CBAM [34] 3.54 0.32 69.33 89.33
+ Triplet Attention [26] 3.51 0.32 72.62 90.77
+ DAS (ours) 3.57 0.35 72.79 90.87
Table 1. Evaluation of image classification models on ImageNet1k dataset, comparing top-1, top-5 accuracies, and computational efficiency.
DAS outperforms ResNet-18, ResNet-50, ResNet-101, MobileNetV2, and various other attention-based models, achieving the best accuracies,
with only a small increase in parameters and FLOPs.
(Eq. 3). This convolution operation changes the number of
channels from α×cto the original input c.
A=σ(LayerNorm (deform (Xc))) (3)
The output from Eq. 3 represents the attention gate. This
gate controls the flow of information from the feature maps,
with each element in the gate tensor having values between
0 and 1. These values determine which parts of the feature
maps are emphasized or filtered out. Lastly, to incorporate
the DAS attention mechanism into the CNN model, we per-form a pointwise multiplication between the original input
tensor and the attention tensor obtained in the previous step.
Xout=X⊙A (4)
The result of the multiplication in Eq. 4 is the input for
the next layer of the CNN model, seamlessly integrating
the attention mechanism, without any need to change the
backbone architecture.
4
BackboneFaster R-CNN on MS COCO (%)
Parameters (M) AP AP 50 AP75 APSAPM APL
ResNet-50 [12] 41.7 36.4 58.4 39.1 21.5 40.0 46.6
ResNet-101 [12] 60.6 38.5 60.3 41.6 22.3 43.0 49.8
SENet-50 [15] 44.2 37.7 60.1 40.9 22.9 41.9 48.2
CBAM-50 [34] 44.2 39.3 60.8 42.8 24.1 43.0 49.8
Triplet Attention-50 [26] 41.7 39.3 60.8 42.7 23.4 42.8 50.3
DAS-50 (ours) 43.0 39.7 60.9 43.2 22.8 43.9 51.9
Table 2. Model performance comparison on MS COCO validation using Faster R-CNN for object detection. DAS surpasses other attention
models and ResNet-101.
Comparison of DAS attention and Deformable Attention
[39]Previous deformable attention mechanism, designed
primarily for transformers, [ 39] employs a fully connected
network (FC) to compute offsets, which may not be optimal
for CNNs. In contrast, DAS attention utilizes a 3×3kernel,
better suited for CNNs. While [ 39] applies deformable atten-
tion exclusively to query features, DAS attention considers
image features holistically. Our attention mechanism oper-
ates as a separate module without necessitating changes to
the main architecture, enhancing its plug-and-play capability
over the transformer-based deformable attention approaches.
4. Experiments
Training Setup For image classification, we used CI-
FAR100 [ 20], Stanford Dogs [ 18], and ImageNet1k [ 6]
datasets, and for object detection, MS COCO [ 22]. We
employed ResNet [ 12] and MobileNetV2 [ 29] architectures
as per [26].
For ImageNet experiments, we adopted settings from
[26]: ResNet training with batch size 256, initial LR 0.1,
and weight decay 1e-4 for 100 epochs. LR scaled at 30th,
60th, and 90thepochs by a factor of 0.1. MobileNetV2: batch
size 96, initial LR 0.045, weight decay 4e-5, LR scaled by
0.98epoch.
For CIFAR100 and Stanford Dogs datasets, we compared
with Triplet Attention [ 26] and Vanilla Resnet. We con-
ducted a hyperparameter search for ResNet-18, and used the
same setup for all of the baselines: 300 epochs, batch size
128, initial LR 0.1, weight decay 5e-4, LR decay at 70th,
130th, 200th, 260thby a scale factor of 0.2. For Stanford
Dogs: batch size 32, LR 0.1, weight decay 1e-4, CosineAn-
nealing LR scheduler, random flip and crop for image pre-
processing.
For object detection, we used Faster R-CNN on MS
COCO with MMdetection toolbox [ 3], with batch size 16,
initial LR 0.02, weight decay 0.0001, and ImageNet-1k pre-
trained backbone. We mitigated noise by initial training of
the backbone, training both the backbone and the rest of
the model for a few epochs. The weights obtained fromthis initial training served as an initialization point for our
subsequent training process. We consistently employed the
SGD optimizer.
4.1. Image Classification
Tab. 3 demonstrates that the addition of Triplet Attention
[26] slightly improves the accuracy of ResNet-18 CIFAR100
(0.3%) but decreases the accuracy by 1.36% on the Stanford
Dogs dataset. However, DAS improves the accuracy of
ResNet-18 by 0.79% and 4.91% on CIFAR100 and Stanford
Dogs, respectively. Similar to ResNet-18, the addition of
Triplet attention [ 26] to ResNet-50 has a negative impact on
the backbone model for Stanford Dogs, while DAS enhances
the backbone model by 2.8% and 4.47% on CIFAR100 and
Stanford Dogs, respectively, showing DAS’s performance
consistency across small and large models.
Interestingly, we observed that our proposed DAS-18
method outperformed not only the base ResNet-18 model
but also deeper architectures on CIFAR100 and Stanford
Dogs datasets, including ResNet-50, while using 2.26G less
FLOPs. This makes DAS-18 a compelling option for mobile
applications.
Results for ImageNet classification are presented in Tab. 1.
When the DAS attention gate is applied to ResNet-18, it
demonstrates remarkable improvements in classification ac-
curacy. The DAS results in a top-1 accuracy of 72.03% and
a top-5 accuracy of 90.70%. This outperforms other existing
methods such as SENet [ 15], BAM [ 27], CBAM [ 34], Triplet
Attention [ 26], and EMCA [ 1] showcasing the efficacy of
DAS in enhancing model performance.
DAS with a depth of 50 achieves a top-1 accuracy of
78.04% and a top-5 accuracy of 94.00%. It achieves the best
performance while using 32% less FLOPs and 1.39M less pa-
rameters compared to the second best performer (GSoP-Net
[10]). ResNet-50 + DAS attention also outperforms ResNet-
101 in terms of top-1 accuracy, with 0.69% more accuracy
at∼60% of FLOPs and number of parameters. ResNet-101
+ DAS attention achieves the best top-1 accuracy (78.62%)
compared to other attention modules with less parameters
5
MethodCIFAR100 Stanford Dogs
(Acc %) (Acc %)
ResNet-18 [12] 78.25 61.50
+ Triplet Attention [26] 78.55 60.14
+ DAS (ours) 79.04 66.41
ResNet-50 [12] 77.74 62.58
+ Triplet Attention [26] 79.22 60.55
+ DAS (ours) 80.54 67.05
Table 3. Performance (%) on CIFAR100 and Stanford Dogs
datasets, with our method DAS, achieving the highest accuracy.
compared to SENet [15] and CBAM [34].
On the lightweight MobileNetV2, DAS maintains its ef-
fectiveness. It achieves a top-1 accuracy of 72.79% and a top-
5 accuracy of 90.87%, outperforming SENet [ 15], CBAM
[34], and Triplet Attention [ 26], while being computationally
efficient with a low FLOP count of 0.35G.
4.2. Object Detection
Tab. 2 shows results from our object detection experiments
using the Faster R-CNN model on the challenging MS
COCO dataset. The metrics used for evaluation include
average precision (AP), AP at different intersections over
union (IoU) thresholds (AP 50, AP 75), and class-specific AP
for small (AP S), medium (AP M), and large (AP L) objects.
The choice of backbone architecture significantly impacts
object detection performance. In our evaluation, ResNet-50,
ResNet-101, SENet-50, CBAM-50, and Triplet Attention-50
serve as strong baselines. Our DAS-50 model surpasses all
other backbones in terms of AP, AP 50, AP 75, AP M, and
APLscores, with a lower number of parameters compared
to ResNet-101, SENet-50 and CBAM-50.
4.3. Design Evolution and Ablation Studies
Before finalizing the design of DAS, we explored two pixel-
wise attention concepts. These are depicted in Fig. 2 (a) and
(b), with corresponding results on the Stanford Dogs dataset
in Table 4.
(a): We concatenated the input with a GridSample of
itself, followed by a convolutional layer that integrated both
the input and information from distant pixels. While this ap-
proach showed potential, it achieved an accuracy of 65.00%
on the Stanford Dogs dataset. GridSample is a differentiable
PyTorch feature that interpolates neighboring pixels spatially
based on a given grid tensor.
(b): We extended the initial concept by using compressed
inputs and GridSample outputs to compute weights for sup-
pressing extraneous information in the features. This re-
finement yielded a modest improvement over the first idea,
achieving an accuracy of 65.21% while reducing computa-Methods in Fig.2 Stanford Dogs
ResNet-18 + Parameters FLOPs Top-1 (%)
a 12.766M 1.99G 65.00
b 11.801M 1.846G 65.21
c (ours) 11.817M 1.856G 66.410
d 11.923M 1.915G 65.338
e 12.048M 1.879G 65.291
f 11.843M 1.849G 66.107
g 11.925M 1.917G 60.851
h 11.819M 1.858G 61.375
Table 4. Ablation study of DAS components in Fig. 2 and explained
in Sec. 4.3: (a, b) Design evolution analyses, (d-h) Component
analyses of the proposed method (c). Evaluation on the Stanford
Dogs dataset reveals the positive influence of each component on
model performance and efficiency.
First Norm. Layer Second Norm. LayerTop-1(%)BN FN IN LN BN FN IN LN
✓ ✓ 65.24
✓ ✓ 64.64
✓ ✓ 66.27
✓ ✓ 64.66
✓ ✓ 65.78
✓ ✓ 65.41
✓ ✓ 65.15
✓ ✓ 65.94
✓ ✓ 65.25
✓ 66.21
✓ ✓ 66.41
Table 5. Ablation study of normalization layers in our attention
gate. We evaluated BatchNorm (BN), Simple Feature Norm (FN),
IntsanceNorm (IN), and LayerNorm (LN) in ResNet-18 + our DAS
attention on Stanford Dogs. Our method (last line) achieved the
best accuracy.
tional overhead.
To evaluate our design decisions (c)we conducted various
ablation studies:
(d)Removing the initial part and relying solely on de-
formable convolution led to reduced accuracy (65.338%),
emphasizing the importance of the first convolution layer.
(e)Removing deformable convolution while keeping the
initial part increased computation and decreased accuracy
(65.291%), indicating the need for multiple layers for precise
attention modeling.
(f)Replacing deformable convolution with depthwise
separable convolutions improved accuracy (66.107%), but
it was still outperformed by our method, highlighting the
advantage of deformable convolution in focusing attention
on relevant information.
6
(a)
 (b)
 (c): proposed method
(d)
 (e)
 (f)
 (g)
 (h)
Figure 2. (a and b): Ablation studies on ideas in Sec. 4.3: (a) Con-
catenating a feature tensor with deformed grids, followed by convo-
lution for global dependencies. (b) Similar to (a) with compressed
channels for reduced FLOPs and parameters. (c) Our method:
channel compression and deformable convolution for attention to
relevant information. (d) to (h): Ablation on each component of (c)
explained in Sec. 4.3. Table 4 demonstrates (c)’s superior accuracy
and computational efficiency.
(g)Excluding attention modules and only using de-
formable convolution drastically decreased accuracy, em-
phasizing the significance of attention behavior.
(h)Similarly, excluding attention modules and using ad-
ditional layers showed low accuracy, emphasizing the prefer-
ence for using these layers as an attention module.
Our attention method (c)outperformed all configurations,
achieving the best accuracy (66.410%). This underscores
the effectiveness of our context-aware attention mechanism
in focusing attention on relevant features even outside of
kernel boundaries and enhancing model performance. Table
5 demonstrates the effect of different normalization layers
on the attention module.
In summary, our experiments demonstrate our method’s
superiority in accuracy and computational efficiency com-
pared to other ideas and configurations, establishing it as a
valuable addition to pixel-wise attention modeling.
We examined the impact of varying the parameter αfrom
0.01 to 1. Increasing αincreases both FLOPs and parameters.1.8 1.9 2.065666768
α= 0.01
α= 0.1
α= 0.2
α= 0.5
α= 1.0
FLOPs (G)Top-1 (%)
Figure 3. Ablation study on compression coefficient α: ResNet18
+ our attention on Stanford Dogs indicates low sensitivity to this
added hyperparameter when α > 0.1. Default αused in our
implementation for this paper is 0.2.
Our findings in Fig. 3 indicate that alpha values greater than
0.1 yield favorable results. Typically, there exists a trade-off
between FLOPs and accuracy. Consequently, we opted for
α= 0.2in the majority of our investigations.
We examined the impact of the number of attention layers.
Adding attention layers after all skip connections slightly
enhances performance but significantly increases FLOPs
and parameters, especially in larger models. Empirically,
our observation is that four attention gate layers strike a
good balance between computation cost and accuracy. We
also conducted studies on attention gate locations, ultimately
choosing an attention model that is simple, efficient, and
accurate for both small and large datasets.
4.4. Salient Feature Detection Effectiveness
The objective of applying an attention mechanism in any
task is to pay increased attention to relevant features, while
at the same time paying less or no attention to irrelevant
features. We believe that the performance improvements
presented in the earlier sections are primarily due to the
effectiveness of our gate in focusing and increasing attention
to salient features in the image. In this section, we visualize
the extent to which our attention mechanism meets the above
objective. For this, we use gradCAM [ 30], a function that
produces a heatmap showing which parts of an input image
were important for a classification decision made by the
trained network. The color scheme used in the heatmap is
red to blue with blue representing lower importance.
Figure 4 shows the heatmaps after block 3 and block 4
for a number of samples for ResNet-50 with and without the
attention gate. These cases clearly show that our attention
7
gate is better at focusing attention on relevant features in the
image.
We have applied our attention gate at the end of each
block in ResNet, so that the network starts focusing attention
on relevant features in the early stages as well. Observing
the change in heatmaps in Fig. 4 from block 3 to block 4,
we can see that attention does indeed shift towards relevant
features when using DAS attention.
Lastly, we define a simple metric for the effectiveness of
a trained network in focusing on relevant features. We base
it on weights output by gradCAM. Since we observed that
gradCAM weights are compressed within the range 0 to 1,
we use antilog scaling of gradCAM weights in the follow-
ing. Let Rdenotes the region(s) containing task-relevant
features ideally identified by a human, but could also be
approximated using a visual grounding tool. Bdenotes the
bounding box within the image which contains R, and is
such that weights outside Bare low (below a threshold), that
is features deemed unimportant by the network are outside
B.Wrdenotes the average weight of features in R.Wnis
the average weight of features in B−R. Salient feature
detection score is,
sfd=Wr/(Wr+Wn) (5)
Wr/Wnprovides a measure of the strength of attention paid
to relevant features in the image. The higher its value, the
more attention is paid to relevant features. On the other hand,
a high value for Wn/Wrimplies that attention is being given
to irrelevant features. sfdwill vary from 0 to 1. A score
closer to 1 implies focused attention to relevant features and a
score closer to 0 implies completely misplaced attention. In-
between values indicate that attention is spread over relevant
and irrelevant features. We use the following procedure for
detecting R and B. We first use Grounding-DINO+SAM
[19,25] to identify the object to be classified in an image. To
avoid manual checking, we accept the possible error in this
operation. This gives us the region R of relevant features.
Outside of R we select the region which as per gradCAM
contains salient pixels. This along with R gives us B. The last
column in Fig. 4 has sfdvalues computed for ResNet-50
and DAS. We also computed sfdvalues for a random sample
of 100 images from ImageNet. The sfdfor ResNet and DAS
are 0.59 and 0.72, respectively, illustrating the strength of
our method in achieving targeted feature attention.
5. Conclusion, Limitations and Extensions
In this paper, we presented the DAS attention gate, a new
self-attention mechanism for CNNs. DAS does not make
use of transformers. Compared to earlier methods for atten-
tion within CNNs, DAS provides dense attention and looks
holistically at the feature context. DAS is very simple – it
combines depthwise separable convolutions (for efficientImage Method GradCam sfd
ResNet
 0.47
DAS
 0.68
ResNet
 0.15
DAS
 0.41
ResNet
 0.87
DAS
 0.99
ResNet
 0.38
DAS
 0.84
ResNet
 0.50
DAS
 0.64
ResNet
 0.53
DAS
 0.91
Figure 4. Analyzing GradCam activations in ResNet and DAS
in Blocks 3 (left), and 4 (right), showcasing the superior saliency
concentration of our method. DAS achieves a higher sfdmetric (5),
emphasizing its capability for attending to salient image features.
representation of the global context) and deformable con-
volutions (for increasing focus on pertinent image regions).
Implementation results indeed show that DAS, though sim-
ple, enables focused attention to task-relevant features in
an image. In our view, its simplicity is its power, as (i) it
can be introduced between any two layers of a CNN de-
signed for any visual task, (ii) does not require any change
to the rest of the network, (iii) provides dense attention,
(iv) provides attention in a holistic fashion, not separating
channel or spatial attention, (v) has just a single additional
hyper-parameter, that is easy to tune, (vi) adds only a small
amount of computation overhead, (vii) is O(n)as opposed
to Transformer-style self-attention’s O(n2), and (viii) yields,
as of today, the best results as compared to all other earlier
proposed CNN attention methods.
One limitation is that the computation overhead can in-
crease significantly when the network has large depth fea-
tures. Hence the value of αhas to be chosen carefully. Too
small a value will result in loss of contextual information
and a large value will increase the amount of computation.
8
While we have demonstrated DAS’s performance for Im-
age Classification and Object Detection, in the future we
want to use it for dense vision tasks such as semantic seg-
mentation and stereo matching where DAS’s dense attention
capability could offer significant advantages.
References
[1]Eslam Mohamed Bakr, Ahmad El-Sallab, and Mohsen Rash-
wan. Emca: Efficient multiscale channel attention module.
IEEE Access , 10:103447–103461, 2022. 4, 5
[2]Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu.
Gcnet: Non-local networks meet squeeze-excitation networks
and beyond. In Proceedings of the IEEE/CVF international
conference on computer vision workshops , pages 0–0, 2019.
2, 4
[3]Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu
Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,
Jiarui Xu, et al. Mmdetection: Open mmlab detection toolbox
and benchmark. arXiv preprint arXiv:1906.07155 , 2019. 5
[4]Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng
Yan, and Jiashi Feng. Aˆ 2-nets: Double attention networks.
Advances in neural information processing systems , 31, 2018.
2, 4
[5]Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang,
Han Hu, and Yichen Wei. Deformable convolutional net-
works. In Proceedings of the IEEE international conference
on computer vision , pages 764–773, 2017. 3
[6]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li
Fei-Fei. Imagenet: A large-scale hierarchical image database.
In2009 IEEE conference on computer vision and pattern
recognition , pages 248–255. Ieee, 2009. 5
[7]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 1
[8]Jun Fu, Jing Liu, Jie Jiang, Yong Li, Yongjun Bao, and Han-
qing Lu. Scene segmentation with dual relation-aware atten-
tion network. IEEE Transactions on Neural Networks and
Learning Systems , 32(6):2547–2560, 2020. 3
[9]Hiroshi Fukui, Tsubasa Hirakawa, Takayoshi Yamashita, and
Hironobu Fujiyoshi. Attention branch network: Learning of
attention mechanism for visual explanation. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 10705–10714, 2019. 4
[10] Zilin Gao, Jiangtao Xie, Qilong Wang, and Peihua Li. Global
second-order pooling convolutional networks. In Proceedings
of the IEEE/CVF Conference on computer vision and pattern
recognition , pages 3024–3033, 2019. 2, 4, 5
[11] Meng-Hao Guo, Tian-Xing Xu, Jiang-Jiang Liu, Zheng-Ning
Liu, Peng-Tao Jiang, Tai-Jiang Mu, Song-Hai Zhang, Ralph R
Martin, Ming-Ming Cheng, and Shi-Min Hu. Attention mech-
anisms in computer vision: A survey. Computational visual
media , 8(3):331–368, 2022. 1, 3[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 2, 3, 4, 5, 6
[13] Qibin Hou, Li Zhang, Ming-Ming Cheng, and Jiashi Feng.
Strip pooling: Rethinking spatial pooling for scene parsing.
InProceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pages 4003–4012, 2020. 3
[14] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea
Vedaldi. Gather-excite: Exploiting feature context in con-
volutional neural networks. Advances in neural information
processing systems , 31, 2018. 2
[15] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation
networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 7132–7141, 2018. 1, 2,
4, 5, 6
[16] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang,
Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross attention
for semantic segmentation. In Proceedings of the IEEE/CVF
international conference on computer vision , pages 603–612,
2019. 2
[17] Mingxin Jin, Huifang Li, and Zhaoqiang Xia. Hybrid atten-
tion network and center-guided non-maximum suppression
for occluded face detection. Multimedia Tools and Applica-
tions , 82(10):15143–15170, 2023. 3
[18] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao,
and Fei-Fei Li. Novel dataset for fine-grained image cat-
egorization: Stanford dogs. In Proc. CVPR workshop on
fine-grained visual categorization (FGVC) . Citeseer, 2011. 5
[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ´ar, and Ross
Girshick. Segment anything, 2023. 8
[20] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 5
[21] HyunJae Lee, Hyo-Eun Kim, and Hyeonseob Nam. Srm:
A style-based recalibration module for convolutional neural
networks. In Proceedings of the IEEE/CVF International
conference on computer vision , pages 1854–1862, 2019. 4
[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings, Part
V 13, pages 740–755. Springer, 2014. 5
[23] Drew Linsley, Dan Shiebler, Sven Eberhardt, and Thomas
Serre. Learning what and where to attend. arXiv preprint
arXiv:1805.08819 , 2018. 3, 4
[24] Lahav Lipson, Zachary Teed, and Jia Deng. Raft-stereo:
Multilevel recurrent field transforms for stereo matching. In
2021 International Conference on 3D Vision (3DV) , pages
218–227. IEEE, 2021. 2
[25] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun
Zhu, and Lei Zhang. Grounding dino: Marrying dino with
grounded pre-training for open-set object detection, 2023. 8
9
[26] Diganta Misra, Trikay Nalamada, Ajay Uppili Arasanipalai,
and Qibin Hou. Rotate to attend: Convolutional triplet at-
tention module. In Proceedings of the IEEE/CVF winter
conference on applications of computer vision , pages 3139–
3148, 2021. 1, 3, 4, 5, 6
[27] Jongchan Park, Sanghyun Woo, Joon-Young Lee, and In So
Kweon. Bam: Bottleneck attention module. arXiv preprint
arXiv:1807.06514 , 2018. 1, 3, 4, 5
[28] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y . Fu,
Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon,
and Christopher R ´e. Hyena hierarchy: Towards larger convo-
lutional language models, 2023. 1
[29] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In Proceedings of the IEEE
conference on computer vision and pattern recognition , pages
4510–4520, 2018. 2, 3, 4, 5
[30] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,
Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-
cam: Visual explanations from deep networks via gradient-
based localization. In Proceedings of the IEEE international
conference on computer vision , pages 618–626, 2017. 2, 7
[31] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field
transforms for optical flow. In Computer Vision–ECCV 2020:
16th European Conference, Glasgow, UK, August 23–28,
2020, Proceedings, Part II 16 , pages 402–419. Springer, 2020.
2
[32] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li,
Honggang Zhang, Xiaogang Wang, and Xiaoou Tang. Resid-
ual attention network for image classification. In Proceedings
of the IEEE conference on computer vision and pattern recog-
nition , pages 3156–3164, 2017. 1
[33] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wang-
meng Zuo, and Qinghua Hu. Eca-net: Efficient channel
attention for deep convolutional neural networks. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 11534–11542, 2020. 1, 2
[34] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So
Kweon. Cbam: Convolutional block attention module. In
Proceedings of the European conference on computer vision
(ECCV) , pages 3–19, 2018. 1, 3, 4, 5, 6
[35] Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, and Gao
Huang. Vision transformer with deformable attention. In
Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 4794–4803, 2022. 2
[36] Qing-Long Zhang and Yu-Bin Yang. Sa-net: Shuffle attention
for deep convolutional neural networks. In ICASSP 2021-
2021 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) , pages 2235–2239. IEEE,
2021. 2
[37] Shanshan Zhong, Zhongzhan Huang, Wushao Wen, Jinghui
Qin, and Liang Lin. Asr: Attention-alike structural re-
parameterization. arXiv preprint arXiv:2304.06345 , 2023.
4
[38] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. De-
formable convnets v2: More deformable, better results. In
Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 9308–9316, 2019. 2, 3[39] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang
Wang, and Jifeng Dai. Deformable detr: Deformable trans-
formers for end-to-end object detection. arXiv preprint
arXiv:2010.04159 , 2020. 2, 5
10"
2308.13696,D:\Database\arxiv\papers\2308.13696.pdf,"In the context of text generation, what trade-off exists between the accuracy of the generated text and the computational cost of generating it?","There is a trade-off between the accuracy of the generated text and the computational cost of generating it.  More accurate text generation often requires more computational resources, while faster generation may result in less accurate outputs.","0
(beam search)1
(LBS-1)2
(LBS-2)3
(LBS-3)
lookahead depth0.125
0.100
0.075
0.050
0.025
0.000negative log-likelihood (Search Error)    
Search Error and UID Error
negative log-likelihood
0.0000.0020.0040.0060.0080.0100.012
stddev of surprisals (UID Error)
stddev of surprisals
Figure 3: Difference in negative log-likelihood (search error)
and average standard deviation of surprisals per sequence
(UID error) of lookahead beam search (LBS) compared to
beam search. The bold line represents the mean over the
beam widths. The shaded area shows the standard error.
• Lookahead depths of up to 3 have little effect on se-
quence length, while beam width has a strong negative
correlation with it (Figure 7).
BLEU Score Figure 1 demonstrates how the lookahead
strategy affects the quality of the results as the lookahead
depth varies. In particular, LBS-2 achieves the best over-
all BLEU score. We observe a reduced improvement with
a lookahead depth of 3 (LBS-3) compared to LBS-2. As
reported in previous work (Stahlberg and Byrne 2019), ex-
haustive search (i.e. LBS- ∞) performs very poorly (See Ap-
pendix).
Why is there a “sweet spot” for lookahead depth? We
observe that a lookahead depth of d= 2 outperforms
d= 0,1,and3(Figure 1). The question is why there is a
“sweet spot” for lookahead depth. Our hypothesis is that this
phenomenon can be explained by the trade-off between the
search error and the UID error . The search error is mea-
sured as the loss of negative log-likelihood compared to
beam search (Figure 3, left axis). We observe that increasing
the lookahead depth reduces the search error, as measured
by the negative log-likelihood. A prior study reports that the
deviation from uniform information density measured by the
standard deviation of surprisals has a negative correlation
with the BLEU score (Meister, Cotterell, and Vieira 2020).
We report the standard deviation of surprisals as UID error
in Figure 3 (right axis). We observe a negative correlation
between lookahead depth and the standard deviation of sur-
prisals.
Overall, we observe that deeper lookaheads improve the
search error, but at the cost of higher UID error at the same
time. These results suggest that a lookahead depth of 2 hap-
pens to be a better trade-off between search error and UID
error in our experimental setting. We also observe a simi-
lar trend for beam width (Figure 4), as indicated by Meister,
Cotterell, and Vieira (2020).
Figure 5 reports the average perplexity. We observe that
5 10 15 20
beam width10.911.011.111.211.3negative log-likelihood (Search Error)
Search Error and UID Error
negative log-likelihood
0.3950.4000.4050.4100.415
stddev of surprisals (UID Error)
stddev of surprisalsFigure 4: Negative log-likelihood (search error) and the av-
erage standard deviation of surprisals per sequence (UID er-
ror) by lookahead beam search (LBS). The bold line repre-
sents the mean over lookahead depth of d= 0,1,2,and3.
The shaded area shows the standard error.
0
(beam search)1
(LBS-1)2
(LBS-2)3
(LBS-3)
lookahead depth1.3201.3221.3241.3261.328perplexity
beam width
k=5
k=10
k=15
k=20
Figure 5: Average perplexity per sentence according to the
model that is being generated from.
increasing the lookahead depth tends to improve the perplex-
ity. Therefore, search error, measured as both negative log-
likelihood and perplexity, decreases with increasing looka-
head depth. Thus, search error alone does not explain why
d= 2has the highest BLEU score.
Figure 6 shows the standard deviation of surprisals and
BLEU for different numbers of lookahead depths and beam
widths. Although LBS has higher BLEU scores than beam
search, it also has a higher average standard deviation of
surprisals per sentence. Therefore, the UID error alone can-
not account for the effect of lookahead depth on the BLEU
scores.
Sequence Length Figure 7 shows the average length of
the output sequence. We observe that while widening the
beam reduces the output sequence length, deepening the
lookahead by up to 3 steps does not. The correlation of
beam width and lookahead depth with sequence length is
0.4050 0.4075 0.4100 0.4125 0.4150 0.4175
stddev of surprisals35.235.435.635.836.0BLEU
depth
beam
LBS-1
LBS-2
LBS-3Figure 6: Average standard deviation of surprisals per se-
quence (UID error) and BLEU with lookahead beam search
(LBS) for different beam widths and lookahead depths.
0
(beam search)1
(LBS-1)2
(LBS-2)3
(LBS-3)
lookahead depth28.028.228.428.6sequence length
beam width
k=5
k=10
k=15
k=20
Figure 7: Average sequence length for varying lookahead
depth and beam width. The correlation of beam width and
lookahead depth with the average sequence length is −0.92
and0.12, respectively.
−0.92and0.12, respectively. While beam width has a clear
negative correlation with output sequence length, lookahead
depth has little effect on sequence length. Thus, average se-
quence length alone does not explain the effect of lookahead
depth on BLEU score.
Running Time We report the wall-clock time of LBS in
Table 1. LBS is significantly slower than beam search es-
pecially when the lookahead depth is large. The wall time
is roughly proportional to the number of calls to the scor-
ing function (see Appendix). Note that the wall-clock time
is hardware dependent. All the experiments are performed
ong4dn.xlarge instances on AWS EC2 (4 vCPU cores,
16 GB memory, and an NVIDIA T4 GPU).
4 Lookbehind Heuristic Beam Search
While LBS outperforms beam search, it requires too muchDecoder k= 5 k= 10 k= 15 k= 20
beam 2.04 3.98 5.44 7.59
LBS-1 22.93 58.55 90.45 138.31
LBS-2 53.91 165.13 302.62 482.58
LBS-3 102.71 329.69 640.80 999.73
Table 1: Average running time (sec) per sentence.
Algorithm 1: Lookbehind Heuristic Beam Search (LHBS)
Input: a set of hypotheses Yt−1of length t−1
Output: a set of hypotheses Ytof length t
1:B′
t,0=∅
2:{y1
t−1,y2
t−1, ...,yk
t−1}=sort(Yt−1)in a descending
order of p(yi
t−1|x)
3:fori∈ {1..k}do
4:Bt,i=B′
t,i−1∪ {yi
t−1◦y|y∈¯V}
5:yi
t= arg maxy∈Bt,ilogpθ(y|x)
6:B′
t,i=Bt,i\ {yi
t}
7:end for
8:return Yt={yi
t,y2
t, ...,yk
t}
computation for sequence generation tasks with large vo-
cabulary sizes (Table 1). We present Lookbehind Heuristic
Beam Search (LHBS), a variant of beam search that heuris-
tically simulates LBS-1 that runs in the same amount of
computation as beam search O(nmaxk|V|). LHBS simulates
LBS by looking behind to consider the past score p(y<t|x)
in addition to the current score.
While beam search determines the next set of hy-
potheses Ytconsidering the score of the current sequence
logpθ(yt|x), LBS with 1-step lookahead (LBS-1) selects Yt
according to both the current score ( logpθ(yt|x)) and the 1-
step future score ( h1(y) = max log pθ(yt+1|x,y)). Since
computing h1(y)requires a computation of the probabil-
ities of succeeding-step, LBS-1 is computationally expen-
sive, with a complexity of O(nmaxk|V|2)in total. LHBS ap-
plies two heuristics to LBS-1 to reduce the computational
overhead. First, we use logpθ(yt−1|x)to account for the
two successive scores (Figure 8). Since logpθ(yt−1|x)is
already computed in the beam search procedure, there is
no additional overhead. Second, we limit the number of hy-
potheses to be considered to k, while heuristically retaining
the most promising ones (Proposition 2). This reduces the
computational complexity of the algorithm to O(k|V|)for
each step.
LHBS gives priority to the hypothesis with the higher
score at step t−1by ordering the hypotheses by their scores
(Line 2 in Algorithm 1). It considers each specific hypothe-
sis in the beam sequentially (Line 3). We maintain a prior-
ity queue Bt,icontaining only children of hypotheses at the
current beam slot and slots before it (Line 4). The highest
scoring candidate is then taken from Bt,iand stored as yi
t
(Lines 5 and 6).3
3In many implementations, including fairseq and Hugging-
Face’s Transformers library, beam search takes top- 2khypotheses
Figure 8: Conceptualized visualization of Lookbehind
Heuristic Beam Search. LBS uses the current score and the
future score to select the next set of hypotheses Ytwhereas
LHBS uses the current score and the previous score.
Formally, we denote yi
tas the i-th hypothesis of the hy-
potheses set Ytwhere i∈ {1..k}.
Y0={BOS}, (8)
Yt=SORT({y1
t, ...,yk
t}), (9)
(10)
where yi
tis generated as follows:
yi
t= arg max
y′∈Bt,ilogpθ(y′|x). (11)
The candidate set Bt,iis defined as:
Bt,i={yt−1◦y|y∈¯V ∧yt−1∈Y1:i
t−1} \ {Y1:i−1
t},(12)
where Y1:i
t={y1
t, ...,yi
t}. By prioritizing hypotheses
deemed promising at the t−1step, LHBS retains some of
these hypotheses in the subsequent t-th step, even if they
are not top- kat the t-th step. This is in contrast to beam
search, which eliminates all non-top- khypotheses at each
iteration. More formally, this procedure guarantees the fol-
lowing properties:
Proposition 2. Att-th iteration of the Lookbehind Heuristic
Beam Search, i-th candidate hypothesis yi
t=yi
t−1◦ytis
guaranteed that
1.yi
t−1has the top- ihighest score among Yt−1
2.yi
thas the top- ihighest score among Bi
t.
The proof is immediate from the construction (Algorithm
1).
The cost of retaining non-optimal hypotheses at the t-th
step is that we have fewer top- khypotheses at the t-th step.
In other words, we seek to deepen the search at the cost of
search width k. We hypothesize that it is an effective strategy
to take depth over width from our analysis (Section 3.2).
instead of top- k. This allows it to pick the first kof those that don’t
predict EOS to continue with. We follow this implementation and
pop the top- 2hypotheses in Lines 5 and 6 to generate 2khypothe-
ses.Dataset Decoder k= 5 k= 10 k= 20
En-Frbeam 42.98 42.95 41.57
LHBS 42.97 43.01 41.64
En-Debeam 29.17 29.14 28.95
LHBS 29.29 29.25 28.97
Table 2: BLEU scores for predictions generated with Look-
behind Heuristic Beam Search (LHBS) and beam search
(WMT’14 En-Fr). The best scores per beam size are bolded.
The best score is underlined.
5 Experiments
We evaluate lookbehind heuristic beam search (LHBS) in
machine translation and text summarization. For all exper-
iments we use publicly available pretrained models and
datasets.
5.1 Machine Translation
To evaluate the performance of LHBS, we decode NMT
models using the proposed decoding strategy. The exper-
iments are performed on the full WMT’14 En-Fr and
WMT’14 En-De test datasets (Bojar et al. 2014). For re-
producibility, we use the pretrained models made publicly
available by fairseq library (Ott et al. 2019). We use BLEU
(Papineni et al. 2002) to evaluate the text quality using the
fairseq-score tool. All models and data information can be
found in the fairseq repository.4On both datasets and across
different beam widths (except for En-Fr with k= 5), the
results indicate that the proposed method produces higher
quality sequences (Table 2).
Prompt-Based Machine Translation using LLM To
evaluate the performance of the proposed method on large
language models (LLM), we decode an LLM using the
proposed decoding strategy. We conduct experiments us-
ing BLOOMZ and mT0 model (Muennighoff et al. 2023)
available in HuggingFace’s Transformers library (Wolf et al.
2020).5We load the model with 8-bit precision to reduce
memory consumption. We use the translation prompt as a
guide to trigger its translation capability, which is shown to
be efficient by Jiao et al. (2023):
Please provide the French translation
for these sentences:
Experiments are performed on the first 200 sentences of
WMT’14 En-Fr dataset (Bojar et al. 2014) due to compu-
tational constraints. Note that because WMT’14 En-Fr test
dataset is public, they are likely to be included in the train-
ing data of the LLM directly or indirectly. We believe that
it does not significantly distort the result of the comparison
of the decoding strategies. Figure 9 reports the BLEU score
of the experiment. We observe that the proposed method is
improved upon beam search.
4https://github.com/facebookresearch/fairseq/tree/main/
examples/translation
5https://huggingface.co/bigscience/bloomz-7b1-mt
4 6 8 10 12 14 16 18 20
beam width35.035.235.4BLEU
BLEU
beam
LHBSFigure 9: Results on prompt-driven machine translation with
LLM (Bloomz and mT0). Evaluated on the first 200 sen-
tences of WMT’14 En-Fr.
Dataset Decoder k= 5 k= 10 k= 20
CNN/DMbeam 40.6 40.2 39.9
LHBS 40.7 40.5 40.2
XSumbeam 35.1 35.1 35.0
LHBS 35.3 35.3 35.1
Table 3: ROUGE-L score for predictions generated with
Lookbehind Heuristic Beam Search (LHBS) and beam
search. The best scores per beam size are bolded. The best
score is underlined.
5.2 Text Summarization
We evaluate the proposed method on two datasets, CNN/DM
(Hermann et al. 2015) and XSum (Narayan, Cohen, and
Lapata 2018). We use the BART-Large model fine-tuned
on each dataset available from fairseq library (Lewis et al.
2020).6We use ROUGE-L (Lin 2004) as the evaluation met-
ric.7Table 3 shows the ROUGE-L score on the two datasets.
On both datasets and across different beam widths, the re-
sults indicate that the proposed method produces higher
quality summarizations.
6 Related Work
The phenomenon that using a larger beam leads to worse
performance has been analyzed in a number of studies
(Koehn and Knowles 2017; Murray and Chiang 2018; Yang,
Huang, and Ma 2018; Stahlberg and Byrne 2019; Cohen and
Beck 2019; Leblond et al. 2021). Many of the authors ob-
serve that widening the beam search degrades performance
due to a bias in sequence models to favor shorter sequences
even with a length penalty. Other authors have investigated
why beam search successfully generates high quality se-
quences. The uniform information density hypothesis (Levy
2005; Levy and Jaeger 2006) is introduced to explain why
6https://github.com/facebookresearch/fairseq/tree/main/
examples/bart
7https://github.com/pltrdy/files2rougebeam search outperforms exhaustive search (Meister, Cot-
terell, and Vieira 2020; Meister et al. 2021). They hypothe-
size that narrowing the width of beam search induces a bias
in the decoding that enforces uniform information density,
resulting in higher quality sequences. Although many have
studied the width of the beam search, little is known about
the depth of the search. Our work extends the analysis to
the search depth and observes a similar trade-off between
search and UID error, which is balanced by the lookahead
depth parameter.
Some authors have studied lookahead strategies for de-
coding. Hargreaves, Vlachos, and Emerson (2021) investi-
gates the greedy roll-out strategy to apply reranking during
decoding instead of only at the end. Lu et al. (2022) evalu-
ated several lookahead strategies to estimate the future score
of the given partial hypothesis. Several works have investi-
gated the lookahead strategy for constraint sentence gener-
ation tasks using Monte Carlo sampling (Miao et al. 2019;
Zhang et al. 2020; Leblond et al. 2021). Our analysis pro-
vides a fundamental insight into why these lookahead strate-
gies can be effective. Furthermore, while these methods re-
quire additional calls to the scoring function, we present a
decoding strategy that does not require any additional calls.
This work focuses on the quality of the text evaluated by
its similarity to the reference text. Previous work has inves-
tigated other factors such as diversity (Vijayakumar et al.
2018; Meister, Forster, and Cotterell 2021), constraints (An-
derson et al. 2017; Hokamp and Liu 2017), or faithfulness
(King et al. 2022; Wan et al. 2023). How the lookahead strat-
egy affects these factors is an open question.
The idea of sequentially processing beam slots in LHBS
is inspired by the monotonic beam search (Lemons et al.
2022). While they use the technique to guarantee the mono-
tonicity of the beam search algorithm, we use it to incorpo-
rate the score of the previous step for hypothesis selection.
7 Conclusions
To study the effect of search depth on the performance of
decoding strategies for text generation models, we introduce
Lookahead Beam Search (LBS). LBS is a generalization
of beam search and exhaustive search that allows control
of the search depth by its hyperparameter. In our machine
translation experiments, we observe that LBS outperforms
beam search, suggesting that there is room for beam search
to improve by searching deeper. We observe that increasing
lookahead depth reduces search error but increases UID er-
ror, similar to the observation reported by Meister, Cotterell,
and Vieira (2020) for increasing beam width. The result sug-
gests that LBS is able to find a better trade-off between the
two errors, thus achieving better performance.
Although promising, LBS is markedly computationally
intensive. We present Lookbehind Heuristic Beam Search
(LHBS), a variant of beam search that heuristically simu-
lates LBS with a 1-step lookahead in the same computational
time as beam search, LHBS is simple, domain-independent,
and requires no additional training. In our empirical evalua-
tion, we find that LHBS improves over vanilla beam search
on machine translation and text summarization tasks overall.
Acknowledgments
We extend our sincere gratitude to all those who offered their
perspectives on this research. We are especially grateful to
Sho Hoshino and Masato Mita for their insightful comments.
References
Anderson, P.; Fernando, B.; Johnson, M.; and Gould, S.
2017. Guided Open V ocabulary Image Captioning with
Constrained Beam Search. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language Pro-
cessing , 936–945. Copenhagen, Denmark: Association for
Computational Linguistics.
Bojar, O.; Buck, C.; Federmann, C.; Research, M.; Haddow,
B.; Koehn, P.; Edinburgh, J. .; Leveling, J.; Monz, C.; Pecina,
P.; Post, M.; Saint-Amand, H.; Google, R. S.; and Specia, L.
2014. Findings of the 2014 Workshop on Statistical Ma-
chine Translation. In the Workshop on Statistical Machine
Translation , 12–58. ISBN 811,203,407668,4.
Chen, Y .; Gilroy, S.; Maletti, A.; May, J.; and Knight, K.
2018. Recurrent Neural Networks as Weighted Language
Recognizers. In Proceedings of the 2018 Conference of the
North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, Volume
1 (Long Papers) , 2261–2271. New Orleans, Louisiana: As-
sociation for Computational Linguistics.
Cohen, E.; and Beck, C. 2019. Empirical Analysis of Beam
Search Performance Degradation in Neural Sequence Mod-
els. In Chaudhuri, K.; and Salakhutdinov, R., eds., Pro-
ceedings of the 36th International Conference on Machine
Learning , volume 97 of Proceedings of Machine Learning
Research , 1290–1299. PMLR.
Gehring, J.; Auli, M.; Grangier, D.; Yarats, D.; and Dauphin,
Y . N. 2017. Convolutional Sequence to Sequence Learning.
In Precup, D.; and Teh, Y . W., eds., Proceedings of the 34th
International Conference on Machine Learning , volume 70
ofProceedings of Machine Learning Research , 1243–1252.
PMLR.
Hale, J. 2001. A Probabilistic Earley Parser as a Psycholin-
guistic Model. In Second Meeting of the North American
Chapter of the Association for Computational Linguistics .
Hargreaves, J.; Vlachos, A.; and Emerson, G. 2021. Incre-
mental Beam Manipulation for Natural Language Genera-
tion. In Proceedings of the 16th Conference of the European
Chapter of the Association for Computational Linguistics:
Main Volume , 2563–2574. Online: Association for Compu-
tational Linguistics.
Hermann, K. M.; Kocisky, T.; Grefenstette, E.; Espeholt, L.;
Kay, W.; Suleyman, M.; and Blunsom, P. 2015. Teaching
machines to read and comprehend. In Advances in neural
information processing systems , volume 28.
Hokamp, C.; and Liu, Q. 2017. Lexically Constrained De-
coding for Sequence Generation Using Grid Beam Search.
InProceedings of the 55th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long Papers) ,
1535–1546. Vancouver, Canada: Association for Computa-
tional Linguistics.Holtzman, A.; Buys, J.; Du, L.; Forbes, M.; and Choi, Y .
2020. The Curious Case of Neural Text Degeneration. In
International Conference on Learning Representations .
Jiao, W.; Wang, W.; Huang, J.; Wang, X.; and Tu, Z. 2023.
Is ChatGPT A Good Translator? Yes With GPT-4 As The
Engine. arXiv:2301.08745.
King, D.; Shen, Z.; Subramani, N.; Weld, D. S.; Beltagy,
I.; and Downey, D. 2022. Don’t Say What You Don’t
Know: Improving the Consistency of Abstractive Summa-
rization by Constraining Beam Search. In Proceedings of the
2nd Workshop on Natural Language Generation, Evalua-
tion, and Metrics (GEM) , 555–571. Abu Dhabi, United Arab
Emirates (Hybrid): Association for Computational Linguis-
tics.
Koehn, P.; and Knowles, R. 2017. Six Challenges for Neural
Machine Translation. In Proceedings of the First Workshop
on Neural Machine Translation , 28–39. Vancouver: Associ-
ation for Computational Linguistics.
Kreutzer, J.; Bastings, J.; and Riezler, S. 2019. Joey NMT:
A Minimalist NMT Toolkit for Novices. In Proceedings of
the 2019 Conference on Empirical Methods in Natural Lan-
guage Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-IJCNLP):
System Demonstrations , 109–114. Hong Kong, China: As-
sociation for Computational Linguistics.
Leblond, R.; Alayrac, J.-B.; Sifre, L.; Pislar, M.; Jean-
Baptiste, L.; Antonoglou, I.; Simonyan, K.; and Vinyals, O.
2021. Machine Translation Decoding beyond Beam Search.
InProceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing , 8410–8434. Online
and Punta Cana, Dominican Republic: Association for Com-
putational Linguistics.
Lemons, S.; Linares L ´opez, C.; Holte, R. C.; and Ruml, W.
2022. Beam Search: Faster and Monotonic. Proceedings
of the International Conference on Automated Planning and
Scheduling , 32(1): 222–230.
Levy, R. 2005. Probabilistic models of word order and syn-
tactic discontinuity . Ph.D. thesis, Stanford University.
Levy, R.; and Jaeger, T. F. 2006. Speakers optimize infor-
mation density through syntactic reduction. In Advances in
neural information processing systems .
Lewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mo-
hamed, A.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L.
2020. BART: Denoising Sequence-to-Sequence Pre-training
for Natural Language Generation, Translation, and Compre-
hension. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics , 7871–7880. On-
line: Association for Computational Linguistics.
Lin, C.-Y . 2004. ROUGE: A Package for Automatic Evalu-
ation of Summaries. In Text Summarization Branches Out ,
74–81. Barcelona, Spain: Association for Computational
Linguistics.
Lu, X.; Welleck, S.; West, P.; Jiang, L.; Kasai, J.; Khashabi,
D.; Le Bras, R.; Qin, L.; Yu, Y .; Zellers, R.; Smith, N. A.;
and Choi, Y . 2022. NeuroLogic A*esque Decoding: Con-
strained Text Generation with Lookahead Heuristics. In
Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics:
Human Language Technologies , 780–799. Seattle, United
States: Association for Computational Linguistics.
Meister, C.; Cotterell, R.; and Vieira, T. 2020. If beam search
is the answer, what was the question? In Proceedings of
the 2020 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , 2173–2185. Online: Associa-
tion for Computational Linguistics.
Meister, C.; Forster, M.; and Cotterell, R. 2021. Determinan-
tal Beam Search. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics and the
11th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , 6551–6562. Online:
Association for Computational Linguistics.
Meister, C.; Pimentel, T.; Haller, P.; J ¨ager, L.; Cotterell, R.;
and Levy, R. 2021. Revisiting the Uniform Information Den-
sity Hypothesis. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing , 963–
980. Online and Punta Cana, Dominican Republic: Associ-
ation for Computational Linguistics.
Meister, C.; Vieira, T.; and Cotterell, R. 2020. Best-First
Beam Search. Transactions of the Association for Compu-
tational Linguistics , 8: 795–809.
Miao, N.; Zhou, H.; Mou, L.; Yan, R.; and Li, L. 2019.
CGMH: Constrained Sentence Generation by Metropolis-
Hastings Sampling. In Proceedings of the AAAI Conference
on Artificial Intelligence , volume 33, 6834–6842.
Muennighoff, N.; Wang, T.; Sutawika, L.; Roberts, A.; Bi-
derman, S.; Le Scao, T.; Bari, M. S.; Shen, S.; Yong, Z. X.;
Schoelkopf, H.; Tang, X.; Radev, D.; Aji, A. F.; Almubarak,
K.; Albanie, S.; Alyafeai, Z.; Webson, A.; Raff, E.; and Raf-
fel, C. 2023. Crosslingual Generalization through Multitask
Finetuning. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Volume 1:
Long Papers) , 15991–16111. Toronto, Canada: Association
for Computational Linguistics.
Murray, K.; and Chiang, D. 2018. Correcting Length Bias
in Neural Machine Translation. In Proceedings of the Third
Conference on Machine Translation: Research Papers , 212–
223. Brussels, Belgium: Association for Computational Lin-
guistics.
Narayan, S.; Cohen, S. B.; and Lapata, M. 2018. Don’t
Give Me the Details, Just the Summary! Topic-Aware Con-
volutional Neural Networks for Extreme Summarization. In
Proceedings of the 2018 Conference on Empirical Methods
in Natural Language Processing , 1797–1807. Brussels, Bel-
gium: Association for Computational Linguistics.
Ott, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.;
Grangier, D.; and Auli, M. 2019. fairseq: A Fast, Extensi-
ble Toolkit for Sequence Modeling. In Proceedings of the
2019 Conference of the North American Chapter of the As-
sociation for Computational Linguistics (Demonstrations) ,
48–53. Minneapolis, Minnesota: Association for Computa-
tional Linguistics.
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.
Bleu: a Method for Automatic Evaluation of Machine Trans-
lation. In Proceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics , 311–318. Philadel-
phia, Pennsylvania, USA: Association for Computational
Linguistics.
Post, M. 2018. A Call for Clarity in Reporting BLEU
Scores. In Proceedings of the Third Conference on Machine
Translation: Research Papers , 186–191. Brussels, Belgium:
Association for Computational Linguistics.
Rush, A. M.; Chopra, S.; and Weston, J. 2015. A Neural At-
tention Model for Abstractive Sentence Summarization. In
Proceedings of the 2015 Conference on Empirical Methods
in Natural Language Processing , 379–389. Lisbon, Portu-
gal: Association for Computational Linguistics.
Stahlberg, F.; and Byrne, B. 2019. On NMT Search Errors
and Model Errors: Cat Got Your Tongue? In Proceedings of
the 2019 Conference on Empirical Methods in Natural Lan-
guage Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-IJCNLP) ,
3356–3362. Hong Kong, China: Association for Computa-
tional Linguistics.
Stahlberg, F.; Hasler, E.; Saunders, D.; and Byrne, B. 2017.
SGNMT – A Flexible NMT Decoding Platform for Quick
Prototyping of New Models and Search Strategies. In Pro-
ceedings of the 2017 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations , 25–
30. Copenhagen, Denmark: Association for Computational
Linguistics.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017.
Attention is All you Need. In Guyon, I.; Luxburg, U. V .;
Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and
Garnett, R., eds., Advances in Neural Information Process-
ing Systems , volume 30. Curran Associates, Inc.
Vijayakumar, A.; Cogswell, M.; Selvaraju, R.; Sun, Q.; Lee,
S.; Crandall, D.; and Batra, D. 2018. Diverse beam search
for improved description of complex scenes. In Proceed-
ings of the AAAI Conference on Artificial Intelligence , vol-
ume 32.
Wan, D.; Liu, M.; McKeown, K.; Dreyer, M.; and Bansal, M.
2023. Faithfulness-Aware Decoding Strategies for Abstrac-
tive Summarization. In Proceedings of the 17th Conference
of the European Chapter of the Association for Computa-
tional Linguistics , 2864–2880. Dubrovnik, Croatia: Associ-
ation for Computational Linguistics.
Welleck, S.; Kulikov, I.; Kim, J.; Pang, R. Y .; and Cho, K.
2020. Consistency of a Recurrent Language Model With
Respect to Incomplete Decoding. In Proceedings of the
2020 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , 5553–5568. Online: Associa-
tion for Computational Linguistics.
Wolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;
Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davi-
son, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y .; Plu,
J.; Xu, C.; Le Scao, T.; Gugger, S.; Drame, M.; Lhoest, Q.;
and Rush, A. 2020. Transformers: State-of-the-Art Natural
Language Processing. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language Process-
ing: System Demonstrations , 38–45. Online: Association for
Computational Linguistics.
Wu, Y .; Schuster, M.; Chen, Z.; Le, Q. V .; Norouzi, M.;
Macherey, W.; Krikun, M.; Cao, Y .; Gao, Q.; Macherey, K.;
Klingner, J.; Shah, A.; Johnson, M.; Liu, X.; Łukasz Kaiser;
Gouws, S.; Kato, Y .; Kudo, T.; Kazawa, H.; Stevens, K.;
Kurian, G.; Patil, N.; Wang, W.; Young, C.; Smith, J.; Riesa,
J.; Rudnick, A.; Vinyals, O.; Corrado, G.; Hughes, M.; and
Dean, J. 2016. Google’s Neural Machine Translation Sys-
tem: Bridging the Gap between Human and Machine Trans-
lation. arXiv:1609.08144.
Yang, Y .; Huang, L.; and Ma, M. 2018. Breaking the Beam
Search Curse: A Study of (Re-)Scoring Methods and Stop-
ping Criteria for Neural Machine Translation. In Proceed-
ings of the 2018 Conference on Empirical Methods in Nat-
ural Language Processing , 3054–3059. Brussels, Belgium:
Association for Computational Linguistics.
Zhang, M.; Jiang, N.; Li, L.; and Xue, Y . 2020. Lan-
guage Generation via Combinatorial Constraint Satisfac-
tion: A Tree Search Enhanced Monte-Carlo Approach. In
Findings of the Association for Computational Linguistics:
EMNLP 2020 , 1286–1298. Online: Association for Compu-
tational Linguistics.
A Implementation of lookahead beam search
Algorithm A.1 describes the procedure of lookahead beam
search. To reduce the computation time, we implement the
evaluation of hdby best-first branch-and-bound search (Al-
gorithm A.1 and A.2). We choose best-first branch-and-
bound over Dijkstra because it requires less memory con-
sumption ( O(d+|V|)). Since the scoring function is mono-
tonically decreasing (Meister, Vieira, and Cotterell 2020),
we can prune a partial hypothesis that is lower than the
current k-th largest score before expanding the hypothesis
further. The function k-th max y∈Y′(f(y))returns the k-th
largest score among Y′if|Y′| ≥kand negative infinity
otherwise. We explore the candidates in best-first order –
the hypothesis with the highest score is explored first. In this
way, we have a higher chance of pruning the less promis-
ing hypothesis, thus reducing computation. Note that the al-
gorithm is guaranteed to find the same path as breadth-first
search, thus preserving the result of the decoding.
B Evaluation of Exhaustive Search (MAP
Decoding)
We perform an exhaustive search on the first 100 sentences
of WMT’14 En-Fr and WMT’14 En-De datasets. The results
are summarized in Table B.1. Overall we observe a signifi-
cant decrease in BLEU and sequence length, as observed in
previous work (Stahlberg and Byrne 2019).
C Additional evaluations of lookahead beam
search
Wall-Clock Time Table C.1 reports the number of calls
to the scoring function (e.g. probabilistic model) by looka-
head beam search. We observe that the number of callsAlgorithm A.1: Lookahead Beam Search
Input: a set of hypotheses Yt−1of length t−1
Output: a set of hypotheses Ytof length t
1:B={yt−1◦y|y∈¯V}
2:{y1
t,y2
t, ...,yb
t}=sort(B)in a descending order of
p(yi
t|x)
3:Y′← ∅
4:b← |¯V|
5:fori∈ {1, ..., b}do
6: iflogpθ(yi
t|x)<k-th max y∈Y′(f(y))then
7: return Yt= arg topky∈Y′(f(y))
8: end if
9:f(yi
t)←Eval (yi
t, d,k-th max y∈Y′(f(y)))
10: iff(yi
t)>k-th max y∈Y′(f(y))then
11: Y′←Y′∪ {yi
t}
12: end if
13:end for
14:return Yt= arg topky∈Y′(f(y))
Algorithm A.2: Eval (yt, d, f max)
Input: a hypothesis yt, a depth d, and a threshold fmax
Output: a score of the hypothesis hd(y)
1:ifd= 0then
2: return logpθ(yt|x)
3:end if
4:B={yt◦y|y∈¯V}
5:{y1
t+1,y2
t+1, ...,yb
t+1}=sort(B)in a descending order
oflogpθ(yi
t+1|x)
6:fori∈ {1..b}do
7: iflogpθ(yi
t+1)< fmaxthen
8: return fmax
9: end if
10: fi←Eval (yi
t+1, d−1, fmax)
11: iffi> fmaxthen
12: fmax←fi
13: end if
14:end for
15:return fmax
Dataset En-Fr En-De
BLEU 2.2 6.0
sequence length 9.169 16.217
negative log-likelihood 8.195 8.246
stddev of surprisals 0.291 0.486
Table B.1: Results of exhaustive search (MAP decoding)
on the first 100 sentences of WMT’14 En-Fr and En-De
datasets.
103
104
number of calls101102103walltime (sec)
depth
beam
LBS-1
LBS-2
LBS-3Figure C.1: Comparison of the average number of calls to
the scoring function to the wall-clock time (WMT’14 En-
Fr).
grows rapidly with increasing lookahead depth. The wall-
clock time is mostly proportional to the number of calls (Fig-
ure C.1). Note that the wall-clock time depends on the hard-
ware. All the experiments are performed on g4dn.xlarge
instances on AWS EC2 (4 vCPU cores, 16 GB memory, and
an NVIDIA T4 GPU).
Results on WMT’14 En-De Table C.2 reports the BLEU
score on the first 100 sentences of WMT’14 En-De dataset.
Overall, we observe that the BLEU score increases with
the lookahead strategy. The highest BLEU score is achieved
when using a beam width of 5 and a lookahead depth of 1
and 2.
Fully Convolutional Decoder To test the effect of the
lookahead strategy on a non-Transformer model, we eval-
uate the performance of LBS on a fully convolutional de-
coder proposed by Gehring et al. (2017). For reproducibility,
we use the pretrained model provided by fairseq.8Table C.3
reports the BLEU score. We observe that LBS outperforms
beam search overall.
Evaluation of LBS-1 on the entire WMT’14 En-Fr To
evaluate the lookahead strategy on larger samples, we eval-
uate LBS-1 on the entire WMT’14 En-Fr dataset. Due to
computational constraints, we present only the evaluation of
LBS-1. Table C.4 reports the BLEU score. We observe that
LBS-1 achieves higher BLEU compared to beam search (ex-
cept for En-Fr with k= 15 ).
Results on the last 50 Sentences of WMT’14 En-Fr We
perform an evaluation on the last 50 sentences of WMT’14
En-Fr. Table C.5 reports the BLEU score. We observe that
LBS improves over beam search overall.
8https://github.com/facebookresearch/fairseq/tree/main/
examples/translationDecoder k= 5 k= 10 k= 15 k= 20
beam 145.86 291.38 436.07 580.99
LBS-1 718.02 1841.83 3142.28 4590.62
LBS-2 2103.73 6270.25 11630.90 17946.10
LBS-3 4656.60 14471.00 27364.80 42597.90
Table C.1: Average number of calls to the scoring function
(probabilistic model) per sentence (WMT’14 En-Fr).
Dataset Decoder k= 5 k= 10 k= 15 k= 20
En-Debeam 22.7 21.9 22.0 21.8
LBS- 1 23.2 22.6 22.2 21.5
LBS- 2 23.2 22.7 22.6 22.6
LBS- 3 23.0 22.7 23.0 23.0
Table C.2: Evaluation of lookahead beam search on the first
100 sentences of WMT’14 En-De datasets. The best for each
beam width is bolded. The best score is underlined.
Dataset Decoder k= 5 k= 10 k= 15 k= 20
En-Frbeam 35.3 35.5 35.4 35.2
LBS-1 35.8 35.7 35.6 35.5
LBS-2 36.1 36.1 35.7 35.7
LBS-3 35.7 35.9 35.6 35.4
Table C.3: BLEU on the first 100 sentences of WMT’14 En-
Fr using a fully convolutional decoder. The best for each
beam width is bolded. The best score is underlined.
Dataset Decoder k= 1 k= 5 k= 10 k= 15
En-Frbeam 34.8 35.8 36.0 36.0
LBS-1 35.2 35.9 36.1 35.9
En-Debeam 28.6 29.3 29.0 28.9
LBS-1 28.8 29.4 29.2 29.0
Table C.4: BLEU on the entire dataset on WMT’14 En-Fr
and En-De. The best for each beam width is bolded. The
best for each dataset is underlined.
Decoder k= 1 k= 5 k= 10 k= 15
beam 37.6 37.2 35.9 35.6
LBS-1 37.3 37.4 36.4 36.5
LBS-2 36.9 38.1 36.2 35.9
LBS-3 36.6 37.4 36.4 36.4
Table C.5: BLEU on the last 50 sentences of WMT’14 En-
Fr. The best for each beam width is bolded. The best score
is underlined."
2010.09273,D:\Database\arxiv\papers\2010.09273.pdf,"How does the proposed neural network architecture address the challenge of classifying objects based on radar reflections, particularly when dealing with objects that have similar reflection characteristics?","The network utilizes a global context layer to capture structural relationships between reflections, allowing it to learn more abstract features that go beyond simple reflection characteristics and improve classification accuracy, especially for objects with similar reflection patterns.","associated reﬂections ( M,N)
1-D convolution ( 1×1)
global context layer(M,16)
1-D convolution ( 1×1)
(M,32)
global max pooling(M,32)
dense(32)
object type ( C)
Fig. 4: Architecture of the proposed neural network for object
type classiﬁcation. All layers until the global max pooling are
invariant of the order and number of associated reﬂections.
Abstract reﬂection-wise features can be learned by using the
proposed global context layer. The size of the representation
is denoted in brackets. For example, ( M,16) is a list of M
reﬂections with 16features.
of global context layers was already used in graph neural net-
works [29]. In the proposed method further 1-D convolutions
follow the global context layer to learn more abstract features,
which capture the structural relationship. For example, con-
sider Cartesian coordinates as input features to a global context
layer. The output features will include the Cartesian positions
as well as the maximum of all Cartesian positions. Subsequent
1-D convolutional layers with this context information can
easily learn the length and width of the object, for example.
C. Radar reﬂection network
The architecture of the proposed radar reﬂection network
(DEEPREFLECS ) is depicted in Fig. 4. The input are the M
associated reﬂections of each object with Nfeatures, which
were provided by the tracking module, see Fig. 2. Then, a 1-
D convolutions with kernel size one follows, which combines
the input features to an intermediate representation. The global
context layer adds global context to the individual reﬂection
features, see Fig. 3. A further 1-D convolutional layer allows
the network to combine the local and global features and
consider the relationship of the reﬂections. Finally, a global
max pooling layer converts the list of features to a single
feature and a fully connected layer calculates the ﬁnal object
class in a one-hot encoding with Cclasses.
The global context layer and the global max pooling layer
do not have trainable parameters. All other layers use ReLU as
nonlinear activation function, except for the last layer, which
uses softmax. The size of the trainable tensors Mandbare
deﬁned by the input and output tensor shapes of each layer.
The input size of the object type classiﬁcation is dynamic.
To apply the proposed network in practice, the Mactual
reﬂections are padded to a list with ﬁxed size (e.g. length
64), such that the input size is larger than the expected input
M. However, only the ﬁrst Mentries of the padded list are
evaluated during the global max pooling operation inside the
global context layer or before the ﬁnal dense layer. By doing
so, the padded list entries are ignored and the network canTABLE II: Dataset used for the experiments.
Class Number of Tracks Number of Samples
Car 574 114,825
Pedestrian 342 30,238
Cyclist 275 13,214
Non-Obstacle 699 55,868
handle any number of input reﬂections. Furthermore, the 1-D
convolutional layers with kernel size 1as well as the pooling
layers are order invariant.
Moreover, ﬁlter pruning and quantization techniques [30],
[31] can be applied to further reduce the network footprint.
Furthermore, the network can be extended for uncertainty
estimation [32], which is relevant for safety-critical automotive
applications.
D. Reﬂection features
The proposed object type classiﬁer uses N= 5 reﬂection
features, which are processed in the network. These are:
•Thex-position of the reﬂection in the coordinate system
of the tracked object.
•They-position of the reﬂection in the coordinate system
of the tracked object.
•The RCS of the reﬂection.
•The range of the reﬂection r.
•The radial velocity vrof the reﬂection, where the ego-
motion of the radar is already compensated.
RCS, rand ego-motion compensated vrare common features
for classiﬁcation, c.f. [22], [23]. The x- and y-position is
converted to the coordinate system of the tracked object, by
subtracting the objects x- and y-position and rotating the
reﬂection positions by the tracked objects heading angle. We
use the object coordinate system and x-/y-position instead
of range and angle because the reﬂection characteristic of
an object is approximately constant in Cartesian coordinates
and range-independent representations facilitates learning of
characteristic patterns.
V. E XPERIMENTS
We use a dataset of road objects (real measurements),
which are relevant in the automotive use-case. The dataset was
collected on a test-track and includes tracks from the C= 4
classes “pedestrian”, “bicycle”, “car” and “non-obstacle”. In
the recording of each track, the host-vehicle with the radar
sensor drives towards the object and brakes shortly before it.
Consequently, each track consists of multiple samples, which
are recorded during the approach. We are using crash-test
dummies for the pedestrian and bicycle tracks because such
maneuvers would be dangerous to real persons. The class
“non-obstacle” contains various overrideable objects which
lead to consistent radar reﬂections, for example a coke can.
The size of the dataset is listed in Tab. II. From each track,
we use only samples closer than 75meters. Pedestrians and
cyclists have fewer samples per track than cars due to the lack
of measurements at larger distance. The cars and non-obstacles
TABLE III: Benchmark comparison of the categorical accu-
racy in % for different classes and methods, evaluated on the
self-recorded dataset described in Tab. II.
Approach Total Car Pedestr. Cyclist Non-Obst.
CRAFTED FOREST 90.4 96.6 78.5 47.0 92.0
GRIDCNN 84.3 86.6 61.9 54.7 95.4
DEEPREFLECS 93.4 95.8 83.9 71.4 97.6
are stationary while pedestrians and bicycles are tangentially
moving. Hence, this dataset is relatively challenging for object
type classiﬁcation because the radial velocity is not sufﬁcient
for accurate classiﬁcation. The dataset is split into 60%
training, 20% validation and 20% test. The split is performed
track-wise (samples of one track must not occur in different
splits) to avoid overﬁtting.
The proposed network is implemented as depicted in Fig. 4.
In total, this network has only 1,284 learnable parameters,
which is small for a neural network object type classiﬁer. In the
experiments, we compare the proposed reﬂection processing
network with two alternative approaches. The ﬁrst alternative
approach equals [24], abbreviated as C RAFTED FOREST . It
uses handcrafted features such as the second order moments
of range and radial velocity. These features are calculated per
object and classiﬁed using a random forest classiﬁer.
The second alternative applies a 2D convolutional neural
network (CNN), abbreviated as G RIDCNN. The input to the
CNN is a grid around the tracked object of size 11×11(4m×
4m) and with two channels. One channel is the sum of the RCS
and the second channel is the average ego-motion compensated
radial velocity of all reﬂections falling into the respective cells.
The particular architecture is similar to [12].
More details on the training are given in Appendix A.
VI. R ESULTS
A. Benchmark
The results of the experiments are described in Tab. III,
where we compare the total categorical accuracy as well as
the categorical accuracy for each of the four classes. It should
be noted that we evaluate the accuracy of single samples,
i.e. a temporal smoothing can improve the numbers for all
classiﬁers.
We observe that all classiﬁers can solve the object type
classiﬁcation task with reasonable total performance. Whereas
cars are classiﬁed easily due to their size and high RCS,
the performance for “pedestrian” and “cyclist” is signiﬁcantly
lower. These classes are often confused with each other and
with the “non-obstacle” class.
The performance of the G RIDCNN is worst for most
classes among the presented approaches. Although it is a
powerful data-driven method, the experiment shows that the
grid representation of radar reﬂection data is inappropriate.
One possible explanation could be the high grid sparsity since
the radar reﬂection data only ﬁll parts of the grid.
The C RAFTED FOREST ranks 2nd in the total accuracy,
which we explain by the information loss through the hand-TABLE IV: Ablation results of the categorical accuracy in
% for different classes and the impact of the global context
layer (GCL) for our approach D EEPREFLECS , evaluated on
the self-recorded dataset described in Tab. II.
Approach Total Car Pedestr. Cyclist Non-Obst.
w/ GCL 93.45 95.75 83.89 71.42 97.58
w/o GCL 92.20 94.86 87.25 58.89 96.35
Diff. +1.25 +0.89 -3.36 +12.53 +1.23
crafted features. D EEPREFLECS outperforms the C RAFTED -
FOREST by3%with respect to the total performance and
by up to 24% for the most challenging class “cyclist”. This
demonstrates the impact of a meaningful input representation
(radar reﬂections as a list) as well as the ability to use data-
driven feature learning.
In summary, D EEPREFLECS outperforms the other ap-
proaches for almost all classes. It further exhibits a compu-
tational complexity, which is orders of magnitude lower than
CRAFTED FOREST or G RIDCNN. This is due to the natural
representation of radar reﬂections as a list as well as the
introduction of the global context layer.
B. Ablation Study
The global context layer can capture more complex object
properties which a radar sensor cannot measure directly such
as width and length, for example. We analyze the impact of
the global context layer on the same dataset and quantify
the difference in performance. Tab. IV shows the accuracy of
our approach D EEPREFLECS with (w/) and without (w/o) the
global context layer (GCL). The total performance decreases
signiﬁcantly without the global context layer, from 93.45%
to92.20%. In the minority class “cyclist”, the difference
is even 12.5%. We explain this by the fact that “cyclist”
is particularly difﬁcult to classify because its reﬂections are
similar to “pedestrian” or “non-obstacle”. Consequently, the
structural relation, which is provided by the global context
layer, improves the performance of this class the most.
VII. C ONCLUSION
To summarize, this paper presented a deep learning classiﬁer
for automotive radar object type classiﬁcation. The input
representation as a list of reﬂections is addressed by the use
of order- and size-invariant neural network layers, such as 1-
D convolution and global max pooling. Further improvements
are achieved through the global context layer which combines
an extraction of local and global features. The beneﬁts of
the proposed approach were demonstrated via experiments on
real data. In detail, we could show that the proposed model
outperforms previous literature in terms of total classiﬁcation
accuracy by up to 8.8%total performance. For future work, we
will investigate whether the global context layer also improves
performance for other complex objects such as trucks or
trailers.
REFERENCES
[1] T. Gandhi and M. M. Trivedi, “Pedestrian protection systems: Issues,
survey, and challenges,” IEEE Transactions on Intelligent Transportation
Systems , vol. 8, no. 3, pp. 413–430, 2007.
[2] D. Feng, C. Haase-Sch ¨utz, L. Rosenbaum, H. Hertlein, C. Glaeser,
F. Timm, W. Wiesbeck, and K. Dietmayer, “Deep multi-modal object
detection and semantic segmentation for autonomous driving: Datasets,
methods, and challenges,” IEEE Transactions on Intelligent Transporta-
tion Systems , 2020.
[3] R. O. Chavez-Garcia and O. Aycard, “Multiple sensor fusion and clas-
siﬁcation for moving object detection and tracking,” IEEE Transactions
on Intelligent Transportation Systems , vol. 17, no. 2, pp. 525–534, 2016.
[4] S. Han, X. Wang, L. Xu, H. Sun, and N. Zheng, “Frontal object
perception for intelligent vehicles based on radar and camera fusion,”
in35th Chinese Control Conference , July 2016.
[5] Milch and M. Behrens, “Pedestrian detection with radar and computer
vision,” tech. rep., Smart Microwave Sensors GmbH, 2001.
[6] M. Tons, R. Doerﬂer, M.-M. Meinecke, , and M. A. Obojski, “Radar
sensors and sensor platform used for pedestrian protection in the EC-
funded project SA VE-U,” in Intelligent Vehicles Symposium , June 2004.
[7] H. Rohling, S. Heuel, and H. Ritter, “Pedestrian detection procedure
integrated into an 24 GHz automotive radar,” in IEEE Radar Conference ,
May 2010.
[8] S. Heuel and H. Rohling, “Pedestrian classiﬁcation in automotive radar
systems,” in 13th International Radar Symposium , May 2012.
[9] A. Bartsch, F. Fitzek, and R. H. Rasshofer, “Pedestrian recognition using
automotive radar sensors,” Advances in Radio Science , vol. 10, pp. 45–
55, 2012.
[10] S. Villeval, I. Bilik, and S. Z. G ¨urbuz, “Application of a 24 GHz
FMCW automotive radar for urban target classiﬁcation,” in IEEE Radar
Conference , May 2014.
[11] Y . Kim and T. Moon, “Human detection and activity classiﬁcation based
on micro-Doppler signatures using deep convolutional neural networks,”
IEEE Geoscience and Remote Sensing Letters , vol. 13, no. 1, pp. 8–12,
2016.
[12] K. Patel, K. Rambach, T. Visentin, D. Rusev, M. Pfeiffer, and B. Yang,
“Deep learning-based object classiﬁcation on automotive radar spectra,”
inIEEE Radar Conference , May 20189.
[13] A. Palffy, J. Dong, J. F. P. Kooij, and D. M. Gavrila, “CNN based road
user detection using the 3d radar cube,” IEEE Robotics and Automation
Letters , vol. 5, no. 2, pp. 1263–1270, 2020.
[14] M. G. Amin, Y . D. Zhang, F. Ahmad, and K. C. D. Ho, “Radar signal
processing for elderly fall detection: The future for in-home monitoring,”
IEEE Signal Processing Magazine , vol. 33, no. 3, pp. 71–80, 2016.
[15] T. Wagner, R. Feger, and A. Stelzer, “Radar signal processing for jointly
estimating tracks and micro-Doppler signatures,” IEEE Access , vol. 5,
no. 2, pp. 1220–1238, 2017.
[16] S. Abdulatif, Q. Wei, F. Aziz, B. Kleiner, and U. Schneider, “Micro-
Doppler based human-robot classiﬁcation using ensemble and deep
learning approaches,” in IEEE Radar Conference , May 2018.
[17] M. Ulrich, T. Hess, S. Abdulatif, and B. Yang, “Person recognition based
on micro-Doppler and thermal infrared camera fusion for ﬁreﬁghting,”
in21st International Conference on Information Fusion , July 2018.
[18] M. Ulrich and B. Yang, “Short-duration Doppler spectrogram for person
recognition with a handheld radar,” in 26th European Signal Processing
Conference , August 2018.
[19] R. Dub ´e, M. Hahn, M. Sch ¨utz, J. Dickmann, and D. Gingras, “Detection
of parked vehicles from a radar based occupancy grid,” in IEEE
Intelligent Vehicles Symposium , June 2014.
[20] J. Lombacher, M. Hahn, J. Dickmann, and C. W ¨ohler, “Potential of
radar for static object classiﬁcation using deep learning methods,” in
IEEE International Conference on Microwaves for Intelligent Mobility ,
May 2016.
[21] R. Prophet, J. Martinez, J. F. Michel, R. Ebelt, I. Weber, and M. V ossiek,
“Instantaneous ghost detection identiﬁcation in automotive scenarios,” in
IEEE Radar Conference , May 2019.
[22] F. Kraus, N. Scheiner, W. Ritter, and K. Dietmayer, “Using machine
learning to detect ghost images in automotive radar,” arXiv:2007.05280,
2020.
[23] O. Schumann, M. Hahn, J. Dickmann, and C. W ¨ohler, “Semantic
segmentation on radar point clouds,” in International Conference on
Information Fusion , June 2018.[24] R. Prophet, M. Hoffmann, M. V ossiek, C. Sturm, A. Ossowska, W. Ma-
lik, and U. L ¨ubbert, “Pedestrian classiﬁcation with a 79 GHz automotive
radar sensor,” in 19th International Radar Symposium , 2018.
[25] N. Scheiner, N. Appenrodt, J. Dickmann, , and B. Sick, “Radar-based
road user classiﬁcation and novelty detection with recurrent neural
network ensembles,” in IEEE Intelligent Vehicles Symposium , June 2019.
[26] Y . Wang and Y . Lu, “Classiﬁcation and tracking of moving objects from
77 GHz automotive radar sensors,” in IEEE International Conference on
Computational Electromagnetics , March 2018.
[27] D. E. Barrick, “FM/CW radar signal and digital processing,” tech. rep.,
National Oceanic and Atmospheric Administration, July 1973.
[28] C. Qi, H. Sun, K. Mo, and L. J. Guibas, “PointNet: Deep learning on
point sets for 3D classiﬁcation and segmentation,” in IEEE Conference
on Computer Vision and Pattern Recognition , July 2017.
[29] J. Gao, C. Sun, H. Zhao, Y . Shen, D. Anguelov, C. Li, and C. Schmid,
“VectorNet: Encoding HD maps and agent dynamics from vectorized
representation,” in IEEE Conference on Computer Vision and Pattern
Recognition , July 2020.
[30] L. Enderich, F. Timm, L. Rosenbaum, and W. Burgard, “Learning
multimodal ﬁxed-point weights using gradient descent,” in European
Symposium on Artiﬁcial Neural Networks , April 2019.
[31] L. Enderich, F. Timm, and W. Burgard, “Holistic ﬁlter pruning for
efﬁcient deep neural networks,” arXiv:2009.08169, 2020, to appear at
WACV , 2021.
[32] D. Feng, Y . Cao, L. Rosenbaum, F. Timm, and K. Dietmayer, “Leverag-
ing uncertainties for deep multi-modal object detection in autonomous
driving,” in IEEE Intelligent Vehicles Symposium , June 2020.
APPENDIX A
TRAINING SETTINGS OF THE EXPERIMENTS
The neural network object type classiﬁers (G RIDCNN,
DEEPREFLECS ) are trained in Tensorﬂow/Python. Each net-
work is trained for 256 epochs with 1024 steps and a batch
size of 512. We implemented a learning rate schedule starting
at0.01and decreasing to 0.0001 over the 256 epochs. For
balanced data we increase sample size of “pedestrian” (2x),
“cyclist” (2x) and “non-obstacle” (4x) by re-sampling (training
dataset only). The validation dataset was used to select the
best epoch of a training. The network inputs are normalized
to zero-mean and unit-variance. The global max pooling with a
masking of unused entries as well as the global context layer
are custom implementations, whereas the 1-D convolutions,
dense layers, and all layers of the G RIDCNNare already
available in Tensorﬂow.
The G RIDCNN consists of three 2-D convolutional layers
with kernel size 3×3(channel size 16,32and64), followed
by three fully-connected layers with 128,32andC= 4
neurons, respectively. This makes a total of 232,628learnable
parameters for the G RIDCNN. Decreasing the network size
in comparison to [12] as well as adding dropout layers was
necessary to avoid overﬁtting, otherwise poor performance was
achieved on the test data.
The feature-based object type classiﬁer is trained using
the implementation of the sklearn library. The number of
decision trees is 100, otherwise default settings were used.
The handcrafted features of [24] are the velocity resolution, the
number of reﬂections per object, the existence of a stationary
(low radial velocity) reﬂection, the average azimuth angle, the
average RCS, the average range, the sum of the object extent in
xandyCartesian coordinates, as well as the interval, variance
and standard deviation of both range and radial velocity. The
sum of the number of nodes in all decision trees of the random
forest is 1,838,940."
1901.02229,D:\Database\arxiv\papers\1901.02229.pdf,"The paper describes a method for detecting adversarial examples by comparing the visual representation of an input image with the visual representation of the codeword that is most highly activated by the network.  What are the potential limitations of this approach, and how might these limitations be addressed?","This approach relies on the assumption that adversarial examples will activate codewords that are visually dissimilar to the input image.  However, this assumption may not always hold, especially if the adversarial attack is sophisticated enough to manipulate the network's internal representations in a way that is not easily detectable by visual inspection.  To address this limitation, one could explore more robust methods for comparing visual representations, such as using a metric that is less sensitive to small perturbations.","Figure 2: Interpretable BoW network for adversarial sample detection. An input image is passed through the network,
and we retrieve the visual codeword with highest activation. We then pass these two images through a two-stream model
which compares them using the Euclidean distance in feature space, so as to determine if they belong to the same class.
ing the clustering procedure. Note that this further encodes
the notion of semantic meaning of each codeword.
To obtain a visual representation of each codeword in
B0, we optimize the input zof the generator, whose weights
are ﬁxed, so as to generate an image that, when passed
through the base network, yields features that are close to
the codeword in the least-square sense. That is, formally,
for each codeword b0,kinB0, we solve
z∗
k= argmin
z∥f(g(z))−b0,k∥2, (5)
whereg(·)represents the generator, and f(·)the base net-
work up to the last average pooling operation. We then
take our codebook Bto be the set of generated features
{f(g(z∗
k))}, and train our BoW model by replacing the
base network classiﬁcation layer with a layer that maps the
BoW representation to the class labels and using the stan-
dard cross-entropy loss.
Note that, while this training procedure may seem costly,
this has no effect on the computational cost at test time.
Indeed, inference only involves a forward pass through our
BoW network, which, compared to the base network, only
requires us to store the additional codebook B; that is, the
generator network is not needed anymore.
3.2. Detecting Adversarial Examples
By providing a visual and semantic meaning related to
the network’s prediction, our interpretable BoW network
can be leveraged to detect adversarial examples. The rea-
soning behind this is the following: Typically, adversarial
attacks aim to add the smallest amount of perturbation to
an image so that the network misclassiﬁes it. While this
perturbation should be imperceptible to humans, it should
strongly affect the resulting deep representation. In our
case, this representation is the BoW one, and, for misclassi-
ﬁcation to occur, the most highly activated codeword shouldtypically be associated to the wrong class. As such, its vi-
sual representation should look signiﬁcantly different from
the adversarial example. We therefore propose to train an
adversary detector network that, given two input images,
predicts whether they belong to the same class or not.
Formally, our detection framework, depicted by Fig. 2,
proceeds as follows. An image I, adversarial or not, is
passed through our interpretable BoW network, and we re-
trieve the visual codeword Vk∗corresponding to the high-
est activation using Eq. 4. We then pass IandVk∗to our
adversary detector, which outputs a binary label indicating
whether the two images belong to the same class or not. If
they don’t, then Iis detected as an adversarial example.
Our detector is a two-stream network that extracts fea-
tures for the two images independently. To train this net-
work, we make use of the contrastive loss [19] , which
aims to make the Euclidean distance between pairs of mis-
matched images larger than a margin m= 1, while mini-
mizing that of matching pairs. Detection is then performed
by comparing the Euclidean distance to the margin.
While, in our experiments, we train the detector to iden-
tify adversarial examples, it can also be used to detect OOD
samples, even without any re-training. In essence, the intu-
ition remains unchanged: An OOD sample will activate a
codeword whose visual representation looks different from
the input image. The detection procedure is thus the same
as in the case of adversarial examples.
4. Experiments
We now empirically evaluate our interpretable BoW net-
works. To this end, we ﬁrst analyze the semantic meaning
of the learned visual dictionaries. We then demonstrate their
beneﬁts to detect adversarial and out-of-distribution sam-
ples. In these experiments, we used the standard benchmark
datasets employed for adversarial sample detection, that is,
4
Dataset Base network BoW network
MNIST 99.23 99.15
FMNIST 92.46 92.06
CIFAR-10 87.54 87.95
SVHN 92.43 91.81
Table 1: Classiﬁcation accuracy (in %) of the base and
BoW networks. Note that both models perform on par, but
the BoW one allows us to obtain a visual interpretation.
MNIST [30], F-MNIST [53], CIFAR-10 [25], SVHN [40].
Implementation details. For our comparisons to be mean-
ingful, we rely on the same base architecture as in [34] on
each dataset. This architecture is ﬁrst trained with a softmax
classiﬁer for 50-100 epochs and used to compute the initial
codebook B0. In parallel, we train either a GAN [13] or a
WGAN [18], depending on the dataset, for 100k iterations.
From the resulting generator and B0, we obtain our inter-
pretable codebook Bfollowing the procedure described in
Section 3.1. We then train the classiﬁcation layer of our
interpretable BoW model for 40epochs.
In all our experiments, we set α= 100 in the BoW soft-
assignment policy of Eq. 3, and use the Adam [23] opti-
mizer with a learning rate of 0.001 and a decay rate of 0.1
applied every 20 epochs. Due to space limitation, we pro-
vide the detail of the base classiﬁer and the GAN archi-
tecture in the supplementary material. The test errors on
MNIST, FMNIST, CIFAR-10, and SVHN using the soft-
max classiﬁer and the BoW one are given in Table 1. In
essence, both classiﬁers perform on par. Note that our goal
here is not to advocate for superior performance of the BoW
model, but rather for its use to provide a visual interpreta-
tion, as discussed below.
4.1. Visualizing BoW Codewords
Each codeword in our BoW model is directly associated
with an image generated by a GAN. In Fig. 3, we visual-
ize these images for MNIST, FMNIST, SVHN and CIFAR-
10. Zooming in on the ﬁgure conﬁrms that the learned
codebooks nicely cover the diversity of the classes in these
datasets. For example, in FMNIST, each garment appears in
a variety of sizes and styles. Similarly, in CIFAR-10, each
object appears in different colors, orientations and in front
of different backgrounds. Furthermore, and more impor-
tantly, each one of these images retains the semantic mean-
ing of the class label for which it was generated. This will
prove key to the success of our adversarial sample detector,
as discussed in the next section.
In the top row of Fig. 4, we show the codewords with
highest activation for a few correctly-classiﬁed images of
each dataset. Note that, in most cases, the corresponding
codeword has the same semantic meaning as the input im-
age, not only because it corresponds to the same class, but
(a) MNIST codewords
 (b) FMNIST codewords
(c) SVHN codewords
(d) CIFAR-10 codewords
Figure 3: Visual interpretations of the BoW codebooks.
Every two rows correspond to one class. Note that these
codewords capture the diversity of the classes in each
dataset (better seen by zooming in).
also because it depicts a visually similar content, e.g., in
terms of color and orientation. More such visualizations are
provided in the supplementary material.
4.2. Adversarial Attacks
We make use of the state-of-the-art attack methods,
FGSM [14], BIM-a [28], BIM-b [28], DeepFool [38] and
CW [5], to evaluate (i) the robustness of our interpretable
BoW model to adversarial samples; and (ii) the effective-
ness of our detection strategy. In both cases, following com-
mon practice [34], we discard the images that were misclas-
siﬁed by the original networks from this evaluation.
To ﬁrst validate our intuition that adversarial samples
will activate codewords corresponding to the wrong classes,
and that these codewords will be visually dissimilar to the
input image, in the bottom row of Fig. 4, we provide the
most highly activated codeword images for a few success-
ful adversarial attacks. Note that these images are indeed
semantically and visually different, which will facilitate the
task of our detector.
4.2.1 Robustness of Interpretable BoW Networks
Before evaluating the effectiveness of our detection strategy,
we study the robustness of our BoW model to the adversar-
ial attacks. In particular, we focus on white-box attacks,
where the attacker has access to the exact model it aims
to fool. There are two ways to attack our BoW model: One
can generate adversarial examples either for the BoW model
5
Normal  samples Adversarial  samples
 Figure 4: Visualization of the most highly activated codeword for normal and adversarial samples. Each two rows
within a block show the input image (top) and corresponding codeword (bottom). The adversarial examples were obtained
with a BIM-a attack. Additional visualizations for other datasets and attacks are provided in the supplementary material.
itself, or for the base network. We refer to the latter as
transferred BoW (T-BoW) attacks. In Table 2, we compare
the success rates of different attack strategies on our BoW
model with those on the base network. Note that attacks
directly targeting our BoW model are signiﬁcantly less suc-
cessful than those on the base network. While transferring
the adversarial samples of the base model to our BoW net-
work is more effective than the direct attacks, the success
rates remain lower than on the base model. Note that, as
shown in the supplementary material, the L2norm of the
perturbations was similar for all network settings. This,
we believe, shows that our BoW models are more robust
than traditional networks to adversarial attacks. Neverthe-
less, some attacks are successful, and we now turn to the
problem of detecting them.
4.2.2 Detecting Adversarial Samples
Adversarial detector. We now evaluate the effectiveness of
the detection strategy introduced in Section 3.2. To this end,
we train our detector using adversarial examples generated
by the white-box attacks discussed above. Speciﬁcally, we
report results obtained with two training strategies. Strategy
1, which is commonly used [34], consists of deﬁning a bal-
anced training set comprised of normal images (positives)
and their adversarial counterparts (negatives). A drawback
of this strategy, however, is that, as shown above, many at-
tacks are unsuccessful with our BoW model, and consid-
ering such samples as negatives essentially adds noise to
our training process, since unsuccessful adversarial samples
will typically activate a codeword that is similar to the in-Dataset Model FGM BIM-a BIM-b CW
MNISTBase 93.68 100.0 100.0 100.0
T- Bow 92.70 38.94 100.0 31.40
BoW 39.23 23.96 23.96 3.9
F-MNISTBase 99.64 100.0 100.0 100.0
T- Bow 98.80 65.52 100.0 49.08
BoW 57.78 32.09 31.97 66.78
SVHNBase 97.0 100.0 100.0 100.0
T- Bow 96.83 93.77 100.0 75.24
BoW 73.39 73.11 72.24 86.42
CIFAR-10Base 82.70 97.10 97.10 100.0
T- Bow 82.66 94.12 97.15 76.92
BoW 60.66 94.18 93.93 99.99
Table 2: Success rates of white-box attacks on the base
network and on our BoW network. Note that, whether
attacked directly (BoW) or via the base network (T-BoW),
our model is more robust to adversarial attacks.
put image. To overcome this, we therefore propose Strategy
2, which consists of using only the successful adversarial
examples as negatives during training. The resulting train-
ing set, however, is then imbalanced. The detailed detector
architectures are provided in the supplementary material.
Comparison with the state of the art. We compare our ap-
proach with the state-of-the-art BU [17] and LID [34] meth-
ods, which have proven more robust than the earlier detec-
tion strategy [6]. In Table 3, we report the area under the
ROC curve (AUROC) for BU, LID and our method, using
both Strategy 1 and Strategy 2. In this case, the results were
6
Dataset Feature FGSM BIM-a BIM-b CW
MNISTKD+BU 95.22/94.11 82.54/72.80 82.17/72.43 50.17/60.01
LID 92.66/91.18 82.24/61.90 83.06/75.61 51.81 /68.46
Ours 100.00/100.0 100.0/100.0 100.0/100.0 50.87/ 100.0
F-MNISTKD+BU 99.33/99.36 91.35/87.32 89.43/85.39 69.68/65.83
LID 93.88/93.93 86.95/81.06 86.83/81.11 73.23/70.84
Ours 100.0/100.0 100.0/100.0 100.0/100.0 83.97/97.96
SVHNKD+BU 78.21/74.24 78.37/73.97 67.96/71.46 88.68/88.62
LID 99.88/99.25 83.45/84.17 85.76/90.42 91.23/91.40
Ours 99.9/100.0 98.71/96.16 99.9/100.0 96.52/96.49
CIFAR-10KD+BU 72.79/69.66 86.23/85.69 60.23/62.52 93.74/93.74
LID 89.67/89.26 85.40/85.02 80.55/82.79 93.57/93.17
Ours 99.37/99.97 93.90/97.47 99.90/99.96 96.52/96.35
Table 3: AUROC scores for BU, LID and our detector for
direct BoW attacks. Left and right numbers correspond to
Strategy 1 and Strategy 2, respectively.
Dataset Feature FGSM BIM-a BIM-b CW
MNISTKD+BU 93.73/93.63 86.09/83.09 79.22/79.22 80.64/80.64
LID 99.17/99.15 99.75/99.75 95.16/96.16 99.01/99.03
Ours 100.00/100.0 100.0/100.0 100.0/100 .99.85/100.0
F-MNISTKD+BU 96.99/97.03 92.60/92.60 97.74/97.74 93.27/93.27
LID 95.68/95.68 95.95/95.95 95.24/95.24 97.31/97.31
Ours 100.0/100.0 99.98/100.0 100.0/100.0 98.26 /97.96
SVHNKD+BU 85.04/85.08 88.06/88.06 99.99/99.99 92.85/92.85
LID 99.88/99.32 88.45/84.17 98.76/99.93 94.23/94.23
Ours 99.99 /100.0 96.66/96.25 100.0/100.0 95.61/96.86
CIFAR-10KD+BU 75.75/74.67 80.02/79.69 99.12/99.24 96.06/96.05
LID 87.16/87.92 82.17/81.77 99.91/99.91 97.54 /97.56
Ours 99.80 /99.71 96.77 /96.87 99.77/99.85 96.70/97.24
Table 4: AUROC scores for BU, LID and our detector
for attacks on the base network. Left and right numbers
correspond to Strategy 1 and Strategy 2, respectively.
obtained using the adversarial examples generated with a
white-box attack on our BoW model directly. As such, BU
and LID were also applied to our BoW model. Note that
we outperform the state of the art in all but one case, by
a particularly large margin when the results are not already
saturated, such as with BIM-a on SVHN and CIFAR-10 and
with FGSM on CIFAR-10. The only case where we don’t is
with CW on MNIST, with Strategy 1. Note, however, that,
as shown in Table 2, in this case, CW only has a 3.9% suc-
cess rate. This results in Strategy 1 having access to very
few training samples. By contrast, with Strategy 2, we out-
perform the baselines for the CW attack by a large margin.
In Table 4, we provide similar results for attacks targeted to
the base network. The conclusions are the same: We out-
perform the baselines in most cases, and by a large margin
where there remained room for improvement.
Similarly to [34], we also evaluate whether our detector
trained for one speciﬁc attack generalizes to other ones. In
Table 5, we compare the generalizability of our approach
and of LID. For each method, we show the results of the
model that was trained on the attack that makes it generalize
best to the other ones. Note that the performance of our
detector is virtually unaffected. While LID also generalizes
well, our method still outperforms it in most cases, and by
a large margin in several scenarios.Dataset Method Train FGSM BIM-a BIM-b CW
MNISTLID FGSM 91.18 65.48 64.42 29.05
Ours BIM-a 100.0 100.0 100.0 97.56
F-MNISTLID FGSM 93.88 82.24 82.58 65.03
Ours CW 97.39 97.13 95.85 97.96
SVHN-10LID FGSM 99.25 77.54 79.77 75.16
Ours BIM-a 91.41 96.25 91.30 94.73
CIFAR-10LID FGSM 89.26 66.55 68.33 66.05
Ours BIM-a 86.90 97.47 95.02 95.44
Table 5: Generalizing to different attacks. We compare
the results of LID and our detector in the scenario where
the detectors were trained for a speciﬁc attack, but tested on
different ones. These results were obtained with Strategy 2.
Dataset Attack success rate Detector AUROC
F-MNIST 33.11 96.22
SVHN 94.58 96.54
CIFAR-10 99.95 97.47
Table 6: Adaptive attacks. Our detector remains robust to
an attacker that knows our detection strategy.
4.3. Attacking the Detector
In the previous set of experiments, we have worked un-
der the assumption that the attacker only had access to our
BoW model, but not to the detector. Here, we remove this
assumption, and study two more challenging scenarios. In
the ﬁrst one, the attacker knows our detection strategy, but
not the model we use. In the second, the attacker also has
access to our detector.
4.3.1 Adaptive Attack
To evaluate the robustness of our approach in the scenario
where the attacker is aware of our detection scheme, we ap-
ply an adaptive CW attack strategy similar to the ones used
in [4, 34] to attack the KD & LID detectors, respectively.
To this end, we modify the objective of the CW attack as
arg min
Iadv∥(I−Iadv∥2
2+α·(
ℓ(Iadv)+∥φ(I)−φ(Iadv)∥2
2).
(6)
The ﬁrst two terms correspond to the original CW attack,
withαbalancing the amount of perturbation and the adver-
sarial strength, that is, how strongly one forces the adversar-
ial image to be misclassiﬁed. The last term directly reﬂects
our detection strategy and encourages the BoW representa-
tion of the real, φ(I), and adversarial, φ(Iadv), images to
be similar. The rationale behind this is that the attack then
aims to ﬁnd an adversarial perturbation such that the sam-
ple is not only misclassiﬁed, but also has a representation
close to that of the real image, thus breaking the premise on
which our detector is built.
In Table 6, we report both the success rate of this adap-
tive attack on our BoW model and the AUROC of our de-
7
DatasetClassiﬁer Detector Attack success rate Attack success rate on detector
attack attack on detector trained with adversarial training
MNISTFGSM FGSM 0.00 0.0
CW CW 100.00 8.0
F-MNISTFGSM FGSM 2.8 0.0
CW CW 100 27.2
Table 7: White-box detector attacks. An attacker that has
access to our BoW network, GAN and detector can indeed
be successful when using the CW attack, but not the FGSM
one. However, adversarial training allows us to robustify
our approach to these attacks, as shown in the right column.
tector, trained with Strategy 2. Note that our detector still
yields high AUROC, thus showing that it remains robust to
an attacker that knows our detection strategy.
4.3.2 White-box Detector Attacks
We now evaluate the robustness of our method in the case
where the attacker has access to all our models, that is, the
BoW model, the GAN and the adversarial sample detector.
Note that access to the parameters of the generator and de-
tector networks is not a mild assumption since information
about the training data is required to compute them.
To attack our complete framework, we generate adver-
sarial images Iadvin a two-step fashion. First, we attack
the BoW classiﬁer to generate an intermediate adversarial
image I0
advthat activates codeword jcorresponding to im-
ageVjin the visual codebook. Second, we attack the de-
tector to misclassify Vjas being similar to the input image.
Since our detector relies on the distance between the input
image and the codeword in feature space, fooling it can be
achieved by ﬁnding a perturbation that solves the optimiza-
tion problem
min
IadvI0
adv−Iadv2
2+α·∥γ(Vj)−γ(Iadv)∥2
2,(7)
whereγ(·)is the feature-extraction part of our detector.
We found that, as is, our detector is indeed vulnerable
to such an attack. To circumvent this, we therefore rely on
the adversarial training strategy of [15], in which the detec-
tor is trained using dynamically-generated adversarial im-
ages. We observed that, after a few epochs of such dynamic
training, our detector becomes robust to these white-box at-
tacks. To evidence this, in Table 7, we provide the results
of a white-box attack on a detector dynamically trained on
limited balanced set of 10K samples and evaluated on 1K
samples. These results show that our approach has become
much more robust to this attack.
4.4. Detecting Out-of-distribution Samples
We now evaluate the use of our approach to detect OOD
samples. Following the setup of [21], we perform experi-
ments using either SVHN [40] or MNIST [25] as trainingIn-Dataset In-DatasetAdversarial examples OOD samples
Baseline [21] / Ours
SVHNCIFAR-10 87.44/ 97.26 87.92/ 99.14
LSUN 89.06/ 99.87 89.06/ 99.98
TinyImageNet 89.97/ 99.76 89.97/ 99.93
MNIST-10Not-MNIST 77.11/ 97.44 77.23/ 99.98
OMNIGLOT 82.11/ 97.01 82.24/ 100.0
CIFAR 79.84/ 99.21 79.84/ 100.0
Table 8: Detecting OOD samples. We compare our de-
tector with the baseline method of [21] and with the recent
ODIN [33] one in the case where we only observe adversar-
ial samples during training (left) and when we have access
to a few OOD samples (right).
datasets from which the in-distribution samples are drawn.
The goal then is to detect OOD samples coming from other
datasets, such as LSUN [54], TinyImageNet [26], Om-
niglot [29], Not-MNIST [3]. We consider two settings: In
the ﬁrst, the detection method does not see anyOOD sam-
ples during training, but has access to adversarial examples
generated by the BIM-a attack; in the second, the detector
has access to 1000 images from the OOD dataset. In Ta-
ble 8, we compare the results of our approach with those
of the baseline method [21] and of ODIN [33]. Note that
we clearly outperform them, both when we see OOD sam-
ples during training and in the more realistic case where we
don’t. We believe that this demonstrates the generality of
our approach.
5. Conclusion
We have introduced a novel approach to interpreting a
CNN’s prediction, by providing the elements in a BoW
codebook with a visual and semantic meaning. We have
then proposed to leverage the visual representation of these
interpretable BoW networks for adversarial example detec-
tion. Our experiments have evidenced that (i) our inter-
pretable BoW networks are more robust to adversarial at-
tacks; (ii) our adversary detection strategy outperforms the
state-of-the-art ones; (iii) our approach could be made ro-
bust to adversarial attacks to the detector itself; (iv) our
framework generalizes to OOD sample detection. In the
future, following the intuition that complex scenes and ob-
jects can be modeled as sets of parts, we plan to extend our
interpretable BoW representation to part-based ones. This,
we believe, will also translate to improved adversary detec-
tion performance, since it will allow us to robustly combine
the decision of each part.
6. Conclusion
We have introduced a novel approach to interpreting a
CNN’s prediction, by providing the elements in a BoW
codebook with a visual and semantic meaning. We have
then proposed to leverage the visual representation of these
8
interpretable BoW networks for adversarial example detec-
tion. Our experiments have evidenced that (i) our inter-
pretable BoW networks are more robust to adversarial at-
tacks; (ii) our adversary detection strategy outperforms the
state-of-the-art ones; (iii) our approach could be made ro-
bust to adversarial attacks to the detector itself; (iv) our
framework generalizes to OOD sample detection. In the
future, following the intuition that complex scenes and ob-
jects can be modeled as sets of parts, we plan to extend our
interpretable BoW representation to part-based ones. This,
we believe, will also translate to improved adversary detec-
tion performance, since it will allow us to robustly combine
the decision of each part.
References
[1] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic.
Netvlad: Cnn architecture for weakly supervised place
recognition. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 5297–5307,
2016. 1, 2, 3
[2] R. Arandjelovic and A. Zisserman. All about vlad. In Pro-
ceedings of the IEEE conference on Computer Vision and
Pattern Recognition , pages 1578–1585, 2013. 1
[3] Y . Bulatov. notmnist dataset. 2011. 8
[4] N. Carlini and D. Wagner. Adversarial examples are not eas-
ily detected: Bypassing ten detection methods. In Proceed-
ings of the 10th ACM Workshop on Artiﬁcial Intelligence and
Security , pages 3–14. ACM, 2017. 7
[5] N. Carlini and D. Wagner. Towards evaluating the robustness
of neural networks. In 2017 IEEE Symposium on Security
and Privacy (SP) , pages 39–57. IEEE, 2017. 2, 5
[6] R. Feinman, R. R. Curtin, S. Shintre, and A. B. Gardner.
Detecting adversarial samples from artifacts. arXiv preprint
arXiv:1703.00410 , 2017. 6
[7] P. Felzenszwalb, D. McAllester, and D. Ramanan. A dis-
criminatively trained, multiscale, deformable part model.
InComputer Vision and Pattern Recognition, 2008. CVPR
2008. IEEE Conference on , pages 1–8. IEEE, 2008. 2
[8] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra-
manan. Object detection with discriminatively trained part-
based models. IEEE transactions on pattern analysis and
machine intelligence , 32(9):1627–1645, 2010. 2
[9] P. F. Felzenszwalb and D. P. Huttenlocher. Pictorial struc-
tures for object recognition. International journal of com-
puter vision , 61(1):55–79, 2005. 2
[10] R. Girdhar and D. Ramanan. Attentional pooling for action
recognition. In Advances in Neural Information Processing
Systems , pages 34–45, 2017. 1, 2
[11] R. Girdhar, D. Ramanan, A. Gupta, J. Sivic, and B. Russell.
Actionvlad: Learning spatio-temporal aggregation for action
classiﬁcation. In CVPR , volume 2, page 3, 2017. 2
[12] Y . Gong, L. Wang, R. Guo, and S. Lazebnik. Multi-scale
orderless pooling of deep convolutional activation features.
InEuropean conference on computer vision , pages 392–407.
Springer, 2014. 2[13] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y . Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems , pages 2672–2680, 2014. 1, 5, 11
[14] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining
and harnessing adversarial examples (2014). arXiv preprint
arXiv:1412.6572 . 2, 5
[15] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining
and harnessing adversarial examples (2014). arXiv preprint
arXiv:1412.6572 . 8
[16] K. Grosse, P. Manoharan, N. Papernot, M. Backes, and
P. McDaniel. On the (statistical) detection of adversarial ex-
amples. arXiv preprint arXiv:1702.06280 , 2017. 2
[17] K. Grosse, P. Manoharan, N. Papernot, M. Backes, and
P. McDaniel. On the (statistical) detection of adversarial ex-
amples. arXiv preprint arXiv:1702.06280 , 2017. 6
[18] I. Gulrajani, F. Ahmed, M. Arjovsky, V . Dumoulin, and
A. C. Courville. Improved training of wasserstein gans. In
Advances in Neural Information Processing Systems , pages
5767–5777, 2017. 5, 11, 12
[19] R. Hadsell, S. Chopra, and Y . LeCun. Dimensionality reduc-
tion by learning an invariant mapping. In null, pages 1735–
1742. IEEE, 2006. 4
[20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
770–778, 2016. 11
[21] D. Hendrycks and K. Gimpel. A baseline for detecting
misclassiﬁed and out-of-distribution examples in neural net-
works. arXiv preprint arXiv:1610.02136 , 2016. 2, 8
[22] H. Jegou, F. Perronnin, M. Douze, J. S ´anchez, P. Perez, and
C. Schmid. Aggregating local image descriptors into com-
pact codes. IEEE transactions on pattern analysis and ma-
chine intelligence , 34(9):1704–1716, 2012. 1, 2
[23] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980 , 2014. 5
[24] J. J. Koenderink and A. J. Van Doorn. The structure of locally
orderless images. International Journal of Computer Vision ,
31(2-3):159–168, 1999. 1, 2
[25] A. Krizhevsky and G. Hinton. Learning multiple layers of
features from tiny images. Technical report, Citeseer, 2009.
5, 8
[26] A. Krizhevsky and G. Hinton. Learning multiple layers of
features from tiny images. Technical report, Citeseer, 2009.
8
[27] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classiﬁcation with deep convolutional neural networks. In
Advances in neural information processing systems , pages
1097–1105, 2012. 2
[28] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial exam-
ples in the physical world. arXiv preprint arXiv:1607.02533 ,
2016. 2, 5
[29] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-
level concept learning through probabilistic program induc-
tion. Science , 350(6266):1332–1338, 2015. 8
[30] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE , 86(11):2278–2324, 1998. 5
9
[31] K. Lee, K. Lee, H. Lee, and J. Shin. A simple uniﬁed frame-
work for detecting out-of-distribution samples and adversar-
ial attacks. arXiv preprint arXiv:1807.03888 , 2018. 2
[32] L.-J. Li, H. Su, L. Fei-Fei, and E. P. Xing. Object bank:
A high-level image representation for scene classiﬁcation &
semantic feature sparsiﬁcation. In Advances in neural infor-
mation processing systems , pages 1378–1386, 2010. 2
[33] S. Liang, Y . Li, and R. Srikant. Enhancing the reliability of
out-of-distribution image detection in neural networks. arXiv
preprint arXiv:1706.02690 , 2017. 2, 8
[34] X. Ma, B. Li, Y . Wang, S. M. Erfani, S. Wijewickrema, M. E.
Houle, G. Schoenebeck, D. Song, and J. Bailey. Character-
izing adversarial subspaces using local intrinsic dimension-
ality. arXiv preprint arXiv:1801.02613 , 2018. 2, 5, 6, 7, 11,
12
[35] A. Mahendran and A. Vedaldi. Understanding deep image
representations by inverting them. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 5188–5196, 2015. 1, 2
[36] A. Mahendran and A. Vedaldi. Visualizing deep convolu-
tional neural networks using natural pre-images. Interna-
tional Journal of Computer Vision , 120(3):233–255, 2016.
1, 2
[37] J. H. Metzen, T. Genewein, V . Fischer, and B. Bischoff.
On detecting adversarial perturbations. arXiv preprint
arXiv:1702.04267 , 2017. 2
[38] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deep-
fool: a simple and accurate method to fool deep neural net-
works. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 2574–2582, 2016. 5
[39] K. K. Nakka and M. Salzmann. Deep attentional structured
representation learning for visual recognition. BMVC 2018 ,
2018. 2
[40] Y . Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y .
Ng. Reading digits in natural images with unsupervised fea-
ture learning. In NIPS workshop on deep learning and un-
supervised feature learning , volume 2011, page 5, 2011. 5,
8
[41] A. Nguyen, A. Dosovitskiy, J. Yosinski, T. Brox, and
J. Clune. Synthesizing the preferred inputs for neurons in
neural networks via deep generator networks. In Advances in
Neural Information Processing Systems , pages 3387–3395,
2016. 2
[42] F. Perronnin, J. S ´anchez, and T. Mensink. Improving
the ﬁsher kernel for large-scale image classiﬁcation. In
European conference on computer vision , pages 143–156.
Springer, 2010. 1, 2
[43] P. Quelhas, F. Monay, J.-M. Odobez, D. Gatica-Perez,
T. Tuytelaars, and L. Van Gool. Modeling scenes with lo-
cal descriptors and latent aspects. In Computer Vision, 2005.
ICCV 2005. Tenth IEEE International Conference on , vol-
ume 1, pages 883–890. IEEE, 2005. 1, 2
[44] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. arXiv preprint arXiv:1511.06434 , 2015. 1
[45] J. S ´anchez, F. Perronnin, T. Mensink, and J. Verbeek. Im-
age classiﬁcation with the ﬁsher vector: Theory and practice.International journal of computer vision , 105(3):222–245,
2013. 1, 2
[46] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam,
D. Parikh, D. Batra, et al. Grad-cam: Visual explana-
tions from deep networks via gradient-based localization. In
ICCV , pages 618–626, 2017. 1, 2
[47] J. Sivic and A. Zisserman. Video google: A text retrieval
approach to object matching in videos. In null, page 1470.
IEEE, 2003. 1, 2
[48] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus. Intriguing properties of neural
networks. arXiv preprint arXiv:1312.6199 , 2013. 2
[49] P. Tang, X. Wang, B. Shi, X. Bai, W. Liu, and Z. Tu.
Deep ﬁshernet for object classiﬁcation. arXiv preprint
arXiv:1608.00182 , 2016. 1, 2, 3
[50] L. Torresani, M. Szummer, and A. Fitzgibbon. Efﬁcient ob-
ject category recognition using classemes. In European con-
ference on computer vision , pages 776–789. Springer, 2010.
2
[51] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang,
X. Wang, and X. Tang. Residual attention network for image
classiﬁcation. arXiv preprint arXiv:1704.06904 , 2017. 2
[52] Z. Wang, H. Li, W. Ouyang, and X. Wang. Learnable his-
togram: Statistical context features for deep neural networks.
InEuropean Conference on Computer Vision , pages 246–
262. Springer, 2016. 2, 3
[53] H. Xiao, K. Rasul, and R. V ollgraf. Fashion-mnist: a
novel image dataset for benchmarking machine learning al-
gorithms. arXiv preprint arXiv:1708.07747 , 2017. 5
[54] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba.
Sun database: Large-scale scene recognition from abbey to
zoo. In Computer vision and pattern recognition (CVPR),
2010 IEEE conference on , pages 3485–3492. IEEE, 2010. 8
[55] M. D. Zeiler and R. Fergus. Visualizing and understanding
convolutional networks. In European conference on com-
puter vision , pages 818–833. Springer, 2014. 2
[56] Q. Zhang, Y . N. Wu, and S.-C. Zhu. Interpretable convo-
lutional neural networks. arXiv preprint arXiv:1710.00935 ,
2(3):5, 2017. 2
[57] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Tor-
ralba. Learning deep features for discriminative localization.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 2921–2929, 2016. 2
10
Below, we provide additional details regarding our archi-
tectures and our experimental results.
7. Architectures
As base networks for our models, we used the same ar-
chitectures as in [34]. These architectures are provided in
Table 9 for the MNIST, FMNIST experiments, in Table 10
for SVHN experiments and in Table 11 for the CIFAR-10
experiments. For the detector in our adversarial example
detection approach, we used the architecture shown in Ta-
ble 12 for all datasets, except for CIFAR-10. In this case,
we used a ResNet-20 [20]1due to the higher image variance
and background complexity of this dataset.
We also trained a GAN [13] for MNIST and FMNIST
and a WGAN [18] for SVHN and CIFAR-10. The archi-
tectures of the generator for MNIST and FMNIST are pro-
vided in Table 13 and in Table 15 for SVHN and CIFAR-
10. Similarly, the discriminator for MNIST and FMNIST is
provided in Table 14 and the one for SVHN and CIFAR-10
in Table 16.
Layer Parameters
Convolution + ReLU 5×5×64
Convolution + ReLU 5×5×64
MaxPool 2×2
Dense + ReLU 128
Softmax 10
Table 9: Base network for MNIST, FMNIST.
Layer Parameters
Convolution + ReLU 3×3×64
Convolution + ReLU 3×3×64
MaxPool 2×2
Convolution + ReLU 3×3×128
Convolution + ReLU 3×3×128
MaxPool 2×2
Dense + ReLU 512
Dense + ReLU 128
Softmax 10
Table 10: Base network for SVHN.
8. Additional Results on Normal Samples
We visualize the learned codewords on various datasets
in Fig. 5. We also provide additional visualizations of the
activated visual codewords for normal samples of MNIST,
FMNIST, SVHN and CIFAR-10 in Fig. 6. We further vi-
sualize the images assigned to a particular codeword in
Figs. 7, 8, 9 and 10 for MNIST, FMNIST, SVHN and
1https://github.com/tensorﬂow/models/tree/master/ofﬁcial/resnetLayer Parameters
Convolution + ReLU 3×3×32
Convolution + ReLU 3×3×32
MaxPool 2×2
Convolution + ReLU 3×3×64
Convolution + ReLU 3×3×64
MaxPool 2×2
Convolution + ReLU 3×3×128
Convolution + ReLU 3×3×128
MaxPool 2×2
Dense + ReLU 1024
Dense + ReLU 512
Softmax 10
Table 11: Base network for CIFAR-10.
Layer Parameters
Convolution + ReLU 5×5×32
MaxPool 2×2
Convolution + ReLU 5×5×64
MaxPool 2×2
Fully Connected + ReLU 128
Fully Connected + ReLU 128
Table 12: Detector for MNIST, FMNIST and SVHN.
Layer Parameters
Input noise, z∈R128Nil
Dense + ReLU 128×4×4
Deconvolution + ReLU 5×5×128
Deconvolution + ReLU 5×5×64
Deconvolution + Sigmoid 5×5×1
Table 13: Generator for MNIST and FMNIST
Layer Parameters
Input image, I∈R28×28Nil
Convolution + LeakyReLU, stride=2 5×5×64
Convolution + LeakyReLU, stride=2 5×5×128
Convolution + LeakyReLU, stride=2 5×5×256
Fully Connected + ReLU 1
Table 14: Discriminator for MNIST and FMNIST
Layer Parameters
Input noise, z∈R128Nil
Dense 128×4×4
ResBlock + Up 128
ResBlock + Up 128
ResBlock + Up 128
Convolution + Tanh 3×3×3
Table 15: Generator for SVHN and CIFAR-10 similar to
the ones used in [18]
CIFAR-10 respectively. Finally, the misclassiﬁed normal
11"
2403.05557,D:\Database\arxiv\papers\2403.05557.pdf,"While previous research has explored both local and global approaches to hierarchical label modeling in human activity recognition (HAR), what limitations do these approaches share, and how does this motivate the development of a new approach?","Both local and global approaches rely heavily on prior knowledge of the label hierarchy, potentially neglecting implicit relationships between label nodes. This limitation motivates the development of a new approach that can learn these hidden relationships, leading to a more optimal modeling of label relationships.","4 J. Zuo, H. Hacid
The hierarchical label modeling in previous studies[18] can be either local
or global approaches. Local approaches [1,5,10] build multiple classifiers at each
hierarchical level. However, they basically ignore the rich structural interactions
between nodes at a global scale, i.e., the activities can share common patterns
even though they do not share the same parent nodes. For instance, in Figure 1,
still with hand waving andwalking with hand waving are two activities under
stillandwalking. Though having different parent nodes, they share the same
action of hand waving . In consequence, the local modeling approaches only cap-
ture limited interactions between neighboring activity nodes in the hierarchy
structure. Previous HAR models [8,4,17,15] usually model the hierarchy in this
manner, i.e., training multiple classifiers at different hierarchical levels.
As for global approaches [10,18,3,13], they build a flat-label classifier for all
classes. Therefore, how to integrate the hierarchy information into the model
becomes the research focus of the recent studies, i.e., building a hierarchy-aware
flat-label classifier. Various work has studied the joint modeling of label and data
embeddings in HTC tasks. For instance, authors in [16,12] designed a general-
ized triplet loss with hierarchy-aware margin, which allows differentiating fine
and coarse-label classes. With more considerations on the hierarchical informa-
tion, the work in [18] introduced Prior Hierarchy Information from the training
set, which serves to encode the label structures. The label structure can be ei-
ther encoded by a Bidirectional Tree-LSTM, or a Graph Convolutional Network
(GCN). Consequently, the hierarchy-aware label embedding can be combined
with text embeddings to feed a multi-label classifier. HiMatch [3] further aligns
the text semantics and label semantics, and adopt a similar Triplet loss with a
hierarchy-aware margin to accelerate the computation process.
However, the above-mentioned work, both local and global approaches, heav-
ily relies on prior knowledge of the label hierarchy information. In consequence,
the implicit, hidden relationships between label nodes are usually ignored, lead-
ing to a less optimal modeling of the label relationships.
3 Problem Formulation
In this section, we formulate our research problems on HAR with learnable label
relationship modeling. Table 1 summarizes the notations used in the paper.
Definition 1. (Hierarchical Human Activity). We denote the Hierarchical Hu-
manActivitydataas D={X, L}withasequenceofactivitysets X={X1, ..., X N}
and a sequence of label sets L={l1, ..., l N}. Each label set licontains a set of
classes, belong to either one or more sub-paths in the hierarchy.
As shown in Figure 1, the hierarchical class labels can be formulated as a
graph structure. Therefore, each label set lirepresents the labels passed through
the root node to a terminal node, that can be a leaf or a non-leaf node.
Definition 2. (Hierarchical Human Activity Recognition). Given a data set
D={X, L}, we aim to learn a multi-label classifier ffromD. For an unseen
activity xi, the classifier fcan accurately predict its label set ˆli={ˆyi}m, where
mis the number of labels.
H-HAR: Re-thinking HAR by Modeling Hierarchical Label Relationship 5
Table 1: Notation
Notation Description
D={X, L} Activity and label sets
X={X1, ..., X N}or{x1, ..., x n}A sequence of activity sets X1, ..., X N, sample x1, ..., x n
L={l1, ..., l N}or{l1, ..., l n}A sequence of label sets l1, ..., l N. (Note: multi-label for xi)
N, n Number of label sets, number of samples
EL={e1, ..., e N} Label embeddings
EX={e′
1, ..., e′
N} Data embeddings
G=<V,E> A graph including the vertex and edge sets
φ:X → RdFeature map function, e.g., a linear layer
Θ Model parameters
Definition 3. (Hierarchical Label Embedding). Given a sequence of label sets
L={l1, ..., l N}, we aim to learn a set of hierarchy-aware label embeddings EL=
{e1, ..., e N}, integrating hierarchical features from Lfor each target instance.
Learning hierarchical human activities requires considering not only the fea-
tures of activity data, i.e., data embeddings, but also relationships ( explicitand
implicit) between activities, i.e., label embeddings. We aim to learn a repre-
sentation space Hwhere the raw activity data are embedded and aligned with
learnable label relationships. The learning objective is to minimize the classifi-
cation loss θ=min θ∈ΘL(fθ(X,EL), Y).
4 Our proposals
To handle the aforementioned challenges, we propose H-HAR, a Hierarchy-aware
modelforHARtasks.AsshowninFigure2,H-HARreliesonagraph-basedlabel
encoder and an activity data encoder. The graph-based label encoder extracts
the complex hierarchical relationships between labels, with a predefined label
hierarchy and a learnable graph structure. The data encoder simply builds data
embeddings of input activities. By aligning the hierarchical label and data em-
beddings, H-HAR is able to learn data representations with hierarchy semantics.
4.1 Hierarchy-Aware Label Encoding
In the label hierarchy, the nodes under the same parent node share similar pat-
terns. Unlike previous studies [8,4,17,15] considering only child-parent and child-
child relationships, we consider global node relationships, coming with more dis-
criminative features. Due to the intricate global relationships among the label
nodes, it is natural for us to represent the label hierarchy as a graph.
Definition 4. (Hierarchy as Graph). We define a label graph G=<V,E>to
represent the hierarchical structure among the labels, where V={v1, v2, ..., v N}
denotes the label set with Nnodes, E={(vi, vj)|vi∈ V, vj∈link(i)}indicates
the directed edge connections between viand it’s linked nodes.
6 J. Zuo, H. Hacid
Hierarchy-aware Label Encoder
Label Embedding 𝐸!∈ℝ""×$!Data Embedding𝐸%𝑖∈ℝ""×$!Cross-entropy LossLabel-data alignment lossMulti-label classifierActivity set 𝑋={𝑥&,…,𝑥'}
Label set L={𝑙&,…,𝑙""}
Joint optimizationLearnable graph 𝐴""!""#Pre-defined graph 𝐴""Graph ConvolutionLabel Embeddings𝐸∈ℝ$×""!…Data-label Hierarchical Embedding Sitting
Run withHand WavingStandingWalking withHand WavingRunWalkingActivity Data EncoderLinearLayer
Feature PropagationPropagated data
Linear LayerLinear LayerSupervised contrastive loss
Fig.2: Global system architecture of H-HAR
Inthelabelgraph G,eachactivityisregardedasanode,andcanbeconnected
or disconnected from others. The graph edges represent the node relationships.
We should note that the relationships are not fully decided by a pre-defined la-
bel hierarchy, i.e., edge connections. As aforementioned in Figure 1, the implicit
hidden relationships exist for unconnected nodes in the pre-defined hierarchy.
Therefore, we propose to learn the implicit node relationships via a learnable
hidden graph. The pre-defined and learnable graphs are jointly considered in a
Graph Convolution Network (GCN) [23], to build hierarchy-aware label embed-
dings.
Predefined graph structure In GCNs [7], the adjacency matrix represents
the graph connections or relationships between the nodes. We define Ai,j=
|H(vi)∩H(vj)|
|H(vi)|as the connection weight between node viandvj, where H(v)de-
notes a set of higher level nodes (i.e., all the parent nodes of v). Intuitively,
|H(vi)∩H(vj)|represents the number of shared parent nodes between viand
vj,Ai,jshows the proportion of common ancestors over the node vi. A larger
value of Ai,jrepresents a closer relationship between viandvj.
Let˜A=I+D−1
2AD−1
2∈ RN×Ndenote the normalized adjacency matrix
with self-loops, where Dis the degree matrix representing the degree of each
vertex in the graph. Given a sequence of label set L={l1, ..., l N}, we define the
intermediate label embeddings E=e(L)∈ RN×dlas the input signals, where e
is the embedding function. Then the label embeddings ELintegrating the graph
structural features is defined as the output of a graph convolution layer [7]:
EL=σ(˜AEWp)∈ RN×dc(1)
where σis ReLU activation, Wp∈ Rdl×dcdenotes GCN’s weight matrix.
Learnablegraphstructure Thehiddeninteractionsallowthemodeltoenrich
the label embeddings from a global view (i.e., interacting nodes from different
layers and branches). To capture the implicit connections, as a complement of
H-HAR: Re-thinking HAR by Modeling Hierarchical Label Relationship 7
the predefined graph, we learn a self-adaptive graph, that does not require any
prior knowledge and is learned end-to-end through stochastic gradient descent.
We initialize two random matrices E1,E2∈ RN×df, representing source and
target node embeddings [14]. We define the self-adaptive adjacency matrix as:
˜Aadp=SoftMax (ReLU (E1ET
2)) (2)
E1ET
2shows the dependency weights between source/target nodes. ReLUserves
to filter weak connections. SoftMax is used to normalize the adjacency matrix.
With the predefined graph in Equation 1, we re-define the graph layer as:
EL=σ(˜AEWp+˜AadpEWadp)∈ RN×dc(3)
4.2 Activity Data Encoding
Raw physical activity data is usually represented as Multivariate Time Series,
i.e.,X={x1, ..., x n} ∈ Rn×m×t, where n, m, trepresent number of instances,
sensors and timestamps. In this paper, we focus on the model’s label encoding
behavior. Therefore, aligned with pre-processed data of multiple HAR datasets
(e.g., DaliAc [8], mHealth [2]), we consider X={x1, ..., x n} ∈ Rn×d, where dis
the input feature dimension. More advanced feature extractors on raw data can
be explored and integrated into our framework. This is orthogonal to our work.
Given X∈ Rn×d, we define the intermediate data embedding Ed=e(X)∈
Rn×dx. Following previous work [18], we further introduce a graph-based fea-
turepropagationmoduletoencodelabelhierarchyinformation.Thepropagation
module first reshapes activity features Edto align with the graph node input:
V=EdWres∈Rn×N×dc(4)
where Wres∈Rdx×N×dc. Then the GCNs built in Equation 3 can be employed
to integrate label hierarchical information:
EX=σ(˜AVWp′+˜AadpVW′
adp)∈ Rn×N×dc(5)
Note that ˜Aand˜Aadpare shared graphs between the label and data encoding.
4.3 Label-data Joint Embedding Learning
Label-data alignment Even though the data embeddings are reshaped to
align with the graph node input, there is no explicit matching between data
embeddings and label embeddings, that contains rich label relationships. To this
end, we jointly built label-data embeddings in the representation space to align
data and label semantics. Concretely, we apply the L2 loss between data and
label embeddings:
Lalign=nX
i=1∥φx(EX(i))−φl(EL)∥2(6)
where φxandφlare linear layers to project EX,ELto a common latent space.
8 J. Zuo, H. Hacid
Class-separable Embedding Building The label-data alignment loss only
captures the correlations between activity data and labels, while the label em-
beddings are not clearly separable. To learn class-separable embeddings, we em-
ploy a supervised contrastive loss [6] to the representation space:
Lcon(EX(i),EX(j), Y) =Y∗ ∥φX(EX(i))−φX(EX(j))∥2
+ (1−Y)∗
max 
0,m2− ||φX(EX(i))−φX(EX(j))∥2	
(7)
where m >0is the margin parameter, Y= 1ifli=lj, otherwise Y= 0.
Classification and Joint Optimization As shown in Figure 1, the hierarchy
canbeflattenedformulti-labelclassification.Thedataembedding EXisfollowed
by a linear layer and a sigmoid function to output the probability on label j:
pij=sigmoid (φX(EX(i)))j(8)
Therefore, a binary cross-entropy loss is applied:
Lce=nX
i=1NX
j=1−yijlog (pij)−(1−yij) log (1 −pij) (9)
where yijis the ground truth: yij= 1ifxicontains a label j, otherwise 0.
We jointly optimize the model by combining the label-data alignment loss,
contrastive loss and cross-entropy loss:
L=Lalign +λ1Lcon+λ2Lce (10)
where λ1,λ2are hyperparameters controlling the weight of the related loss.
During inference, we only use the Activity Data Encoder for classification.
5 Experiments
In this section, we validate H-HAR with real-life human activity datasets. The
experiments were designed to answer the following Research Questions (RQs):
RQ 1H-HAR Performance : How does H-HAR compare to other (hierarchical)
models in HAR tasks?
RQ 2Label Encoding Efficiency : How effective is our graph-based label encod-
ing compared to other label modeling methods in HAR?
RQ 3Impact of Joint Optimization : What are the benefits of using multiple
objective functions together in improving HAR model performance?
5.1 Experimental Settings
Dataset Descriptions We choose DaliAc [8] and UCI HAPT [9] as testing
datasets because of their rich label relationships. As shown in Figure 3, the
H-HAR: Re-thinking HAR by Modeling Hierarchical Label Relationship 9
DaliAc dataset contains 13 activities collected by 19 participants. The UCI
HAPT dataset was collected from 30 volunteers, with 6 basic activities and
6 postural transitions. We follow [4,11] as for the data preprocessing and train-
ing/testing split. As both datasets are relatively class-balanced, for simplicity,
we report the average accuracy of all classes in each dataset.
VacuumingRunningSweepingAscending stairsDescending stairsBicycling 50 wattBicycling 100 wattSittingLyingStandingWalkingRope jumpingWashing dishesRestHouseBicycleDaLiAc Base ActivityWalking upstairsWalking downstairsStand-to-sitSit-to-lieSittingLyingStandingWalkingStaticPostural transitionsUCI HAPT Base Activity
Stand--to-lieLie-to-sitSit-to-standLie-to-stand
Fig.3: Predefined label hierarchy in DaLiAc and UCI HAPT.
Execution and Parameter Settings The proposed model is implemented
in PyTorch 1.6.0 and is trained using the Adam optimizer in one single Nvidia
A100 (40G). We set an adaptive learning rate regarding training epochs, i.e., the
learning rate starts from 0.01 and decreases by half every training epoch. We set
the balancing weight λ1=λ2= 1.
Baselines For HAR tasks, much of the existing research adopts local-based
approaches. These typically involve constructing multiple classifiers in a top-
down manner. They can be essentially simplified to a flat classifier model when
not considering the predefined label hierarchy. Therefore, we selected popular
conventional ML models for evaluation, including AdaBoost, kNNs (k=7), SVM,
and Multi-layer Perceptron (MLP) with a Softmax activation function.
It’s important to note that while there are numerous advanced models that
could potentially yield superior HAR performance, our focus is primarily on
examining the impact of label relationship modeling within HAR tasks, rather
than identifying the most advanced model architectures.
Additionally, we assessed the performance of these models both with and
without considering label hierarchy. For the baseline models, we did not in-
corporate any predefined hierarchy. In contrast, for H-HAR, we substituted the
graph-based label encoding layer with a linear layer. We also extended our evalu-
ation to both single-label and multi-label classification tasks to comprehensively
understand the models’ behavior.
5.2 Experimental results
Table 2 presents a comparison of the accuracy of various HAR models, both
with and without considering label hierarchy (denoted as w/o H. and w/ H.
10 J. Zuo, H. Hacid
respectively). With advanced label-data embedding learning and joint classifier
building, it is not surprising that H-HAR shows superior performances of others.
However, the results offer several key insights:
–Robustness in Multi-label Classification ( RQ 1): While there is a general
decline in model performance for multi-label classification tasks, H-HAR
exhibits a relatively small decrease compared to other baseline methods.
This suggests that H-HAR is robust in differentiating between parent and
child node classes.
–Improvement with Predefined Label Hierarchy: The introduction of a pre-
defined label hierarchy significantly enhances the performance of baseline
models, particularly noted in the SVM on the DaLiAc dataset with an im-
provement of over 30%. As illustrated in Figure 3, building classifiers at each
layereffectivelyreducesthelearningcomplexitybyleveragingrichpriorlabel
knowledge.
–SuperiorityofNeuralNetwork-BasedApproaches:Neuralnetwork-basedmod-
els generally outperform traditional ML models in this context, where the
data is straightforward, and the feature space is limited. Exploring more
advanced network architectures could further augment the model’s perfor-
mance, which is orthogonal to this work.
However, due to a larger parameter space, H-HAR performs less efficient than
MLP,taking39sforonetrainingepoch,comparedto12sforMLP.Conventional
ML models are not compared on efficiency as they are running on CPU.
Table 2: Accuracy (%) comparison between models w/o or w/ label hierarchy
AdaBoost kNN SVM MLP H-HAR
Dataset Classifier w/o H. w/ H. w/o H. w/ H. w/o H. w/ H. w/o H. w/ H. w/o H. w H.
DaLiAcsingle-label 80.0 86.64 68.71 85.48 54.13 87.12 88.92 94.62 91.64 97.43
multi-label 76.28 83.34 64.53 76.32 52.34 82.34 88.32 92.43 90.98 97.23
UCI HAPTsingle-label 88.96 92.39 75.62 88.92 89.26 94.25 90.54 96.77 95.45 97.98
multi-label 84.23 89.23 72.43 84.34 87.23 92.34 90.23 95.88 94.32 97.82
5.3 Ablation study
To understand why our model performs effectively, we conduct ablation studies
on various parameters that might impact or enhance the model’s performance.
Specifically, as detailed in Table 3, we examine several H-HAR variants:
–Label Hierarchy
•None: replace the graph modeling layer in Equation 3 with a linear layer;
•ˆA: only use the predefined label hierarchy for label modeling;
•ˆAadp: only employ a learnable graph-based label modeling.
–Feature Propagation (None): replace Feature Propagation by a linear layer
–Objective Function
•Lalign+Lce: label-data alignment loss with cross-entropy loss;
H-HAR: Re-thinking HAR by Modeling Hierarchical Label Relationship 11
Table 3: Ablation study: model accuracy (%) w.r.t. various parameters
Label Hierarchy Feat. Propag. Objective FunctionH-HARDataset Classifier None ˆA ˆAadp None Lalign+LceLcon+LceLce
DaLiAcsingle-label 91.64 94.23 97.69 96.23 94.32 97.33 94.42 97.43
multi-label 90.98 94.12 97.21 95.67 93.23 96.59 92.38 97.23
UCI HAPTsingle-label 95.45 97.32 98.20 97.45 97.28 97.89 96.73 97.98
multi-label 94.32 97.24 97.12 97.12 96.52 97.65 95.72 97.82
•Lcon+Lce: contrastive loss with cross-entropy loss;
•Lce: only cross-entropy loss.
Fromthe results, weobservethat i)The modelperforms betterin single-label
classification with just the learnable graph than when combined with a prede-
fined label hierarchy. This suggests that learning relationships directly from data
can be more effective than using pre-set connections ( RQ 2); ii) Adding feature
propagation improves the model’s performance. This likely happens because it
helps align data better with the graph’s structure; iii) The biggest boost in per-
formance comes from supervised contrastive learning, which helps build class-
separable embeddings. Joint optimization of these techniques also helps enhance
the model’s overall effectiveness ( RQ 3).
6 Discussions and Conclusion
Modeling and integrating label relationships into HAR models allows regular-
izing the representation space, thus building better feature embeddings. The
proposed H-HAR brings multiple research opportunities, which are not fully
addressed in the paper: i) the hierarchy-aware label modeling allows us to han-
dle data with heterogeneous-granular labels, leading to less effort and better
flexibility in practice for data annotations; ii) the contrastive learning can be
further explored in the context of label relationship modeling. For instance, a
hierarchy-aware margin parameter can be investigated [3]; etc.
Conclusion In this work, we propose H-HAR and rethink Human Activity
Recognition (HAR) tasks from a perspective of graph-based label modeling.
The proposed hierarchy-ware label encoding can be seamlessly integrated into
other HAR models to improve further models’ performance. For future work,
one can be exploring more complex data with a deeper hierarchy and intricate
label relationships. Human activities with multi-modality will also be one of the
research directions in the future.
References
1. Banerjee,S.,Akkaya,C.,Perez-Sorrosal,F.,Tsioutsiouliklis,K.:Hierarchicaltrans-
fer learning for multi-label text classification. In: ACL. pp. 6295–6300 (2019)
2. Banos, O., Garcia, R., Saez, A.: MHEALTH Dataset. UCI Machine Learning
Repository (2014), DOI: https://doi.org/10.24432/C5TW22"
2106.14647,D:\Database\arxiv\papers\2106.14647.pdf,"How can the use of explainable AI techniques, such as SHAP and LIME, contribute to the development of more robust and adaptable cybersecurity systems, particularly in the context of detecting and classifying novel attack types?","Explainable AI techniques like SHAP and LIME can help generate human-understandable explanations for model predictions, enabling security analysts to understand the reasoning behind attack classifications. This allows for the development of more robust systems that can adapt to new attack types by providing insights into the features that characterize them.","4 attack should be intuitive for an analyst to understand the type of attack. Then we could map the 
auto generated attack names to the act ual attack names by taking feedback from the security 
analyst. We trained  an isolation forest anomaly detection algorithm on the NSL -KDD dataset. This 
algorithm  ‘isolates’ observations by recursively splitting the data into parts based on a random 
threshold value  till points are isolated. Anomalies are points with shortest splits since they are 
different from rest. We see that the model gets an acceptable f1 score of 0.9 2 and 0.9 1 on normal 
and attack labels as shown in the table below.  
 
Fig. 3. Classification report for Isolation Forest on  NSL-KDD training data 
4 GENERATING EXPLANATIONS  
SHAP [10] summary plot is global explanation of a model which combines feature importance with 
feature effect. Shapely value for a feature and particular sample is re presented by a point on a 
summary plot. Features are on Y axis and shapely values are on X -axis. Colors are used to represent 
low/high values. Features are arranged according to their importance, top feature in the summary 
plot is most important whereas bo ttom one is the least.  
 
Fig. 1. SHAP summary plot for isolation forest model  

5 SHAP global explanations are drawn by considering complete/partial dataset. SHAP local 
explanations consider only specific instance at a time and generated explanation, it shows which 
feature values are taking decision towards positive and which are taking towards negative. Figure 
6 shows local explanation where probability of the output being attack is 1.00 and features along 
with their values are shown below such as ‘dst_host_same_srv_rate’, ‘same_srv_rate’, 
‘service_private’ and so on. Features pushing the prediction higher are shown in red and those 
pushing predictions to lower are shown in blue. Note that these explanation changes as we 
chan ge the input instance.  
 
Fig. 2. SHAP force plot used for local explanations to explain a particular instance where output probability 
of ‘attack’ is 1 and shows features contributing in decision  
We have trained a ML model to identify if the network traffic is ‘normal’ or ‘attack’. We see from 
SHAP values that distinct attack types show different dependencies on features while predicting 
attack. We will use a BRCG [8] algorithm that can summarize the model prediction by using  rules. 
We use BRCG to extract rules out of the data as follows:  
 
Predict Y=1(Attack) if ANY of the following rules are satisfied, otherwise Y=0 (Normal):  
· wrong_fragment > 0.00  
· src_bytes <= 0.00 AND dst_host_diff_srv_rate > 0.01  
· dst_host_count <= 0.0 4 AND protocol_type_icmp  
· num_compromised > 0.00 AND dst_host_same_srv_rate > 0.98  
· srv_count > 0.00 AND protocol_type_icmp AND service_urp_i not  
 
Performance of the BRCG algorithm:  
Training Accuracy = 0.9823  
Test Accuracy = 0.7950  
Which means by applying just these rules one can get ~80% accurate results on unseen test data. 
We can generate these rules with predictions and have these verified with security domain 
experts to make sure our model has captured the domain correctly. However, to get to a zero -

6 shot capability we need to generate labels for detected anomalies using explanations. For this we 
will explore local explanations that try and explain individual predictions. We will use a library 
called LIME (Local Interpretable Model -Agnostic  Explanations) [7] for this.  
 
Fig. 3. Explaining individual prediction of deep learning classifier using LIME  [7] 
LIME [7] generates local explanations. In the following figure explanation is shown to determine if 
classification  result is ‘Normal’ or ‘Attack’ along with probability and original instance values. 
Colors are used to highlight which features contributes to which class. Features in orange colour 
contributes to ‘attack’ and blue contributes to ‘normal’ category.  
 
Table 1a. Similar instances predicted as attack 1b. Use of weights to show similarity  
The person who takes final decision based on model’s output can get understanding of model’s 
decision if we show instances from the training dataset which are similar in dif ferent ways to test 
instance we want to understand. We considered first instance from test dataset for which model 
prediction is ‘attack’. Table 3a shows similar instances from training data, similarity is indicated by 
the weight mentioned in last row.  

7 It also provides human friendly explanations showing feature values in terms of weight. More the 
weight, more the similarity. Above two tables - table 3a and table 3b represent five closest 
instances to the test instance. Based on the weights mentioned, we can see that instance under 
column 0 is the most representative of test instance as weight it 0.93. These tables would help 
the analyst to take final decision more confidently.  
For end users, ML models should be transparent. They should get answers to thei r all queries 
such as why model made certain decision, which factors led to this decision, by making what 
changes model’s decision can be changed etc. CEM algorithm helps us to answer all these end 
user questions.  
We considered one particular instance wher e prediction made was ‘normal’, CEM shows us how 
decision can be changed by making minimal changes in the feature values.  
CEM can also highlight minimal set of features along with their values that would maintain the 
prediction made by the model. Looking a t the statistics of explanations given by the CEM algorithm 
over bunch of applicants, one can get insight into what minimal set of features play important role. 
It is also possible to get values of these features for every type of attack.  
 
Table 2. Perti nent negative and pertinent positives for an instance  
 
5. CONCLUSIONS  
Figure 6 below shows the SHAP force plot for group of points from test dataset. We combined 50 
points from each category - normal and 3 types of attacks and plotted a force plot for this  shown 
below. This is basically created by taking multiple force plots for a single instance (shown in figure 
5) rotating them by 90 degrees and stacking them horizontally. We see a clear separation on type 
of attack defined by the explanations.  We see tha t between the isolation forest anomaly detector 
is able to separate between normal and anomalous points. Then using SHAP explainer we can auto -
assign labels to the predicted labels using combination of features that appear as important. The 
SHAP scores sho w a distinct pattern of explanations for each type of anomaly.  

8  
Fig. 4. SHAP force plot for 4 types of data points in NSL -KDD dataset  
 
We combined data in NSL -KDD for normal traffic flow and 3 types of attacks and plotted a force 
plot for this shown below is figure 6. We see a clear separation on type of attack defined by the 
explanations. Prominence of blue values indicate tendency towards prediction of 0 indicating a 
normal traffic flow. When red values are promin ent it indicates an attack and using explainability 
we see the features that are more influencing the value being red or 1 – indicating attack. The red 
patterns for attacks also show a clear separation based on type of attacks. This gives us confidence 
that the labels generated will have enough uniqueness to satisfy a zero -shot learning requirement.  
Now we can auto -generate labels indicating a type of attack. Table 1 below shows the auto -
generated labels corresponding to above attack types. We selected 5 ra ndom points for each attack 
and show the actual and auto -generated label.  
 
 
Table 1.  Mapping the actual and auto -generated labels for few random data points  
We see from above that the auto -generated label is uniquely mapped to actual label – except for 
a case for portsweep. Even though there are multiple auto -generated labels for a unique attack, 
we could allow multiple mappings to the same attack in our learning system. For the portsweep 
ACTUAL LABEL AUTO-GENERATED LABEL
guess_passwd dst_host_rerror_rate-hot-service
guess_passwd dst_host_rerror_rate-hot-service
guess_passwd dst_host_rerror_rate-hot-service
guess_passwd dst_host_rerror_rate-hot-service
guess_passwd dst_host_rerror_rate-hot-service
portsweep dst_host_same_srv_rate-service-src_bytes
portsweep dst_host_same_srv_rate-service-src_bytes
portsweep dst_host_same_src_port_rate-service-src_bytes
portsweep dst_host_same_srv_rate-service-src_bytes
portsweep dst_host_same_srv_rate-service-src_bytes
portsweep dst_host_same_srv_rate-service-src_bytes
warezmaster dst_host_count-dst_host_rerror_rate-src_bytes
warezmaster dst_host_count-dst_host_rerror_rate-src_bytes
warezmaster dst_host_count-dst_host_rerror_rate-src_bytes
warezmaster dst_host_count-dst_host_rerror_rate-src_bytes
warezmaster dst_host_count-dst_host_rerror_rate-src_bytes
9 example above both the generated labels – “dst_host_same_srv_rate -service -src_bytes” and 
“dst_host_same_src_port_rate -service -src_bytes” have a domain meaning which a security 
analyst can understand. This is particularly helpful for new and unseen attacks since we could 
auto generate labels like this in a zero -shot learnin g setting and make our cybersecurity systems 
more adaptive to novel attack types.  
NSL-KDD is a pretty old and exhausted dataset in cybersecurity with all the attacks known and 
labelled. We are working with a major cybersecurity provider to build this auto -labelling system 
on a real -world product which sees unknown attacks and is used in a dedicated security 
operations center (SoC). This will help us evaluate the true potential of this technology and how it 
can help improve the cyber defense capability.  
 
REFERENCES  
1. Google Colab Notebook with reproducible code for this paper.  
https://colab.research.google.com/drive/13OIwHn6wdCaJNCDE5Ze9FR7PfFmd9WOu?usp=sharing   
2. M. Tavallaee, E. Bagheri, W. Lu, and A. Ghorbani, “A Detailed Analysi s of the KDD CUP 99 Data 
Set,”  Submitted to Second IEEE Symposium on Computational Intelligence for Security and Defense 
Applications (CISDA) , 2009.  
3. D. L. Marino, C. S. Wickramasinghe and M. Manic, ""An Adversarial Approach for Explainable AI in 
Intrusion D etection Systems,""  IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics 
Society , Washington, DC, 2018, pp. 3237 -3243, doi: 10.1109/IECON.2018.8591457.  
4. L. Santos, C. Rabadao, and R. Goncalves, ‘‘Intrusion detection systems in Internet of T hings: A 
literature review,’’ in Proc. 13th Iberian Conf. Inf. Syst. Technol. (CISTI), Jun. 2018, pp. 1 –7. 
5. G. Serpen and E. Aghaei, ‘‘Host -based misuse intrusion detection using PCA feature extraction and 
kNN classification algorithms,’’ Intell. Data Anal. , vol. 22, no. 5, pp. 1101 –1114, Sep. 2018.  
6. Chen, T. & Guestrin, C., 2016. XGBoost: A Scalable Tree Boosting System. In  Proceedings of the 22nd 
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . KDD &#x27;16. 
New York, NY, USA: ACM, pp. 785 –794. Available at: http://doi.acm.org/10.1145/2939672.2939785.  
7. Knowledge discovery in databases DARPA archive. Task Description. 
http://kdd.ics.uci.edu,/databases/kddcup99/ta sk.html   
8. Ribeiro, Marco & Singh, Sameer & Guestrin, Carlos. (2016). “Why Should I Trust You?”: Explaining 
the Predictions of Any Classifier. 97 -101. 10.18653/v1/N16 -3020.  
9. O. G. D. W. Sanjeeb Dash, ""Boolean Decision Rules via Column Generation,"" in Advances  in Neural 
Information Processing Systems 31 , 2018.  
10. Sweetviz is an open source Python library that generates beautiful, high -density visualizations to 
kickstart EDA https://github.com/fbdesignpro/swee tviz 
11. SHAP (SHapley Additive exPlanations)  is a game theoretic approach to explain the output of any 
machine learning model. https://github.com/slundberg/shap   
12. Bukovcikova, Zuzana & Sopiak, Dominik & Oravec,  Milos & Pavlovicova, Jarmila. (2017). Face 
verification using convolutional neural networks with Siamese architecture. 205 -208. 
10.23919/ELMAR.2017.8124469.  
13. Koch, Gregory R.. “Siamese Neural Networks for One -Shot Image Recognition.” (2015).  
14. Shou -Ching Hsi aoa, Da -Yu Kaob, Zi -Yuan Liua, Raylin Tsoa, “Malware Image Classification Using 
One-Shot Learning with Siamese Networks” (2019).  
10 15. Gunning, David. (2019). DARPA's explainable artificial intelligence (XAI) program. IUI '19: Proceedings 
of the 24th Internation al Conference on Intelligent User Interfaces. ii -ii. 10.1145/3301275.3308446.  
16. Dattaraj Rao, Shruti Mittal and Ritika S, “Siamese Neural Networks for One -shot detection of Railway 
Track Switches” arXiv:1712.08036  
17. Pushpankar Kumar Pushp, Muktabh Mayank Sriva stava, “Train Once, Test Anywhere: Zero -Shot 
Learning for Text Classification”  arXiv:1712.05972  
18. Brown, Tom & Mann, Benjamin & Ryder, Nick & Subbiah, Melanie & Kaplan, Jared & Dhariwal, 
Prafulla & Neelakantan, Arvind & Shyam, Pranav & Sastry, Girish & Aske ll, Amanda & Agarwal, 
Sandhini & Herbert -Voss, Ariel & Krueger, Gretchen & Henighan, Tom & Child, Rewon & Ramesh, 
Aditya & Ziegler, Daniel & Wu, Jeffrey & Winter, Clemens & Amodei, Dario. (2020). Language 
Models are Few -Shot Learners.   
19. Gunning, David. (20 19). DARPA's explainable artificial intelligence (XAI) program. IUI '19: 
Proceedings of the 24th International Conference on Intelligent User Interfaces. ii -ii. 
10.1145/3301275.3308446.  
20. Xu, Feiyu & Uszkoreit, Hans & Du, Yangzhou & Fan, Wei & Zhao, Dongyan & Zhu, Jun. (2019). 
Explainable AI: A Brief Survey on History, Research Areas, Approaches and Challenges. 10.1007/978 -
3-030-32236 -6_51.  
21. Hadi, Ali. (2019). Using Zeek for Network Investigations. 10.13140/RG.2.2.26634.11201.  
cing "
1801.04492,D:\Database\arxiv\papers\1801.04492.pdf,"The paper describes a method for deriving explicit convergence rates for optimization algorithms using the Integral Quadratic Constraint (IQC) framework.  What specific challenges are encountered when applying this framework to derive convergence rates for algorithms with a large number of free variables, and how are these challenges addressed in the paper?","The paper highlights the difficulty of finding an appropriate ansatz (a set of initial assumptions) when dealing with algorithms with many free variables. To address this, the authors simplify the problem by fixing certain variables and using numerical experiments to identify patterns and relationships between the remaining variables, which then guide the derivation of the ansatz.","condition for positive deﬁnitiveness known as Sylvester’s
criterion [17].
Replacing (12)–(17) in P′, the ﬁrst minor is given by
f=κω2
β(κ−1)>0. (33)
The second minor is⏐⏐⏐⏐d e
e f⏐⏐⏐⏐=β(κ−1)(β(−κ) +β+ 2κω)
κ2, (34)
whose sign is dictated by β(−κ) +β+ 2κωand which, by
substituting (25)–(26), becomes
β(−κ)+β+2κω=(κ−1)(
2κ+√2κ−1−3)
2(
κ+√2κ−1)>0,(35)
sinceκ≥1.
The third minor is just the determinant of P′, which is
1
ωβ3(1
κ−1)3(ω−1) +β2(1
κ−1)2(1
κ+ 3ω−5)
+ 2β(1
κ−1)
ω(1
κ+ω−3)
−1
2ω(
−1
κ+ω+ 1)2.(36)
We can use (11) to simplify this expression to
β2(κ−1)2((ω−1)(β(−κ) +β+κω) +ω)
κ3ω, (37)
whose sign is dictated by (ω−1)(β(−κ) +β+κω) +ω. If
we substitute (25)–(26) we obtain
(ω−1)(β(−κ) +β+κω) +ω=(κ−1)(√2κ−1−1)
2κ(
κ+√2κ−1)
>0
(38)
sinceκ>1.
We now provide our main result, which directly follows
from our previous theorems and a simple rescaling argument.
Theorem 7. Letf∈Sp(m,L)andκ=L/m≥1. Consider
Algorithm 1 to solve the optimization problem (1). Ifα=1
L
andβ=2κ−√2κ−1−1
2(κ+√2κ−1), then
∥xt−x∗∥≤C0C1τt, (39)
whereC0=√
∥x1−x∗∥2+∥x0−x∗∥2,C1>0is a
function ofκ, and
τ=√
1−√2κ−1
κ. (40)
Proof. We can assume, without loss of generality, that κ>1.
The caseκ= 1 follows by a continuity argument, applying a
small quadratic perturbation to fand letting the perturbation
converge to zero.
The convergence rate of Algorithm 1 on fwithα= 1/Lis
the same as its convergence rate on ˆf=f/L∈Sp(m,1)with
α= 1. In this setting, Theorem 3 and Theorem 6 tell us that
the conditions to apply Theorem 2 hold for our choice of α
andβ. Furthermore, according to Theorem 5, for this choice
ofαandβ, the convergence rate τsatisﬁes (40).IV. T HE PATHWAY TOWARDS THE PROOF
The reader might have noticed that our previous proofs
amount to substituting expressions into conditions and sub-
sequently checking that these conditions are satisﬁed. It is
enlightening to explain how we obtained these expressions in
the ﬁrst place. Speciﬁcally, how did we obtain (12)–(17) from
which all other formulas follow? In a nutshell, we built our
ansatz based on numerical experimentation. Reveling this path
might be useful for other researchers to use the IQC framework
to derive explicit formulas for other algorithms as well.
First, we reduce the number of variables in the problem by
settingλ= 1,ρ=τandα=L= 1.
Second, we ﬁx β > 0andκ∈(0,1), and use a convex
optimization solver to numerically ﬁnd the smallest τfor
which (9) is satisﬁed under the assumption that P≻0. Let
Hbe the right hand side of (9) multiplied by −1. To ﬁnd this
τ, we start with τ= 0.5and check if the SDP
min
P1 s.t.H⪰0andP⪰0 (41)
has a feasible solution2. In the afﬁrmative case, we reduce
τ, otherwise we increase τ. Notice that the eigenvalues of H
increase monotonically with τ. Hence, we can use bisections to
ﬁnd the smallest possible τin a few steps. After this procedure
is done, we check if P≻0. If this does not hold, we try a
differentβand/orκ.
Third, we repeat this procedure for several pairs of (β,κ).
For each pair, we obtain numerical values for PandHsuch
thatH⪰0andP≻0hold. From these numerical values, we
try to identify some very simple properties that HorPmight
satisfy for all tested values of βandκ. Labeling the entries
ofPas in Theorem 3, the properties that we can easily guess
based on our numerical experiments are the following:
1) Recall that P=P⊤=[a b c
b d e
c e f]
. Then,
e=ω−d, (42)
d=β(1−m). (43)
2) Let ∆ibe the principal minor of Hobtained by remov-
ing theith row and column. We observe ∆i= 0 for
i= 1,..., 4;
3) Let ∆1,2;1,2be the principal minor of Hobtained from
removing the 1st and 2nd column/row from H. We
observe that ∆1,2;1,2= 0.
Fourth, we replace (42) and (43) into Hand we solve the
condition ∆1= 0 fora. This leads to
a=1
ω(−β+ 2cω−fω+βm+ 2ω). (44)
We substitute this expression into Hand solve ∆3= 0 forb,
yielding
b=1
2((2β+ 1)(m−1) +ω). (45)
2Note that the standard formulation of convex optimization problems, and
existing solvers, does not allow us to enforce P≻0. This is why we enforce
P⪰0and later check if P≻0.
Again, we substitute this expression in Hand solve ∆4= 0
forc, obtaining
c=1
z(x±√y), (46)
where
x=−2β(m−1)ω((β+ 1)(m−1)−f)
−(2β+ 1)(m−1)ω2+ω3, (47)
y=ω(−4β2(m−1)2(ω−2)−4β(m−1)ω(m+ω−3)
+ω(−m+ω+ 1)2)(βf(m−1) +ω2),(48)
z= 2β(m−1)ω. (49)
We substitute the expression for cwith+sign inHand solve
∆1,2;1,2= 0 forf. This leads to
f=−ω2
β(m−1). (50)
Finally, we eliminate f,dandcfrom equations (42), (44)
and (46). This leads to (12)–(17), observing that m=s=κ−1
whenL= 1. Note that (11) can be obtained from (12)–(17)
by forcingH⪰0(see the proof of Theorem 3).
V. N UMERICAL RESULTS AND DISCUSSION
We ﬁrst note that our optimal choice for βin (25) is
numerically very close, but not equal, to Nestervo’s choice
in (2); see Figure 3 (left). Our convergence rate for NAM
is almost indistinguishable to τLGin Figure 1, and it is
indistinguishable from the curve obtained by running the
Matlab code of [7] for the plot of τLGwith our optimal choice
ofαandβ. However, plotting τLGfor the choice in (7) gives a
numerical rate that is better than the one derived in this paper;
see Figure 3 (right). This shows that we have not extracted
the best possible convergence rate for NAM from the IQC
framework. Indeed, we assumed that ρ=τandα= 1/L
which might be suboptimal. We did so because we were unable
to ﬁnd an ansatz without restricting αorρ. There are too many
free variables to perform closed form calculations, e.g. could
not solve some of the resulting polynomial equations.
We know that any bound produced by the IQC-framework
must be above or equal to τLQin Figure 3. It is an important
open question to know what is the best possible bound that
the IQC-framework can produce. Can it reach τLQ?
VI. C ONCLUSION AND FUTURE WORK
We have derived a new, improved, and explicit convergence
rate of Nesterov’s accelerated method for strongly convex
functions. Our numerical experiments using the IQC frame-
work [7] show that our results can be further improved. Future
work should include deriving better and explicit convergence
rates using the IQC framework, and demonstrating that these
cannot be improved. It would also be important to know if
IQC allows us to prove the best possible upper bound on
the convergence rate of Nesterov’s method. To do so, one
would have to produce a family of “bad” functions for which
the convergence rate of Nesterov’s method matches the rate
obtained from IQC.
10010110210300.20.40.60.81κβ=eq. (2)
β=eq. (25)
10010110210300.20.40.60.81
κτNGτLQτ=eq. (26)τLGwith
α,β as in
in eq. (7)
Fig. 3. Left: There is a very small difference between the standard choice
forβgiven in (25) and our optimal choice of βin (2). Right: It is possible
to obtain better rates than the one we derived in this paper if we choose α
andβas in (7).
ACKNOWLEDGMENT
This work was partially funded by NIH/1U01AI124302 and
NSF/IIS-1741129.
REFERENCES
[1] A. Beck and M. Teboulle, “A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems,” SIAM. J. Imaging Sciences ,
vol. 2, pp. 183–202, 2009.
[2] S. Becker, J. Bobin, and E. J. Cand `es, “NESTA: A Fast and Accurate
First-Order Method for Sparse Recovery,” SIAM J. Imaging Sci. , vol. 4,
pp. 1–39, 2011.
[3] J. A. Tropp, J. N. Laska, and M. F. Duarte, “Beyond Nyquist: Efﬁ-
cient Sampling of Sparse Bandlimited Signals,” IEEE Transactions on
Information Theory , vol. 56, pp. 520–544, 2010.
[4] “An accelerated gradient method for trace norm minimization.”
[5] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, “On the importance of
initialization and momentum in deep learning,” International Conference
on Machine Learning , vol. 28, no. 3, pp. 1139–1147, 2013.
[6] Y . Nesterov, Introductory Lectures on Convex Optimization: A Basic
Course . Springer, 2013.
[7] L. Lessard, B. Recht, and A. Packard, “Analysis and Design of Opti-
mization Algorithms via Integral Quadratic Constraints,” vol. 26, no. 1,
pp. 57–95, 2016.
[8] G. Franc ¸a and J. Bento, “An Explicit Rate Bound for Over-Relaxed
ADMM,” IEEE International Symposium on Information Theory (ISIT) ,
pp. 2104–2108, 2016.
[9] Z. Allen-Zhu and L. Orecchia, “Linear Coupling: An Ultimate Uniﬁca-
tion of Gradient and Mirror Descent,” 2014.
[10] Y . Nesterov, “Smooth minimization of non-smooth functions,” Mathe-
matical Programming , vol. 103, pp. 127–152, 2005.
[11] W. Su, S. Boyd, and E. Cand `es, “A differential equation for modeling
Nesterov’s accelerated gradient method: theory and insights,” Journal of
Machine Learning Research , vol. 17, no. 153, pp. 1–43, 2016.
[12] N. Flammarion and F. Bach, “From Averaging to Acceleration, There
is Only a Step-size,” Conference on Learning Theory , vol. 40, 2015.
[13] Y . Arjevani, S. Shalev-Shwartz, and O. Shamir, “On lower and upper
bounds in smooth and strongly convex optimization,” Journal of Ma-
chine Learning Research , vol. 17, no. 126, pp. 1–51, 2016.
[14] S. Bubeck, Y . T. Lee, and M. Singh, “A geometric alternative to
Nesterov’s accelerated gradient descent,” 2015.
[15] F. Zhang, The Schur complement and its applications . Springer Science
& Business Media, 2006, vol. 4.
[16] A. Ben-Israel and T. N. Greville, Generalized inverses: theory and
applications . Springer Science & Business Media, 2003, vol. 15.
[17] C. D. Meyer, Matrix analysis and applied linear algebra . Siam, 2000,
vol. 2."
2101.10025,D:\Database\arxiv\papers\2101.10025.pdf,How do the authors of this paper suggest that graph neural networks (GNNs) can be used to improve the accuracy of transformer fault diagnosis compared to traditional methods?,The authors propose using spectral-based GCNs to improve transformer fault diagnosis by representing similarity metrics between samples as an adjacency matrix and using graph convolutional layers to capture the complex nonlinear relationship between dissolved gas data and fault types.,"TABLE  I 
MAIN CHARACTERISTICS OF VARIOUS PARADIGMS  
Paradigms  Variant s Main function  Ref 
GCNs  Spectral -based  Nets Extracting  latent 
representation  of the 
graph -structured data  [20],[24]-
[27] 
Spatial -based Nets  [29]-[32] 
GRNNs  Graph GRUs  Solving  the long -term 
dependencies of the 
graph -structured data  [39] 
Tree LSTM  [40] 
GATs  GAT  Incorporating  attention 
mechanism s to the 
propagation step  of 
GNNs  [43] 
GaAN  [44] 
GGNs  GAEs  Genera ting new 
graph -structured data  [46] 
GVAEs  [47] 
GGANs  [48] 
STGNNs  RNN -based  Nets  Learning hidden patterns 
from spatial -temporal 
graphs  [49]-[53] 
CNN -based  Nets  [54]-[56] 
Hybrid 
forms of 
GNNs  GRL  Improving control and 
perception of 
graph -structured data  [59]-[61] 
GTL  Transferring of 
knowledge  in graph 
domains  [62]-[64] 
B. Graph Convolutional Networks  
This section will discuss GCNs that are generalized from 
Euclidean domain s to graph domain s. The existing GCNs 
mainly include  two categories: spectral -based  GCNs  and 
spatial -based  GCNs  [24]. Since both categories of GCNs  have a 
large number of variants, only a few classic models are listed to 
illustrate the principle and str ucture.  
1) Spectral -based  GCNs . Unlike the images in Euclidean 
domain s, the graph -structured  data does not have the 
characteristics of translation invariance, which makes it hard to 
directly define the convolution al operation in  graph domain s. In 
2014,  a spectral network was proposed in  [20]. It transforms  the 
samples  into the Fourier  domain s to perform convolution al 
operations through Fourier transform, and then th e samples are 
transformed back to the graph domain s through inverse Fourier 
transform.  Specifically,  the graph convolutional operation of 
the sample 
nxR  with a filter 
ngR  can be defined as:  
  1*T T T
W g x x g U U x U g Ug U x  
F F F
 (3) 
where * is the graph convolutional operation ; 
 is the 
Hadamard product operation ; and 
diag( )T
Wg U g  is the filter  
parameterized by  
nWR . 
In [25], the Chebyshev spectral CNN (ChebNet)  that 
approximate 
Wg  by the truncated expansion of Chebyshev 
polynomials  
()kTx  up to Kth order  is proposed:  
'
0 max2*K
k k n
kLg x W T I x 

                   (4) 
1 2 0 1 ( ) 2 ( ) ( ), ( ) 1, ( )k k kT x xT x T x T x T x x   
    (5) 
where 
' KWR  is a vector that consists of  Chebyshev 
coefficients ; and 
max  is the largest eigenvalue . Because the 
operation is a Kth order polynomial in the Laplacian , it is the 
K-localized . Moreover , a new graph convolutional network is proposed to 
approximat e ChebNet by assuming  that 
max 2  and 
1K  
[26]. Its mathematical formula is : 
11
'' 22
01 *g x W x W D AD x
               (6) 
To alleviate over -fitting problems and restrain the number of 
parameters, it further assume s that 
''
01 W W W  , which lead 
to the following formula : 
11
22*n g x W I D AD x

               (7) 
Furthermore , the renormalization trick  is utilized to avoid 
vanishing gradients problems  in GCN [27]: 
11
22ˆ ˆ ˆ ˆ *,n g x WD AD X A A I  
            (8) 
where 
ˆD  denotes  the diagonal matrix of node degrees  with 
1ˆ ˆn
ii ij
jDA

. 
Finally, t he Eq. (8)  can be generalize d to the multi -channel 
convolution  [11], [27]: 
11
'' 22
outˆˆ ˆ Z D AD XW
                     (9) 
where 
ncXR  denotes  a input signal  with f filters and c input 
channels ; 
outnfZR  denotes  an output signal ; and 
'' cfWR  
denotes the filter parameters . 
2) Spatial -based  GCNs . Analogous to the convolutional 
operation in the Euclidean domain s, spatial -based GCNs 
directly define the convolutional operation on the graph 
domain s by operating on spatially close neighbors.  The key 
challenges of these spatial -based GCNs are to define 
convolutional operations with the different number of 
neighborhoods and to keep the local invariance  [28], [29]. 
In 2015, Neural FPs  defined the convolutional operation  by 
using different weight matrices for nodes with different degrees  
[30]. Its mathematical formula is : 
()
() 11
FP, FP FP FP FP, FP,
1,Nv
Nv t t t t t
v t v i
iH X W X H H
   
       (10) 
where is 
FP,t
vH  the hidden state of node  v of the Neural FPs at 
time step  t; 
FP()  is the activation function  of Neural FPs , such 
as rectified linear unit  (ReLU) ; 
()Nv  is the neighborhood  of 
node  v; and 
()Nv
tW  is a weight matrix with the degree  
()Nv  at 
time step  t. The main shortcoming of this convolutional 
operation is that it cannot be applied to graphs of large -scale 
with large node degrees.  
Moreover,  a diffusion -convolutional neural network (DCNN) 
is proposed to define the neighborhood for nodes by transition 
matrices  in [31]. Its mathematical formula is : 
  DC DC DC DC DCt t tH W P X
                  (11) 
where 
DCt n fXR  is the input data at time step  t (f is the 
number of features  and n is the number of node s); 
DC()  is the 
activation function of DCNN; 
DCtW  is the weight tensor of 
 5 
DCNN at time step  t; The dimensions of  the hidden state  
DCtH  at 
time step  t and input data 
DCtX  are the same; and the 
degree -normalized transition matrix 
DCP  can be obtained by  
from the  adjacency matrix 
1
DCP D A . Simulation results  have 
shown that DCNN is not only suitable for graph  classification, 
but can also be applied to edges  classification tasks , which 
require  augmenting the adjacency matrix and transforming  
edges to nodes . 
Furtherm ore, a dual-graph convolutional network  (DGCN ) 
that consists of two graph convolutional layers in parallel  is 
proposed to jointly account for the global consistency and local  
consistency  in [32]. The first graph convolutional layer is the 
same as Eq. ( 8). The second graph convolutional layer 
substitutes the positive pointwise mutual information (PPMI) 
matrix for the adjacency matrix:  
11
1 22
DG DG P P P DG DGt t tH D X D H W 

             (12) 
where 
PD  is the diagonal degree matrix of  PPMI matrix  
PX;
DG()
 is the activation function of DGCN ; 
DGtH  is the output 
data of DGNN at time step  t; and 
DGtW  is the weight tensor of 
DGNN at time step  t. 
3) Comparison between spectral  and spatial -based  GCNs.  
The main differences between spectral -based GCNs and 
spatial -based GCNs are as follows:   
Firstly, spectral -based GCNs either need to deal with the 
whole graph s simultaneously or perform eigenvector 
computation , which lead s to more computations induced by the 
forward and inverse graph Fourier transforms  [33]. Relatively, 
spatial -based GCNs are  extensible  to large -scale graph s, since 
they directly define convolutional operations in  graph domain s. 
The computation of spatial -based GCNs can be performed in a 
batch of nodes in place of the whole graph -structured data . 
Secondly, spectral -based GCNs which rely on the Fourier 
transform generalize unfavorably  to various graphs. Any 
perturbations in the graph-structured  data will cause the 
eigenbasis to change, because they assume that the graph s are 
fixed  [34]. In contrast, spatial -based GCNs perform graph 
convolutional operations locally on each node, and the weights 
of networks can be easily shared across different locations.  
Thirdly, most of spectral -based GCNs are limited  to handle 
undirected graphs  [12], while spatial -based GCNs are more 
flexible to handle multisource graphs , such as directed graphs  
[31], heterogeneous graphs  [35], edge inputs  [36], and signed 
graphs  [37], since these graphs can be easily  incorporated into 
the aggregation function.  
In general, spectral -based GCNs perform convolution al 
operations in spectral domains through the complex Fourier 
transform , while spatial -based GCNs  directly define 
convolution al operations in  graph domain s. Therefore, 
spatial -based GCNs show  strong er generalization  and 
flexibility compared with  most of  spectral -based GCNs.  In fact, 
both the spatial -based and spectral -based GCNs are developing 
unceasi ngly. For example, Ref. [ 34] is a kind of spectral -based 
GCNs that is much efficient already. Therefore, the effectiveness of spectral -based GCNs is also being developed.  
C. Graph Recurrent Neural Networks  
The GRNNs are designed for problems defined in graph  
domains  (e.g., classifications in graph -level and node -level) 
which require outputting sequences.  This section will introduce 
two popular variants of the GRNNs . 
To solve long -term dependencies in the graph -structured  
data and reduce the restrictions in GN Ns, there is increasing 
interest in extending gate mechanism s from RNNs , such as  
gated recurrent unit s (GRUs)  and long short -term memories  
(LSTMs) [38], to GRNNs.  For example, the graph GRUs  are 
introduced into the propagation step  in [39]. Specifically , the 
gated GNNs  expand  the RNNs  to a fixed number of T steps, and 
calculate  the gradient s by back-propagat ing time.  The basic 
mathematical formula of the propagation is  
 
 
  
11
1
1
Z
1
R
1
1,,
ˆtanh
ˆ 1Tt T t t
v v n
t z t z t
v v v
t r t r t
v v v
t t t t t t
v v v v v v
t t t t t
v v v v va A h h b
z W a U h
r W a U h
h W a U r h
h z h z h









  
             (13) 
where 
vA  is a part of the adjacency matrix, which is used to 
represent the connection between node v and its neighbors ; b is 
the offset vector ; 
zW  and 
zU  are the weight s of the update  
gate; 
rW  and 
rU  are the weight s of the reset gate; 
t
vW and 
t
vU
 are the weight s of the g ated GNNs at time step t; 
t
vz is the 
update  gate; 
()Z  is the activation function of the update  gate; 
R()
 is the activation function of the reset  gate; 
t
vh is the 
hidden state  at time step t; and 
t
vr  is the reset gate . These gates 
like update function s of GRU s which combine  the previous 
time step and information  of other nodes to update the hidden 
state of each node.  
Similar to GRU s, LSTM s are also a popular framework for 
improving the effectiveness of  long-term information 
propagation.  The basic LSTM s architecture called the 
child -sum tree -LSTM is proposed in  [40]. It includes the input 
gate 
vi, memory unit  
vc, output gate  
vo, and hidden state
vh . In 
addition, it replaces the single forget gate with a forget gate 
vkf  
for each child k, which results in node v aggregate information 
from its child nodes accordingly . Its mathematical formula is:  
 
 
 
 
11
()
1
I
1
F
1
O
1
1
()ˆ
ˆ
ˆ
ˆ
ˆ tanh
tanhtt
vk
k N v
t i t i t i
v v v
t f t f t f
vk v k
t o t o t o
vk v k
t u t u t u
v v k
t t t t t
v v v vk k
k N v
t t t
v v chh
i W X U h b
f W X U h b
o W X U h b
u W X U h b
c i u f c
h o c










  

  
  

  





       (14) 
where 
t
vX  is the input data of node v at time step t; 
I()  is the 
activation function of the input gate ; 
F()  is the activation 
function of the forget gate ; 
O()  is the activation function of 
the output gate ; 
iW and 
iU  are the weight s of the input gate ; 
fW
 and 
fU  are the weight s of the forget gate ; 
oW and 
oU  
are the weight s of the output gate ; 
uW and 
uU  are the weight s 
of the child -sum tree -LSTM ; 
ib is the bias vector  of the input 
gate; 
fb is the bias vector  of the forget gate ; 
ob is the bias 
vector  of the output gate ; 
ub is the bias vector  of the child -sum 
tree-LSTM ; and 
t
vh  is the hidden state  at time step t. 
D. Graph Attention Networks  
In the above -mentioned GCNs, the neighborhood of nodes is 
aggregated with equal or predefined weights. Nevertheless , the 
impact s of neighbors may vary greatly  [41]. Therefore, they 
should be learned in the process of training, not predetermined. 
Activated by attention mechanism s, GAT s introduce  the 
attention mechanism into graph domain s by revising the graph 
convolutional operation  [42]: 
1
GATs, GATs GATs GATs,
()t t t t
i ij j
j N iH W H


             (15) 
where 
1
GAT,t
i H  is the hidden state of node  i of GATs at time step  
t+1; 
()Ni  is the neighborhood  of node  i; 
GATstW  is the weights 
of GATs at time step  t; 
GATs()  is the activation function of 
GATs; and 
t
ij is the attention coefficient of node j to node i at 
time step  t. Its mathematical formula is:  

SM
()exp
expt
ij tt
ij ij t
ik
k N ie
e
e

            (16) 
  LR GATs GATs, GATs GATs, ||t t t t t
ij i je F W H W H
           (17) 
where 
||  is the concatenation operation ; 
LR is the  leaky 
rectified linear unit  (LeakyReLU) function ; 
SM is the  Softmax 
function ; and 
F  is a function  (e.g., multi -layer perceptron)  to 
be learned.  
Furthermore, the multi -head attention mechanism is utilized 
to stabilize the learning process. The  K independent attention 
mechanisms are used to calculate hidden states and then 
concatenate these features  in graph attention network  (GAT)  
[43], which lead to two different output representations:  
1
GAT1, GAT1 GAT, GAT,
1 ()||Kt k t t
i ij k j
k j N iH W H 
 

                (18) 
1
GAT 2, GAT 2 GAT, GAT,
1 ( )1K
t k t t
i ij k j
k j N iH W HK


          (19) 
where 
||  is the concatenation operation ; 
GAT1 is the activation 
function of the first GAT;  
GAT 2 is the activation function of the 
second GAT;  
1
GAT1,t
i H  is the hidden state of node  i of the first 
GAT at time step  t+1; 
1
GAT 2,t
i H  is the hidden state of node  i of the second GAT at time step t+1; and 
k
ij  is normalized 
attention coefficient calculated  by the kth attention mechanism . 
Another popular variant is the gated attention network 
(GaAN) which employs the dot product attention and key-value 
attention mechanism  [44]. To replace the average operation,  the 
self-attention mechanism is employed to gather information 
from different heads . 
E. Graph Generative Networks  
The purpose of GGNs  is to generate some new 
graph -structured  data by learning a series of given historical 
samples . Similar to  generative networks in Euclidean  domain s, 
the existing GGNs  mainly include  the graph automatic 
encoder s (GAE s), variational graph auto -encoder s (VGAE s), 
and graph generative adversarial network s (GGAN s) [45]. 
The GAE s consist  of an encoder and a decoder  [46]. Firstly, 
the features X and the adjacency matrix A of the nodes are fed to 
the encoder to obtain the embedding matrix 
GAEZ  of the 
graph -structured  data:  
GAE GCN , Z X A
                           (20) 
Then, t he decoder of GAE s aims to reconstruct the graph 
adjacency matrix by feeding the embedding matrix 
GAEZ  from 
the encoder:  
 GAE GAE GAETA Z Z
                          (21) 
where 
A
  is the reconstructed adjacency matrix ; and 
GAE()  is 
the activation function of GAEs.  
The new graph -structured  data generated by GAE s lacks 
diversity and the number is limited. To overcome these 
shortcomings, V GAEs introduce  probability  to GAE s [47]. The 
loss function of VGAE s is the variational l ower bound : 

VGVG VG VG ( , )log | KL | , || ( )q Z X Ap A Z q Z X A p Z        
(22) 
where 
 KL  denotes the Kullback -Leibler divergence; p(ZVG) 
is the Gaussian distribution ; ZVG is the Gaussian noise;
VG|p A Z
 is the inner product between latent variables ; 
VG|, q Z X A
 is empirical  distribution of nodes, which is used 
to approximates the prior distribution . VGAE can only 
approximate the lower bound of logarithm likelihood  of the 
nodes , which results in the limited quality of the new 
graph -structured data . 
In order to improve the quality of the generated 
graph -structured  data, the adversarial loss  function of GGAN s 
is introduced into the training process  [48]. Given the graph G, 
GGAN s aim to train the generator and discriminator  by 
historical samples . Specifically,  the g enerator 
 |;iG G v v  
attempts to  fit the  real connected distribution 
real |i P v v  of the 
nodes as much as possible  and generates the most likely nodes  
connected with node 
iv  from the nodes set  to deceive the 
discriminator . On the contrary , the d iscriminator 
 ,;iD D v v  
tries to  identify the connectivity for the nodes  pair 
( , )ivv  and 
outputs a value that represents the probability that whether the 
node is ground -truth neighbors of 
iv  or the one generated by 
 7 
the generator . Formally, the generator and discriminator are 
playing the two -player min -max game, and the loss function of 
GGAN s is: 
  
 ~P real( | )
1
~ ( | ; )min max log ( , ; )
log(1 ( , ; ))iG D
iGn
v v i D
i
v G v i DL E D v v
E D v v




 
  (23) 
where n is the total number of nodes ; 
D is the parameters of 
the discriminator  to be learned; and 
G  is the parameters of the 
generator  to be learned.  The parameters of the generator and 
discriminator are updated during the training process by 
alternately maximizing and minimizing the loss function.  
F. Spatial-Temporal Graph Neural  Networks  
Normally , the structure and feature information of many 
graph -structured  data may change with time. For example, the 
power curves of adjacent wind farms have temporal -spatial 
correlation , which will change with  times and environmental 
factor s, such as wind speed a nd wind direction . There is a need  
to consider spatial dependence when forecast ing the output  
powers of wind farms  [49]. To predict values or graph labels of 
nodes, the STGNNs are designed to capture temporal -spatial 
dependencies of graphs simultaneously.  Existing STGNNs 
mainly include  two categories:  RNN s-based models and 
CNN s-based models.  
For the RNN s-based models, they try to capture 
temporal -spatial dependencies by filtering hidden states and 
input data passed to recurrent units using graph convolutional 
operations  [44]. Normally, t he mathematical formula of RNN s 
is shown as follows:  
 1
RNN RNN RNN RNN RNN RNN RNNt t t t t tH W X U H b  
         (24) 
where  
RNNtW  and 
RNNtU  are the weights  of RNN s; 
RNNtb  is the 
offset vector ; 
RNNtX is the feature matrix of nodes at time step  t; 
RNN()
 is the activation function of RNN s; and 
RNNtH  is the 
hidden states  of RNN s at time step t. After inserting graph 
convolution al operations, Eq. ( 24) becomes : 
 1
RG RNN RNN RNN RNN RNN GCN( , ; ) GCN( , ; )t t t t t tH X A W H A U b  
(25) 
where 
RGtH  is the hidden states  of RNN s-based models  at time 
step t; and A is the adjacency matrix . 
On this basis,  the diffusion convolutional RNN s combine  a 
GRU s with diffusion graph convolutional layers in  [50], while 
the graph convolutional recurrent network s (GCRN s) 
incorporates the LSTM s into ChebNet in  [51]. 
In addition, some works utilize edge -level RNN s and 
node -level RNN s to deal with different aspects of temporal 
features  [52]. For example, a recurrent structure with the 
edge-level RNN s and the node -level RNN s is proposed to 
forecast labels of nodes at each time step in  [53]. The temporal 
information of nodes and edge s passes through the edge -level 
RNN s and node -level RNN s respectively. The output of the 
edge-level RNN s is used as the input data of the node -level to 
merge spati al information.  
The RNN s-based models have gradient vanishing problems 
and time -consuming iterative propagation. Relatively, the CNN s-based models handle the temporal -spatial graph s in a 
non-recursive manner, which has the advantages of the stable 
gradient,  parallel computing, and low memory requirement  
[54]. For example,  A PGC layer and a 1 -D convolutional layer 
are used to build temporal -spatial block in  [55]. The CGCN s 
[56] combine  one-dimensional convolutional layers with 
ChebNet,  which builds a temporal -spatial block by sequentially 
integrating  graph convolutional layer s and gated 
one-dimensional convolutional layer s. 
G. Hybrid forms of  GNNs  
In addition to the GNNs mentioned above, there are also 
some extended models , such as GRL  [57] and GTL  [58]. 
The combination of RL and GNNs has led to a new research 
field named GRL, which  integrates the decision -making of RL 
and perception of GNNs.  Therefore, GRL can be applied to a 
variety of task s requiring both the precise control and  rich 
perception of graph -structured  data.  Many researches on GRL 
are currently being conducted. For example,  to predict  
chemical reaction product s, a graph transformation policy 
network is proposed in  [59], which combines the strengths of 
RNNs and RL to learn the chemical reactions directly from raw 
data with minimal prior knowledge.  Specifically,  it uses the 
RNN s to memorize the forecasting seq uences and  the GCN s to 
learn the representations of nodes . In [60], a graph 
convolutional policy network that consists of GCNs and the RL 
is proposed to discover novel molecules with specific 
properties . The generative network  is considered as an RL 
agent performing in the graph generative environment . In 
addition,  the graph generation is regarded as a Markov decision 
process , where edges and nodes  are added . Simil arly, the 
GGANs are proposed to directly operate on small molecular 
graphs in [61]. It combines a reinforcement learning objective 
with GNNs to encourage the generat ion of small molecular 
graphs with specific desired chemical properties.  
GTL  is a research topic  in machine learning, which aims to 
store prior knowledge when solving old problems and applying 
it to new but related problems  [62]. Although GNNs hav e 
shown superior performance in various fields, training 
dedicated GNNs will be costly for large -scale graphs.  In this 
case, a practically useful and  theoretically grounded f ramework 
is proposed for the transfer learning of GNNs in  [63]. The 
proposed novel views towards the important graph information 
and activate the capturing of it as the goals of transferable 
GNNs training, which motivates the design of GNNs 
frameworks.  In [64], a model containing transfer learning and 
GNNs is proposed to solve related tasks in the target domain 
without training a new model from scratch by transferring the 
natural geometric information learned in the source domain . 
III. APPLICATIONS IN POWER SYSTEMS  
After years of development, some  papers about the 
application of GNNs  have been published, and most of them 
have been published since 2018. However, most of the se 
applications are focused on  computer science  and biolog y, such 
as social network s, link prediction, protein structure generation , 
and natural language processing . The application s of GNN s in 
power system s are relatively limited.  Table II lists some  
existing publications  of GNN s in power systems, cover ing fault 
scenario application , time series  prediction  of RES and load s, 
power flow calculation , data generation , etc. In the future, it 
may be divided into more categories as the number of 
publications increases.  To serve as a catalyst for further study 
of applications, this section reviews these  available literatures  
and discusses  oriented research es. 
TABLE  II 
EXIST ING APPLICATIONS OF GRAPH NEURAL NETWORKS  
Area  Category  Involved GNNs  Ref 
Fault 
scenario 
application  Transformer fault 
diagnosis  Spectral -based 
GCNs  [67] 
Fault location  Spectral -based 
GCNs  [69],[70] 
Fault detection and 
isolation  Spectral -based 
GCNs  [72],[76] 
Power outages 
prediction  Spectral -based 
GCNs  [74] 
Time series  
prediction  Solar  power 
prediction  Spectral -based 
GCNs  [78]-[81] 
Wind  power  or wind 
speed prediction  Graph LSTMs and 
Spectral -based 
GCNs  [82]-[84] 
Residential load  
prediction  Spectral -based 
GCNs  [85] 
Power flow 
calculation  Power flow 
approximation  Spatial -based 
GCN s, GAEs  [86]-[88][90] 
Optimal power flow Spectral -based 
GCNs  [6],[91] 
Optimal load shedding  Spectral -based 
GCNs  [92] 
Data 
generation  Scenario generation  Spectral -based 
GCNs  and GAEs  [93],[94] 
Synthetic feeder 
generation  Spectral -based 
GCNs  and GGANs  [95] 
Others  Coupled  power  and 
transportation 
networks  Spectral -based 
GCNs  and GRL  [96] 
Line flow control  Spectral -based 
GCNs  and GRL  [97] 
Maintenance tasks  Hybrid forms  
of GNNs  [98] 
Operation of 
distributed energy 
resources  Spectral -based 
GCNs  [99], [100]  
Safe operation of 
power grids  Spectral and 
spatial -based 
GCNs  [101] -[103]  
Synchrophasors 
recovery  Graph LSTMs, 
Spectral -based 
GCNs  and GGANs  [104]  
Transient stability 
assessment  Graph LSTMs, 
Spectral -based 
GCNs  [105]  
Wind power 
estimation  Physics -induced 
GNNs  [106]  
A.  Fault  scenario application  
In this section , the four categories applications  given under 
fault scenario  and some potential research direction  are 
discussed. The main aspects in this relation  are to detect failures 
in order to avoid power outages and ensure the safe operation of 
the pow er system.  
A1. Transformer fault diagnosis  
One often used diagnostic method within  transformer fault diagnosis is by applying dissolved gas analysis.  the fault 
diagnosis of the dissolved gas analysis data is crucial  to 
diagnose  the incipient faults of power transformers as early as 
possible. Most of the existing methods for transformer fault 
diagnosis can be classified into the following t hree categories: 
model -based methods  (e.g., CNNs), distance -based methods  
(e.g., k-nearest neighbors ), and their hybrid forms  (e.g., 
ensemble models) . Specifically, distance -based methods 
attempt to calculate the similarity metrics between and the 
historical samples and the samples with unknown labels  [65], 
but they are hard to mine the complex non -linear relationship 
between dissolved gas data and labels. In contrast, model -based 
methods predict the type of fault through a classifier trained 
with historica l samples  [66], while they ignore the similarity 
metrics or do not use them  directly. Furthermore, the hybrid 
model can not only account for similarity metrics, but also train 
a classifier for fault diagnosis.  GCN can be considered as a kind 
of hybrid form. On one hand,  similarity metrics between 
labeled samples and unknown samples can be represented by an 
adjacency matrix.  On another  hand, GCNs can accurately 
explore  the complex relationship between dissolved gas data 
and fault types through graph convolutional layers.  As shown 
in Fig. 3, the  spectral -based GCN s are proposed to improve the 
accuracy of transformer fault diagnosis  in [67]. The simulation 
results show that the performance of th e GCN s is better than 
those of the traditional methods , such as multi -layer 
perceptions  (MLP s), support vector machines  (SVM s), CNNs, 
and k-nearest neighbors (KNN s) in different data volumes and 
input features.  Although GCNs show strong performance in 
transformer fault diagnosis, there are still some potential  
directions for further research : 1) Since the size of the 
adjacency matrix depend s on the number of samples, GCNs 
need to be retrained when the number of samples changes, 
which lead to that  the existing GCNs methods are  difficult to  
use for on -line fault diagnosis. It needs further study on how to 
avoid retraining GCN s for transformer fault diagnosis.  For 
example, i t may be considered that new samples and their most 
similar samples have the same connection relationship with 
others. In this case, GCNs can identify new samples without 
repeated training.  2) The existing paper  only analyzes  the 
performance of spectral -based GCNs for transformer fault 
diagnosis. This work may be extended to spatial -based GCNs 
and then explore the performance s of different GCNs for 
transformer fault diagnosis.  
 
Fig. 3. Transformer fault diagnosis via the spectral -based GCNs  [67].  
ReLU ReLUSoftmax
Graph 
convolutional layerGraph 
convolutional layerDense layer(X,A)…
Output 
fault typeInput feature matrix 
and adjacency matrix
 9 
A2. Fault location  
Traditional methods of fault location for distribution 
network s mainly include voltage sag -based methods, 
impedance -based methods, traveling wave -based methods, and 
auto-mated outage mapping methods. Although they have their 
own advantages, there are two main challenges  [68]: Firstly, 
they cannot flexibly combine measurement data from different 
buses, especially in the case of data loss.  Secondly,  most of 
traditional methods have difficulties in modeling  the topol ogy 
of the distribution network. In order to solve these problems, 
Ref. [69] employ s the graph GRUs to automatically localize the 
faults of distribution networks. Th e feeder topology is 
represented by graph edges and problem data (e.g., 
measurements and electrical characteristics) is regarded as 
graph nodes.  Similarly, the spectral -based GCNs are proposed 
to explore comprehensive information from multiple 
measurement units and capture the spatial correlations among 
buses  in [70]. As shown in Tab le III, s imulation results show 
that classification accuracy and one -hop accuracy of GCNs are 
higher than  those of some  machine learning methods , such as 
the hybrid model of the principal component analyses  (PCA s) 
and SVMs , hybrid model of  PCA s and random forests  (RFs) , 
and MLPs . Furthermore,  there are still some potential 
directions for further research:  1) The effectiveness of GCNs in 
more realistic settings needs to be further studied. For example, 
field data can be utilized to fine -tune the pre -trained model 
through graph transfer learning.  Especially trained GCNs may 
be transferred to other distribution netw orks with different 
topologies . 2) The difference between spectral -based  GCNs 
and spatial -based GCNs on the performance of large -scale 
distribution networks can be analyzed, and the impact of 
different RES on GCNs can be discussed.  
TABLE  III 
FAULT LOCATION  ACCURACIES OF DIFFERENT  
METHODS IN THE IEEE  123  BUS SYSTEM  
Model  Accuracy  One-hop accuracy  
PCA s+SVM s 94.60%  98.31%  
PCA s+RFs 94.96%  99.28%  
MLPs  84.64%  96.38%  
GCNs  99.26%  99.93%  
 
A3. Fault detection and isolation  
Traditional methods for fault detection and isolation are used 
to identify and isolate faults on the level of a single component 
by accounting  for features from this component and 
corresponding  components  [71]. These methods are not good 
enough , since they are independently applied to a single 
component without explicitly considering the dependencies 
among multiple components coexisting in power systems. The 
interaction between components bri ngs challenges to fault 
isolation. In addition, the traditional methods do not consider 
the network structure when designing the fault diagnosis , which 
causes over -fitting problems. To solve these problems, the 
connected components in power systems can be represented as 
a weighted undirected graph structure  [72]. Then, local 
relationships between power variables in different components 
of the distribution network are explored by GCNs to improve 
fault detection and isolation. Simulation s show  that GCNs are significantly better than several baselines. Moreover, the 
community -varying GCNs can be used to explore highly 
correlated components in distribution networks, so as to 
improve performance for fault detection and isolation.  
A4. Power outages prediction  
Power outages have an important impact on economic 
development because of the strong correlation between  power 
energy and productive sectors. Traditional methods ignore the 
connection relationship of measurement data, resulting in their 
limited accuracy  [73]. In order to improve the accuracy of 
predicting power outages, a new method based on GNNs is 
proposed to process weather measurements  [74]. Specifically, 
the structure of weather stations is regarded as a graph where 
the edges denote the dis tance between these stations, and each 
node represents a station. Then, weather measurements at 
stations are modeled as features of each node, and the 
corresponding topology is utilized to process these features. 
The simulation results show that GNNs signi ficantly improve 
the accuracy of power outage prediction. Moreover , this 
framework may be extended to communities. Each community 
is modeled as a node, and the connection relationship between 
communities is modeled as branches. Then, the correlation of 
power between communities can be further analyzed.  
As a further example of power outages, a case with a PV 
plant is considered.  Generally,  PV plants are installed in remote 
places where the weather may be very bad, which leads to PV 
plants  failure is difficult to predict. Traditional methods either 
have low accuracy or require a large number of historical 
samples to train the classifier, which is not suitable for 
photovoltaic fault classification  [75]. Therefore, a graph signal 
processing technique is proposed to detect PV faults with a 
limited amount of labeled samples  [76]. The simulation results 
show that the graph -based classifier  has higher accuracy and 
lower computational cost than traditional methods , such as 
KNN s, SVM s, and random forest s. In Euclidean  domain s, 
Siamese networks and matching networks show go od 
performance for few -shot learning. Similarly, it can also  
employ  GCNs or GRNNs to construct graph Siamese networks, 
which may be suitable for fault diagnosis of the 
graph -structured dataset with small samples.  
A5. Further ideas for fault scenario applic ation  
In addition to the above applications of fault scenario s, there 
are some potential directions worthy of further study:  1) 
Existing work s show  that ensemble learning can significantly 
improve predictive performance by using multiple learning 
algorithms. Therefore,  it may ensemble  multiple GNNs or 
combine GNN s with traditional DNN s in the future . 2) In 
Euclidean  domain s, the pooling operation of CNNs loses a lot 
of feature information of input data, which limits the accuracy  
of fault diagnosis.  In order to solve this problem, the capsule 
network  with primary capsule layers and digital capsule layers 
is proposed to significantly improve the accuracy of fault 
diagnosis. Similarly, the capsule network may also be extended 
to graph domain s. 3) For GCNs in  [67], their adjacency matrix 
represents the similarity metrics between samples, and the 
graph convolutional layer capture s the complex nonlinear 
relationship  between  features of samples and labels. This 
framework may  be suitable for transformer fault diagnosis  and 
other classification tasks in the power system, such as power 
quality disturbance classification and transient stability 
assessment  of power systems . 
B. Time series prediction   
In this section, the three  categories applications given under 
time series prediction  and some potential research direction are 
discussed.  
B1. Solar power prediction  
Accurately predicting short -term power s is of great 
significance for power systems with high penetration of RES, 
because PV plant and wind turbine  have a great impact on the 
economic and stable operation of power systems. However, 
traditional methods cannot accurately capture the temporal and 
spatial correlation s of photovoltaic stations  [77], because 
high-dimensional input vectors require a large number of free 
parameters  in traditional machine learning models, while the 
gradient descent method based on the loss function cannot be 
effective to adjust a large number of free parameters. In [78], 
the CNNs -based STGNNs are proposed to leverage 
spatial -temporal coherence among PV systems. While in  [79], 
the GAEs are employed to capture the spatial -temp oral 
manifold of power loads and PV power, which are considered 
as spatial -temporal graphs representing the measurements of 
units via nodes and reflecting the mutual correlation between 
the units via edges. Similarly, the convolutional GAEs are 
devised to predict probabilistic solar irradiance in different 
multiple measurement sites, which are modeled as an 
undirected graph  [80]. In order to improve the accuracy of 
photovoltaic prediction, a hybrid algorithm with the LSTM s 
and GCN s is proposed in  [81]. Specifically, the LSTM s are 
employed to e xtract the temporal features of the photovoltaic 
power curve s, and GCNs are used to capture the spatial 
correlation between multiple adjacent PV plants . Each PV plant 
is regarded as a node, and the features of each node include 
historical power and weather  data. If the correlation coefficient 
between the two PV plant s is greater than the threshold, they 
are considered to be connected. As shown in Tab le IV, the 
mean absolute error  (MAE) , mean absolute percentage error  
(MAPE) , root mean squared error  (RMSE)  of the proposed 
hybrid model are smaller than  those of traditional methods , 
such as the LSTM s and MLP s. Although this model shows 
strong performance in short -term prediction of PV power, there 
are still some potential directions for further research:  1) More 
combinations of algorithms can be tried. For example, if the 
performance of the GRU s to capture temporal  correlation is 
similar to that of  LSTM s, and the computing time is less than 
that of  LSTM s. The performance of the combination of the 
GRU s and GCNs for short -term photovoltaic power can be 
explored in the future.  2) The RNN s in Euclidean  domains  have 
been extended to the GRNN s in graph domains , and ha ve 
shown outstanding performance in natural language processing. 
Perhaps the temporal and sp atial correlation of the PV plants  
can be captured by GRNN s. 
 
 TABLE  IV 
DAY-AHEAD FORECASTING RESULTS OF DIFFERENT METHODS  
Models  MAPE  RMSE  MAE  
LSTMs+GCNs  28.30%  1.10 0.79 
LSTMs  54.17%  2.19 1.58 
MLPs  43.35%  1.78 1.31 
 
B2. Wind power  and wind speed  prediction  
Compared with onshore wind power s, the fluctuations and 
intermittence of offshore wind power s are stronger , which 
poses  great challenges to the operation and planning of power 
systems.  Traditional methods are difficult to fully exploit the 
spatial property  of offshore wind power s, resulting in low 
forecasting accuracy. To simultaneously represent the temporal 
and spatial information of wind farms, a superposition graph 
neural network  (SGNN) is proposed in  [82], which refers to the 
superposition structure of CNNs on feature channel and the 
feature transfer method of GNNs.  Specifically, many nearby 
wind turbines are form of the wind farms. They are co nsidered 
as the graph -structured data that represent the direct spatial 
property among wind turbines. To the temporal -spatial 
property of wind farms, the encapsulated data structure is 
designed by overlaying spatial maps at different nodes and time 
horizon s. Then, a SGNN is employed to extract feature s, so as 
to maximize the utilization of temporal -spatial property.  The 
simulation results show that SGNN can accurately capture the 
temporal and spatial features of wind farms and achieve higher 
accuracy than traditional methods.  In [83], the spectral -based 
GCNs are presented to learn the interconnection among 
multiple wind farms for wind speed prediction. Similarly, a 
new temporal and spatial wind speed feature learning 
framework is proposed  to combine graph deep learning and 
rough set theory in [84]. As shown in Fig. 4, the wind  farms are 
modeled as the graph -structured  data where nodes with high 
wind direction s and speed correlations are connected by edges. 
Then, the recurrent LSTM s are used to extract temporal 
features of each wind site, which are fed to the spectral -based 
GCNs. To learn robust features, rough set theory is embedded 
in the GCNs by extracting in terval lower -bound and 
upper -bound filtering parameters. Simulation results show that 
the model has better performance than popular deep learning 
architectures , such as  deep belief networks  (DBN s). 
Furthermore,  there are still some potential directions for  further 
research:  1) The S GNN s may be extended to other prediction 
tasks, such as irregular distribution point clouds of PV systems . 
2) An effective graph construction method is able to accelerate 
the efficiency of preprocessing in extracting spatial 
characteristic s. Nevertheless , The SGNN s only use  a feasible 
construction method without involving  more theories about 
computer graphics, which can be further studied.  
 11 
 
Fig. 4. Structure of spatio -temporal graph deep neural network for short -term 
wind spe ed prediction  [84].  
B3. Residential load prediction  
Short -term load prediction is an important part of providing a 
stable power supply to all electricity consumers on power 
systems. In order to accurately predi ct household load s, a novel 
method that consists of graph spectral clustering and 
non-intrusive  load monitoring is proposed in  [85]. Specifically, 
the aggregate d power curves are decomposed into the power 
curve of the individual appliance which is forecasted separately. 
Then, the total power curve is obtained by aggregating the 
forecasted power curve of individual appliances. Every 
electrical appliance is regarded  as a node, and branches are 
constructed by functions of state duration probabilities of 
appliances. Simulation results show that this model is more 
accurate compared to existing approaches , such as similar 
profile load forecast and autoregressive integrat ed moving 
average.  
B4. Further ideas for time series prediction  
In addition to the above applications of prediction for RES 
and loads, there are some potential directions worthy of further 
study:  1) In Euclidean  domain s, CNN s and LSTM s are often 
combined to predict powers of RES or power loads, since 
CNN s are better at automatically extracting the features of 
input data and representing the complex nonlinear relationship 
between the features and the real powers, while LSTM s are 
better a t capturing the temporal correlation of time series. 
Therefore, they are often used to construct hybrid models. In 
the same way, it may also combine GCNs and GRNN s to 
construct a hybrid model with their advantages in  graph 
domain s. 2) Temporal convolutiona l network (TCN) is a novel 
neural network originated from the 1 -dimensional CNN s. It 
keeps the powerful ability of feature extraction of  CNN s, and is 
very suitable for forecasting time series.  TCN may also be 
extended to  graph  domain s, and test its perform ance in load and 
renewable energy power prediction.  3) Like fault diagnosis, 
ensemble learning can also be used to improve the forecasting 
performance  of GNNs . It may even  ensemble the traditional 
RNN s and GRNN s, as well as the impact of different ensemble  
frameworks on the accuracy.  
C. Power flow calculation  
The power flow calculation of the power system is the basis 
for operation and planning.  In this section, the three categories 
applications given under power flow calculation and some 
potential research di rection are discussed.  
C1. Power flow approximation  
Due to the growing integration of RES and an increasing 
amount of power equipment, the physical model s of power system s are becoming more complex, which leads to the longer 
computing time of power flow calculation. This requires further 
development of fast power flow approximation methods. 
Traditional methods (e.g., MLP s and CNN s) do not exploit the 
intrinsic network topology of power system s, resulting in low 
accuracy. Therefore, the spatial -based GCNs are employed to 
approximate the power flow in  [86]. Specifically, the active 
power and reactive power of each node are regarded as features, 
and the power flow of ea ch branch and the voltage of each node 
are defined as corresponding tags to be predicted. Simulation 
results show that the spatial -based GCNs provide a highly 
accurate power flow approximation and outperform classical 
methods on large -scale power system s. In [87], the voltage 
magnitude and voltage angle are estimated by the 
spectral -based  GCNs. In  [88], the ChebNet is employed to 
calculate distribution characteristics of power flow without the 
prior knowledge. The GCNs show shorter computation time 
and higher accuracy than the conventional M onte-Carlo 
method. To decompose and solve the power flow equations of 
transmission networks, the spectral -based GCNs are presented 
to provide a geometric picture of the electrical variables  in [89]. 
Similarly, to calculate the power flow quickly and in parallel, 
the GNNs are proposed to minimize the violation of 
Kirchhoff’s law at each node during training  in [90]. Unlike 
traditional methods, th is graph neural solver learns by itself and 
does not imitate the output data of the Newton -Raphson solver. 
Simulation results show that the GNNs can perform predictions 
faster than traditional methods , such as the Newton -Raphson 
solver.  Although this model shows strong performance in 
power flow calculation of power systems, there are still some 
potential directions for further research:  1) The distribution 
networks are constantly cha nging. When the nodes and 
branches of the distribution networks increase, the trained 
models cannot be used directly. In the next step, it can  study 
how to fine -tune the trained model through transfer  learning so 
that the model can be applied to the expand ed distribution 
networks.  2) Alternative GCNs model and further investigation 
about architecture improvements may be considered in the 
future.  3) In Euclidean  domain s, adversarial training has been 
successfully applied to regression tasks.  The adversarial 
training , such as GGAN s may also be introduce d into the power 
flow calculation.  
C2. Optimal power flow  
In addition to being used for power flow calculation, GNNs 
can also be further applied to the optimal power flow of 
distribution networks. For example, the spectrum -based GCNs 
are designed to optimize the reactive power of distribution 
networks in  [6]. Specifically,  the adjacency matrix is used to 
represent the topol ogy information between the nodes in 
distribution networks, so as to mine the correlation of nodes. 
Then, the deep graph convolutional layer is used to capture the 
complex nonlinear relationship between the state of the power 
equipment and the power loads.  Simulation results show that 
the performance of this model is better than those of traditional 
data-driven methods , such as CNN s, MLP s, and case-based 
reasoning . Similarly, the GNNs are designed to approximate 
the optimal power flow solution in  [91]. GNNs are local 
Graph 
modelingLSTM LSTM LSTM… …
… …
Temporal feature 
extractionRough layer… …
Spatio-temporal 
pattern recognitionOutput layer
…
Rough layer"
2011.08651,D:\Database\arxiv\papers\2011.08651.pdf,"How does the choice of hypothesis space for the critic affect the variance of mutual information estimates, and what are the implications for bias-variance tradeoff?","The choice of hypothesis space for the critic significantly impacts the variance of mutual information estimates. A more complex hypothesis space can lead to higher variance, while a simpler space may result in lower variance but potentially higher bias. This tradeoff is crucial for achieving accurate and reliable estimates of mutual information.","produces a D-dimensional embedding in an RKHS for any
inputx. Any function in this RKHS is represented by a D-
dimensional vector w, such thatf(x) =w⊺φ(x).
ASKL represented the RKHS generated by the above feature
mapping as a two layer neural network with cosine activations
shown in Fig. 1. The hidden layer of this neural network
represents the feature mapping φ(x), its trainable parameters
are the frequency samples {ωi,ω′
i}from spectral distribution
S(ω,ω′). The parameters wof the ﬁnal output layer represent
functions in the RKHS. The output of the ﬁnal layer is the
inner product f(x) =⟨w,φ(x)⟩H. A RKHS can be learned
by optimizing this neural network using a stochastic gradient
descent method. During the optimization, a spectral distribu-
tions is learned implicitly through learning the parameters of
the hidden layer{ωi,ω′
i}. In this work, the critic’s hypothesis
space is restricted to an RKHS using the neural network
architecture Fig. 1 and ASKL. For more information on ASKL
refer to [20]. Any further reference to ASKL critic refers to
the neural network architecture shown in Fig. 1.
IV. T HEORY & O URAPPROACH
Our goal is to estimate the mutual information, I(X;Y),
between two RVs XandY, fromni.i.d samples,{xi,yi}n
i=0
from joint distribution PXYandmi.i.d samples,{x′
i,y′
i}m
i=0
from the product of marginal distributions PX⊗PY. As, the
true underlying probability distributions are unknown, we use
empirical approximations of the variational lower bounds of
MI deﬁned as:
ˆIn,m
TUBA (fθ,S) =EPn
XY[fθ(x,y)]−
EPm
X⊗Pm
Y[
efθ(x,y)]
a−log(a) + 1 (5)
ˆIn,m
DV(fθ,S) =EPn
XY[fθ(x,y)]−
log(
EPm
X⊗Pm
Y[
efθ(x,y)])
(6)
Where,Sis the set of n,m i.i.d samples {xi,yi}n
i=1.
{x′
i,y′
i}m
i=1,Pn
XY andPm
X⊗Pm
Yare empirical distribu-
tions corresponding to samples {xi,yi}n
i=1and{x′
i,y′
i}m
i=1,
respectively, EPn
XY[f(x,y)] =1
n∑n
i=1f(xi,yi)and
EPm
X⊗Pm
Y[f(x,y)] =1
m∑m
i=1f(x′
i,y′
i).
A. Theoretical Guarantees
In this subsection the generalization behaviour of the empir-
ical estimates, ˆIn,m
TUBA andˆIn,m
DVare discussed. We derive gen-
eralization error bound for the empirical estimates using data-
driven Rademacher complexity of general critic’s hypothesis
space. We also bound the empirical Rademacher complexity
of the ASKL critic’s hypothesis space.
Generalization error quantiﬁes the out of sample behaviour
of an estimator. Formally, generalization error is deﬁned as the
maximum possible deviation of the empirical estimates from
true values. If empirical estimate ˆIis an unbiased estimate,
then variance of this empirical estimate is upper bounded
by the expectation of squared generalization error. Hence,
generalization error is an indicator of the variance of theestimate. The following theorem bounds the generalization
error of ˆIn,m
TUBA andˆIn,m
DV.
Theorem 2 (Generalization Error Bounds): Assume, that the
hypothesis spaceFof the critic is uniformly bounded by M,
that is|f(x,y)|≤M∀f∈F&∀(x,y)∈X×Y ,M <∞.
For a ﬁxed δ >0generalization errors of ˆIn,m
TUBA andˆIn,m
DV
can be bounded with probability of at least 1−δ, given by
sup
f∈F(
ITUBA (f)−ˆIn,m
TUBA (f))
≤4ˆRn(F)+8
aeMˆRm(F)
+4M
nlog(4
δ)
+8MeM
amlog(4
δ)
+√(
4M2
n+(eM−e−M)2
a2m)
log(2
δ)
2(7)
sup
f∈F(
IDV(f)−ˆIn,m
DV(f))
≤4ˆRn(F) + 8e2MˆRm(F)
+4M
nlog(4
δ)
+8Me2M
mlog(4
δ)
+√(
4M2
n+(e2M−1)2
m)
log(2
δ)
2(8)
Where, sample set SforˆIn,m
TUBA andˆIn,m
DVis assumed to be
known, and ˆRn(F)and ˆRm(F)are empirical Rademacher
averages of the hypothesis space Ffor different sample sizes.
To formulate the generalization error bounds given in the
above theorem, we used McDairmid’s inequality to bound
generalization error by expected generalization error over
sample set S. Then we use lemma A5 given in [40] to
bound the expected error by Rademacher complexity. Further,
Rademecher concentration inequality, lemma A4 also given in
[40] is used to arrive at the ﬁnal theoretical guarantees. Refer
to Appendix B for detailed proof. Error bounds for INWJ and
IMINE are derived by substituting the parameter ain bound
7 withe, and with exponential moving average of ef
θ(x,y)
across mini-batches, respectively. IJSusesINWJ lower bound
to estimate MI, hence generalization error of IJSis bounded
by generalization error bound of INWJ . Similarly, ISMILE
usesIDVlower bound to estimate MI and its generalization
error is bounded by error bound of IDV
The generalization error bounds depend on the empirical
Rademacher complexities and eM. Our ﬁnding on the de-
pendence of the generalization error on eMis conﬁrmed by
similar observation made in [41] on the sample complexity of
MINE estimator. From the error bounds, it can be inferred
that high empirical Rademacher complexity of the critic’s
hypothesis space leads to high generalization error, hence
high variance estimates. Therefore, variance of these estimates
can be effectively reduced by choosing a hypothesis space
for critic with low Rademacher complexity. However, it is
also necessary to keep the hypothesis space rich enough to
induce low bias. Though these bounds apply to all hypothesis
spaces including the space of functions that are learned by
a fully connected neural network, emperical estimation of
(a) Comparison on 20 dimensional correlated Gaussian dataset
(b) Comparision on cubed 20 dimensional correlated Gaussian dataset
Fig. 2. Qualitative comparison between ASKL and baseline critic on four diferent variational lower bounds of MI, INWJ ,IMINE ,IJS, andISMILE . MI
estimates on Gaussian correlated and cubed Gaussian correlated datasets are plotted in (a) and (b), respectively. MI estimate by the proposed ASKL critic are
in blue and the estimates of baseline critic are depicted in orange. The solid plotted lines are exponentially weighted moving average of these estimates. ASKL
critic etimates are more stable in comparison to baseline estimates on all lower bounds of MI and both datasets. A speciﬁc case of estimation instability can
be noticed in IMINE (ﬁrst row second plot) based estiamtion of MI using baseline critic architecture when the true MI is higher than 16, whereas, ASKL
critic computes stable MI estimates even at higher values.
Rademacher complexity for a fully connected neural network
is an open area of research. We restrict the critic neural
networks hypothesis space to RKHS by using ASKL to
gain insights into variational lower bound estimates of MI.
The empirical Rademacher complexity of the ASKL critic’s
hypothesis space can be upper bounded as shown by the
following theorem,
Theorem 3: The empirical Rademacher average of the
RKHSFto which ASKL critic belongs can be bounded as
following
ˆRn(F)≤B
n√n∑
i=1∥φ(xi)∥2
2≤B√n
WhereB= sup
f∈F∥w∥2.
We used the Cauchy-Schwarz inequality to bound the com-
plexity of the ASKL critic, for detailed proof refer to Appendix
A. Note that, the second inequality in the above theorem is true
only in the case of ASKL critic. Using the above theorem we
can decrease the complexity by decreasing the largest possible
norm of RKHS representation of functions wor decreasing
the frobenius norm of the feature mapping matrix. In the next
subsection, we present an optimization procedure to decrease
the empirical Rademacher complexity by penalizing ∥w∥2and
∥φ(X)∥Fto control the bias-variance tradeoff. Using second
inequality, and penalizing ∥w∥2it is possible to carve out the
regularisation used by Nguyen et al. [18] to control hypothesis
space complexity.TABLE I
REGULARIZATION WEIGHTS
Lower Bound λ1λ2
NWJ [42] 0.001 0.001
MINE [11] 0.001 0.001
JS [12] 1e-5 1e-5
SMILE [13] 1e-4 0.001
B. Training Methodology
We train an ASKL critic neural network shown in Fig.
1 to simultaneously maximize empirical estimate of MI and
minimize regularization terms deﬁned below. The overall
training objective is:
argmin
θ−ˆI(fθ,S) +λ1∥w∥2+λ2∥φ(S;θ)∥F (9)
Where, ˆIcan be an empirical estimate of any variational
lower bound of MI, ˆIn,m
NWJ ,ˆIn,m
MINE ,ˆIn,m
JS orˆIn,m
SMILE . And
θis the set of trainable parameters w,Ω, and Ω′. GAN
discriminator objective is maximized in cases where ˆIis
ˆIn,m
JSorˆIn,m
SMILE . In this work, regularization terms ∥w∥2and
∥φ(S;θ)∥Fappear in upper bound of empirical Rademacher
complexity of ASKL critic’s hypothesis space. Bias-variance
tradeoff is controlled by tuning hyperparameters, λ1andλ2.
We use mini-batch stochastic gradient decent to train the
estimator.
V. E XPERIMENTS
We empirically validate our claims on two different toy
datasets which have been widely used by other MI estimation
(a) Bias, variance and RMSE of ASKL critic estimates for different batch
sizes.
(b) Bias, variance and RMSE of baseline critic estimates for different
batch sizes.
Fig. 3. Bias, variance, and RMSE values of ASKL critic and baseline critic estimates averaged over 50 experimental trials are shown in ﬁgures (a) and (b),
respectively. In each ﬁgure ﬁrst, second and third rows contain bias, RMSE and variance plots. Each column corresponds to different lower bound, and in
each plot different plotted lines correspond to different batch sizes. ASKL critic etimates are less biased and exhibit lower variance compared to baseline
critic estimates on all variational lower bounds.
methods [11]–[13], (1) correlated Gaussian dataset, where
samples of two RVs (X,Y )are drawn from a 20 dimensional
Gaussian distribution with correlation ρbetween each dimen-
sion ofXandY. The correlation ρis increased such that
I(X;Y)increases in steps of 2 every 4000 training steps,
and (2) cubed Gaussian dataset, same as in (1) but we apply
a cubic non-linearity to Yto get samples ( x,y3). As, mutual
information remains unchanged by application of deterministic
functions on random variables, I(
X;Y3)
=I(X;Y). Fur-
ther, is it important to note that previous methods increased
the correlation ρtill the true MI is increased to 10. In our
experimental analysis, we increased the correlation ρtill the
true MI is 20 to demonstrate that ASKL critic produces low
variance estimates even at high values of MI.
For comparative analysis we train ASKL critic and a
baseline critic on four different lower bounds, namely INWJ ,
IMINE ,IJS, andISMILE . The baseline critic is a fully con-
nected neural network with ReLU activations. This baseline
has been used by previous estimation methods that consider
the universal approximation property of neural networks [11]–
[13]. ASKL critic with regularised space complexity computes
low variance stable variational lower bound estimates of MI
in comparison to baseline critic.
Code for this paper are available at https://cvit.iiit.ac.in/
projects/mutualInfo/.
A. Training Details
For ASKL critic, Dis set to 512, that is 512 spectral
samples are used for estimation. The multiplicity factors for
each of the regularization terms used for different estimators
are given in Table I. For our baseline critic, we used a 3 layer
neural network with 256 units in each hidden layer. Unless
mentioned otherwise, batch size is set to 64. We use Adamoptimizer [43] with β1= 0.9andβ2= 0.999. Learning rates
are set to 10−3and5×10−4for ASKL and baseline critics,
respectively.
We test the validity of our claim that constraining the critic
to RKHS should lead to better bias-variance tradeoff in three
different experimental setups, (1) qualitatively compare the
variance of MI estimates between ASKL critic and baseline
critic on four different variational lower bounds of MI. These
experiments are performed on both toy datasets described
above, batch size is ﬁxed at 64 sample, (2) quantitatively
compare the average bias, variance, and the root mean square
error (RMSE) between the true and empirical estimates of MI
over 50 experimental trials. These quantitative comparisons
are made over a range of batch sizes to depict the robustness
of our estimates with varying batch sizes, (3) quantitatively
demonstrate the efﬁcacy of the proposed regularisation terms
in controlling bias-variance tradeoff of ASKL critic’s space
complexity by varying the regularisation hyperparameters λ1
andλ2for∥w∥2and∥φ(X)∥F, respectively. In experiment
(3), bias-variance values are estimated over 50 experiments.
Both experients (1) and (2) are run on correlated Gaussian
dataset. We further elaborate on each of these experimental
results in the next subsection.
B. Results
Qualitative comparison between ASKL critic and baseline
critic on four different variational lower bounds of MI has
been shown in Fig. 2. Fig. 2(a) and Fig. 2(b) demonstrate
the comparative results on the 20 dimensional correlated
Gaussian dataset and the cubed correlated Gaussian dataset,
respectively. In can be seen that maximisation using ASKL
critic tends to produce stable estimates in comparison to
their baseline counterpart. A particular instance of numerical
(a) Bias of ASKL critic based estimators for different conﬁgurations of
regularization weights
(b) Variance of ASKL critic based estimators for different conﬁgurations
of regularization weights
Fig. 4. Bias-variance tradeoff for different values of λ1andλ2for estimation using ASKL critic. Figures (a) and (b) show bias and variance plots, respectively.
In both ﬁgures each row corresponds to a single λ1value and each column corresponds to a single λ2value. These ﬁgures quantitatively demonstrate the
efﬁcacy of the proposed regularisation terms ( λ1andλ2for∥w∥2and∥φ(X)∥F, respectively) in controlling bias-variance tradeoff of ASKL critic’s space
complexity.
instability in baseline critic estimates can be observed in the
plot corresponding to IMINE when the true MI is higher
than 16. Estimates by ASKL critic does not suffer from such
instability and it is to be noted that the ASKL critic also
produces comparatively low variance MI.
We compute bias, variance, and root mean square error
of the estimated MI values to quantitatively evaluate the
proposed ASKL critic’s performance against the baseline. The
bias, variance, and RMSE values have been averaged over 50
experimental trials. Fig. 3(a) and Fig. 3(b) show the computed
values for the ASKL critic and the baseline, respectively.
These plots conclusively demonstrate that the ASKL critic
estimates have lower bias and variance characteristics in com-
parison to the baseline critic. Lower variance characteristics
of the ASKL critic can be explained by observing that the
empirical Rademacher complexity of ASKL critic’s hypothesis
space is bounded, theorem 3. Hence, generalization error is
guaranteed to be upper bounded. Lower bias in estimates can
be attributed to better control over bias-variance tradeoff.
Experimental results shown in Fig. 3, demonstrates the
effect of change in batch size on the variance of ASKL
and baseline critic estimates. It can be observed that with an
increase in batch size the variance of both ASKL and baseline
estimates decreases. This is due to the fact that the empirical
Rademacher complexity is inversely proportional to the sample
size (refer Appendix A for deﬁnition). Hence, an increase
in batch size leads to a decrease in empirical Rademacher
complexity and, corresponding decrease in variance of theMI estimates. Another key observation on the variance of MI
estimates which holds for both critics is that with an increase
in true MI the variance of the empirical estimates increases.
This observations can be explained by noticing the effect of
increase in the value of true MI on the log likelihood density
ratio between the joint and product of marginal distributions,
log(dPXY/dPX⊗PY). The absolute value of the log density
ratio evaluated at any given sample increases with increase in
MI. The optimal critics for variational lower bound estimates
of MI depend on the log density ratio. Hence, to match the
increase in log density ratio the constant Mwhich uniformly
bounds the critic’s hypothesis space also increases. As de-
scribed in theorem 2, the generalization error bounds depend
on both empirical Rademacher complexity and eM, hence, an
increase inMleads to an increase in variance of MI estimates.
Bias-variance tradeoff for different values of λ1andλ2
in ASKL critic, ﬁgure 4. Figures 4(a) and 4(b) are the bias
and variance plots, respectively. The left top most plots in
both ﬁgures, 4(a) and 4(b) correspond to λ1andλ2set to 0,
respectively. It can be seen in these plots that even without any
explicit regularisation estimates using ASKL critic have lower
bias and lower variance in comparison to the baseline critic.
This veriﬁes our claim that constraining the complexity of the
hypothesis space leads to signiﬁcant improvement in reliability
of these estimates. It is evident from these plots that regular-
ization weights are also effective in controlling the bias, as λ1
andλ2increase the estimates get biased in negative direction.
This demonstrates the efﬁcacy of the proposed regularization
terms in inducing effective bias-variance tradeoff.
VI. C ONCLUSION
In the proposed work, we successfully demonstrate the
effect of controlling the complexity of critic’s hypothesis space
on the variance of sample based empirical estimates of mutual
information. We negate the high variance characteristics of
variational lower bound based estimates of MI by constructing
the critic’s hypothesis space in a Reproducing Kernel Hilbert
Space, which corresponds to a critic learned using Auto-
mated Spectral Kernel Learning architecture. By analysing
the generalisation bounds using Radmacher complexity of the
constrained critic space, we demonstrate effective regulari-
sation of bias-variance tradeoff on four different variational
lower bounds of Mutual information. In larger scheme of
Explainable-AI, this work theoretically motivates the implica-
tions of understanding the effect of regulating the complexity
of deep neural network based critic hypothesis spaces on the
bias-variance tradeoff of variational lower bound estimators of
mutual information.
REFERENCES
[1] T. Q. Chen, X. Li, R. B. Grosse, and D. K. Duvenaud, “Isolating sources
of disentanglement in variational autoencoders,” in Advances in Neural
Information Processing Systems , 2018, pp. 2610–2620.
[2] H. Kim and A. Mnih, “Disentangling by factorising,” arXiv preprint
arXiv:1802.05983 , 2018.
[3] A. A. Alemi, B. Poole, I. Fischer, J. V . Dillon, R. A. Saurous, and
K. Murphy, “Fixing a broken elbo,” arXiv preprint arXiv:1711.00464 ,
2017.
[4] R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal, P. Bach-
man, A. Trischler, and Y . Bengio, “Learning deep representations
by mutual information estimation and maximization,” arXiv preprint
arXiv:1808.06670 , 2018.
[5] X. Chen, Y . Duan, R. Houthooft, J. Schulman, I. Sutskever, and
P. Abbeel, “Infogan: Interpretable representation learning by information
maximizing generative adversarial nets,” in Advances in neural informa-
tion processing systems , 2016, pp. 2172–2180.
[6] Y . Li, “Which way are you going? imitative decision learning for path
forecasting in dynamic scenes,” in The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , June 2019.
[7] N. Tishby, F. C. Pereira, and W. Bialek, “The information bottleneck
method,” arXiv preprint physics/0004057 , 2000.
[8] N. Tishby and N. Zaslavsky, “Deep learning and the information bot-
tleneck principle,” in 2015 IEEE Information Theory Workshop (ITW) .
IEEE, 2015, pp. 1–5.
[9] A. A. Alemi, I. Fischer, J. V . Dillon, and K. Murphy, “Deep variational
information bottleneck,” arXiv preprint arXiv:1612.00410 , 2016.
[10] A. v. d. Oord, Y . Li, and O. Vinyals, “Representation learning with
contrastive predictive coding,” arXiv preprint arXiv:1807.03748 , 2018.
[11] M. I. Belghazi, A. Baratin, S. Rajeshwar, S. Ozair, Y . Bengio,
A. Courville, and D. Hjelm, “Mutual information neural estimation,”
inInternational Conference on Machine Learning , 2018, pp. 531–540.
[12] B. Poole, S. Ozair, A. van den Oord, A. A. Alemi, and G. Tucker, “On
variational lower bounds of mutual information,” in NeurIPS Workshop
on Bayesian Deep Learning , 2018.
[13] J. Song and S. Ermon, “Understanding the limitations of variational
mutual information estimators,” arXiv preprint arXiv:1910.06222 , 2019.
[14] S. Ghimire, P. K. Gyawali, and L. Wang, “Reliable estimation of
kullback-leibler divergence by controlling discriminator complexity in
the reproducing kernel hilbert space,” arXiv preprint arXiv:2002.11187 ,
2020.
[15] A. M. Fraser and H. L. Swinney, “Independent coordinates for strange
attractors from mutual information,” Physical review A , vol. 33, no. 2,
p. 1134, 1986.
[16] Y .-I. Moon, B. Rajagopalan, and U. Lall, “Estimation of mutual infor-
mation using kernel density estimators,” Physical Review E , vol. 52,
no. 3, p. 2318, 1995.[17] A. Kraskov, H. St ¨ogbauer, and P. Grassberger, “Estimating mutual
information,” Physical review E , vol. 69, no. 6, p. 066138, 2004.
[18] X. Nguyen, M. J. Wainwright, and M. I. Jordan, “Estimating divergence
functionals and the likelihood ratio by convex risk minimization,” IEEE
Transactions on Information Theory , vol. 56, no. 11, pp. 5847–5861,
2010.
[19] M. D. Donsker and S. S. Varadhan, “Asymptotic evaluation of certain
markov process expectations for large time. iv,” Communications on
Pure and Applied Mathematics , vol. 36, no. 2, pp. 183–212, 1983.
[20] J. Li, Y . Liu, and W. Wang, “Automated spectral kernel learning,” arXiv
preprint arXiv:1909.04894 , 2019.
[21] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y . Bengio, “Generative adversarial nets,” in
Advances in neural information processing systems , 2014, pp. 2672–
2680.
[22] K. Ahuja, “Estimating kullback-leibler divergence using kernel ma-
chines,” arXiv preprint arXiv:1905.00586 , 2019.
[23] J. Shawe-Taylor, N. Cristianini et al. ,Kernel methods for pattern
analysis . Cambridge university press, 2004.
[24] B. Scholkopf and A. J. Smola, Learning with kernels: support vector
machines, regularization, optimization, and beyond . MIT press, 2001.
[25] G. R. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, and M. I.
Jordan, “Learning the kernel matrix with semideﬁnite programming,”
Journal of Machine learning research , vol. 5, no. Jan, pp. 27–72, 2004.
[26] C. Cortes, M. Mohri, and A. Rostamizadeh, “L2 regularization for
learning kernels,” arXiv preprint arXiv:1205.2653 , 2012.
[27] ——, “Learning non-linear combinations of kernels,” in Advances in
neural information processing systems , 2009, pp. 396–404.
[28] A. Wilson and R. Adams, “Gaussian process kernels for pattern discov-
ery and extrapolation,” in International conference on machine learning ,
2013, pp. 1067–1075.
[29] M. L ´azaro-Gredilla, J. Qui ˜nonero-Candela, C. E. Rasmussen, and A. R.
Figueiras-Vidal, “Sparse spectrum gaussian process regression,” The
Journal of Machine Learning Research , vol. 11, pp. 1865–1881, 2010.
[30] W. Rudin, Fourier analysis on groups . Wiley Online Library, 1962,
vol. 121967.
[31] A. M. Yaglom, “Correlation theory of stationary and related random
functions.” Volume I: Basic Results. , vol. 526, 1987.
[32] Y .-L. K. Samo and S. Roberts, “Generalized spectral kernels,” arXiv
preprint arXiv:1506.02236 , 2015.
[33] J.-F. Ton, S. Flaxman, D. Sejdinovic, and S. Bhatt, “Spatial mapping
with gaussian processes and nonstationary fourier features,” Spatial
statistics , vol. 28, pp. 59–78, 2018.
[34] A. G. Wilson, Z. Hu, R. Salakhutdinov, and E. P. Xing, “Deep kernel
learning,” in Artiﬁcial Intelligence and Statistics , 2016, pp. 370–378.
[35] A. Wilson and H. Nickisch, “Kernel interpolation for scalable structured
gaussian processes (kiss-gp),” in International Conference on Machine
Learning , 2015, pp. 1775–1784.
[36] A. G. Wilson, Z. Hu, R. R. Salakhutdinov, and E. P. Xing, “Stochastic
variational deep kernel learning,” in Advances in Neural Information
Processing Systems , 2016, pp. 2586–2594.
[37] C.-L. Li, W.-C. Chang, Y . Mroueh, Y . Yang, and B. P ´oczos, “Implicit
kernel learning,” arXiv preprint arXiv:1902.10214 , 2019.
[38] H. Xue, Z.-F. Wu, and W.-X. Sun, “Deep spectral kernel learning,”
inProceedings of the 28th International Joint Conference on Artiﬁcial
Intelligence . AAAI Press, 2019, pp. 4019–4025.
[39] A. Berlinet and C. Thomas-Agnan, Reproducing kernel Hilbert spaces
in probability and statistics . Springer Science & Business Media, 2011.
[40] P. L. Bartlett, O. Bousquet, S. Mendelson et al. , “Local rademacher
complexities,” The Annals of Statistics , vol. 33, no. 4, pp. 1497–1537,
2005.
[41] D. McAllester and K. Stratos, “Formal limitations on the measurement
of mutual information,” arXiv preprint arXiv:1811.04251 , 2018.
[42] S. Nowozin, B. Cseke, and R. Tomioka, “f-gan: Training generative
neural samplers using variational divergence minimization,” in Advances
in neural information processing systems , 2016, pp. 271–279.
[43] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
arXiv preprint arXiv:1412.6980 , 2014.
Reducing the Variance of Variational Estimates
of Mutual Information by Limiting the Critic’s
Hypothesis Space to RKHS Appendix
P Aditya Sreekar, Ujjwal Tiwari and Anoop Namboodri
Center for Visual Information Technology
International Institute of Information Technology, Hyderabad
A Rademacher Complexity
In problems pertinent to machine learning obtaining practical generalization
error bound is crucial for proper model selection. Generalization error bounds
are typically contained by a measure of the complexity of the learning model’s
hypothesis space, for example, the covering number of the hypothesis function
space. The data-driven Rademacher’s complexity used in this work is described
as follows:
Let (X,P) be a probability space and Fbe the class of measurable functions
fromXtoR. Consider X1,X2,...,Xnto beni.i.d data samples from P, with
the corresponding empirical distribution denoted by Pn. Now, letσ1,σ2,...,σn
benindependent discrete random variables for which Pr(σ= 1) =Pr(σ=
−1) =1
2known as the Rademacher random variables. Then, for any f∈F we
deﬁne
Rn(f) =1
nn∑
i=1σif(Xi), RnF= sup
f∈FRnf
ˆRn(F) =Eσ[Rn(F)],Rn(F) =E[Rn(F)](1)
Where, Eσdenotes expectation with respect to the Rademacher random vari-
ables,{σi}n
i=1. And Eis the expectation with respect to Rademacher random
variables and data samples, {Xi}n
i=1.Rn(F) and ˆRn(F) are the Rademacher
average and empirical (conditional) Rademacher average of F, respectively. In-
tuitive reason for Rn(F) as a measure of complexity is that it quantiﬁes the
extent to which a function from the class Fcan correlate to random noise,
a function belonging to a complex set can correlate to any random sequence.
For a comprehensive overview of Rademacher averages and it’s properties refer
to [1–3]. Results from the aforementioned research work that have been used in
the proofs related to our work are mentioned below.
The following is the concentration inequality that depicts the relation be-
tween Rademacher averages and empirical Rademacher averages. The deriva-
1arXiv:2011.08651v1  [cs.LG]  17 Nov 2020
tion utilizes Talagrand’s inequality, kindly refer to Lemma A.4 in [3] for full
derivation.
Lemma A.1. LetFbe a class functions with range [a,b]. For ﬁxedδ>0, with
probability of atleast 1−δ,
Rn(F)≤inf
α∈(0,1)(1
1−αˆRn(F) +(b−a)log(1
δ)
4nα(1−α))
The expected maximum deviation of empirical means from actual can be
bounded by Rademacher averages as shown in the following bound. Check
Lemman A.5 in [3] for derivation.
Lemma A.2. For any class of function Fwe have,
max(
Esup
f∈F(EP[f]−EPn[f]),Esup
f∈F(EPn[f]−EP[f]))
≤2Rn(F)
Where, EPn[f] is the empirical mean given nsamples from Pgiven by
1
n∑n
i=1f(Xi). Using Lemma A.1 and Lemma A.2, one can relate expected
maximum deviation of empirical estimate from actual value to the empirical
Rademacher averages.
We would like to point a minor error in the derivation of the generalization
error bound in et al. [4] where Lemma A.2 has been used. In their work left
hand side of the bound has been misinterpreted as maximum deviation instead
of expected maximum deviation. To relate maximum deviation to Rademacher
average we need another bound before Lemma A.2 which relates maximum devi-
ation to expected maximum deviation. We will look at this corrected approach
in the next section where we derive the generalization results for our work.
The following simple structural result can be used to express Rademacher
averages for a complex class of functions in terms of Rademacher averages of
simple class of functions.
Lemma A.3. Ifφ:R→Ris Lipschitz with constant Lφand satisﬁes φ(0) = 0 ,
thenRn(φ◦F)≤2LφRn(F)
Next, we look at the empirical Rademacher average for the class of functions
represented by our ASKL critic.
Theorem A.4. The empirical Rademacher average of the RKHS Flearned by
the ASKL critic can be bounded and is described as follows,
ˆRn(F)≤B
n√n∑
i=1∥φ(xi)∥2
2≤B√n
Where,B= sup
f∈F∥w∥2.
2
Proof.
ˆRn(F) =1
nEσ[
sup
f∈F(n∑
i=1σif(xi))]
=1
nEσ[
sup
f∈F(w⊺Φσ)]
Here Φσ=∑n
i=1σiφ(xi) is aDdimensional vector
ˆRn(F) =1
nEσ[
sup
f∈F(w⊺Φσ)]
≤1
nEσ[
sup
f∈F(∥w∥2∥Φσ∥2)]
(2)
≤B
nEσ[∥Φσ∥2]
≤B
n√
Eσ[∥Φσ∥2
2] (3)
Where step 2 is a direct implication of the Cauchy-Schwarz inequality.
Eσ[
∥Φσ∥2
2]
=Eσ
n∑
i=1n∑
j=1σiσjφ(xi)⊺φ(xj)
=n∑
i=1∥φ(xi)∥2
2
=1
2Dn∑
i=1D∑
j=1cos((
ωj−ω′
j)⊺xi)
+ 1≤n(4)
From 3 and 4 we have the ﬁnal result.
B Generalization Error Bounds
In this section we derive the generalization error bounds contributed in the scope
of paper. We represent joint distribution, PXYasPand the product of marginal
distributions, PX⊗PY, asQ. Both distribution are deﬁne on measurable space
(X×Y,ΣXY).PnandQmrepresents the corresponding empirical distributions
and the pair ( x,y) is referred as z. The proofs use McDiarmid’s inequality which
is described as follows:
Lemma B.1 (McDiarmid’s inequality) .LetX1,...,Xnbe independent random
variables taking values in a set X, and assume that φ:Xn→Rsatisﬁes
sup
x1,...,xn,x′
i∈X|φ(x1,...,xn)−φ(x1,...,xi−1,x′
i,xi+1,...,xn)|≤ci
for every 1≤i≤n.
3"
1908.06487,D:\Database\arxiv\papers\1908.06487.pdf,"In the context of addressing class imbalance in machine learning, what are the potential drawbacks of solely reducing the number of majority samples to match the number of minority samples, and how might future research address these drawbacks?","This approach can lead to the loss of valuable information from the majority class, especially when the imbalance ratio is high. Future research could explore methods to retain more information from the majority class while still achieving a balanced dataset, perhaps by using the accuracy of predicting majority samples as a parameter for selecting which samples to keep.","4
Algorithm 2 Soft Under-sampling Using Neural Network
1:n1← −number of samples of the minority class
2:n2← −number of samples of the majority class
3:m← −number of attributes
4:majSamples [1. . . n 2]← −Samples of the Majority class
5:minSamples [1. . . n 1]← −Samples of the Minority class
6:ifm > threshold then
7: model← −autoencoder.train (minSamples [1. . . n 1])
8:else
9: model← −simpleANN.train (minSamples [1. . . n 1])
10:end if
11:distArray← −{}
12:for eachx∈n2do
13: x′← −model.predict (x)
14: d← −||x−x′||2
2
15: index← −x.index
16: distArray← −distArray∪{d, index}
17:end for
18:sortedMinorityIndexList =Sort the indices of n1
samples according to descending order of distance
19:lastMid avg=Average distance of half of the mi-
nority samples whose indices are in ﬁrst half of
sortedMinorityIndexList
20:Maxdist =Maximum distance calculated among the
minority samples’ with their prediction
21:foreach sample in n2do
22: Predict the majority sample’s attribute using the trained
model
23: Calculate euclidean distance between real and predicted
attributes of the sample
24: Map this distance with index of the sample
25:end for
26:sortedMajorityList =Sort the indices of n2samples
according to descending order of distance
27:selectedIndices = []
28:foreach index in sortedMajorityList do
29: dist=MajoritySamples [index ].dist
30: ifdist > Maxdist ordist > lastMid avgthen
31: selectedIndices.append (index )
32: end if
33:end for
34:X1= []
35:foreach index in selectedIndices do
36: Append( X1, MajoritySamples [index ])
37:end for
38:finalData =Combine (X1, MinoritySamples [1. . . n 1])
B. Evaluation Criteria
For evaluating the performance of our proposed algo-
rithm, we use some ROC (Receiver Operating Characteristics)
curve [ 26] based performance metrics. Let +,- represent positive
and negative class labels. Table II called confusion matrix
represents performance of classiﬁcation algorithm. Based on the
confusion matrix in Table II the performance metrics as deﬁned
in this section are used to evaluate learning of imbalanced data
sets by our proposed algorithms.Table II: Confusion Matrix
Predicted
+ -Actual+ True Positive (TP) False Negative (FN)
- False Positive (FP) True Negative (TN)
For comparing the performance of different undersampling
algorithms on classiﬁcation, we use the metric Area under the
Receiver Operating Characteristics (ROC) curve[ 26], the area
under ROC curve is popularly known as AUC. AUC value
measures the degree of separability between classes. Higher
value of AUC indicates that the model is more capable of
distinguishing the classes than a model with lower AUC value.
The problem with imbalanced dataset is that any machine
learning algorithm trained on these data becomes more biased
towards the majority class. In addition, overlapping of samples
from different classes also poses a problem to the performance
of the model because it can not distinguish between classes.
This phenomenon is reﬂected in lower AUC value during
evaluation. Under-sampling potentially can solve the problem
of imbalance by removing some samples from the majority
class and thus by making the dataset more balanced. AUC
value becomes higher when trained with these balanced data.
In Table V, VIII and XI and XIV, we showed the AUC
values of different machine learning models on some originally
imbalanced datasets [ 27] resampled by several under-sampling
techniques. The G-mean is deﬁned as the square root of the
product of true positives (TP) and false positives (FP). The
equation is as follows.
G−mean =√
TP×TN (1)
The F1 measure is another popular performance metric to
evaluate the performance of classiﬁcation algorithms which is
deﬁned as follows.
F1 =2×precision×recall
precision +recall(2)
The terms precision and recall in this formula refer to the ratio
of true positives (TP) and false positives (FP) respectively to
the total number of samples, deﬁned as follows:
precision =TP
TP+FP
recall =TP
TP+FN
C. Description of Dataset
We have used four real world datasets to do experiment on
the proposed algorithms. All of them are from UCI machine
learning repository [ 28]. The imbalanced ratio is deﬁned as
Nmaj/Nmin. The description of the data sets are available in
Table III.
5
Table III: Description of Dataset
Dataset #attribute #min #maj Ratio
Ionosphere 34 126 225 1.78
Balance 4 49 576 11.8
Pima 8 268 500 1.9
Satimage 36 626 5809 9.27
The number of majority samples selected by each under
sampling algorithms are described in Table IV.
Table IV: Number of Majority Samples Chosen by each
undersampler
Dataset ENN AKNN NM1 NM2 NM3 NUS1 NUS2 CC NCR TLL RUS
Ionosphere 216 215 126 126 99 126 105 126 146 225 126
Balance 452 427 49 49 49 49 161 49 544 571 49
Pima 279 249 268 268 268 268 204 268 261 450 268
Satimage 5319 5213 626 626 626 626 3045 626 5449 5770 626
In almost all cases, we found that, our proposed undersam-
plers, NUS1 and NUS2 outperform all other undersamplers
in case of almost all training algorithms. NUS1 and NUS2
resample the data in such a way that they become more
separable as noted from Figures 3 and 4. This leads to higher
AUC, G-mean and F1 values and hence better performance.
It is to be noted that we have used a number of classiﬁers
to verify that the proposed undersampling algorithms are not
classiﬁer dependent.
Table V: AUC values of Balance dataset using various classi-
ﬁers
Balance Dataset
Method GradBoost SGD KNN RF LR
ENN 0.498±0.060 0 .500±0.006 0 .554±0.111 0 .498±0.030 0 .500±0.000
AKNN 0.511±0.063 0 .500±0.000 0 .595±0.090 0 .516±0.058 0 .500±0.000
NM1 0.417±0.193 0 .497±0.145 0 .513±0.188 0 .278±0.147 0 .526±0.179
NM2 0.783±0.155 0 .496±0.155 0 .744±0.144 0 .772±0.157 0 .415±0.190
NM3 0.447±0.152 0 .494±0.129 0 .636±0.202 0 .366±0.209 0 .492±0.175
NUS1 0.887±0.119 0.860±0.215 0.971±0.086 0 .897±0.117 0.897±0.122
NUS2 0.809±0.148 0.768±0.181 0.842±0.114 0 .796±0.128 0.721±0.143
CC 0.964±0.087 0 .476±0.142 0 .582±0.162 0.944±0.098 0 .359±0.154
NCR 0.494±0.022 0 .501±0.008 0 .518±0.066 0 .497±0.008 0 .500±0.000
TLL 0.495±0.013 0 .500±0.000 0 .496±0.031 0 .493±0.010 0 .500±0.000
RUS 0.561±0.181 0 .519±0.099 0 .657±0.201 0 .522±0.201 0 .487±0.194
Table VI: G-Mean values of Balance dataset using various
classiﬁers
Balance Dataset
Method GradBoost SGD KNN RF LR
ENN 0.081±0.275 0 .000±0.000 0 .313±0.366 0 .019±0.151 0 .000±0.000
AKNN 0.211±0.340 0 .000±0.000 0 .451±0.276 0 .147±0.340 0 .000±0.000
NM1 0.366±0.185 0 .192±0.471 0 .483±0.211 0 .213±0.230 0 .513±0.209
NM2 0.766±0.178 0 .103±0.360 0 .724±0.158 0 .780±0.161 0 .368±0.232
NM3 0.391±0.209 0 .171±0.436 0 .622±0.201 0 .295±0.212 0 .447±0.182
NUS1 0.871±0.129 0.809±0.428 0.969±0.108 0 .895±0.130 0.895±0.150
NUS2 0.791±0.182 0.736±0.232 0.824±0.177 0 .770±0.166 0.661±0.208
CC 0.957±0.076 0 .126±0.351 0 .567±0.214 0.945±0.097 0 .347±0.178
NCR 0.019±0.149 0 .000±0.000 0 .155±0.345 0 .013±0.123 0 .000±0.000
TLL 0.000±0.000 0 .000±0.000 0 .006±0.089 0 .000±0.000 0 .000±0.000
RUS 0.528±0.244 0 .206±0.416 0 .605±0.261 0 .518±0.201 0 .472±0.224Table VII: F1 values of Balance dataset using various classiﬁers
Balance Dataset
Method GradBoost SGD KNN RF LR
ENN 0.935±0.021 0 .933±0.216 0 .944±0.014 0 .943±0.012 0 .949±0.004
AKNN 0.936±0.022 0 .946±0.004 0 .952±0.014 0 .944±0.010 0 .946±0.004
NM1 0.370±0.253 0 .390±0.580 0 .523±0.196 0 .158±0.231 0 .541±0.256
NM2 0.796±0.178 0 .372±0.586 0 .771±0.121 0 .795±0.169 0 .405±0.228
NM3 0.382±0.254 0 .340±0.536 0 .631±0.194 0 .250±0.264 0 .482±0.214
NUS1 0.890±0.118 0 .888±0.145 0.976±0.088 0 .908±0.099 0 .907±0.100
NUS2 0.930±0.060 0 .877±0.220 0 .949±0.033 0 .930±0.045 0 .920±0.041
CC 0.951±0.099 0 .308±0.590 0 .515±0.251 0 .946±0.105 0 .331±0.244
NCR 0.953±0.010 0 .957±0.004 0 .955±0.010 0.955±0.006 0 .957±0.003
TLL 0.953±0.012 0.959±0.003 0 .952±0.011 0 .953±0.009 0.959±0.003
RUS 0.533±0.196 0 .427±0.529 0 .566±0.286 0 .515±0.251 0 .455±0.236
Table VIII: AUC values of Pima dataset using various classiﬁers
Pima Dataset
Method GradBoost SGD KNN RF LR
ENN 0.859±0.068 0 .826±0.079 0 .857±0.067 0 .867±0.060 0 .852±0.068
AKNN 0.880±0.061 0 .829±0.116 0.887±0.059 0.887±0.059 0.877±0.056
NM1 0.746±0.066 0 .736±0.136 0 .714±0.057 0 .727±0.081 0 .753±0.054
NM2 0.782±0.080 0 .735±0.108 0 .763±0.067 0 .777±0.081 0 .766±0.067
NM3 0.659±0.069 0 .638±0.096 0 .605±0.072 0 .646±0.067 0 .664±0.093
NUS1 0.836±0.059 0 .805±0.105 0 .817±0.056 0 .845±0.058 0 .823±0.056
NUS2 0.865±0.075 0.852±0.092 0 .846±0.065 0 .870±0.070 0 .854±0.065
CC 0.680±0.091 0 .644±0.095 0 .632±0.078 0 .686±0.088 0 .679±0.090
NCR 0.870±0.059 0 .806±0.078 0 .858±0.053 0 .858±0.055 0 .836±0.063
TLL 0.754±0.061 0 .728±0.121 0 .730±0.064 0 .755±0.073 0 .732±0.059
RUS 0.724±0.079 0 .705±0.090 0 .700±0.086 0 .731±0.070 0 .727±0.085
Table IX: G-Mean values of Pima dataset using various
classiﬁers
Pima Dataset
Method GradBoost SGD KNN RF LR
ENN 0.853±0.061 0 .822±0.107 0 .854±0.069 0 .860±0.061 0 .852±0.058
AKNN 0.874±0.065 0.837±0.144 0.883±0.053 0.880±0.058 0.873±0.046
NM1 0.739±0.067 0 .702±0.139 0 .705±0.083 0 .725±0.059 0 .752±0.073
NM2 0.778±0.084 0 .711±0.209 0 .760±0.086 0 .777±0.083 0 .765±0.082
NM3 0.644±0.080 0 .542±0.300 0 .601±0.107 0 .638±0.086 0 .660±0.079
NUS1 0.841±0.065 0 .805±0.120 0 .813±0.063 0 .842±0.064 0 .818±0.071
NUS2 0.886±0.065 0 .825±0.138 0 .837±0.065 0 .865±0.071 0 .850±0.057
CC 0.681±0.072 0 .594±0.256 0 .628±0.067 0 .682±0.072 0 .676±0.085
NCR 0.861±0.064 0 .783±0.119 0 .854±0.061 0 .858±0.062 0 .832±0.066
TLL 0.754±0.079 0 .709±0.172 0 .729±0.803 0 .749±0.081 0 .715±0.078
RUS 0.721±0.076 0 .646±0.177 0 .703±0.062 0 .727±0.075 0 .727±0.083
Table X: F1 values of Pima dataset using various classiﬁers
Pima Dataset
Method GradBoost SGD KNN RF LR
ENN 0.854±0.073 0 .805±0.140 0 .847±0.079 0 .857±0.068 0 .847±0.072
AKNN 0.880±0.065 0 .855±0.077 0.883±0.073 0.890±0.065 0 .874±0.074
NM1 0.739±0.088 0 .708±0.205 0 .691±0.085 0 .730±0.077 0 .751±0.078
NM2 0.770±0.079 0 .721±0.216 0 .746±0.090 0 .773±0.075 0 .760±0.097
NM3 0.639±0.078 0 .597±0.322 0 .587±0.095 0 .643±0.094 0 .654±0.097
NUS1 0.841±0.059 0 .802±0.158 0 .820±0.059 0 .852±0.062 0 .821±0.066
NUS2 0.841±0.065 0.869±0.119 0 .868±0.071 0 .892±0.055 0.880±0.063
CC 0.678±0.103 0 .636±0.255 0 .640±0.094 0 .684±0.086 0 .672±0.101
NCR 0.862±0.057 0 .804±0.123 0 .853±0.066 0 .861±0.067 0 .834±0.079
TLL 0.687±0.107 0 .608±0.317 0 .658±0.097 0 .686±0.092 0 .653±0.100
RUS 0.732±0.078 0 .665±0.327 0 .702±0.067 0 .727±0.067 0 .728±0.076
Table XI: AUC values of Satimage dataset using various
classiﬁers
Satimage Dataset
Method GradBoost SGD KNN RF LR
ENN 0.838±0.032 0 .556±0.149 0 .895±0.032 0 .832±0.036 0 .516±0.015
AKNN 0.852±0.045 0 .566±0.171 0 .902±0.031 0 .852±0.045 0 .516±0.013
NM1 0.823±0.042 0 .566±0.111 0 .790±0.056 0 .834±0.042 0 .705±0.058
NM2 0.831±0.052 0 .649±0.192 0 .832±0.044 0 .851±0.042 0 .733±0.043
NM3 0.692±0.050 0 .524±0.061 0 .688±0.043 0 .715±0.048 0 .572±0.056
NUS1 0.994±0.009 0.993±0.014 0.999±0.004 0.997±0.006 0.976±0.023
NUS2 0.897±0.036 0.993±0.014 0.920±0.027 0.899±0.035 0.862±0.038
CC 0.912±0.035 0 .545±0.142 0 .863±0.046 0 .818±0.044 0 .793±0.048
NCR 0.820±0.043 0 .545±0.142 0 .885±0.034 0 .818±0.044 0 .515±0.010
TLL 0.760±0.044 0 .543±0.149 0 .833±0.039 0 .765±0.044 0 .511±0.011
RUS 0.868±0.033 0 .657±0.151 0 .879±0.037 0 .880±0.034 0 .709±0.043
6
Table XII: G-Mean values of Satimage dataset using various
classiﬁers
Satimage Dataset
Method GradBoost SGD KNN RF LR
ENN 0.825±0.054 0 .286±0.466 0 .888±0.038 0 .818±0.047 0 .000±0.000
AKNN 0.843±0.050 0 .297±0.524 0 .899±0.043 0 .841±0.042 0 .000±0.000
NM1 0.805±0.046 0 .370±0.405 0 .792±0.042 0 .817±0.044 0 .724±0.053
NM2 0.817±0.042 0 .504±0.524 0 .826±0.055 0 .839±0.052 0 .645±0.055
NM3 0.696±0.053 0 .243±0.391 0 .688±0.054 0 .719±0.052 0 .596±0.049
NUS1 0.995±0.008 0.993±0.014 0.999±0.004 0.998±0.005 0.976±0.022
NUS2 0.893±0.036 0 .874±0.074 0 .920±0.033 0 .894±0.032 0.862±0.038
CC 0.909±0.041 0 .545±0.142 0 .858±0.034 0 .902±0.047 0 .765±0.041
NCR 0.801±0.052 0 .234±0.469 0 .879±0.034 0 .797±0.045 0 .000±0.000
TLL 0.731±0.051 0 .207±0.479 0 .824±0.039 0 .738±0.063 0 .000±0.000
RUS 0.867±0.041 0 .573±0.363 0 .878±0.034 0 .878±0.043 0 .688±0.046
Table XIII: F1 values of Satimage dataset using various
classiﬁers
Satimage Dataset
Method GradBoost SGD KNN RF LR
ENN 0.974±0.006 0 .886±0.273 0 .985±0.005 0 .976±0.005 0 .945±0.003
AKNN 0.977±0.006 0 .887±0.203 0 .986±0.005 0 .979±0.006 0 .944±0.003
NM1 0.821±0.044 0 .471±0.558 0 .792±0.054 0 .833±0.042 0 .715±0.054
NM2 0.825±0.042 0 .645±0.161 0 .821±0.057 0 .843±0.047 0 .705±0.055
NM3 0.703±0.057 0 .443±0.536 0 .687±0.051 0 .721±0.051 0 .573±0.060
NUS1 0.994±0.009 0.993±0.014 0.999±0.004 0.997±0.006 0.975±0.024
NUS2 0.975±0.008 0.965±0.019 0 .975±0.007 0 .975±0.008 0.968±0.008
CC 0.913±0.030 0 .723±0.115 0 .863±0.036 0 .975±0.006 0 .770±0.053
NCR 0.971±0.006 0 .901±0.224 0 .978±0.005 0 .975±0.006 0 .946±0.002
TLL 0.966±0.006 0 .897±0.224 0 .969±0.008 0 .969±0.006 0 .949±0.002
RUS 0.868±0.033 0 .635±0.197 0 .871±0.041 0 .877±0.036 0 .672±0.060
Table XIV: AUC values of Ionosphere dataset using various
classiﬁers
Ionosphere Dataset
Method GradBoost SGD KNN RF LR
ENN 0.916±0.072 0 .842±0.089 0 .823±0.089 0 .929±0.050 0 .839±0.085
AKNN 0.918±0.058 0 .845±0.085 0 .820±0.078 0 .925±0.052 0 .837±0.089
NM1 0.914±0.084 0 .804±0.119 0 .794±0.095 0 .926±0.071 0 .808±0.084
NM2 0.936±0.064 0 .809±0.125 0 .777±0.111 0 .940±0.056 0 .823±0.095
NM3 0.898±0.079 0 .768±0.200 0 .771±0.096 0 .903±0.082 0 .797±0.099
NUS1 0.935±0.059 0.859±0.171 0.889±0.075 0 .940±0.060 0.856±0.104
NUS2 0.933±0.073 0.868±0.208 0.879±0.068 0 .940±0.059 0.878±0.065
NCR 0.934±0.059 0.859±0.109 0 .859±0.098 0.943±0.063 0 .854±0.084
TLL 0.913±0.083 0 .834±0.116 0 .780±0.102 0 .920±0.075 0 .825±0.093
RUS 0.904±0.091 0 .831±0.103 0 .818±0.088 0 .910±0.074 0 .828±0.077
CC 0.902±0.090 0 .812±0.118 0 .791±0.102 0 .908±0.084 0 .818±0.085
Table XV: G-mean values of Ionosphere dataset using various
classiﬁers
Ionosphere Dataset
Method GradBoost SGD KNN RF LR
ENN 0.916±0.073 0 .835±0.097 0 .802±0.111 0 .927±0.052 0 .825±0.103
AKNN 0.912±0.059 0 .839±0.092 0 .798±0.099 0 .923±0.054 0 .822±0.105
NM1 0.916±0.079 0 .796±0.133 0 .775±0.117 0 .925±0.071 0 .798±0.096
NM2 0.932±0.053 0 .801±0.132 0 .753±0.128 0 .939±0.057 0 .811±0.107
NM3 0.901±0.088 0 .750±0.268 0 .748±0.117 0 .902±0.084 0 .789±0.104
NUS1 0.932±0.059 0.859±0.299 0.858±0.083 0.944±0.052 0.853±0.104
NUS2 0.927±0.075 0 .839±0.385 0.870±0.079 0 .939±0.061 0.870±0.072
CC 0.896±0.073 0 .801±0.153 0 .768±0.130 0 .901±0.082 0 .808±0.099
NCR 0.934±0.048 0 .853±0.122 0 .846±0.117 0 .942±0.065 0 .844±0.095
TLL 0.913±0.071 0 .825±0.129 0 .752±0.129 0 .918±0.079 0 .809±0.113
RUS 0.897±0.072 0 .825±0.111 0 .800±0.107 0 .909±0.075 0 .821±0.084Table XVI: F1 scores of Ionosphere dataset using various
classiﬁers
Ionosphere Dataset
Method GradBoost SGD KNN RF LR
ENN 0.901±0.090 0 .803±0.117 0 .782±0.131 0 .917±0.061 0 .803±0.118
AKNN 0.904±0.071 0 .808±0.110 0 .778±0.118 0 .913±0.063 0 .801±0.127
NM1 0.912±0.084 0 .793±0.119 0 .794±0.095 0 .925±0.071 0 .781±0.115
NM2 0.933±0.067 0 .799±0.114 0 .723±0.155 0 .937±0.058 0 .794±0.125
NM3 0.901±0.077 0 .789±0.129 0 .723±0.143 0 .909±0.075 0 .786±0.117
NUS1 0.932±0.068 0.879±0.127 0.847±0.094 0.943±0.055 0.841±0.118
NUS2 0.937±0.068 0.883±0.128 0.861±0.090 0.943±0.053 0.861±0.080
NCR 0.929±0.066 0 .844±0.119 0 .833±0.135 0 .939±0.070 0 .830±0.110
TLL 0.896±0.105 0 .794±0.147 0 .714±0.161 0 .904±0.093 0 .783±0.134
RUS 0.901±0.094 0 .821±0.108 0 .780±0.127 0 .909±0.077 0 .807±0.098
CC 0.897±0.097 0 .805±0.110 0 .741±0.158 0 .904±0.088 0 .792±0.118
V. U NDERSAMPLING ON ARTIFICIAL DATASET
Th datasets on which we experimented so far have lots of
features, which makes it difﬁcult to visualize actually how
the undersamplers undersample those datasets. Hence we have
used two artiﬁcial datasets to visualise the effect of different
under-samplers using scikit-learn package [ 22]. The ﬁrst dataset
consists of two features, which makes it easy to plot the
dimensions and visualize the data. There are 1000 majority
samples and 100 minority samples in the dataset. So, the ratio
of majority samples to minority samples is 10:1. The centers
of two clusters are [0.0 0.0] and [2.0 2.0] respectively. The
standard deviation of the cluster samples from its center are
1.5 and 0.5 each. The effect of each undersampler is shown in
the Figure 3.
Next, we generated the second dataset where the majority
and minority samples are overlapping in nature.For the second
dataset, the ratio of majority to minority is 3.33 : 1 . In this
case, the number of majority samples were same as before
but the number of minority samples were 300. We choose
the center of the two classes to be [0.0 0.0] and [0.02 0.05]
respectively to introduce the overlapping criteria. The standard
deviation of the two cluster samples from the center were
[1.51.5]respectively. The result of each sampler is shown in
Figure 3 and Figure 4.
VI. C OMPARISON BETWEEN THE PROPOSED ALGORITHMS
It is observed from the classiﬁcation results and the ﬁgures
that refered to the effect of each under-sampler in the data
that NUS1 performs well when there exists less overlapping
in data. NUS1 algorithm actually retains those majority data
points which are most distant from most of the minority
data points. But in case of overlapped data, there could be
some minority samples overlapped with the retained majority
data. In case of non-overlapped data, this problem is minimal.
Hence, NUS1 makes balanced dataset linearly separable. On
the other hand, NUS2 ﬁnds the perimeter of minority samples
by calculating the average distance of minority samples to
its generator samples generated by the model. Then it retains
those majority samples that are outside of the perimeter. By this
way, overlapping is removed. Hence NUS2 performs better in
classifying overlapping data. However, the choosing of distance
whether maximum or average is a tunable parameter. We can
indirectly verify the nature of the data by these two proposed
methods.
7
Figure 3: Artiﬁcial dataset resampling with various undersamplers
Figure 4: Artiﬁcial overlapped dataset resampled by various undersamplers
A. Case study
Now we observe a particular case which may arise due to a
certain distribution of the data. It may happen that majority class
data consists of outliers or data points that are at far distances
from the minority data points and also the ratio of majority to
minority is very high. In this case, the outliers from the majority
data points should be removed ﬁrst before implementing the
proposed hard and soft undersampling algorithms. In Figure 5
we have generated an artiﬁcial dataset using scikit-learn [ 22]
package. The ratio of majority to minority is 500 : 50 . The
two proposed algorithms NUS-1 and NUS-2 always select
the 50 points that are located far from minority class at the
time of undersampling. In case of outlier, it may happen that
the algorithms always choose the outlier data points at the
time of undersampling. We have shown the data and effect of
undersampling algorithms on data points in Figure 5.
VII. C ONCLUDING REMARKS
In this paper, we proposed two algorithms to solve the class
imbalance problem. The main target of this paper is to balance
the data i.e. bring down the number of majority samples to
the number of minority samples. This approach might result
Figure 5: NUS-1 and NUS-2 both are trying to choose far
samples from minority class. In the ﬁgure, the coordinate of
the centers are chosen as [0.0,0.0],[5.0,5.0]respectively. The
standard deviation from center are 2.5for the ﬁrst and 5.5
for the later one. We have used make_blob function from
scikit-learn [22] to generate the data points.
8
into some drawbacks. If the majority to minority ratio is vary
high, there is a high probability of loosing information from
majority class. In this scenario, we can use the accuracy of
predicting majority samples as a parameter to choose which
batch of majority samples should be considered to mitigate the
loss. Future works may address this issue.
REFERENCES
[1]Bartosz Krawczyk, Mikel Galar, Łukasz Jele ´n, and Francisco Herrera.
Evolutionary undersampling boosting for imbalanced classiﬁcation of
breast cancer malignancy. Applied Soft Computing , 38:714–726, 2016.
[2]Sun Choi, Young Jin Kim, Simon Briceno, and Dimitri Mavris. Prediction
of weather-induced airline delays based on machine learning algorithms.
In2016 IEEE/AIAA 35th Digital Avionics Systems Conference (DASC) ,
pages 1–6. IEEE, 2016.
[3]Wei Wei, Jinjiu Li, Longbing Cao, Yuming Ou, and Jiahang Chen.
Effective detection of sophisticated online banking fraud on extremely
imbalanced data. World Wide Web , 16(4):449–475, 2013.
[4]CJ Van Rijsbergen. Information retrieval 2nd edition butterworths. London
available on internet , 1979.
[5]Jin Huang and Charles X Ling. Using auc and accuracy in evaluating
learning algorithms. IEEE Transactions on knowledge and Data
Engineering , 17(3):299–310, 2005.
[6]Miroslav Kubat, Stan Matwin, et al. Addressing the curse of imbalanced
training sets: one-sided selection. In Icml, volume 97, pages 179–186.
Nashville, USA, 1997.
[7]Yanmin Sun, Andrew KC Wong, and Mohamed S Kamel. Classiﬁcation of
imbalanced data: A review. International Journal of Pattern Recognition
and Artiﬁcial Intelligence , 23(04):687–719, 2009.
[8]Guillaume Lemaître, Fernando Nogueira, and Christos K. Aridas.
Imbalanced-learn: A python toolbox to tackle the curse of imbalanced
datasets in machine learning. Journal of Machine Learning Research ,
18(17):1–5, 2017.
[9]Ivan Tomek. A generalization of the k-nn rule. IEEE Transactions on
Systems, Man, and Cybernetics , (2):121–126, 1976.
[10] Jorma Laurikkala. Improving identiﬁcation of difﬁcult small classes by
balancing class distribution. In Conference on Artiﬁcial Intelligence in
Medicine in Europe , pages 63–66. Springer, 2001.
[11] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip
Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal
of artiﬁcial intelligence research , 16:321–357, 2002.
[12] Hui Han, Wen-Yuan Wang, and Bing-Huan Mao. Borderline-smote: a new
over-sampling method in imbalanced data sets learning. In International
conference on intelligent computing , pages 878–887. Springer, 2005.
[13] Gustavo EAPA Batista, Ronaldo C Prati, and Maria Carolina Monard. A
study of the behavior of several methods for balancing machine learning
training data. ACM SIGKDD explorations newsletter , 6(1):20–29, 2004.
[14] Haibo He, Yang Bai, Edwardo A Garcia, and Shutao Li. Adasyn:
Adaptive synthetic sampling approach for imbalanced learning. In 2008
IEEE International Joint Conference on Neural Networks (IEEE World
Congress on Computational Intelligence) , pages 1322–1328. IEEE, 2008.
[15] Dennis L Wilson. Asymptotic properties of nearest neighbor rules using
edited data. IEEE Transactions on Systems, Man, and Cybernetics ,
(3):408–421, 1972.
[16] Inderjeet Mani and I Zhang. knn approach to unbalanced data distribu-
tions: a case study involving information extraction. In Proceedings of
workshop on learning from imbalanced datasets , volume 126, 2003.
[17] Leo Breiman. Random forests. Machine learning , 45(1):5–32, 2001.
[18] Jerome H Friedman. Greedy function approximation: a gradient boosting
machine. Annals of statistics , pages 1189–1232, 2001.
[19] RO Duda and PE Hart. Pattern classiﬁcation and scene analysis–john
wiley & sons. New York, NY , 1973.
[20] Tong Zhang. Solving large scale linear prediction problems using
stochastic gradient descent algorithms. In Proceedings of the twenty-ﬁrst
international conference on Machine learning , page 116. ACM, 2004.
[21] Raymond E Wright. Logistic regression. 1995.
[22] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel,
M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas,
A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.
Scikit-learn: Machine learning in Python. Journal of Machine Learning
Research , 12:2825–2830, 2011.
[23] Stefan Van Der Walt, S Chris Colbert, and Gael Varoquaux. The numpy
array: a structure for efﬁcient numerical computation. Computing in
Science & Engineering , 13(2):22, 2011.[24] J. D. Hunter. Matplotlib: A 2d graphics environment. Computing in
Science & Engineering , 9(3):90–95, 2007.
[25] François Chollet et al. Keras. https://keras.io, 2015.
[26] Tom Fawcett. An introduction to roc analysis. Pattern recognition letters ,
27(8):861–874, 2006.
[27] Zejin Ding. Diversiﬁed ensemble classiﬁers for highly imbalanced data
learning and their application in bioinformatics. 2011.
[28] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
Md. Adnan Arefeen Md. Adnan Arefeen is currently
working as a lecturer at United Intenational Univer-
sity, Bangladesh. He completed his graduation from
Bangldesh University of Engneering and Technology.
He is persuing his post graduation degree in the same
university.
Sumaiya Tabassum Nimi Sumaiya Tabassum Nimi
completed her Bachelor of Science in Computer Sci-
ence and Engineering from Bangladesh University of
Engineering and Technology, Bangladesh. Currently
she is working as a lecturer at United International
University.
M Sohel Rahman Prof. Dr. M Sohel Rahman
received his PhD from King’s College, University
of London. Currently, he is working as a professor
at Bangladesh University of Engineering and Tech-
nology. His research area includes bioinformatics,
algorithms, strings, musicology, graph theory, ma-
chine learning."
1901.00466,D:\Database\arxiv\papers\1901.00466.pdf,"The paper describes a method for predicting the final resting position and rotation of a 3D object after an impulse.  What are the key physical properties of the object that the model learns to extract from the point cloud representation, and how do these properties influence the object's motion?","The model learns to extract the object's mass, moment of inertia, and the shape of the contact surface. These properties influence the object's motion by determining its linear and angular velocities, as well as how friction affects its rotation.","Figure 2. Problem input and impulse coordinates. Our method
uses point clouds (red spheres) of object shapes, the impulse vector
J(black arrow), and the impulse position (red circle) to learn and
predict dynamics. The object is initially at rest on a plane. We use
impulse coordinates , aligned with the direction of the impulse, to
represent the input to our model.
datasets contain different sets of objects and are summa-
rized in Figure 3. Each dataset is split into training (80%)
and test sets (20%). Training objects are simulated with
a different random scale from 0.5 to 1.5 for x, y, and z
directions in order to increase shape diversity. There are
two primitive object datasets used for evaluating on rela-
tively low shape diversity. The Box dataset is a single cube,
whereas the Cylinders set contains a variety of cylin-
der shapes. There are four datasets which contain every-
day objects taken from the ShapeNet [8] dataset. These ex-
hibit wide shape diversity and offer a more challenging task.
Lastly, we have a dataset which combines all of the objects
from the previous six to create a large and extremely di-
verse set of shapes. This Combined dataset is split roughly
evenly between shape categories (around 7000–10000 sim-
ulations per category). In total, we use 793distinct object
shapes and ran 98826 simulations to generate our data.
Pre-processing : Outlier simulations where the object
translated more than 7 meters or rotated more than 3000◦
are removed. We also transform all data to a coordinate
system where the x-axis is aligned with the direction of the
applied impulse—we refer to these as impulse coordinates
(see Figure 2). Motion will be most prominent along the
direction of the impulse, thus using impulse coordinates al-
lows our network to better learn object dynamics.
5. Method
To predict ﬁnal rest position and total rotation after an
impulse, we use a neural network trained on simulated data.
5.1. Network Architecture
A straightforward approach may combine all inputs into
one vector and use a multi-layer neural network to directly
regress the ﬁnal state. As we show in Section 6, this na ¨ıve
Figure 3. Data distribution. We generated our data using 6 cate-
gories of shapes both from ShapeNet and primitive shapes ( Box
andCylinders are combined into Primitives ). For each
shape category, we ran thousands of simulations. In total, we use
793 unique object shapes and ran 98826 simulations.
approach cannot learn the intricacies of the non-linear mo-
tion of the object before it comes to rest. We take a more
principled approach and inform the design of our network
based on our understanding of physical laws and priors.
From Equation 1, we observe that the linear and angu-
lar velocities depend on: (1) the applied impulse magni-
tude, direction, position, and its angular impulse ( r×J),
and (2) the shape of the object which affects its mass mand
moment of inertia I. We therefore base our network design
on learning important information related to the applied im-
pulse and shape of the object. Our network (see Figure 4) is
composed of two main branches whose output features are
jointly used to make a ﬁnal position and rotation prediction.
Impulse Processing : The top branch in our network
is the impulse processing branch which takes the applied
impulse, its position, and 4 pairwise terms as input, and
outputs an impulse feature . The 4 pairwise terms are the
products of the components of the impulse with those of the
impulse position r. The aim of this branch is to learn the
effect of the impulse and the angular impulse on the motion
of the object. Since the impulse is parallel to the ground, we
only provide the 2D impulse vector and its position relative
to the center of mass. We observe that the angular impulse
is a cross product ( r×J) which could be difﬁcult to learn.
We encourage the network to learn this relationship by pro-
viding the pairwise product between randJ.
Shape Processing : The bottom shape processing branch
is designed to extract salient shape features that are cru-
cial to making accurate predictions. As seen in Equation 1,
object geometry affects both linear and angular velocities
through its mass (which depends on volume) and moment
of inertia about the vertical axis. The aim of this branch is
to help the network develop notions of volume, mass, and
inertia from a point cloud representation. It must also learn
the effect of the area and shape of the bottom contacting
surface which determines how friction affects total rotation.
Figure 4. Model architecture. Our network takes the impulse, its position, additional pairwise terms, and the object point cloud as input and
predicts the ﬁnal resting position and total rotation that the object undergoes. Since the object’s initial and ﬁnal position are on the plane,
we use only 2 translation and 1 rotation parameters. Numbers in bracket indicate the output size of each layer, + sign is concatenation,
MLP indicates multilayer perceptron, and FC indicates a fully-connected layer. Optional branches are shown in dashed boxes.
To effectively learn this, we use PointNet [30]. As shown in
Figure 4, the initial object point cloud is fed to the PointNet
classiﬁcation network which outputs a global feature that is
further processed to output our ﬁnal shape feature . We use
batch normalization following every layer in the network
besides the ﬁnal output layer.
Prediction : After concatenating the impulse and shape
features, we jointly predict ﬁnal position and total rotation
with a 6-layer multilayer perceptron (MLP). Position is a
2×1 vector and rotation a 1 ×1 vector since the object rests
on the plane in its ﬁnal state. Because rotation affects ﬁ-
nal position, jointly predicting them with the same network
provides improved performance.
Optional Branches : We add two optional branches
(dashed boxes in Figure 4) that are only used to investigate
if our network learns notions of mass, moment of inertia,
and linear and angular velocities (see Section 6). Each op-
tional branch is a single fully-connected layer. The ﬁrst
branch takes a feature from the shape processing branch
to predict the mass and moment of inertia from the point
cloud. The second predicts initial linear and angular veloc-
ities from a feature in the ﬁnal prediction MLP.
5.2. Loss Functions & Training
The goal of the network is to minimize the error be-
tween the predicted and ground truth position and rotation.
We found that using an L2loss for translation and rota-
tion caused the network to focus too much on examples
with large error due to their large translation and total ro-
tation. Instead we propose to use a form of relative error:
for translation we penalize the relative distance between the
predicted ﬁnal position ˆPfand ground truth Pf, and for ro-
tation we use a relative L1error between the predicted total
rotation ˆθand the ground truth θ. We sum the values inthe denominator of the rotation loss Lθto avoid exploding
losses when ground truth rotation is near zero:
Lp=∥Pf−ˆPf∥
∥Pf∥,Lθ=|ˆθ−θ|
|ˆθ|+|θ|. (2)
Our ﬁnal loss is the sum of the two L=Lp+Lθ.
We train all branches of our network jointly using the
Adam [18] optimization algorithm with a starting learning
rate of 0.005 which is exponentially decayed to 1 ×10−5
during training. In the shape processing branch, PointNet
weights are pretrained on ModelNet40 [41], then ﬁne-tuned
during our training process. We train the network for 200
epochs with a batch size of 128 on a single NVIDIA Titan
X GPU. Before training, 20% of the objects in the training
split are set aside as validation data. During training, eval-
uation is performed on the validation set every 5 epochs.
The model weights which result in the lowest validation loss
throughout training are used as the ﬁnal model. In total, our
network architecture has about 2.8 million parameters.
6. Experiments
In this section, we present extensive experiments and
evaluation on the generalization capability of our approach,
justify design choices through ablation studies, and present
comparison to baselines as well as previous work.
Evaluation Metrics : For all experiments, we report
mean relative and absolute errors for position and total ro-
tation. Since absolute errors increase for large motions
and datasets have different distributions, relative error is
the better indicator of model performance making different
datasets comparable. For position, we use the same relative
error used for the loss (Equation 2). For rotation, we report
ImpulseGen, Single ImpulseGen, Combined ObjGen, Single ObjGen, Combined ObjGen, Leave-One-Out
Dataset Position Rotation Position Rotation Position Rotation Position Rotation Position Rotation
Box 2.8 (4.5) 10.9 (8.7) 4.5(8.1) 9.3(7.1) 2.8(4.5) 10.9 (8.7) 3.3(5.0) 9.3(7.2) 4.1(6.6) 11.1 (7.9)
Cylinders 5.4 (10.0) 12.2 (50.5) 6.4(12.0) 16.2 (56.8) 5.9(11.0) 11.5 (48.5) 6.4(11.9) 17.7 (75.7) 8.5(14.9) 17.7 (72.1)
Mugs 3.6 (6.4) 7.2(11.2) 4.2(7.1) 8.9(14.8) 11.8 (20.2) 10.7 (16.9) 8.1(14.1) 10.8 (15.6) 8.9(15.8) 13.5 (21.9)
Trashcans 3.9 (6.7) 9.2(12.6) 4.7(8.5) 9.6(13.7) 6.4(11.4) 11.0 (15.1) 6.2(11.1) 13.0 (19.4) 5.6(10.0) 12.9 (16.0)
Bottles 3.3 (6.2) 5.6(21.5) 6.2(11.1) 12.7 (42.6) 10.2 (19.1) 14.7 (72.0) 9.3(16.9) 15.6 (62.1) 17.4 (28.3) 24.0 (81.7)
Speakers 10.0 (21.6) 14.6 (63.5) 9.1(20.0) 11.4 (47.5) 11.3 (24.9) 13.3 (51.9) 8.9(19.4) 11.4 (43.4) 42.7 (86.1) 160.5 (586.1)
Combined - - 6.2(11.9) 11.8 (34.4) - - 6.9(13.0) 13.2 (39.5) - -
Table 1. Results for impulse generalization (ImpulseGen) and object generalization (ObjGen) experiments. For position, mean relative %
error (and absolute centimeter error in parentheses) is reported. For rotation, mean relative % error (and absolute degree error) is
shown. Single indicates a different model trained for each shape category, Combined indicates a single model trained on the Combined
dataset, and Leave-One-Out indicates models trained on the Combined dataset with the evaluated category left out.
abinned relative error
ηθ=|ˆθ−θ|/b
⌈|θ|/b⌉. (3)
θis the ground truth total rotation and ˆθis the prediction.
For all results we use a bin of b= 30◦. This metric prevents
relative rotation error from exploding when ground truth ro-
tation is near zero but still penalizes poor predictions.
6.1. Impulse Generalization
We ﬁrst perform impulse generalization (ImpulseGen)
experiments to evaluate our model’s robustness to new im-
pulsive forces applied to known objects—an important ca-
pability that helps generalize to novel settings and has been
demonstrated in previous work [7]. For these experiments,
train and test sets contain the same objects but with different
simulations. Model performance is tested after training on
both single object categories and across multiple categories.
Single Category : A separate model is trained for each
distinct object category. The second and third columns of
Table 1 show the mean relative percent errors (absolute er-
rors are in brackets) for position and total rotation of the
six trained single-category models. As previously hypoth-
esized, the network has a harder time predicting total rota-
tion than position resulting in higher errors. However, hav-
ing seen all objects during training, these models make ex-
tremely accurate predictions on novel impulse forces.
Combined Categories : A single model is trained on
theCombined dataset which contains all object categories.
We evaluate this model on both the Combined dataset and
individual datasets so that performance can be compared
to single-category trained models. As shown in columns
four and ﬁve of Table 1, the combined model performs only
slightly worse on the individual category datasets compared
to single-category training. This suggests the network has
effectively learned how varying the force affects angular im-
pulse, and linear and angular velocities in order to perform
well on such a wide range of objects.6.2. Object Generalization
We next perform object generalization (ObjGen) exper-
iments to evaluate whether the learned model is able to
apply accurate dynamics predictions to unseen objects—a
crucial ability for autonomous systems in unseen environ-
ments. Since it is impossible to experience all objects that
an agent will interact with, we would like knowledge of
similarly-shaped objects to inform reasonable predictions
about dynamics in new settings. For these experiments, we
split datasets based on unique objects such that no test ob-
jects are seen during training . Furthermore, the impulses
applied for test objects are disjoint from the training objects
similar to the ImpulseGen experiments. Since our network
is designed speciﬁcally to process object shape and learn
relevant physical properties, we expect it to extract general
features allowing for accurate predictions even on novel ob-
jects. Similar to the ImpulseGen experiment, we evaluate
models trained on both single and combined categories.
Single Category : Results for testing data when a sep-
arate network is trained for each category are shown in Ta-
ble 1 under the blue heading. As expected, the error is
slightly higher but still within 5% of the ImpulseGen single-
category results in most cases. This indicates that the net-
work is able to generalize to unseen objects within the same
shape category. The blue curves in Figure 5 summarize
single-category performance over entire test sets. For po-
sition, around 90% of predictions for all object categories
fall under 20% relative error, while for rotation this number
falls closer to 80-85% especially for the larger and more
diverse Bottles andSpeakers datasets.
Combined Categories : Performance of the model
when trained on the Combined dataset and then evaluated
on all individual datasets is shown under the orange head-
ing in Table 1. In general, performance is very similar to
training on individual datasets and even improves errors in
some cases, for example position predictions for Bottles ,
Mugs , and Speakers . This indicates that exposing the
network to larger shape diversity at training time can help
focus learning on underlying physical relationships rather
Figure 5. Comparison of performance training on single object categories (blue), the full Combined dataset (orange), and Combined
dataset with the evaluated category left out (green). Curves show cumulative fraction of test examples under a certain relative error.
than properties of single or small groups of objects. Im-
provements and drops in performance are indicated by the
blue and orange curves plotted in Figure 5. In order to main-
tain this high performance, the network is likely learning a
general approach to extract salient physical features from
the diverse objects in the Combined dataset rather than
just memorizing how speciﬁc shapes behave.
Physical Understanding : To explore the implicit fea-
ture space of our learned model, we conduct two experi-
ments that require additional supervised outputs from the
network on top of position and total rotation (modiﬁcations
detailed in dashed boxes in Figure 4). In the ﬁrst experi-
ment, we output the mass and moment of inertia of the ob-
ject using a feature from the shape processing branch. Af-
ter training on the combined dataset, the network achieves
8.7% and 3.5% relative errors for moment of inertia and
mass , respectively, without degrading performance on the
main objective. In the second experiment, we supervise ini-
tial linear andangular velocity from a feature in the ﬁnal
prediction branch and are able to achieve 3.5% and 5.8%
relative error, respectively, without affecting the main task.
Since there is no signiﬁcant change in network performance
on ﬁnal state prediction, we conclude that the model has al-
ready developed implicit notions of these physical proper-
ties, resulting in minimal change to its learned feature space
when additional supervised objectives are added.
Out of Category : Lastly, we evaluate performance on
the extreme task of generalizing outside of trained object
categories . For this, we create new combined datasets each
with one object category left out of the training set. We
then evaluate its performance on objects from the left out
category. Results for these experiments are shown under
the green heading in Table 1 and by the green curves in
Figure 5. The network is able to achieve good results on all
left-out object categories except for Speakers . As seen in
Figure 3, Speakers contributes the most unique objects to
theCombined dataset by far; without them, the networkmay not see enough diversity in training to perform well.
Overall, this result shows that we can still make accurate
predictions for objects from completely different categories
in spite of their shape not being close to the trained objects.
The model seems to have developed a deep understanding
of how shape affects dynamics through mass, moment of
inertia, and contact surface in order to generalize to novel
categories at test time. Some predictions from leave-one-
out trained models are visualized in Figure B.2.
6.3. Ablation Study
We compare our proposed architecture to a number of
ablations and modiﬁcations to justify design decisions. A
performance comparison of all model variants is shown in
Figure 7. Every model is trained and evaluated on the
Combined dataset split by unique objects. The ﬁrst two
ablations justify some physically-informed design choices.
The no pairwise products (NPP) model removes the input
augmentation of pairwise terms from the impulse process-
ing branch, while the no impulse coordinates (NIC) model
uses data that does not use impulse coordinates. The next
two models justify our use of PointNet for feature extraction
and 3D point clouds as input. In the ﬁrst model, we replace
the PointNet module in the shape processing branch with
a feed-forward network (NP-MLP). The second (NP-VGG)
shows the advantage of using 3D data to provide shape in-
formation instead of images by replacing PointNet with the
convolutional layers of VGG-16 [34]. This version of the
network is trained with images of the objects in each sim-
ulation, and performs signiﬁcantly worse than using point
cloud data. This indicates that learning to understand subtle
shape variation is imperative to making accurate dynamics
predictions for 3D objects. The last model is a straightfor-
ward baseline MLP that takes in all input data concatenated
together to make position and rotation predictions. The poor
performance of this network highlights the advantage of us-
ing a branched structure.
Figure 6. Sample predictions from models trained on the Combined dataset with one category left out. Initial object state is shown in
shaded grey, ground truth ﬁnal state is in transparent grey, and network prediction is in transparent red. Relative errors are shown. From
left to right the second row shows best performance (green), average performance (orange), and Speakers failure cases (red).
Figure 7. Comparison of architecture variants trained and evalu-
ated on the object generalization Combined dataset. Curves show
cumulative fraction of test examples under a certain relative error.
Dashed lines indicate the mean relative error. The proposed archi-
tecture (Ours) is shown in blue.
6.4. Comparison to Other Work
We compare our method to the hierarchical relation net-
work (HRN) [28] to highlight the differences between ﬁnal
rest state (our work) and their multi-step predictions. Both
models are trained on a small dataset of 1519 scaled boxes
simulated in the NVIDIA FleX engine [24], then evaluated
on 160 held out simulations. Since HRN makes predictions
for the next time step, to infer ﬁnal rest state the model must
roll out over roughly 20 steps for each simulation. Our
model averages 6.8% and11.4% relative error for posi-
tion and rotation, respectively, while HRN achieves 12.2%
and58.0% . It is clear that directly predicting ﬁnal state al-
lows for more accurate long-term observations, especially
for complex rotation motion. However, HRN predicts a de-
tailed trajectory of object motion at 10 Hz which we cannot.7. Limitations and Future Work
Our approach has many limitations and there remains
room for future exploration. In this work, we took a dif-
ferent approach to previous work by predicting the ﬁnal
state of a 3D rigid object instead of multi-step predictions.
We believe that future work should consider closing the
loop by predicting both the ﬁnal state as well as multiple
intermediate states. Our method is fully supervised and
does not explicitly model physical laws like some previ-
ous work [35]—we plan to explore this in future work. We
show our results on purely synthetic data with no noise and
assume that a complete point cloud of an object is available
which may not be the case with real-world depth sensing.
We ignore the physical parameter estimation problem and
assume constant friction and density. We also ignore free
3D dynamics and complex phenomena such as collisions
which are important directions for future work. We believe
that our approach provides a strong foundation for develop-
ing methods for these complex motions.
8. Conclusion
We presented a method for learning to predict the posi-
tion and total rotation of a 3D rigid object subjected to an
impulse and moving along a plane. Our method is capa-
ble of generalizing to previously unseen object shapes and
new impulses not seen during training. We showed that this
challenging dynamics prediction problem can be solved us-
ing a neural network architecture inspired by physical laws
and priors. We train our network on 3D point clouds of a
large shape collection and a large synthetic dataset with ex-
periments showing that we are able to accurately predict the
ﬁnal state of 3D rigid objects with complex dynamics.
Acknowledgements : This work was supported by a
grant from the Toyota-Stanford Center for AI Research,
NSF grant IIS-1763268, and a Vannevar Bush Faculty Fel-
lowship.
References
[1] Bullet physics engine. https://pybullet.org . 3
[2] Unity game engine. https://unity3d.com . 3
[3] P. Agrawal, A. Nair, P. Abbeel, J. Malik, and S. Levine.
Learning to poke by poking: Experiential learning of intu-
itive physics. In Proceedings of the 30th Conference on Neu-
ral Information Processing Systems (NIPS) , 2016. 1, 2
[4] R. Baillargeon and S. Hanko-Summers. Is the top object
adequately supported by the bottom object? young infants’
understanding of support relations. Cognitive Development ,
5(1):29–53, 1990. 1
[5] P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, and
K. kavukcuoglu. Interaction networks for learning about
objects, relations and physics. In Proceedings of the 30th
International Conference on Neural Information Processing
Systems (NIPS) , pages 4509–4517, 2016. 1, 2
[6] P. W. Battaglia, J. B. Hamrick, and J. B. Tenenbaum. Simula-
tion as an engine of physical scene understanding. Proceed-
ings of the National Academy of Sciences , 110(45):18327–
18332, 2013. 2
[7] A. Byravan and D. Fox. Se3-nets: Learning rigid body mo-
tion using deep neural networks. In 2017 IEEE International
Conference on Robotics and Automation (ICRA) , 2017. 1, 2,
6
[8] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
et al. Shapenet: An information-rich 3d model repository.
arXiv preprint arXiv:1512.03012 , 2015. 2, 4
[9] M. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum.
A compositional object-based approach to learning physical
dynamics. In Proceedings of the 5th International Confer-
ence on Learning Representations (ICLR) , 2017. 1, 2
[10] S. Ehrhardt, A. Monszpart, N. J. Mitra, and A. Vedaldi.
Learning A Physical Long-term Predictor. arXiv preprint,
arXiv:1703.00247 , Mar. 2017. 2
[11] S. Ehrhardt, A. Monszpart, N. J. Mitra, and A. Vedaldi. Un-
supervised intuitive physics from visual observations. arXiv
preprint, arXiv:1805.05086 , 2018. 2
[12] S. Ehrhardt, A. Monszpart, A. Vedaldi, and N. J. Mitra.
Learning to Represent Mechanics via Long-term Extrapo-
lation and Interpolation. arXiv preprint arXiv:1706.02179 ,
June 2017. 2
[13] C. Finn, I. Goodfellow, and S. Levine. Unsupervised learn-
ing for physical interaction through video prediction. In Pro-
ceedings of the 30th International Conference on Neural In-
formation Processing Systems (NIPS) , pages 64–72, 2016. 1,
2
[14] C. Finn and S. Levine. Deep visual foresight for planning
robot motion. In International Conference on Robotics and
Automation (ICRA) , 2017. 2
[15] M. Fraccaro, S. Kamronn, U. Paquet, and O. Winther. A
disentangled recognition and nonlinear dynamics model forunsupervised learning. In Advances in Neural Information
Processing Systems (NIPS) , 2017. 2
[16] K. Fragkiadaki, P. Agrawal, S. Levine, and J. Malik. Learn-
ing visual predictive models of physics for playing bil-
liards. In Proceedings of the 4th International Conference
on Learning Representations (ICLR) , 2016. 1, 2
[17] J. B. Hamrick, R. Pascanu, O. Vinyals, A. Ballard, N. Heess,
and P. Battaglia. Imagination-based decision making with
physical models in deep neural networks. In Advances
in Neural Information Processing Systems (NIPS), Intuitive
Physics Workshop , 2016. 2
[18] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. In International Conference for Learning Rep-
resentations (ICLR) , 2015. 5
[19] A. Lerer, S. Gross, and R. Fergus. Learning physical in-
tuition of block towers by example. In Proceedings of the
33rd International Conference on International Conference
on Machine Learning (ICML) , pages 430–438, 2016. 2
[20] A. M. Leslie. The perception of causality in infants. Percep-
tion, 11(2):173–186, 1982. 1
[21] W. Li, S. Azimi, A. Leonardis, and M. Fritz. To fall or not to
fall: A visual approach to physical stability prediction. arXiv
preprint, arXiv:1604.00066 , 2016. 2
[22] W. Li, A. Leonardis, and M. Fritz. Visual stability prediction
for robotic manipulation. In 2017 IEEE International Con-
ference on Robotics and Automation (ICRA) , pages 2606–
2613, May 2017. 2
[23] Z. Liu, W. T. Freeman, J. B. Tenenbaum, and J. Wu. Physical
primitive decomposition. In Proceedings of the 15th Euro-
pean Conference on Computer Vision (ECCV) , 2018. 2
[24] M. Macklin, M. M ¨uller, N. Chentanez, and T.-Y . Kim. Uni-
ﬁed particle physics for real-time applications. ACM Trans-
actions on Graphics (TOG) , 33(4):153, 2014. 8
[25] M. Mirza, A. Courville, and Y . Bengio. Generalizable
Features From Unsupervised Learning. arXiv preprint,
arXiv:1612.03809 , 2016. 2
[26] R. Mottaghi, H. Bagherinezhad, M. Rastegari, and
A. Farhadi. Newtonian image understanding: Unfolding the
dynamics of objects in static images. In Proc. Computer Vi-
sion and Pattern Recognition (CVPR) , 2016. 1, 2
[27] R. Mottaghi, M. Rastegari, A. Gupta, and A. Farhadi. “what
happens if...” learning to predict the effect of forces in im-
ages. In Proceedings the 14th European Conference on Com-
puter Vision (ECCV) , 2016. 1, 2
[28] D. Mrowca, C. Zhuang, E. Wang, N. Haber, L. Fei-Fei, J. B.
Tenenbaum, and D. L. K. Yamins. Flexible neural repre-
sentation for physics prediction. In Proceedings of the 32nd
International Conference on Neural Information Processing
Systems (NIPS) , 2018. 2, 3, 8
[29] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. P. Singh. Action-
conditional video prediction using deep networks in atari
games. In Advances in Neural Information Processing Sys-
tems (NIPS) , 2015. 2
[30] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep
learning on point sets for 3d classiﬁcation and segmentation.
Proc. Computer Vision and Pattern Recognition (CVPR),
IEEE , 1(2):4, 2017. 2, 5
[31] R. Riochet, M. Y . Castro, M. Bernard, A. Lerer, R. Fergus,
V . Izard, and E. Dupoux. Intphys: A framework and bench-
mark for visual intuitive physics reasoning. arXiv preprint,
arXiv:1803.07616 , 2018. 2
[32] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer
Vision (IJCV) , 115(3):211–252, 2015. 10
[33] A. Sanchez-Gonzalez, N. Heess, J. T. Springenberg,
J. Merel, M. Riedmiller, R. Hadsell, and P. Battaglia. Graph
networks as learnable physics engines for inference and con-
trol. In Proceedings the 35th International Conference on
Machine Learning (ICML) , 2018. 2
[34] K. Simonyan and A. Zisserman. Very deep convolu-
tional networks for large-scale image recognition. CoRR ,
abs/1409.1556, 2014. 7, 10
[35] R. Stewart and S. Ermon. Label-free supervision of neural
networks with physics and domain knowledge. In Proc. of
AAAI Conference on Artiﬁcial Intelligence , 2017. 2, 8
[36] Z. Wang, S. Rosa, B. Yang, S. Wang, N. Trigoni, and
A. Markham. 3d-physnet: Learning the intuitive physics
of non-rigid object deformations. In Proceedings of the
26th International Joint Conference on Artiﬁcial Intelli-
gence, IJCAI-18 , pages 4958–4964, 2018. 2
[37] N. Watters, A. Tacchetti, T. Weber, R. Pascanu, P. Battaglia,
and D. Zoran. Visual interaction networks. arXiv preprint,
arXiv:1706.01433 , 2017. 1, 2
[38] J. Wu, J. J. Lim, H. Zhang, J. B. Tenenbaum, and W. T. Free-
man. Physics 101: Learning physical object properties from
unlabeled videos. In Proceedings of the 27th British Machine
Vision Conference (BMVC) , 2016. 2
[39] J. Wu, E. Lu, P. Kohli, W. T. Freeman, and J. B. Tenenbaum.
Learning to see physics via visual de-animation. In Proceed-
ings of the 31st Conference on Neural Information Process-
ing Systems (NIPS) , 2017. 2
[40] J. Wu, I. Yildirim, J. J. Lim, W. T. Freeman, and J. B. Tenen-
baum. Galileo: Perceiving physical object properties by in-
tegrating a physics engine with deep learning. In Proceed-
ings of the 29th Conference on Neural Information Process-
ing Systems (NIPS) , pages 127–135, 2015. 2
[41] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3d shapenets: A deep representation for volumet-
ric shapes. In Computer Vision and Pattern Recognition
(CVPR) , 2015. 5
[42] T. Ye, X. Wang, J. Davidson, and A. Gupta. Interpretable
intuitive physics model. In Proceedings the 15th Euro-
pean Conference on Computer Vision (ECCV) , pages 89–
105, 2018. 2
[43] R. Zhang, J. Wu, C. Zhang, W. T. Freeman, and J. B. Tenen-
baum. A comparative evaluation of approximate probabilis-
tic simulation and deep neural networks as accounts of hu-
man physical scene understanding. In Annual Meeting of the
Cognitive Science Society , 2016. 2A. Appendix: Implementation Details
Here we provide additional details of our simulation
pipeline and baseline implementations.
Simulation Procedure : The simulation pipeline is in-
troduced in Section 4 of the main paper. Prior to simulation,
some pre-computation is done on object shapes to extract
accurate physical parameters, namely the mass and moment
of inertia. To calculate these values, we voxelize each shape
using a grid with a cell side length of 2.5 cm. From this we
approximate the volume of the object which can be used to
calculate the mass given density. Additionally, we compute
a discretized approximation of the moment of inertia about
the shape’s principle axes. These calculated mass and mo-
ment of inertia values are used directly to parameterize the
simulated rigid bodies within the the Bullet physics engine.
Calculating physical parameters in this way ensures con-
sistency across all simulated shapes rather than relying on
the physics engine to calculate them using a mesh collider
which can be extremely inconsistent and inaccurate.
For each performed simulation, the amplitude of the ran-
domly applied impulse is scaled by the object mass to en-
sure similar distributions for small and large objects alike.
When choosing the impulse to apply, we also ensure that the
line deﬁned by the impulse vector and its position passes
within a certain radius of the object’s center of mass. Con-
straining the impulse in this way gives control over the
amount of rotation the object will undergo, allowing for a
reasonable distribution. For Speakers this radius is 40
cm because these objects tend to have a large moment of in-
ertia about the vertical axis. For all other object categories
this radius is set to 10 cm. Simulated objects have a fric-
tion coefﬁcient of 0.7 in the Bullet physics engine while the
ground plane has a coefﬁcient of 0.1.
For each object, we also sample a point cloud from the
surface to use as input to our model. To ensure uniform
sampling, we ﬁrst oversample (by a factor of 3) the mesh
surface area. We then sub-sample these points using fur-
thest point sampling to obtain our ﬁnal 1024 points.
Baselines : We now detail implementations for the base-
lines presented in Section 6.3 of the main text. For the no
PointNet MLP (NP-MLP) baseline, we replace the PointNet
module with a small MLP made up of 2 fully-connected
(FC) layers each with 512 nodes. This is followed by the
usual shape processing branch.
For the no PointNet VGG (NP-VGG) baseline, we re-
place the PointNet module with a modiﬁed version of VGG-
16 [34]. We use all convolutional layers from VGG (up
through the 5th max pooling layer), but modify the ﬁnal
2 FC layers to each have 1024 nodes. We initialize the
weights of the convolutional layers from a model pretrained
on ImageNet [32]. During training, we ﬁne tune the weights
for our problem. The input to NP-VGG is a single 112 ×112
image of the simulated object rather than a point cloud (see
Figure A.1. Examples of object images used as input to the NP-
VGG baseline model.
Figure A.1). In these images, the object is placed against
a plain black background and the camera is positioned in
line with the applied impulse (offset in the -x direction in
the impulse coordinate system described in Section 4 of the
main paper).
The basic MLP baseline ﬂattens the point cloud input
and concatenates it to all other inputs to process this whole
vector with a series of FC layers: 512x3 (3 layers with out-
put size 512), 256x3, 128x2, 64x1, 32x1, and then the ﬁnal
output of 3 (2 position and 1 total rotation). The poor perfor-
mance of this baseline is likely due to the lack of a branched
structure which overwhelms the network with point cloud
information making it difﬁcult to pick out the important im-
pulse input.
B. Appendix: Additional Results
In this section we provide some additional results to
those presented in Section 6 of the main paper.
Impulse Generalization : Figure B.1 summarizes the
performance of our method evaluated on novel impulse
forces applied to known objects (the ImpulseGen exper-
iments presented in the main paper). These curves plot
the fraction of test examples (y-axis) for which model pre-
dictions fall under a certain relative error threshold (x-
axis). The blue curves indicate models that were trained
on datasets of individual objects categories (six different
models, one for each column). The orange curves show
performance of a single model trained on the Combined
dataset then evaluated on each individual category sepa-rately. In some cases ( Box andSpeakers rotation), the
combined training offers improved performance, while in
others ( Bottles ) the model seems to beneﬁt from train-
ing on individual categories.
Out of Category Object Generalization : Additional
visualizations for the out-of-category generalization experi-
ments are shown in Figure B.2. For each of these examples,
the model was trained on the Combined dataset with the
evaluated object category left out. Some failure cases are
presented in Figure B.3.
Comparison to Baselines : Figure B.4 compares results
from our out-of-category trained model to that of two base-
lines presented in the main text—the no PointNet MLP (NP-
MLP) and VGG (NP-VGG) models. Note that our model
was trained on the Combined dataset with the evaluated
category left out, while the two baseline methods were
trained on the full Combined dataset split by objects. Each
row shows predictions for each model on the same simula-
tion. We see that our proposed method provides more accu-
rate predictions than both of these baselines even though it
did not see the test object category at training time."
2302.03064,D:\Database\arxiv\papers\2302.03064.pdf,"How do the authors address the challenge of generating realistic ultrasound data for training deep learning models, considering the limitations of existing methods?","The authors propose a novel pipeline for generating randomized and quasi-realistic simulation input data, incorporating anatomical priors and random parameterization to create a large, heterogeneous dataset of in-silico phantoms for full-wave ultrasound simulations.","Though all of the above-mentioned methods have made great contribu-
tions towards real-time sound speed estimation, the problem of sound speed
estimation remains a challenging and important task. Recently, there has
been growing interest in methods for sound speed estimation built upon the
advances in computer vision with the advent of performant neural networks
as universal estimators.
Feigin et al. (2019) proposed pulse-echo sound speed estimation with
a deep neural network based on VGG (Simonyan and Zisserman, 2014).
The network was trained to map the raw channel data from three plane
wave transmits to a sound speed distribution. Training data was generated
by simulating ultrasonic interrogations of media containing randomly po-
sitioned ellipses of varying ultrasonic properties with the k-Wave software
package (Treeby and Cox, 2010). Each plane wave used a separate 64 ele-
ment sub-aperture of a 128 element transducer to interrogate the medium at
a pre-deﬁned angle. The trained network was evaluated on a simulated test
set and achieved a mean absolute error of 12.5 ±16.1 m/s. Nonetheless, when
applying the method to in-vivo data the resulting estimated values were both
outside the training data range and the normal envelope of healthy tissue,
indicating a large domain shift between the simulation and in-vivo data.
A new investigation on the use of deep learning for ultrasound sound
speed reconstruction was presented by Jush et al. (2021a), in which the net-
work multi-input architecture of Feigin et al. (2019) is extended to map IQ
data with separate I and Q branches to sound speed distribution maps. The
k-Wave suite was again used to simulate random ellipses in media, although
only one plane wave transmission was simulated. The evaluation showed
accurate sound speed estimations, but the method was solely evaluated on
simulated data similar to the training set data and did not incorporate fea-
tures commonly observed in real-world ultrasound signals, such as thermal
noise.
Two challenges for deep learning sound speed estimation models are data
collection and labeling. For supervised deep learning, there is not yet an
accurate way to manually label ultrasound signals with a local sound speed.
For this reason, full-wave simulations are used to create a paired dataset of
known sound speed distributions and their respective channel data. This
approach is nevertheless challenged by the requirement for simulations to
be carefully parameterized in order to accurately model transducer charac-
teristics and tissue property distributions. Previous works have proposed a
variety of methods to create in-silico phantoms for the generation of ultra-
4
sound data. Some have generated randomly positioned ellipses of varying
ultrasonic properties to create in-silico phantoms that are randomly param-
eterizable (Feigin et al., 2019; Jush et al., 2021b, 2020). These works show-
cased phantoms that are fully parameterizable with geometric shapes but
did not incorporate anatomical priors of a target anatomy. Others have
sampled a virtual breast model containing complex anatomical geometries
through a novel combination of image-based landmark structures based on
MRI data from the NIH Visible Human Project and randomly distributed
structures (Wang et al., 2015). Karamalis et al. (2010) modiﬁed a fetus
dataset presented by Jensen and Munk (1997) for discretized Westervelt sim-
ulations. Salehi et al. (2015) evaluated their hybrid ultrasound simulation ap-
proach using phantoms based on segmented MRI volumes of patient data fur
intra-operative registration. While these works took care to model anatom-
ical geometries in their simulation phantoms, they were limited in scale by
the availability of MRIs as anatomical priors and could not be randomly gen-
erated. Therefore, bridge anatomical realism and random parameterization
is required to generate large, heterogeneous, and quasi anatomically realistic
datasets of in-silico phantoms for full-wave ultrasound simulations.
2.1. Our Contributions
This study proposes a novel pipeline for sound speed estimation of medical
ultrasound for breast tissue. We create a large-scale synthetic ultrasound
dataset of breast ultrasound images, including breast gland, skin, and two
types of lesions. Afterward, a Deep Neural Network (DNN) is trained on IQ
demodulated synthetic aperture data to estimate sound speed. Finally, we
show the capabilities of the proposed method with an extensive evaluation
of simulated, phantom, and in-vivo data. Our contributions are:
•A novel approach to generating randomized and quasi-realistic simula-
tion input data
•A deep neural network (DNN) trained on beamformed IQ data gener-
ated from angled plane wave transmission to estimate the distribution
of sound speed in a medium
•First quantitative results of a DNN on phantom and in-vivo data con-
sistent with traditional sound speed measurements
•Evaluation of temporal estimation consistency that displays invariance
to artifacts such as thermal noise on sequential measurements
5
Importantly, this work is meant to show the viability of using neural network
to infer sound speed given a small number of angled plane-wave acquisitions
which have been simulated from in-silico phantoms. An exhaustive evaluation
and comparison of network architectures and methods is left to future works.
3. Methods
The following section describes the methodology used in order to train
a DNN on quasi-realistic in-silico simulations for the purpose of generaliz-
able sound speed estimation with real transducer data. To this end, we will
cover the generation and parameterization of a quasi-realistic in-silico breast
phantom, the simulation process using the k-Wave suite, the proposed data
processing and augmentation steps for training a DNN, as well as the archi-
tecture and structure of the DNN. The methods and notation are formally
described in this section, while the selected parameter values are presented
in Section 4.
3.1. In-Silico Phantom
The three dimensional ultrasound simulations developed in this work are
generated to model human breast tissue and are comprised of three basic
elements; a tissue property map, which deﬁnes the spatial distribution of
anatomies in the domain such as skin, lesion, cyst and breast gland, a scat-
ter distribution ﬁeld, which deﬁnes the location and relative intensity of all
scatterers and a tissue property model, which deﬁnes voxel-wise tissue prop-
erties of sound speed, density, non-linearity (B/A) and attenuation in the
simulation domain. The combination and random parameterization of these
elements generates a large heterogeneous dataset used for sound speed esti-
mation.
The cuboid in-silico phantom domain is deﬁned on a Cartesian grid of
pointspi∈X×Y×Z,whereX={0,1x,2x,...,xd},Y={0,1y,2y,...,yd},
andZ={0,1z,2z,...,zd}.x,y, andzare the spatial resolution of the grid
andxd,ydandzdare the respective grid dimensions.
3.1.1. Tissue Type Label Map
The in-silico simulations include phantoms containing the breast anatomy,
skin, breast gland, breast cysts, and breast lesions resembling ﬁbroadenomas
and glandular tissue (Smithuis et al., 2010). The tissue type label map as-
signs every voxel the class of its respective tissue type distributed in space.
6
Simulated 
B-mode 
Sound Speed 
Medium 
1580 
1560 
1540 
Sound Speed 
Target 
1520 
1500 1580 
1570 
1560 
1550 
1540 
1530 
1520 
1510 
Figure 1: (a) Simulated ultrasound B-mode image with background approximating glan-
dular breast tissue and an anechoic cyst (Smithuis et al., 2010). (b) Sound speed of
the simulated medium. (c) Sound speed target used for model optimization with region-
average sound speed values. Note that two sound speed values are used for the background,
and a single sound speed value is used for the cyst.
Both cysts and lesions in the dataset are modeled by elliptical inclusions but
are diﬀerentiated by cysts being anechoic and lesions having either positive
or negative echogenicity. The cyst/lesion mask is deﬁned as an ellipse Ein
space projected onto the aforementioned Cartesian grid and randomly pa-
rameterized by the position of its center ( xc,yc)∈[X×Y], the lengths of its
radiiri={1,2}∈R∀ri<min (xd,yd),and an orientation angle θ∈[0,π].
E(xp,yp,r1,r2,xc,yc) =((xp−xc)·cos(θ) + (yp−yc)·sin(θ))2
r1+
((xp−xc)·sin(θ)−(yp−yc)·cos(θ))2
r2.(1)
The 2D ellipse is projected in the elevational plane to generate a 3D inclusion.
The skin mask is deﬁned as a simple linear mask at the top of the cuboid
and parameterized in depth for varying skin thickness within anatomical
7
norms of 0.7 to 3 mm (Huang et al., 2008). All remaining unassigned voxels
are assigned to the breast gland class.
In total, six combinations of the above-mentioned tissue classes are de-
ﬁned; namely, cyst with skin, lesion with skin, skin, breast gland, lesion, and
cyst. All classes are created over a breast gland background, and the breast
gland class represents a phantom without any other anatomical structures.
3.1.2. Scatterer Distribution
Next, a spatial distribution of scatterers and their relative intensity is
generated. In 3D, the discretized scatterer density
ρs=ns
λ3·x·y·z,ρs∈[0,1]
wherensis the number of scatterers in an imaging resolution voxel (IRV).
The unit-less discretized scatterer density ρsdeﬁnes the fraction of all voxels
in a region that are labeled as scatterers such that the number of scatterers
per IRV is upheld. The size of the IRV in 3D is approximated as λ3whereλ
is the wavelength of the transmit pulse.
The scatterer intensity distribution, i.e., how strongly the scatterer at
a given location deviates from average medium properties, is modeled by a
uniform distribution of intensity for every scatterer in the domain. For this,
every point in 3D space is assigned a sample value from the distribution
U∼U [−0.5,0.5]to create a spatial white noise distribution for sound speed
and density. This white noise distribution is jointly sampled by a Bernoulli
distribution B∼B(1,ρs) to determine the location of the scatterers in the
3D grid. The Bernoulli distribution represents the likelihood that a given
point in 3D space is a scatterer and is parameterized by the scatterer density
ρs. By jointly sampling these two distributions, a resulting ﬁnal scatterer
distribution is characterized by randomly located discretized scatterers with
random intensities.
3.1.3. Tissue Property Model
The tissue property model takes the previously deﬁned spatial distri-
bution of tissue types as well as the spatial scatterer distribution in order
to generate realistic in-scilico phantoms of varying tissue classes. A diﬀer-
ent method of mapping the tissue properties onto the spatial distribution is
employed for each tissue class. The model utilized for both the skin and le-
sion classes is straightforward, while the breast gland model is slightly more
8
complex for added realism. Breast gland tissue class is modeled by using
a 2D Gaussian random ﬁeld (GRF) to model the correlated variation of
breast gland tissue properties. A 2D Gaussian ﬁlter gof the size ( xf,yf)
is deﬁned as g(u,v) =1
2πσ2e−u2+v2
2σ2whereu∈[−xf
2,xf
2] andv∈[−yf
2,yf
2]
and is convolved with a 2D random ﬁeld of size F= [0,xd+xf]×[0,yd+
yf] where the ﬁeld elements Fi∼U [0,1]in order to generate a GRF that mim-
ics breast gland. The resulting GRF is then normalized ( µ= 0) and scaled
to the interval [−0.5,0.5], and two sub-regions of foreground and background
via a randomly thresholded to create a random binary map of breast gland
regions. Values below the threshold are assigned to the background (low
echogenicity), and all above are assigned to the foreground (high echogenic-
ity). The foreground and background deﬁne the varying echogenicity regions
in the breast gland class of the resulting image. This value distribution is
the basis of the breast gland model and is later scaled with the mean sound
speed and augmented with the scaled scatterer map. For a given class re-
gion, the tissue properties are assigned as a random uniformly sampled mean
sound speed value combined with a scaled scatterer ﬁeld intensity to induce
the echogenicity to create the resulting in-silico phantom used for simulation.
The ranges of the sound speed and contrast for each given class can be seen
in Table 1 and were chosen following (Bamber, 1983). As shown empirically
in (Mast, 2000), the values of sound speed and density can be approximated
to be linear, and the density map is set to be proportional to the sound speed
map by a factor of αρ. The attenuation and non-linearity values are constant,
as listed in Table 2.
Lastly, the in-silico phantom sound speed map in Figure 1 (a) is averaged
by region (two sound speeds for breast gland, one for cyst/lesion, and one
for skin) in order to form a target average sound speed map, as shown in
Figure 1 (c), suitable for training our deep model. The averaged sound speed
map is used only as a training label and not for the k-Wave simulation. The
original in-silico phantoms are then utilized in k-Wave to generate simulated
RF channel signals from pulse-echo ultrasound.
3.2. Data Processing
Neural networks perform best when the dataset they are trained on ac-
curately represents the dataset they will see at “inference time” or when
they are deployed. Therefore, a large heterogeneous dataset is desirable to
train robust neural networks. To increase the heterogeneity of the training
9
Table 1: Mean sound speed range and scatter contrast per class used for our breast
ultrasound dataset simulation.
Tissue Class Mean sound speed Range Scatter Contrast
Cyst (Bamber, 1983) [1500, 1620] -
Lesion (Bamber, 1983) [1488, 1512] ±10-30 dB
Skin [1540, 1670] 10 dB
Breast Gland (Bamber, 1983) [1480, 1528] 12 dB
data, data augmentation is often applied. In computer vision, these augmen-
tations can include rotations, ﬂips, and deformations of the training data.
Augmentation is applied at train time given an augmentation likelihood.
Furthermore, augmentation is randomly parameterized at every invocation.
We propose the use of ultrasound-speciﬁc augmentation techniques. We pro-
pose augmenting convolved with the transducer’s impulse response with a
random relative bandwidth and the addition of thermal noise on the channel
data via thermal noise augmentation (TNA) Huang et al. (2021).
Thermal noise is an artifact resulting from electronic noise in ultrasound
devices but is missing from ultrasound simulations (Hyun et al., 2019). TNA
is performed by adding white thermal noise to channel data with an aug-
mentation likelihood pTNA. TNA can be added on top of the clutter and
aberration noise generated by the forward process of the k-Wave simulation.
Here, TNA is parameterized by an upper and lower bound in noise amplitude
relative to the transmit signal’s Root Mean Square (RMS). This uniform pa-
rameterization distribution is randomly sampled via the method proposed
by Hyun et al. (2019). Due to the constant TNA and attenuating tissue
model, SNR reduces over depth.
Next, a start delay is applied to the RF channel signal to align a deﬁned
t0 for every transmitted plane wave correctly. t0deﬁnes a standardized po-
sition in space at which the propagation time t= 0, i.e. the starting time
of wave propagation. This aligns multiple transmissions through a medium
during beamforming independently of steering and focus. The deﬁnition t0
can vary between devices and simulation platforms. In this work, t0is deﬁned
as the end of the pulse of the last ﬁring element for a given transmission.
This deﬁnition removes the transmitted pulse from the training data, which
would otherwise introduce a large amplitude discrepancy and impair network
training. Then, the complex IQ signal is generated via the Hilbert transform,
10
34Max Pooling 2DEncoder
UnpoolingDecoder2D ConvPReluBatch Norm2D ConvPReluBatch Norm2D ConvPReluBatch NormDense BlockSkip ConnectionsBottleneckDense ConnectionFigure 2: Overview of the proposed architecture. Our model is composed of an encoder
that individually processes three beamformed IQ images, whose features are concatenated
after their individual dense blocks, a bottleneck, and a decoder that utilizes unpooling and
produces the sound speed estimations. Dense skip connections are used within each dense
block and long-term skip connections are placed between encoder and decoder to enhance
gradient ﬂow and maintain feature quality.
and each plane wave is beamformed individually via dynamic receive beam-
forming with an assumed sound speed c0to generate complex beamformed
IQ images similar to (St¨ ahli et al., 2020). Lastly, the complex IQ components
from the same spatial location are mapped to the channel dimension of the
convolutional neural network.
3.3. Network Architecture
We modify a deep fully convolutional neural network Fbased on (J´ egou
et al., 2017) and with modiﬁcations from (He et al., 2015) and (Ulyanov
et al., 2016) described below to take, as input, three beamformed IQ images
of a medium (one for each angled plane wave transmission) and output an
estimated sound speed map of the medium deﬁned as
F:CN×M↦→RN×M,
11"
2111.04089,D:\Database\arxiv\papers\2111.04089.pdf,"In the context of sampling from log-concave distributions, what is the primary challenge in achieving a runtime that depends logarithmically on the desired error in the infinity-distance metric, and how does the proposed approach address this challenge?","The primary challenge is that existing methods rely on discrete-space Markov chains, which require a grid with cell size inversely proportional to the error, leading to a polynomial dependence on the error in the runtime. The proposed approach overcomes this by introducing a continuous-space algorithm that transforms TV-bounded samples into samples with bounded infinity-distance, allowing the use of continuous-space Markov chains with step sizes independent of the error.","Lipschitz log-concave distributions. Unlike the TV-distance case where algorithms whose running time
depends logarithmically on the error are known (e.g., [1, 29, 34]), the best available bounds for sampling
withinO(ε)inﬁnity-distance [20, 2] have runtime that is polynomial in1
εand a relatively large polynomial
ind.
Our contributions. We present a new approach to output samples, which come with d∞bounds, from
a log-concave and log-Lipschitz distribution constrained to a a convex body. Speciﬁcally, when K:={θ:
Aθ≤b}is a polytope (where one is given Aandb) our main result (Theorem 2.1) guarantees samples from
a distribution that is within O(ε)error in d∞and whose runtime depends logarithmically on1
εcompared
to the polynomial dependence of [2]. Our approach departs from prior works that construct Markov chains
on a1
ε2-discretization of Kto achieve a sample with εinﬁnity-distance error, and we present a method
(Algorithm 1) to directly convert continuous samples from Kwith total-variation bounds to samples with
inﬁnity bounds (Theorem 2.2). This continuous-space approach also allows us to obtain an improvement
on the dimension din the running time when Kis a polytope by plugging in TV-distance running time
bounds for the Dikin Walk Markov chain of [34]. As immediate applications, we obtain faster algorithms
for differentially private empirical risk minimization (Corollary 2.4) and low rank approximation (Corollary
2.5).
2 Results
LetB(v,s) :={z∈Rd:∥z−v∥2≤s}andωdenote the matrix-multiplication constant.
Theorem 2.1 (Main result) There exists an algorithm which, given ε,L,r,R > 0,A∈Rm×d,b∈Rm
that deﬁne a polytope K:={θ∈Rd:Aθ≤b}contained in a ball of radius R, a pointa∈Rd
such thatKcontains a ball B(0,r)of smaller radius r, and an oracle for the value of a convex function
f:K→Rd, wherefisL-Lipschitz, and deﬁning πto be the distribution π∝e−f, outputs a point from
a distribution νsuch that d∞(ν,π)<ε. Moreover, with very high probability1, this algorithm takes O(T)
function evaluations and O(T×mdω−1)arithmetic operations, where T=O((m2d3+m2dL2R2)×[LR+
dlog(Rd+LRd
rε)]).
In comparison to the polynomial in1
εruntime bounds of [2], Theorem 2.1 guarantees a runtime that is
logarithmic in1
ε, and also improves the dependence on the dimension d, in the setting where Kis a polytope.
Speciﬁcally, [2] show that the number of steps of the grid walk to sample from πwith inﬁnity-distance error
at mostεis
O(1
ε2(d10+d6L4R4)×polylog(1
ε,1
r,R,L,d))
(Lemma 6.5 in the Arxiv version of [2]). When applying their algorithm to the setting where fis con-
strained to a polytope K={x∈Rd:Ax≤b}, each step of their grid walk Markov chain requires
computing a membership oracle for Kand the value of the function f. The membership oracle can be
computed in O(md)arithmetic operations. Thus the bound on the number of arithmetic operations for
each step of their grid walk is O(md)(provided that each function evaluation takes at most O(md)arith-
metic operations). Thus the bound on the number of arithmetic operations to obtain a sample from πis
O(1
ε2(md11+md7L4R4)×polylog(1
ε,1
r,R,L,d )). Thus, Theorem 2.1 improves on this bound by a factor
of roughly1
ε2m3d8−ω. For example, when m=O(d), as may be the case in differentially private applica-
tions, the improvement is1
ε2d5−ω.
We note that the bounds of [2] also apply in the more general setting where Kis a convex body with
membership oracle. One can extend our bounds to achieve a runtime that is logarithmic in1
ε(and polynomial
1The number of steps is O(τ×T), where E[τ]≤3,P(τ≥t)≤(2
3)tfort≥0, andτ≤O(dlog(R
r) +LR)w.p. 1.
4
ind,L,R ) in the more general setting where Kis a convex body with membership oracle; we omit the details
(see Remark 2.3).
Moreover, we also note that while there are several results which achieve O(δ)TV bounds in time
logarithmic in1
δ, TV bounds do not in general imply O(ε)bounds on the KL or Renyi divergence, or on the
inﬁnity-distance, for any δ >0.2On the other hand, an ε-inﬁnity-distance bound does immediately imply
a bound ofεon the KL divergence DKL, andα-Renyi divergence Dα, sinceDKL(µ,π)≤d∞(µ,π)and
Dα(µ,π)≤d∞(µ,π)for anyα>0and any pair of distributions µ,π. Thus, under the same assumptions
onKandf, Theorem 2.1 implies a method of sampling from a Lipschitz concave log-density on Kwithε
KL and Renyi divergence error in a number of arithmetic operations that is logarithmic in1
ε, with the same
bound on the number of arithmetic operations.
The polynomial dependence on1
εin [2] is due to the fact that they rely on a discrete-space Markov chain
[1], on a grid with cells of width w=O(ε
L√
d), to sample from πwithinO(ε)inﬁnity-distance. Since their
Markov chain’s runtime bound is polynomial in w−1, they get a runtime bound for sampling within O(ε)
inﬁnity-distance that is polynomial in1
ε. The proof of Theorem 2.1 bypasses the use of discrete grid-based
Markov chains by introducing Algorithm 1 which transforms any sample within δ=O(εe−d−nLR)TV
distance of the distribution π∝e−fon the continuous setK(as opposed to a discretization of K), into
a sample within O(ε)inﬁnity-distance from π. This allows us to make use of a continuous-space Markov
chain, whose step size is not restricted to a grid of width O(ε
L√
d)and is instead independent of ε, to obtain
a sample within O(ε)inﬁnity-distance from πin time that is logarithmic in1
ε.
Theorem 2.2 (Main technical contribution: Converting TV bounds to inﬁnity-distance bounds) There
exists an algorithm (Algorithm 1) which, given ε,r,R,L > 0, a membership oracle for a convex body K
contained in a ball of radius Rand containing a ball B(0,r), and an oracle which outputs a point from a
distribution µwhich has TV distance
δ≤O
ε×(
R(dlog(R/r) +LR)2
εr(−d
e−LR(
(
from a distribution π∝e−fwheref:K→Ris anL-Lipschitz function (see Appendix 5 for the exact
values ofδand related hyper-parameters), outputs a point ˆθ∈Ksuch that the distribution νofˆθsatisﬁes
d∞(ν,π)≤ε. Moreover, with very high probability3, this algorithm ﬁnishes in O(1)calls to the sampling
and membership oracles, plus O(d)arithmetic operations.
To the best of our knowledge Theorem 2.2 is the ﬁrst result which for any ε,L,r,R > 0, when provided
as input a sample from a continuous-space distribution on a convex body Kwithin some TV distance
δ=δ(ε,r,R,L )>0from a given L-log-Lipschitz distribution πonK, whereKis contained in a ball of
radiusRand containing a ball of smaller radius r, outputs a sample with distribution within inﬁnity-distance
O(ε)fromπ. This is in contrast to previous works [2] (see also [20] which applies only to the special case
whereπis the uniform distribution on K) which instead require as input a sample with bounded TV distance
from the restriction of πon a discrete grid onK, and then convert this discrete-space sample into a sample
2For instance, if π(θ) = 1 with support on [0,1], for everyδ >0there is a distribution νwhere∥ν−π∥TV≤2δand yet
d∞(ν,π)≥DKL(ν,π)≥1
2. (ν(θ) =e1
δonθ∈[0,δe−1
δ],ν(θ) =1−δ
1−δe−1
δon(δe−1
δ,1]andν(θ) = 0 otherwise)
3Algorithm 1 ﬁnishes in τcalls to the sampling and membership oracles, plus O(τd)arithmetic operations, where E[τ]≤3
andP(τ≥t)≤(2
3)tfor allt≥0andτ≤5dlog(R
r) + 5LR+ 2w.p. 1.
5
within inﬁnity-distance O(ε)from the continuous-space distribution π:K→R.
Algorithm 1: Interior point TV to inﬁnity-distance converter
Input:d∈N
Input: A membership oracle for a convex body K∈Rdand anr>0such thatB(0,r)⊆K.
Input: A sampling oracle which outputs a point from a distribution µ:K→R
Output: A point ˆθ∈K.
1Hyperparameters: ∆>0,τmax∈N(set in Appendix 5)
2fori= 1,...,τ maxdo
3 Sample a point θ∼µ
4 Sample a point ξ∼Unif(B(0,1))
5 SetZ←θ+ ∆rξ
6 Setˆθ←1
1−∆Z
7 Ifˆθ∈K, output ˆθwith probability1
2and halt. Otherwise, continue.
8end
9Sample a point ˆθ∼Unif(B(0,r))
10Output ˆθ
Remark 2.3 (Extension to convex bodies with membership oracles) We note that Theorem 2.1 can be
extended to the general setting where Kis an arbitrary convex body in a ball of radius Rand contain-
ing a ball of smaller radius r, and we only have membership oracle access to K. Namely, one can plug
in the results of [29] to our Theorem 2.2 to generate a sample from a L-Lipschitz concave log-density on
an arbitrary convex body Kin a number of operations that is (poly)-logarithmic in1
ε,1
rand polynomial on
d,L,R . We omit the details.
Applications to differentially private optimization. Sampling from distributions with O(ε)inﬁnity-
distance error has many applications to differential privacy. Here, the goal is to ﬁnd a randomized mech-
anismh:Dn→R which, given a dataset x∈Dnconsisting of ndatapoints, outputs model parameters
ˆθ∈R in some parameter space R, which minimize a given (negative) utility function f(θ,x), under the
constraint that the output ˆθpreserves the pure ε-differential privacy of the data points x. A randomized
mechanismh:Dn→R is said to be ε-differentially private if for any datasets x,x′∈D which differ by a
single datapoint, and any S⊆R , we have that
P(h(x)∈S)≤eεP(h(x′)∈S);
see [14].
As one application of Theorem 2.1, we consider the problem of ﬁnding an (approximate) minimum ˆθ
of an empirical risk function f:K×Dn→Runder the constraint that the output ˆθisε-differentially
private, where f(θ,x) :=∑n
i=1ℓi(θ,xi). Following [2], we assume that the ℓi(·,x)areL-Lipschitz for all
x∈Dn,i∈N, for some given L>0. In this setting [2] show that the minimum ERM utility bound under
the constraint that ˆθis pureε-differentially private, Eˆθ[f(ˆθ,x)]−minθ∈Kf(θ,x) = Θ(dLR
ε), is achieved
if one samples ˆθfrom the exponential mechanism π∝e−ε
2LRfwith inﬁnity-distance error at most O(ε).
Plugging Theorem 2.1 into the framework of the exponential mechanism, we obtain a pure ε-differentially
private mechanism which achieves the minimum expected risk (Corollary 2.4, see Section 6.1 for a proof).
Corollary 2.4 (Differentially private empirical risk minimization) There exists an algorithm which, given
ε,L,r,R> 0,A∈Rm×d,b∈Rmthat deﬁne a polytope K:={θ∈Rd:Aθ≤b}contained in a ball of
radiusRand containing a ball B(0,r)of smaller radius r, and a convex function f(θ,x) :=∑n
i=1ℓi(θ,xi),
6
where eachℓi:K→RisL-Lipschitz, outputs a random point ˆθ∈Kwhich is pure ε-differentially private
and satisﬁes
Eˆθ[f(ˆθ,x)]−min
θ∈Kf(θ,x)≤O(dLR
ε)
.
Moreover, this algorithm takes at most T×mdω−1arithmetic operations plus Tevaluations of the function
f, whereT=O((m2d3+m2dn2ε2)×(εn+d)log2(nRd
rε)).
Corollary 2.4 improves on the previous bound [2] of O((1
ε2(m+n)d11+ε2n4(m+n)d7)×polylog(nRd
rε)))
arithmetic operations by a factor of roughly max(
d8−ω
ε2m2,1
εm2nd5)
, in the setting where the ℓiareL-Lipschitz
on a polytope Kand eachℓican be evaluated in O(d)operations. See Appendix 6.1 for a proof of this
corollary.
As another application, we consider the problem of ﬁnding a low-rank approximation of a sample co-
variance matrix Σ =∑n
i=1uiu⊤
iwhere the datapoints ui∈Rdsatisfy∥ui∥≤1, in a differentially private
manner. Given any k > 0, the goal is to ﬁnd a (random) rank- kprojection matrix Pwhich maximizes
the average variance EP[⟨Σ,P⟩]of the matrix Σ(also reffered to as the utility of P), under the constraint
that the mechanism which outputs the matrix Pisε-differentially private. This problem has many applica-
tions to statistics and machine learning, including differentially private principal component analysis (PCA)
[7, 3, 15, 24].
When privacy is not a concern, the solution Pwhich maximizes the variance is just a projection ma-
trix onto the subspace spanned by top- keigenvectors of Σ, and the maximum variance satisﬁes ⟨Σ,P⟩=∑k
i=1λi, whereλ1≥···≥λd>0denote the eigenvalues of Σ. However, when privacy is a concern,
there is a tradeoff between the desired privacy level εand the utility EP[⟨Σ,P⟩], and the maximum utility
EP[⟨Σ,P⟩]one can achieve decreases with the privacy parameter ε. The best current utility bound for an
ε-differentially private low rank approximation algorithm was achieved in [24], who show that one can ﬁnd
a pureε-differentially private random rank- kprojection matrix Psuch that EP[⟨Σ,P⟩]≥(1−δ)∑k
i=1λi
whenever∑k
i=1λi≥Ω(
dk
εδlog1
δ)
for anyδ > 0. To generate the matrix P, their algorithm gener-
ates a sample, with inﬁnity-distance error O(ε), from a Lipschitz concave log-density on a polytope, and
transforms this sample into a projection matrix P. The sampling algorithm used in [24] has a bound of
poly(1
ε,d,λ 1−λd)arithmetic operations and they leave as an open problem whether this can be improved
from a polynomial dependence on1
εto a logarithmic dependence on1
ε. Corollary 2.5 shows that a direct
application of Theorem 2.1 resolves this problem. (See Section 6.2 for a proof.)
Corollary 2.5 (Differentially private low rank approximation) There exists an algorithm which, given a
sample covariance matrix Σ =∑n
i=1uiu⊤
ifor datapoints ui∈Rdsatisfying∥ui∥≤ 1, its eigenvalues
λ1≥...λd>0, an integerk, andε,δ > 0, outputs a random rank-k symmetric projection matrix Psuch
thatPisε-differentially private and satisﬁes the utility bound
EP[⟨Σ,P⟩]≥(1−δ)k∑
i=1λi(Σ)
whenever∑k
i=1λi(Σ)≥Cdk
εδlog1
δfor some universal constant C > 0. Moreover the number of arithmetic
operations is logarithmic in1
εand polynomial in dandλ1−λd.
3 Proof overviews
Given anyε, and a function f:Rd→R, the goal is to sample from a distribution π(θ)∝e−f(θ), constrained
to ad-dimensional convex body Kwith inﬁnity-distance error at most O(ε)in a number of arithmetic
7
operations that is logarithmic in1
ε. We assume that Kis contained in a ball of some radius R > 0and
contains a ball of some radius r > 0, andfisL-Lipschitz. In addition we would also like our bounds
to be polylogarithmic in1
r, and polynomial in d,L,R with a lower-order dependence on the dimension d
than currently available bounds for sampling from Lipschitz concave log-densities on a polytope in inﬁnity-
distance [2]. We note that since whenever Kis contained in a ball of radius Rand contains a ball B(0,r)
of smaller radius r, we also have that B(0,r)⊆K⊆B(0,2R), without loss of generality, we may assume
thatB(0,r)⊆K⊆B(0,R)as this would only change the bounds provided in our main theorems by a
constant factor.
The main ingredient in the proof of Theorem 2.1 is Theorem 2.2 that uses Algorithm 1 to transform a
TV-bounded sample into a sample from πwith error bounded in d∞. Subsequently, we invoke Theorem 2.1
whenKis given as a polytope K:={x∈Rd:Ax≤b}and plug in the Dikin Walk Markov chain of [34]
which generates independent samples from πwith bounded TV error. We ﬁrst present an overview of the
proof of Theorem 2.2. (The full proof has been omitted to space restrictions and presented in Appendix 5.)
The proof of Theorem 2.1 is presented in Section 3.2.
3.1 Converting samples with TV bounds to inﬁnity-distance bounds; proof of Theorem 2.2
Impossibility of obtaining log-dependence on inﬁnity-distance via grid walk. One approach is to
observe that if e−fhas support on a discrete space Swith at most|S|points, then any νsuch that∥ν−
π∥TV≤εalso satisﬁes
d∞(ν,π)≤2|S|maxz∈Se−f(z)
minz∈Se−f(z)×ε
for anyε≤minz∈Sπ(z). This suggests forming a grid GoverK, then using a discrete Markov chain to
generate a sample θwithinO(ε)TV distance of the discrete distribution πG∝e−fwith support on the
gridG, and then designing an algorithm which takes as input θand outputs a point with bounded inﬁnity-
distance to the continuous distribution π. This approach was used in [20] in the special case when πis
uniform onK, and then extended by [2] to log-Lipschitz log-concave distributions. In their approach, [2]
ﬁrst run a “grid-walk” Markov chain on a discrete grid in a cube containing K. They then apply the bound
from [1] which says that the grid walk obtains a sample Zwithin TV distance O(δ)from the distribution
∝e−f(restricted to the grid) in time that is polylogarithmic in1
δand quadratic in a−1, whereais the
distance between neighboring grid points. Since their grid has size |S|= Θ((R
a)d), a TV distance of O(δ)
automatically implies an inﬁnity-distance of O(δc|S|), wherecis the ratio of the maximum to the minimum
probability mass satisfying c≤eLRsincefisL-Lipschitz on K⊆B(0,R). Thus, by using the grid walk
to sample within TV-distance δ=O(
ε
|S|c)
from the discrete distribution πG, they obtain a sample Zwhich
also has inﬁnity-distance O(ε)fromπG. Finally, to obtain a sample from the distribution π∝e−fon the
continuous space K, they sample a point uniformly from the “grid cell” [Z−a,Z+a]dcentered atZ.
SincefisL-Lipschitz, the ratioe−f(Z)
e−f(w)is bounded by O(ε)for allwin the grid cell [Z−a,Z+a]das long
asa=O(
ε
L√
d)
, implying that the sample is an inﬁnity-distance of O(ε)fromπ∝e−f. However, since
the running time bound of the grid walk is quadratic in a−1, the grid coarseness a=O(
ε
L√
d)
needed to
achieveO(ε)inﬁnity-distance from πleads to a running time bound which is quadratic in1
ε.
To get around this problem, rather than relying on the use of a discrete-space Markov chain such as
the grid walk to sample within O(ε)inﬁnity-distance from π, we introduce an algorithm (Algorithm 1)
which transforms any sample within δ=O(
εe−d−LR)
TV distance from the distribution π∝e−fon the
continuous spaceK(as opposed to a grid-based discretization of K), into a sample within O(ε)inﬁnity-
distance from π. This allows us to make use of a continuous-space Markov chain, such as the Dikin walk of
8
[34], whose step-size is not restricted by a grid of coarseness w=O(
ε
L√
d)
and instead is independent of
ε, in order to generate a sample within O(ε)inﬁnity-distance from πin runtime that is logarithmic in1
ε.
Converting continuous space TV-bounded samples to inﬁnity-distance bounded samples. As discussed
in Section 1, there are many Markov chain results which allow one to sample from a log-concave distribution
onKwith error bounded in weaker metrics such as total variation, Wasserstein, or KL divergence. However,
when sampling from a continuous distribution, bounds in these metrics do not directly imply bounds in
inﬁnity-distance. And techniques used to prove bounds in weaker metrics do not easily extend to methods
for bounding the inﬁnity-distance; see Section 4.1.
Convolving with continuous noise. As a ﬁrst attempt, we consider the following simple algorithm: sample
a pointθ∼µfrom a distribution µwith total variation error ∥µ−π∥TV≤O(ε). SincefisL-Lipschitz,
for any ∆<ε
Land any ball B(z,∆)in the ∆-interior ofK(denoted by int∆(K); see Deﬁnition 5.1), we
can obtain a sample from a distribution νsuch that log(ν(z)
µ(z))
≤εfor allz∈int∆(K)by convolving µ
with the uniform distribution on the ball B(0,∆). Sampling from this distribution νcan be achieved by ﬁrst
samplingθ∼µand then adding noise ξ∼Unif(B(0,∆))to the sample θ.
Unfortunately, this simple algorithm does not allow us to guarantee that log(ν(z)
µ(z))≤εat pointsz /∈
int∆(K)which are a distance less than ∆from the boundary of K. To see why, suppose that K= [0,1]d
is the unit cube, that fis constant on K, and consider a point w= (1,..., 1)at the corner of the cube K.
In this case we could have that ν(z)≤2−dπ(z)for allzin some ball containing w, and hence d∞(ν,π) =
sup⏐⏐⏐log(ν(z)
π(z))⏐⏐⏐≥dlog(2) , no matter how small we make ∆.
Stretching the convex body to handle points close to the boundary. To get around this problem, we would
like to design an algorithm which samples from some distribution νsuch that⏐⏐⏐logν(z)
π(z)⏐⏐⏐≤εfor allz∈K,
including at points znear the corners of K. Towards this end, we ﬁrst consider the special case where K
is itself contained in the ∆-interior of another convex body K′, the function f:K→Rextends to an
L-Lipschitz function on K′(also referred to here with slight abuse of notation as f) and where we are able
to sample from the distribution ∝e−fonK′withO(ε)total variation error. If we sample θ∼e−fonK′
with total variation error O(δ)whereδ≤εe−dlog(R), add noiseξ∼Unif(B(0,∆)) toθfor∆ =δ
LR, and
then rejectθ+ξonly if it is not in K, we obtain a sample whose distribution is O(ε)from the distribution
∝e−fonKin inﬁnity-distance.
However, we would still need to deﬁne and sample from such a convex body K′, and to make sure that
K′is not too large when compared to K; otherwise the samples from the distribution ∝e−fonK′may
be rejected with high probability. Moreover, another issue we need to deal with is that fmay not even be
deﬁned outside of K.
To get around these two problems, in Algorithm 1, we begin by taking as input a point θ∼µsampled
from some distribution µsupported on Kwhere∥µ−π∥TV≤δfor someδ≤εe−dlog(R), and add
noiseξ∼unif(B(0,∆r))in order to sample from a distribution ˆµwhich satisﬁes⏐⏐⏐logˆµ(z)
π(z)⏐⏐⏐≤εfor all
z∈int∆r(K). Hereris the radius of the small ball contained in K; the choice of radius ∆rfor the noise
distribution is because we will show in the following paragraphs that to sample from the distribution πon
Kwith inﬁnity-distance error εit is sufﬁcient sample a point in the ∆r-interior ofKand to then apply a
“stretching” operation to K.
We still need a method of sampling within O(ε)inﬁnity-distance error of πon all ofK, including in
the regionK\int∆r(K)near the boundary of K. Towards this end, after Algorithm 1 generates a point
Z=θ+ξfrom the above-mentioned distribution ˆµ, it then multiplies Zby1
1−∆and returns the resulting
point ˆθ:=1
1−∆Zif it isK, in other words, if Z∈(1−∆)K. If we can show that (1−∆)K⊆int∆r(K),
then this would imply that⏐⏐⏐logˆµ(z)
π(z)⏐⏐⏐≤εfor allz∈(1−∆)K, and hence that the distribution of ˆνof the
9
C
<latexit sha1_base64=""+rw+Q0H7WTA6XlCgRzrL2/Omu3U="">AAAB83icdVDLSgMxFM34rPVVdekmWARXQ6attrMrduOygn1AZyiZNNOGZjJDkhHK0N9w40IRt/6MO//GTFtBRQ8EDufcyz05QcKZ0gh9WGvrG5tb24Wd4u7e/sFh6ei4q+JUEtohMY9lP8CKciZoRzPNaT+RFEcBp71g2sr93j2VisXiTs8S6kd4LFjICNZG8rwI6wnBPGvNi8NSGdk1160jFyIbObXGZdUQ171CThU6NlqgDFZoD0vv3igmaUSFJhwrNXBQov0MS80Ip/OilyqaYDLFYzowVOCIKj9bZJ7Dc6OMYBhL84SGC/X7RoYjpWZRYCbzjOq3l4t/eYNUhw0/YyJJNRVkeShMOdQxzAuAIyYp0XxmCCaSmayQTLDERJua8hK+fgr/J92K7VTtym2t3Lxe1VEAp+AMXAAH1EET3IA26AACEvAAnsCzlVqP1ov1uhxds1Y7J+AHrLdPK5qRyA==</latexit>ˆ✓<latexit sha1_base64=""DweDyBAwjFPSqeTwgWKgj7rvswE="">AAAB83icdVDJSgNBEO2JW4xb1KOXxiB4GnqSaDK3oBePEcwCmSH0dDpJk56F7hohDPkNLx4U8erPePNv7CyCij4oeLxXRVW9IJFCAyEfVm5tfWNzK79d2Nnd2z8oHh61dZwqxlsslrHqBlRzKSLeAgGSdxPFaRhI3gkm13O/c8+VFnF0B9OE+yEdRWIoGAUjed6YQubBmAOd9YslYlddt0ZcTGziVOsXFUNc95I4FezYZIESWqHZL757g5ilIY+ASap1zyEJ+BlVIJjks4KXap5QNqEj3jM0oiHXfra4eYbPjDLAw1iZigAv1O8TGQ21noaB6QwpjPVvby7+5fVSGNb9TERJCjxiy0XDVGKI8TwAPBCKM5BTQyhTwtyK2ZgqysDEVDAhfH2K/yftsu1U7PJttdS4WsWRRyfoFJ0jB9VQA92gJmohhhL0gJ7Qs5Vaj9aL9bpszVmrmWP0A9bbJ/o2klA=</latexit>Z<latexit sha1_base64=""wAvjoDlBHHbabhdvT/3mYhYTjws="">AAAB6HicdVDJSgNBEK2JW4xb1KOXxiB4GnriYMZb0IvHBMyCyRB6Oj1Jm56F7h4hhHyBFw+KePWTvPk3dhZBRR8UPN6roqpekAquNMYfVm5ldW19I79Z2Nre2d0r7h80VZJJyho0EYlsB0QxwWPW0FwL1k4lI1EgWCsYXc381j2TiifxjR6nzI/IIOYhp0QbqX7bK5awjT3XOXcRtl0Xl72KIdhxLzwHOTaeowRL1HrF924/oVnEYk0FUarj4FT7EyI1p4JNC91MsZTQERmwjqExiZjyJ/NDp+jEKH0UJtJUrNFc/T4xIZFS4ygwnRHRQ/Xbm4l/eZ1Mh54/4XGaaRbTxaIwE0gnaPY16nPJqBZjQwiV3NyK6JBIQrXJpmBC+PoU/U+aZds5s8t1t1S9XMaRhyM4hlNwoAJVuIYaNIACgwd4gmfrznq0XqzXRWvOWs4cwg9Yb58gvI0p</latexit>q<latexit sha1_base64=""ArVCBdXSd0sxNGHpxqheWxSOrpc="">AAAB6HicdVDJSgNBEK2JW4xb1KOXxiB4GnriYMZb0IvHBMwCyRB6Oj1Jm57F7h4hhHyBFw+KePWTvPk3dhZBRR8UPN6roqpekAquNMYfVm5ldW19I79Z2Nre2d0r7h80VZJJyho0EYlsB0QxwWPW0FwL1k4lI1EgWCsYXc381j2TiifxjR6nzI/IIOYhp0QbqX7XK5awjT3XOXcRtl0Xl72KIdhxLzwHOTaeowRL1HrF924/oVnEYk0FUarj4FT7EyI1p4JNC91MsZTQERmwjqExiZjyJ/NDp+jEKH0UJtJUrNFc/T4xIZFS4ygwnRHRQ/Xbm4l/eZ1Mh54/4XGaaRbTxaIwE0gnaPY16nPJqBZjQwiV3NyK6JBIQrXJpmBC+PoU/U+aZds5s8t1t1S9XMaRhyM4hlNwoAJVuIYaNIACgwd4gmfr1nq0XqzXRWvOWs4cwg9Yb59DmI1A</latexit>0<latexit sha1_base64=""k0D05+bV3gB6CFj3h9eDStqywdY="">AAAB6HicdVDLSgNBEOz1GeMr6tHLYBA8hZkoJrkFvXhMwDwgWcLsZDYZM/tgZlYIS77AiwdFvPpJ3vwbZ5MIKlrQUFR1093lxVJog/GHs7K6tr6xmdvKb+/s7u0XDg7bOkoU4y0WyUh1Paq5FCFvGWEk78aK08CTvONNrjO/c8+VFlF4a6YxdwM6CoUvGDVWauJBoYhLGGNCCMoIqVxiS2q1aplUEcksiyIs0RgU3vvDiCUBDw2TVOsewbFxU6qMYJLP8v1E85iyCR3xnqUhDbh20/mhM3RqlSHyI2UrNGiufp9IaaD1NPBsZ0DNWP/2MvEvr5cYv+qmIowTw0O2WOQnEpkIZV+joVCcGTm1hDIl7K2IjamizNhs8jaEr0/R/6RdLpHzUrl5UaxfLePIwTGcwBkQqEAdbqABLWDA4QGe4Nm5cx6dF+d10briLGeO4Aect0/HXYzu</latexit>r<latexit sha1_base64=""LPq8AaXFcBKK0ep6VEiu8gv4w10="">AAAB6HicdVDLSgNBEJz1GeMr6tHLYBA8hZkoJrkFvXhMwDwgWcLspDcZM/tgZlYIS77AiwdFvPpJ3vwbZ5MIKlrQUFR1093lxVJoQ8iHs7K6tr6xmdvKb+/s7u0XDg7bOkoUhxaPZKS6HtMgRQgtI4yEbqyABZ6Ejje5zvzOPSgtovDWTGNwAzYKhS84M1ZqqkGhSEqEEEopzgitXBJLarVqmVYxzSyLIlqiMSi894cRTwIIDZdM6x4lsXFTpozgEmb5fqIhZnzCRtCzNGQBaDedHzrDp1YZYj9StkKD5+r3iZQFWk8Dz3YGzIz1by8T//J6ifGrbirCODEQ8sUiP5HYRDj7Gg+FAm7k1BLGlbC3Yj5minFjs8nbEL4+xf+TdrlEz0vl5kWxfrWMI4eO0Qk6QxRVUB3doAZqIY4APaAn9OzcOY/Oi/O6aF1xljNH6Aect08rdI0w</latexit>p
<latexit sha1_base64=""mTCmyPuBs8Pl0/9w2vdwqAE6VGE="">AAAB6HicdVDLSgNBEOz1GeMr6tHLYBA8hZkoJrkFvXhMwDwgWcLsZDYZM/tgZlYIS77AiwdFvPpJ3vwbZ5MIKlrQUFR1093lxVJog/GHs7K6tr6xmdvKb+/s7u0XDg7bOkoU4y0WyUh1Paq5FCFvGWEk78aK08CTvONNrjO/c8+VFlF4a6YxdwM6CoUvGDVWasaDQhGXMMaEEJQRUrnEltRq1TKpIpJZFkVYojEovPeHEUsCHhomqdY9gmPjplQZwSSf5fuJ5jFlEzriPUtDGnDtpvNDZ+jUKkPkR8pWaNBc/T6R0kDraeDZzoCasf7tZeJfXi8xftVNRRgnhodsschPJDIRyr5GQ6E4M3JqCWVK2FsRG1NFmbHZ5G0IX5+i/0m7XCLnpXLzoli/WsaRg2M4gTMgUIE63EADWsCAwwM8wbNz5zw6L87ronXFWc4cwQ84b58obI0u</latexit>K<latexit sha1_base64=""HQfadDQrozr2PpvG928HQmjcOjI="">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexGQY9BL4KXBMwDkiXMTnqTMbOzy8ysEEK+wIsHRbz6Sd78GyfJHjSxoKGo6qa7K0gE18Z1v53c2vrG5lZ+u7Czu7d/UDw8auo4VQwbLBaxagdUo+ASG4Ybge1EIY0Cga1gdDvzW0+oNI/lgxkn6Ed0IHnIGTVWqt/3iiW37M5BVomXkRJkqPWKX91+zNIIpWGCat3x3MT4E6oMZwKnhW6qMaFsRAfYsVTSCLU/mR86JWdW6ZMwVrakIXP198SERlqPo8B2RtQM9bI3E//zOqkJr/0Jl0lqULLFojAVxMRk9jXpc4XMiLEllClubyVsSBVlxmZTsCF4yy+vkmal7F2UK/XLUvUmiyMPJ3AK5+DBFVThDmrQAAYIz/AKb86j8+K8Ox+L1pyTzRzDHzifP6NTjNM=</latexit>Figure 1: The construction used in the proof of Lemma 5.1.
returned point ˆθsatisﬁes
⏐⏐⏐⏐logˆν(z)
π((1−∆)z)⏐⏐⏐⏐≤⏐⏐⏐⏐logˆµ((1−∆)z)
π((1−∆)z)⏐⏐⏐⏐+ log1
(1−∆)d≤O(ε)
for allz∈K. SincefisL-Lipschitz we have⏐⏐⏐logπ(θ)
π((1−∆)θ)⏐⏐⏐=O(ε)for allθ∈K, and hence we would
then have that the distribution ˆνof the point ˆθreturned by Algorithm 1 satisﬁes
⏐⏐⏐⏐logˆν(z)
π(z)⏐⏐⏐⏐=⏐⏐⏐⏐logˆν(z)
π((1−∆)z)⏐⏐⏐⏐+O(ε)≤O(ε)∀z∈K. (1)
However, for (1) to hold, we still need to show that (1−∆)K⊆int∆r(K)(proved in Lemma 5.1). In
other words, we would like to show that for any point Z∈(1−∆)K, there is a ball B(Z,∆r)centered
atZof radius ∆rcontained in K. To show this fact, it is sufﬁcient to consider the convex hull Cof
B(0,r)∪{
1
1−∆Z}
⊆K, and show that it contains the ball B(Z,∆r). Towards this end, we make the
following geometric construction: we let pbe a point such that the line pˆθis tangent to B(0,r), andqthe
point onpˆθwhich minimizes the distance ∥q−Z∥2(see Figure 1). Since ∠0pˆθand∠Zqˆθare both right
angles, we have that the triangles 0pˆθandZqˆθare similar triangles, and hence that∥Z−q∥2
r=∥Z−ˆθ∥2
∥ˆθ−0∥2. In
other words,
∥Z−q∥2=∥Z−ˆθ∥2
∥ˆθ−0∥2×r=Z−1
1−∆Z
21
1−∆Z
2×r= ∆×r,
implying a ball of radius ∆rcentered atZis contained inC⊆K, and hence that
(1−∆)K⊆int∆r(K).
Bounding the inﬁnity distance error. To complete the bound on the inﬁnity-distance of the distribution ˆνof
the point returned by Algorithm 1 to the target distribution π, we must show both a lower bound (Lemma
5.5) and an upper bound (Lemma 5.6) on the ratioˆν(θ)
π(θ)at every point θ∈K. Both the upper and lower
bounds are necessary to bound the inﬁnity-distance d∞(ˆν(θ),π(θ)) = supθ∈K⏐⏐⏐logˆν(θ)
π(θ)⏐⏐⏐.
10
Both Lemmas 5.5 and 5.6 require the input point to have TV error δ<ε (R
∆r)−de−LR. The term (R
∆r)−d
is a lower bound on the ratio of the volume of Kto the volume of the smoothing ball B(0,∆r); this bound
holds sinceKis contained in a ball of radius R. The terme−LRis a lower bound on the ratiominw∈Kπ(w)
maxw∈Kπ(w)
of the minimum value of the density πto the maximum value of πat any two points in K; this bound holds
sincefisL-Lipschitz.
The above choice of δensures that in any ball B(z,∆r)with center zin the ∆r-interior ofK, the
distribution µof the input point, which satisﬁes ∥µ−π∥TV≤δ, will have between e−εandeεtimes the
probability mass which the target distribution πhas inside the ball B(z,∆r). Thus, when the distribution µ
is smoothed by adding noise uniformly distributed on a ball of radius ∆r, the smoothed distribution ˜ν(θ)is
withine−εandeεtimes the target probability density π(θ)at any point θin the ∆r-interior ofK, allowing
us to bound the inﬁnity distance error of the smoothed distribution ˜νat any point θin the ∆r-interior ofK.
We then apply this fact, together with Lemma 5.1 which says that for any point θ∈Kthe point (1−∆)θ
is in the ∆r-interior ofK, to bound the distribution νof the output point (after the stretching operation) as
follows,
ν(θ)≥(1−∆)d˜ν((1−∆)θ)Lemma 5.1
≥ (1−∆)dπ((1−∆)θ)×e−ε≥π(θ)e−ε
2. (2)
Here our choice of hyperparameter ∆≤ε
max(d,LR )ensures that (1−∆)d= Ω(1) and, sincefisL-
Lipschitz, that π((1−∆)θ)≥e−επ(θ). This proves the lower bound (Lemma 5.5). The proof of the upper
bound (Lemma 5.6) follows in a similar way as equation (2) but with the inequalities going in the opposite
direction.
Bounding the number of iterations and concluding the proof of Theorem 2.2. We still need to deal with
the problem that the point ˆθmay not be accepted. If this occurs, roughly speaking, we repeat the above
procedure until a point ˆθwith distribution ˆνis accepted. To bound the number of iterations, we show
thatˆθis inKwith high probability. Towards this end, we ﬁrst use the facts that fisL-Lipschitz and
K⊆B(0,R), to show that the probability a point sampled from π∝e−flies inside (1−∆)Kis at least
(1−∆)de−L∆R≥9
10(Lemma 5.2). Lemma 5.2 says that if you stretch the polytope by a factor of1
1−∆, then
most of the volume of the stretched polytope (1
1−∆)Kremains inside the original polytope K. The term
(1−∆)dis just the ratio of the volume of (1−∆)Kto the volume of K. And, since fisL-Lipschitz, the
terme−L∆Rbounds the ratioπ(θ)
π(1
1−∆θ)of the target density at any point θ∈Kto the value of πat the point
1
1−∆θto which the stretching operation transports θ, whenever1
1−∆θ∈K. The choice of hyperparameter
∆≤ε
max(d,LR )ensures that the acceptance probability (1−∆)de−L∆Rguaranteed by Lemma 5.2 is at least
9
10. Since the convex body (1−∆)Kcontains the ball B(0,r
2), applying Lemma 5.1 a second time (this
time to the convex body (1−∆)K) we get that
(1−3∆)K⊆int∆r((1−∆)K).
Thus, by Lemma 5.2 we have that θlies inside int∆r((1−∆)K)with probability at least9
10−δ≥8
10
(asθis sampled from πwith TV error≤δ). Therefore, since ξ∼B(0,∆r), we must also have that the
probability that the point ˆθ=1
1−∆(θ+ξ)is inK(and is therefore not rejected) is greater than8
104. This
implies that the number of iterations until our algorithm returns a point ˆθis less thank>0with probability
at least 1−2−k, and the expected number of iterations is at most 2 (proved in Corollary 5.4).
Since each iteration requires one random sample θfrom the distribution µ, and one call to a membership
oracle forK(to determine if1
1−∆Z∈K), the number of sampling oracle and membership oracle calls
4In Algorithm 1 we reject ˆθwith a slightly higher probability to ensure that, in differential privacy applications, in addition to
the privacy of the point returned by the algorithm, the runtime is also ε-differentially private.
11"
1805.05855,D:\Database\arxiv\papers\1805.05855.pdf,"While many algorithms are inspired by natural phenomena, some are considered ""social"" algorithms due to their reliance on interactions between agents. What are the key characteristics that distinguish these social algorithms from other nature-inspired algorithms, and how do these characteristics contribute to their effectiveness in solving optimization problems?","Social algorithms are distinguished by their use of a population of multiple agents, each representing a solution, and their reliance on interactions between these agents to explore the solution space. These interactions, often driven by stochastic processes, allow for both local and global search, enabling the algorithms to escape local optima and find more globally optimal solutions.","time, the artiﬁcial bee colony (ABC) algorithm was developed by D. Ka raboga in 2005 (Karaboga
2005). All these algorithms are bee-based algorithms and they all u se some (but diﬀerent) aspects of
the foraging behaviour of social bees.
Then, in late 2007 and early 2008, the ﬁreﬂy algorithm (FA) was deve loped by Xin-She Yang,
inspired by the ﬂashing behaviour of tropic ﬁreﬂy species (Yang 200 8). The attraction mechanism,
together with the variation of light intensity, was used to produce a nonlinear algorithm that can deal
with multiomodal optimization problems. In 2009, cuckoo search (CS ) was developed by Xin-She
Yang and Suash Deb, inspired by the brood parasitism of the reprod uction strategies of some cuckoo
species (Yang and Deb 2009). This algorithm simulated partly the com plex social interactions of
cuckoo-host species co-evolution. Then, in 2010, the bat algorith m (BA) was developed by Xin-She
Yang, inspired by the echolocation characterisitics of microbats (Y ang 2010b), and uses frequency-
tuning in combination with the variations of loudness and pulse emission rates during foraging. All
these algorithms are can be considered as social algorithms becaus e they use the ‘social’ interactions
and their biologically-inspired rules.
There are other algorithms developed in the last two decades, but t hey are not social algorithms.
For example, harmony search is a music-inspired algorithm (Geem et a l. 2001), while gravitational
search algorithm (GSA) is a physics-inspired algorithm (Rashedi et a l. 2009). In addition, ﬂower
pollination algorithm (FPA) is an algorithm inspired by the pollination feat ures of ﬂowering plants
(Yang 2012) with promising applications (Yang et al. 2014, Rodrigues et al. 2016). All these
algorithms are population-based algorithm, but they do not strictly belong in swarm intelligence
based or social algorithms. A wider range of applications of nature- inspired algorithms can be found
in the recent literature (Yang et al. 2015, Yang and Papa 2016, Yan g 2018).
As the focus of this work is on social algorithms, we will now explain som e of the social algorithms
in greater details.
2 Algorithms and Optimization
In order to demonstrate the role of social algorithms in solving optim ization problems, let us ﬁrst
brieﬂy outline the essence of an algorithm and the general formulat ion of an optimization problem.
2.1 Essence of an Algorithm
An algorithm is a computational, iterative procedure. For example, N ewton’s method for ﬁnding the
roots of a polynomial p(x) = 0 can be written as
xt+1=xt−p(xt)
p′(xt), (1)
wherextis the approximation at iteration t, andp′(x) is the ﬁrst derivative of p(x). This procedure
typically starts with an initial guess x0att= 0.
In most cases, as along as p′̸= 0 and x0is not too far away from the target solution, this
algorithm can work very well. As we do not know the target solution x∗= limt→∞xtin advance,
the initial guess can be an educated guess or a purely random guess . However, if the initial guess
is too far way, the algorithm may never reach the ﬁnal solution or sim ply fail. For example, for
p(x) =x2+9x−10 = (x−1)(x+10), we know its roots are x∗= 1 and x∗=−10. We also have
p′(x) = 2x+9 and
xt+1=xt−(x2
t+9xt−10)
2xt+9. (2)
If we start from x0= 10, we can easily reach x∗= 1 in less than 5 iterations. If we use x0= 100, it
may take about 8 iterations, depending on the accuracy we want. I f we start any value x0>0, we
can only reach x∗= 1 and we will never reach the other root x∗=−10. If we start with x0=−5, we
can reach x∗=−10 in about 7 steps with an accuracy of 10−9. However, if we start with x0=−4.5,
the algorithm will simply fail because p′(x0) = 2x0+9 = 0.
4
This has clearly demonstrated that the ﬁnal solution will usually depe nd on where the initial
solution is.
This method can be modiﬁed to solve optimization problems. For examp le, for a single objective
function f(x), the minimal and maximal values should occur at stationary points f′(x) = 0, which
becomes a root-ﬁnding problem for f′(x). Thus, the maximum or minimum of f(x) can be found by
modifying the Newton’s method as the following iterative formula:
xt+1=xt−f′(xt)
f′′(xt). (3)
ForaD-dimensionalproblemwithanobjective f(x)withindependentvariables x= (x1,x2,...,xD),
the above iteration formula can be generalized to a vector form
xt+1=xt−∇f(xt)
∇2f(xt), (4)
where we have used the notation convention xtto denote the current solution vector at iteration t
(not to be confused with an exponent).
In general, an algorithm Acan be written as
xt+1=A(xt,x∗,p1,...,pK), (5)
which represents that fact that the new solution vector is a funct ion of the existing solution vector xt,
some historical best solution x∗during the iteration history and a set of algorithm-dependent para m-
etersp1,p2,...,pK. The exact function forms will depend on the algorithm, and diﬀeren t algorithms
are only diﬀerent in terms of the function form, number of paramet ers and the ways of using historical
data.
2.2 Optimization
In general, an optimization problem can be formulated in a D-dimensional design space as
minimize f(x),x= (x1,x2,...,xD)∈RD, (6)
subject to
hi(x) = 0,(i= 1,2,...,M), gj(x)≤0,(j= 1,2,...,N), (7)
wherehiandgjare the equality constraints and inequality constraints, respectiv ely. In a special
case when the problem functions f(x),hi(x) andgj(x) are all linear, the problem becomes linear
programming, which can be solved eﬃciently by using George Dantzig’s Simplex Method. However,
in most cases, the problem functions f(x),hi(x) andgj(x) are all nonlinear, and such nonlinear op-
timization problems can be challenging to solve. There are a wide class o f optimization techniques,
including linear programming, quadratic programming, convex optimiz ation, interior-point method,
trust-region method, conjugate-gradient methods (S¨ uli and M ayer 2003, Yang 2010c) as well as evo-
lutionary algorithms (Goldberg 1989), heuristics (Judea 1984) and metaheuristics (Yang 2008, Yang
2014b).
An interesting way of looking at algorithms and optimization is to consid er an algorithm system
as a complex, self-organized system (Ashby 1962, Keller 2009), bu t nowadays researchers tend to look
at algorithms from the point of view of swarm intelligence (Kennedy et al. 2001, Engelbrecht 2005,
Fisher 2009, Yang 2014b).
2.3 Traditional Algorithms or Social Algorithms?
As there are many traditional optimization techniques, a natural q uestion is why we need new algo-
rithms such as social algorithms? One may wonder what is wrong with t raditional algorithms? A
short answer is that there is nothing wrong. Extensive literature a nd studies have demonstrated that
traditional algorithms work quite well for many diﬀerent types of pr oblems, but they do have some
serious drawbacks:
5
•Traditional algorithms are mostly local search, there is no guarant ee for global optimality for
most optimization problems, except for linear programming and conv ex optimization. Con-
sequently, the ﬁnal solution will often depend on the initial starting points (except for linear
programming and convex optimization).
•Traditional algorithms tend to be problem-speciﬁc because they us ually use some information
such as derivatives about the local objective landscape. They can not solve highly nonlinear,
multimodal problems eﬀectively, and they struggle to cope with prob lems with discontinuity,
especially when gradients are needed.
•These algorithms are largely deterministic, and thus the exploitation ability is high, but their
exploration ability and diversity of solutions are low.
Social algorithms, in contrast, attempts to avoid these disadvant ages by using a population-based
approach with non-deterministic or stochastic components to enh ance their exploration ability. Com-
paredwith traditionalalgorithms, metaheuristicsocial algorithmar emainly designed for globalsearch
and tend to have the following advantages and characteristics:
•Almost all social algorithms are global optimizers, it is more likely to ﬁnd the true global opti-
mality. They are usually gradient-free methods and they do not use any derivative information,
and thus can deal with highly nonlinear problems and problems with disc ontinuities.
•They often treat problems as a black box without speciﬁc knowledge , thus they can solve a
wider range of problems.
•Stochasticcomponentsinsuchalgorithmscanincreasethe explora tionabilityandalsoenablethe
algorithms to escape any local modes (thus avoiding being trapped lo cally). The ﬁnal solutions
tend to ‘forget’ the starting points, and thus independent of any initial guess and incomplete
knowledge of the problem under consideration.
Though with obvious advantages, social algorithms do have some dis advantages. For example,
the computational eﬀorts of these algorithms tend to be higher th an those for traditional algorithms
because more iterations are needed. Due to the stochastic natur e, the ﬁnal solutions obtained by such
algorithms cannot be repeated exactly, and multiple runs should be c arried out to ensure consistency
and some meaningful statistical analysis.
3 Social Algorithms
The literature of social algorithms and swarm intelligence is expanding rapidly, here we will introduce
some of the most recent and widely used social algorithms.
3.1 Ant Colony Optimization
Ants are social insects that live together in well-organizedcolonies w ith a population size rangingfrom
about 2 million to 25 million. Ants communicate with each other and intera ct with their environment
in a swarm using local rules and scent chemicals or pheromone. There is no centralized control. Such
a complex system with local interactions can self-organize with emer ging behaviour, leading to some
form of social intelligence.
Based on these characteristics, the ant colony optimization (ACO) was developed by MarcoDorigo
in 1992 (Dorigo 1992), and ACO attempts to mimic the foraging behav iour of social ants in a
colony. Pheromone is deposited by each agent, and such chemical w ill also evaporate. The model for
pheromone deposition and evaporation may vary slightly, depend on the variants of ACO. However,
in most cases, incremental deposition and exponential decay are u sed in the literature.
From the implementation point of view, for example, a solution in a netw ork optimization problem
can be a path or route. Ants will explore the network paths and dep osit pheromone when it moves.
The quality of a solution is related to the pheromone concentration o n the path. At the same time,
6
pheromone will evaporate as (pseudo)time increases. At a junctio n with multiple routes, the probabil-
ity of choosing a particular route is determined by a decision criterion , depending on the normalized
concentration of the route, and relative ﬁtness of this route, co mparing with all others. For example,
in most studies, the probability pijof choose a route from node ito nodejcan be calculated by
pij=φα
ijdβ
ij∑n
i,jφα
ijdβ
ij, (8)
whereα,β >0 are the so-called inﬂuence parameters, and φijis the pheromone concentration on the
route between iandj. In addition, dijis the desirability of the route (for example, the distance of
the overall path). In the simplest case when α=β= 1, the choice probability is simply proportional
to the pheromone concentration.
It is worth pointing out that ACO is a mixed of procedure and some simp le equations such as
pheromonedepositionand evaporationaswellasthe path selection probability. ACOhasbeen applied
to many applications from scheduling to routing problems (Dorigo 199 2).
3.2 Particle Swarm Optimization
Manyswarmsin naturesuchasﬁsh andbirds canhavehigher-levelb ehaviour, but they all obeysimple
rules. For example, a swarm of birds such as starlings simply follow thr ee basic rules: each bird ﬂies
according to the ﬂight velocities of their neighbour birds (usually abo ut seven adjacent birds), and
birds on the edge of the swarm tend to ﬂy into the centre of the swa rm (so as to avoid being eaten by
potential preditors such as eagles). In addition, birds tend to ﬂy t o search for food or shelters, thus
a short memory is used. Based on such swarming characteristics, p article swarm optimization (PSO)
was developed by Kennedy and Eberhart in 1995, which uses equatio ns to simulate the swarming
characteristics of birds and ﬁsh (Kennedy and Eberhart 1995).
Forthe easeofdiscussions below, let us use xiandvito denote the position (solution) and velocity,
respectively, of a particle or agent i. In PSO, there are nparticles as a population, thus i= 1,2,...,n.
There are two equations for updating positions and velocities of par ticles, and they can be written as
follows:
vt+1
i=vt
i+αǫ1[g∗−xt
i]+βǫ2[x∗
i−xt
i], (9)
xt+1
i=xt
i+vt+1
i, (10)
whereǫ1andǫ2are two uniformly distributed random numbers in [0,1]. The learning par ameters α
andβare usually in the range of [0,2]. In the above equations, g∗is the best solution found so far
by all the particles in the population, and each particle has an individua l best solution x∗
iby itself
during the entire past iteration history.
It is clearly seen that the above algorithmic equations are linear in the sense that both equation
only depends on xiandvilinearly. PSO has been applied in many applications, and it has been
extended to solve multiobjective optimization problems (Kennedy et al. 2001, Engelbrecht 2005).
However, there are some drawbacks because PSO can often have so-called premature convergence
when the population loses diversity and thus gets stuck locally. Cons equently, there are more than 20
diﬀerent variants to try to remedy this with various degrees of impr ovements.
3.3 Bees-inspired Algorithms
Bees such as honeybees live a colony and there are many subspecies of bees. Honeybees have three
castes, including worker bees, queens and drones. The division of la bour among bees is interesting,
and worker bees forage, clean hive and defense the colony, and th ey have to collect and store honey.
Honeybees communicate by pheromone and ‘waggle dance’ and othe r local interactions, depending
on species. Based on the foraging and social interactions of honey bees, researchers have developed
various forms and variants of bees-inspired algorithms.
The ﬁrst use of bees-inspired algorithms was probably by S. Nakran i and C. A. Tovey in 2004
to study web-hosting servers (Nakrani and Tovey 2004), while slig htly later in 2004 and early 2005,
7
Yang used the virtual bee algorithm to solve optimization problems (Y ang 2005). At around the same
time, D. Karabogaused the artiﬁcial bee colony (ABC) algorithm to c arry out numerical optimization
(Karaboga2005). Inaddition,Phametal. (2005)usedbeesalgorit hmtosolvecontinuousoptimization
and function optimization problems. In about 2007, Afshar et al. (2 007) used a honey-bee mating
optimization approach for optimizing reservoir operations.
For example, in ABC, the bees are divided into three groups: forage r bees, onlooker bees and
scouts. For each food source, there is one forager bee who shar es information with onlooker bees after
returning to the colony from foraging, and the number of forager bees is equal to the number of food
sources. Scout bees do random ﬂight to explore, while a forager at a scarce food source may have to
be forced to become a scout bee. The generation of a new solution vi,kis done by
vi,k=xi,k+φ(xi,k−xj,k), (11)
which is updated for each dimension k= 1,2,...,Dfor diﬀerent solutions (e.g., iandj) in a population
ofnbees (i,j= 1,2,...,n). Here,φis a randomnumber in [-1,1]. Afood sourceis chosenby a roulette-
based probability criterion, while a scout bee uses a Monte Carlo style randomization between the
lower bound (L) and the upper bound (U).
xi,k=Lk+r(Uk−Lk), (12)
wherek= 1,2,...,Dandris a uniformly distributed random number in [0,1].
Bees-inspired algorithms have been applied in many applications with div erse characteristics and
variants (Pham et al. 2005, Karaboga 2005).
3.4 Bat Algorithm
Bats are the only mammals with wings, and it is estimated that there ar e about 1000 diﬀerent
bat species. Their sizes can range from tiny bumblebee bats to giant bats. Most bat species use
echolocation to a certain degree, though microbats extensively us e echolocation for foraging and
navigation. Microbats emit a series of loud, ultrasonic sound pules an d listen their echoes to ‘see’
their surrounding. The pulse properties vary and correlate with th eir hunting strategies. Depending
on the species, pulse emission rates will increase when homing for pre y with frequency-modulated
short pulses (thus varying wavelengths to increase the detection resolution). Each pusle may last
about 5 to 20 milliseconds with a frequency range of 25 kHz to 150 kHz , and the spatial resolution
can be as small as a few millimetres, comparable to the size of insects t hey hunt.
Bat algorithm (BA), developed by Xin-She Yang in 2010, uses some ch aracteristics of frequency-
tuning and echolocation of microbats (Yang 2010b, Yang 2011). It also uses the variations of pulse
emission rate rand loudness Ato control exploration and exploitation. In the bat algorithm, main
algorithmic equations for position xiand velocity vifor batiare
fi=fmin+(fmax−fmin)β, (13)
vt
i=vt−1
i+(xt−1
i−x∗)fi, (14)
xt
i=xt−1
i+vt
i, (15)
whereβ∈[0,1] is a random vector drawn from a uniform distribution so that the f requency can vary
fromfmintofmax. Here,x∗is the current best solution found so far by all the virtual bats.
From the above equations, we can see that both equations are linea r in terms of xiandvi. But,
the control of exploration and exploitation is carried out by the var iations of loudness A(t) from a
high value to a lower value and the emission rate rfrom a lower value to a higher value. That is
At+1
i=αAt
i, rt+1
i=r0
i(1−e−γt), (16)
where 0 < α <1 andγ >0 are two parameters. As a result, the actual algorithm can have a weak
nonlinearity. Consequently, BA can have a faster convergence ra te in comparison with PSO. BA has
been extended to multiobjective optimization and hybrid versions (Y ang 2011, Yang 2014b).
8
3.5 Fireﬂy Algorithm
There are about 2000 species of ﬁreﬂies and most species produce short, rhythmic ﬂashes by bio-
luminescence. Each species can have diﬀerent ﬂashing patterns an d rhythms, and one of the main
functions of such ﬂashing light acts as a signaling system to communic ate with other ﬁreﬂies. As light
intensity in the night sky decreases as the distance from the ﬂashin g source increases, the range of
visibility can be typically a few hundred metres, depending on weather conditions. The attractiveness
of a ﬁreﬂy is usually linked to the brightness of its ﬂashes and the timin g accuracy of its ﬂashing
patterns.
Based on the above characteristics, Xin-She Yang developed in 200 8 the ﬁreﬂy algorithm (FA)
(Yang 2008, Yang 2010a). FA uses a nonlinear system by combing th e exponential decay of light
absorption and inverse-square law of light variation with distance. I n the FA, the main algorithmic
equation for the position xi(as a solution vector to a problem) is
xt+1
i=xt
i+β0e−γr2
ij(xt
j−xt
i)+α ǫt
i, (17)
whereαis a scaling factor controlling the step sizes of the random walks, while γis a scale-dependent
parameter controlling the visibility of the ﬁreﬂies (and thus search m odes). In addition, β0is the
attractiveness constant when the distance between two ﬁreﬂies is zero (i.e., rij= 0). This system is
a nonlinear system, which may lead to rich characteristics in terms of algorithmic behaviour.
Since the brightness of a ﬁreﬂy is associated with the objective land scape with its position as
the indicator, the attractiveness of a ﬁreﬂy seen by others, dep ending on their relative positions and
relative brightness. Thus, the beauty is in the eye of the beholder. Consequently, a pair comparison
is needed for comparing all ﬁreﬂies. The main steps of FA can be summ arized as the pseudocode in
Algorithm 1.
Initialize all the parameters α,β,γ,n ;
Initialize a population of nﬁreﬁes;
Determine the light intensity/ﬁtness at xibyf(xi);
whilet <MaxGeneration do
forAll ﬁreﬂies ( i= 1 :n)do
forAll other ﬁreﬂies ( j= 1 :n) withi̸=j(inner loop) do
ifFireﬂyjis better/brighter than ithen
Move ﬁreﬂy itowardsjusing Eq.(17);
end
end
Evaluate the new solution;
Accept the new solution if better;
end
Rank and update the best solution found;
Update iteration counter t←t+1;
Reduceα(randomness strength) by a factor;
end
Algorithm 1: Fireﬂy algorithm.
It is worth pointing out that αis a parameter controlling the strength of the randomness or
perturbationsin FA.The randomnessshouldbe graduallyreducedt ospeedup theoverallconvergence.
Therefore, we can use
α=α0δt, (18)
whereα0is the initial value and 0 < δ <1 is a reduction factor. In most cases, we can use δ= 0.9 to
0.99, depending on the type of problems and the desired quality of solu tions.
If we look at Eq.(17) closely, we can see that γis an important scaling parameter. At one extreme,
we can set γ= 0, which means that there is no exponential decay and thus the vis ibility is very high
(all ﬁreﬂies can see each other). At the other extreme, when γ≫1, then the visibility range is very
9
short. Fireﬂies are essentially ﬂying in a dense fog and they cannot s ee each other. Thus, each ﬁreﬂy
ﬂies independently and randomly. Therefore, a good value of γshould be linked to the scale or limits
of the design variables so that the ﬁreﬂies within a range are visible to each other. This range is
determined by
L=1√γ, (19)
whereLthe typical size of the search domain or the radius of a typical mode shape in the objective
landscape. If there is no prior knowledge about its possible scale, we can start with γ= 1 for most
problems.
Infact, sinceFAisanonlinearsystem,ithastheabilitytoautomatica llysubdividethewholeswarm
into multiple subswarms. This is because short-distance attraction is stronger than long-distance
attraction, and the division of swarm is related to the mean range of attractiveness variations. After
division into multi-swarms, each subswarm can potentially swarmarou nd a local mode. Consequently,
FA is naturally suitable for multimodal optimization problems. Further more, there is no explicit use
of the best solution g∗, thus selection is through the comparison of relative brightness ac cording to
the rule of ‘beauty is in the eye of the beholder’.
It is worth pointing out that FA has somesigniﬁcant diﬀerences from PSO. Firstly, FA is nonlinear,
while PSO is linear. Secondly, FA has an ability of multi-swarming, while PSO cannot. Thirdly, PSO
uses velocities (and thus have some drawbacks), while FA does not u se velocities. Finally, FA has
some scaling control by using γ, while PSO has no scaling control. All these diﬀerences enable FA to
search the design spaces more eﬀectively for multimodal objective landscapes.
FA has been applied to a diverse range of applications and has been ex tended to multiobjective
optimization and hybridization with other algorithms (Yang 2014a, Ya ng 2014b).
3.6 Cuckoo Search
In the natural world, among 141 cuckoo species, 59 species engag e the so-called obligate brood para-
sitism (Davies 2011). These cuckoo species do not build their own nes ts and they lay eggs in the nests
of host birds such as warblers. Sometimes, host birds can spot the alien eggs laid by cuckoos and thus
can get rid ofthe eggsor abandon the nest by ﬂying awayto build a ne w nest in a new locationso as to
reduce the possibility of raising an alien cuckoo chick. The eggs of cuc koos can be suﬃciently similar
to eggs of host birds in terms the size, color and texture so as to inc rease the survival probability of
cuckoo eggs. In reality, about 1/5 to 1/4 of eggs laid by cuckoos will be discovered and abandoned by
hosts. In fact, there is an arms race between cuckoo species and host species, forming an interesting
cuckoo-host species co-evolution system.
Basedthe abovecharacteristics,Yang and Deb developedin 2009t he cuckoosearch(CS) algorithm
(Yang and Deb 2009). CS uses a combination of both local and global search capabilities, controlled
by a discovery probability pa. There are two algorithmic equations in CS, and one equation is
xt+1
i=xt
i+αs⊗H(pa−ǫ)⊗(xt
j−xt
k), (20)
wherext
jandxt
kare two diﬀerent solutions selected randomly by random permutatio n,H(u) is a
Heaviside function, ǫis a random number drawn from a uniform distribution, and sis the step size.
This step is primarily local, though it can become global search if sis large enough. However, the
main global search mechanism is realized by the other equation with L´ evy ﬂights:
xt+1
i=xt
i+αL(s,λ), (21)
where the L´ evy ﬂights are simulated (or drawn random numbers) b y drawing random numbers from
a L´ evy distribution
L(s,λ)∼λΓ(λ)sin(πλ/2)
π1
s1+λ,(s≫0). (22)
Hereα >0 is the step size scaling factor.
By looking at the equations in CS carefully, we can clearly see that CS is a nonlinear system due to
the Heaviside function, discovery probability and L´ evy ﬂights. The re is no explicit use of global best
10
Table 1: Characteristics and properties of social algorithms.
Components/characteristics Role or Properties
Population (multi-agents) Diversity and sampling
Randomization/perturbations Exploration and escape local optima
Selection and elitism Exploitation and driving force for convergence
Algorithmic equations Iterative evolution of solutions
g∗, but selection of the best solutions is by ranking and elitism where the current best is passed onto
the next generation. In addition, the use of L´ evy ﬂights can enha nce the search capability because a
fraction of steps generated by L´ evy ﬂights are larger than thos e used in Gaussian. Thus, the search
steps in CS are heavy-tailed (Pavlyukevich 2007, Reynolds and Rhod es 2009). Consequently, CS can
be very eﬀective for nonlinear optimization problems and multiobject ive optimization (Gandomi et
al. 2013, Yang and Deb 2013; Yang 2014a, Yildiz 2013). A relatively co mprehensive literature review
of cuckoo search has been carried out by Yang and Deb (2014).
3.7 Other Algorithms
As wementioned earlier, the literatureis expandingand moresociala lgorithmsarebeing developedby
researchers, but we will not introduce more algorithms here. Inst ead, we will focus on summarizing
the key characteristics of social algorithms and other population- based algorithms so as to gain a
deeper understanding of these algorithms.
4 Algorithm Analysis and Insight
4.1 Characteristics of Social Algorithms
Though diﬀerent social algorithms have diﬀerent characteristics a nd inspiration from nature, they do
share some common features. Now let us look at these algorithms in t erms of their basic steps, search
characteristics and algorithm dynamics.
•All social algorithms use a population of multiple agents (e.g., particles , ants, bats, cuckoos,
ﬁreﬂies, bees, etc.), each agent corresponds to a solution vecto r. Among the population, there
is often the best solution g∗in terms of objective ﬁtness. Diﬀerent solutions in a population
represent both diversity and diﬀerent ﬁtness.
•Theevolutionofthepopulationisoftenachievedbysomeoperators (e.g., mutationbyavectoror
by randomization), often in terms of some algorithmic formulas or eq uations. Such evolution is
typically iterative, leading to evolution of solutions with diﬀerent prop erties. When all solutions
become suﬃciently similar, the system can be considered as converg ed.
•All algorithms try to carry out some sort of both local and global se arch. If the search is mainly
local, it increases the probability of getting stuck locally. If the sear ch focuses too much on
global moves, it will slow down the convergence. Diﬀerent algorithms may use diﬀerent amount
of randomization and diﬀerent portion of moves for local or global s earch so as to balance
exploitation and exploration (Blum and Roli 2001).
•Selection of the better or best solutions is carried out by the ‘surviv al of the ﬁttest’ or simply
elitismsothatthebestsolution g∗iskeptinthepopulationinthenextgeneration. Suchselection
essentially acts a driving force to drive the diverse population into a c onverged population with
reduced diversity but with a more organized structure.
These basic components, characteritics and their properties can be summarized in Table 1.
11"
2203.04386,D:\Database\arxiv\papers\2203.04386.pdf,"The paper describes a method for identifying divergent subgroups within datasets.  What are the potential benefits of using a sparsity-based feature selection method, like the one proposed in the paper, before performing subgroup discovery?",Sparsity-based feature selection can significantly reduce the computational time required for subgroup discovery by limiting the search space to a smaller set of relevant features. This can also improve the interpretability of the identified subgroups by providing a more concise and focused description of the characteristics that define them.,"Figure 1: The overview of proposed framework (SAFS) in which features are selected prior to
automated divergence subgroup detection using sparsity-based evaluation of their associations with
the outcome.
discovery metrics such as odds ratio to quantify its divergence from the remaining subset of the input
data.
2.2 Automated Feature Selection
The sparsity-based automatic feature selection (SAFS) component in Fig. 1 is tasked with selecting
the topKfeatures from a given M-dimensional feature space that are more useful for the subsequent
anomalous subgroup discovery step. Unlike the existing wrappers and embedded techniques in the
state-of-the-art feature selection, SAFS is model-free and does not require training of a particular
model. Similar to other ﬁlter-based techniques, SAFS employs a principled objective measure but to
evaluate the association between the outcome variable and each unique value of input feature. To
this end, SAFS uses Yule’s Y-coefﬁcient [ 32], which is a normalized odds ratio. We select Yule’s
Y-coefﬁcient as it is proven to be an objective measure that satisﬁes the most fundamental and
additional proprieties for a good measure [33].
Given a feature fmwithCmunique values, we manually stratify Dper each feature value ˆfu
m∈ˆfm,
resulting two subsets Du
mand˜Dum, whereD=Du
m⋃˜DumandDu
m=D|ˆfm=ˆfu
m. For example,
say a feature under-consideration is fm=Sex with three unique values in D:ˆf1
m=Female ,
ˆf2
m=Male andˆf3
m=Unknown/Missing . Then stratifying for ˆf1
mgivesD1
mcontaining all
samples in theDwithSex=Female whereas˜D1mcontaining the remaining samples in Dwith
4
Sex=Female orSex=Unknown/Missing . In order to compute the Yule’s Y coefﬁcient, we
generate a 2×2contingency table from Du
mand˜Dumas follows,
Y=1 Y=0
Du
mαβ
˜Dumδγ
whereαis the number of samples in Du
mwith the binary outcome Y= 1,βis the number of samples
inDu
mwithY= 0. Similarly, δis the number of samples in ˜DumwithY= 1,γis the number
of samples in ˜DumwithY= 0. Note thatα+βis size ofDu
mandδ+γis the size of ˜Dum, i.e.,
N=α+β+δ+γ. Yule’s Y objective measure for the feature value ˆfu
mis computed as
ou
m=√
P(Dum,Y= 1)P(˜Dum,Y= 0)−√
P(Dum,Y= 0)P(˜Dum,Y= 1)
√
P(Dum,Y= 1)P(˜Dum,Y= 0) +√
P(Dum,Y= 0)P(˜Dum,Y= 1)(1)
ou
m=√αγ−√βδ
√αγ+√βδ(2)
Onceou
mis computed for u= [1,2,···,Cm]of featurefm, we we employ Gini-index [ 26] to evaluate
the sparsity of Yule’s Y coefﬁcient across the feature values. Gini-index is selected as it is the only
sparsity measure that satisﬁes the required six properties as described in [ 26]. These six properties
include Dalton’s four laws ( Robin Hood, Scaling, Rising Tide and Cloning ) in addition to Bill
Gates andBabies [26]. Gini-index is computed over the ranked vector of objective measures− →om=
[o(1)
m,o(2)
m,···,o(Cm)
m]in ascending order, i.e., o(1)
m≤o(2)
m≤···≤o(Cm)
m , and (1),(2),···(Cm)are
the indices after the sorting operation. Then the Gini-index ( ηm) offmis then computed as
ηm= 1−2Cm∑
i=1o(i)
m
||− →om||1(Cm−i+1
2
Cm) (3)
where||·|| 1represents the l1norm. The Gini-index is computed for all the features m= [1,2,···,M]
and they are ranked in decreasing order where a feature with the largest Gini-index takes the top spot.
The summary of the steps for sparsity-based feature selection is shown in Algorithm 1. The follow
up divergent subgroup detection step takes the top K <M features as described below.
2.3 Subgroup Discovery via Subset Scanning
We employ Multi-Dimensional Subset Scanning (MDSS) [ 14] from the anomalous pattern detection
literature in order to identify signiﬁcantly divergent subset of samples. MDSS could be posed as
a search problem over possible subsets in a multi-dimensional array to identify the subsets with a
systematic deviation between observed outcomes (i.e., yi) and expectation of the outcomes, the latter
of which could be set differently for variants of the algorithm. In the simple automatic stratiﬁcation
setting, the expectation is the global outcome average µg=∑N
i=1yi
NinDr. The deviation between the
expectation and observation is evaluated by maximizing a Bernoulli likelihood ratio scoring statistic
for a binary outcome, Γ(·). The null hypothesis assumes that the likelihood of the observed outcome
in each subgroupDsis similar to the expected, i.e., H0:odds(Ds) =µg
1−µg; while the alternative
hypothesis assumes a constant multiplicative increase in the odds of the observed outcome in the
anomalous or extremely divergent subgroup Da, i.e.,H1:odds(Da) =qµg
1−µgwhereq̸= 1(q>1
for extremely over observed subgroup (e.g., high risk population); and 0<q< 1for extremely under
observed subgroup (e.g., low risk population). The anomalous scoring function for a subgroup ( Ds)
with referenceDris formulated as, Γ(Ds,Dr)and computed as:
Γ(Ds,Dr) = max
qlog(q)∑
i∈Syi−NS∗log(1−µg+qµg), (4)
whereNSis the number of samples in Ds. Divergent subgroup identiﬁcation is iterated until
convergence to a local maximum is found, and the global maximum is subsequently optimized using
5
Algorithm 1: Pseudo-code for the proposed automated feature selection (SAFS) based on the
Gini sparsity of Yule’s Y coefﬁcients as objective measure between features and a binary outcome.
input : Dataset:D={(xi,yi)|i= 1,2,···,N},
Set of features:F= [f1,f2,···,fm,···,fM],
Required number of features: K.
output : Set of selected features: Fr
1η←ZerosArray (M);
2forfminFdo
3 ˆfm←IdentifyUniqueValues (fm);
4Cm←|ˆfm|;
5om←ZerosArray (Cm);
6 foru←1toCmdo
7Du
m←Stratification (D,ˆfu
m);
8 ˜Dum←Du
m-Du
m;
9ou
m←YuleY (Du
m,˜Dum);
10ηm←GiniSparsity (om);
11I←SortDescendingIndices (η);
12Fs←F[I];
13Fr←TopK (Fs,K);
14returnFr
multiple random restarts. The characterization of the identiﬁed anomalous subgroup ( Da) includes
quantifying the anomalousness score Γ(Da,Dr), the analysis of the anomalous features and their
values ˆFa, the size of the subgroup NS, the odds ratio between Daand˜Daand95% Conﬁdence
Interval (CI) of the odds ratio, the signiﬁcance tests quantiﬁed using empirical p-value, and the time
elapsed to identify Da.
3 Experimental Setup
3.1 Datasets
We employ two publicly available tabular datasets to validate the proposed feature selection framework
and identify divergent subgroups in these datasets. These datasets are the Medical Information Mart
for Intensive Care (MIMIC-III) [ 27] and All State Claim Severity dataset (Claim) [ 28]. MIMIC-III is
a freely accessible critical care dataset recording vital signs, medications, laboratory measurements,
observations and notes charted by care providers, ﬂuid balance, procedure codes, diagnostic codes,
imaging reports, hospital length of stay, and survival data. We selected a study cohort of adult patients
(16 years or older) who were admitted to the ICU for the ﬁrst time. The length of stay was greater
than a day, with no hospital readmissions, no surgical cases, and having at least one day one chart
events. The ﬁnal cohort consisted of 19,658rows of patients data. We constructed M= 41 features
(15numerical and 26categorical) based on observations made on the ﬁrst 24 hours of ICU admission.
The numerical features are later discretized. We deﬁned the target outcome as a binary indicator
variableyisuch thatyi= 1for patients who died within 28 days of the onset of their ICU admission,
andyi= 0otherwise.
The Claim dataset is released by Allstate, an US-based insurance company as a Kaggle challenge [ 28]
to predict the severity of the claims. In our validation, we use 185,000training claim examples with
109anonymized categorical features in our validation, and the numeric lossfeature is used as the
outcome of interest. We also transform the outcome to a binary variable using the median loss as a
threshold, i.e., loss values greater than equal to the median loss are set to yi= 1, and loss values less
than the median are set to yi= 0.
6
3.2 Existing methods selected for comparison
In order to compare our sparsity-based feature selection framework, we selected multiple existing
methods in the state-of-the-art for selecting features from tabular data. These methods are Filter,
Wrapper, and Embedded methods [21, 23, 22].
Filter-based methods exploit the statistical characteristics of input data to select features independent
of any modeling algorithms [ 23]. In this study, we implemented a ﬁlter method based on mutual
information gain [22, 34]. Features are ranked in decreasing order of their mutual information, and
top-ranked features are assumed to be more important than low-ranked features.
Wrapper methods [ 23,21,22] measure the usefulness of features by learning a stepwise Ordinary
Least Squares regression and dropping less signiﬁcant features recursively until a stopping rule, such
as the required top Kfeatures, is reached. In this study, we implemented a wrapper method using
recursive feature elimination [35].
Embedded methods select features based on rankings from a model. We implemented this by
employing two tree-based classiﬁers, i.e., CatBoost [ 36] and XGBoost [ 30] and a Committee vote [ 23]
from both. Unlike wrapper method, embedded methods begin with the ﬁtting of the tree-based
model followed by ranking features based on their importance. In the case of Committee-based
selection, the importance score from each of the two tree-based models is normalized using min-max
scaling separately. Then the average of these importance scores is computed to rank the features.
The three embedded methods above require calibration of the model. We also experimented with
Shap [ 25]-value based feature importance using XGBoost classiﬁer but using the default setting
without calibration (herein referred to as Fast-Shap ).
3.3 Setup
We set-up the subgroup discovery task as form of automatic data stratiﬁcation use cases in the two
datasets as follows. The subgroup in MIMIC-III dataset refers to a subset of patients with highest
death risk compared to the average population in that dataset ( µg= 17.2%). On the other hand, the
subgroup discovery task in the Claim dataset is formulated as identifying a subset of claims with the
highest severity compared to the µg= 42.2%of claims possess higher or equal to the median loss.
For each dataset, we conducted subgroup discovery using the top Kfeatures selected by the different
feature selection methods examined. Speciﬁcally, we experimented with top K∈{5, 10, 15, 20, 25,
30, 35, 40, 41} features for the MIMIC-III dataset, and top K∈{10, 20, 30, 40, 50, 60, 70, 80, 90,
100, 109} features for the Claim dataset. Note that K= 41 andK= 109 represent using the entire
original feature set in MIMIC-III and Claim datasets, respectively.
To compare the different feature selection methods, we measured the computation times elapsed
to rank the features as the ﬁrst performance metric. Furthermore, we explore the similarity of the
feature ranking across these methods using rank-based overlap [ 37]. In addition, the output of the
subgroup discovery algorithm using top Kfeatures were also evaluated to inspect the usefulness of
the selected features to detect divergent subgroup. We also compared the amount of time elapsed to
identify the subgroup across different top Kvalues to determine the amount of computation time
saved by using the selected top Kfeatures rather than the whole input feature set. Lastly, we used
the Jaccard similarity is employed to evaluate the similarity of the anomalous samples detected by
using the selected top Kand the whole features. Note that all the experiments are conducted on a
MacBook Pro with macOS Big Sur OS v11.6, 2.9 GHz Quad-Core Intel Core i7 (processor) and 16
GB 2133 MHz LPDDR3 (memory).
4 Results and Discussion
Table 1 presents the time duration in seconds (s) elapsed by each feature selection method to rank
features in the two datasets used for validation. The results show that the proposed sparsity-based
feature selection framework achieved the ranking with the shortest duration: 70.5seconds in Claim
and just 3.3seconds in MIMIC-III datasets, resulting an average reduction of feature selection time
by a factor of 104×(in Claim) and 81×(in MIMIC-III) compared to the existing methods. As
expected, the wrapper method took the longest duration as it involves recursive ﬁtting of a model and
7
Table 1: Comparison of elapsed times (in seconds) by different feature selection methods for ranking
given feature sets across two validation datasets: Claim and MIMIC-III . The Proposed method
achieved the least elapsed time in both the datasets.
Datasets
Method Claim MIMIC-III
Wrapper[21, 23] 16444.3 777.4
Committee[23] 13565.6 652.7
XGBoost[30] 12306.1 124.7
CatBoost[31] 1259.5 21.4
Filter[23] 494.4 14.4
Fast-Shap[25] 114.1 11.3
Proposed 70.5 3.3
back-ward elimination of features. Considerably, the embedded methods also elapsed considerable
time, particularly in the larger dataset due to the need for hyper-parameter tuning before selection. The
committee vote approach also took longer than any of the XGBoost- and CatBoost-based embedded
methods as it tries to normalize and merge their separate rankings.
The pairwise similarity of rankings from different feature selection methods are illustrated in Fig. 2a
for the MIMIC-III dataset and in Fig. 2b for the Claim datasets using rank-based overlap scores.
In both datasets, it is clear that the ranking from Committee vote achieved the higher similarity
with CatBoost and XGBoost rankings. This is expected as the Committee vote is generated from
the two rankings. The feature rankings from Fast-Shap, relatively, resemble the other embedded
techniques, i.e., XGBoost and CatBoost. The overlapping of Fast-Shap and XGBoost are lower than
the overlapping of CatBoost and XGBoost even though the only difference between Fast-Shap and
XGBoost is that in the latter, the tuning of model parameters was conducted during ﬁtting while
Fast-Shap uses the default XGBoost parameters without tuning. The proposed SAFS method outputs
a higher similarity of ranking with that of Filter method as both employ objective measures to quantify
the association between features and outcome without training any model. The feature ranking from
the Wrapper method are most different from the other methods suggesting that it is the least effective
ranking method despite taking the longest duration to rank the features (see Table 1).
Figure 3 shows a steep increase in the detection time (in seconds) with the increase in the number of
features, thereby validating the plausibility of feature selection prior to the subgroup detection step.
While it is clear that subgroup discovery is more efﬁcient using a lower number of features selected in
a principled way, it is important to evaluate the consistency of features identiﬁed to characterize the
divergent subgroups detected. Ideally, similar descriptions of anomalous features should be achieved
from scanning across different top Kfeatures. To this end, we employed Upset plots to visualize the
similarity of the features in the anomalous subsets identiﬁed by scanning over the top Kfeatures
using our proposed SAFS method as shown in Fig. 4. In MIMIC-III dataset (see Fig. 4a), two features
curr_service andurineoutput_cat appears in the anomalous subset across all the different top K
values.The feature psychoses is also shown to appear in 8of9differentKvalues. In the Claims
dataset (Fig. 4b), three features ( cat80, cat1 and cat94 ) are consistently detected as a subset of the
anomalous features at least 10times out of 11differentKvalues. This validates the consistency of
the identiﬁed group across different top Kvalues and reafﬁrms the beneﬁt of using fewer selected
features, which achieve similar subgroup but with signiﬁcantly reduced computation time.
Table 2 presents a detailed comparison of the identiﬁed subgroups across different top kfeatures.
The details include the number of features and the corresponding number of values describing the
divergent subgroup, the number of samples in the subgroup, and percentage of the whole data in
the subgroup. The table also shows the odds ratio (and its 95% CI) of the outcome in the identiﬁed
subgroup compared to the complement subset, and the signiﬁcance of the divergence as measured by
an empirical p-value. Table 2 shows the detected subgroup details for the Claim dataset. It is clear
that by just using 20 features ( 18.3%of the whole features), we identify a subgroup with 44181 (24%
of the whole data) characterized by the following logical combinations of features (and values) :cat80
(’B’ or ’C’) and cat1 (’A’) and cat94 (’B’ or ’C’ or ’D’) . The odds of experiencing severe claims
is8.79(95% CI:8.58,9.01) higher in the identiﬁed subgroup than its complement. This identiﬁed
subgroup is similar to the subgroup identiﬁed using the whole 109features that results a subgroup
8
(a) MIMIC-III dataset
(b) Claim dataset
Figure 2: Rank-based overlap scores quantifying the pairwise similarity of feature rankings by
different selection methods.
of4201 samples ( 23% of whole data) and odds ratio of 9.30(95% CI:9.07,9.53). The samples
identiﬁed by using K= 20 andK= 109 features have a Jaccard similarity index of 0.95. Table 2
shows the same trend of achieving similar divergent subgroup using fewer number of features in
the MIMIC-III dataset. Speciﬁcally, using just 20features ( 48.8%of whole features), we identiﬁed
subgroup size of 3183 (16% of whole data) with a 3.86(95% CI:3.54,4.20) increase in odds of
experiencing death within 28days of the onset of their ICU admission. This group is described with
the following combination of features (and values): psychoses (’0’) andurineoutput_cat (’1’ or ’2’)
9
(a) MIMIC-III dataset
(b) Claim dataset
Figure 3: Comparison of the time taken to discover the divergent subgroup across different top K
features. Note K= 41 andK= 109 mean the whole feature set is used by the detection algorithm.
anddrug_abuse (’0’) andcurr_service (’0’ or ’5’ or ’6’ or ’8’) anddepression (’0’) ,insurance (’1’
or ’2’ or ’3’ or ’4’) andvent_ﬁrstday (’1’) . This is similar to the group identiﬁed using the whole
feature set (K= 41 ), which results in a divergent subgroup of 3078 (16% of whole data) with odds
ratio of 3.95(95% CI:3.63,4.31), achieving a Jaccard similarity of 0.93. The results also show that
the number of features and their unique values to describe the identiﬁed subgroup unnecessarily
grows with a larger number of top Kfeatures. This validates the beneﬁts of prior feature selection
framework before subgroup detection to achieve better interpretability of the identiﬁed subgroup and
signiﬁcantly reduce the computation time.
Limitations : The proposed feature selection is shown to achieve competitively or better perfor-
mance than existing feature selection techniques as it does not require ﬁtting of a particular model
(compared to existing Wrappers and Embedded methods), and it exploits the variation of objective
measures across the unique values of a feature (compared to existing Filter methods). However, its
limitation is mainly rests on its requirement to discretize numeric features into bins.
5 Conclusion and Future work
The model-centric approach has grown over the years to solve problems across different domains
using sophisticated models trained on large datasets. While methods for data-centric insight extraction
has not been given enough attention, they possess bigger potential to understand, clean, and valuate
data thereby improving the capability for more efﬁcient performance using less resources. Automatic
10
(a) MIMIC-III dataset
(b) Claim dataset
Figure 4: Upset plots to visualize the similarity of identiﬁed anomalous features that describe the
detected divergent groups, across the two datasets (MIMIC-III and Allstate Claims), using different
topKfeatures selected by the proposed SAFS.
divergent subgroup detection could be employed to answer different data-related questions, e.g., what
are the high-risk population towards a particular disease? Such automated techniques often do not
require prior assignment of a feature of interest, and they are scalable to high-dimensional feature
input. However, detection of such subset of the data requires searching across potentially exponential
combinations of input features that grow along with the number of input features. To this end, we
propose a sparsity-based automated feature selection (SAFS) framework for divergent subgroup
discovery that signiﬁcantly reduces the search space and consequently, the amount of time required to
complete the discovery and to improve the interpretation of the identiﬁed subgroups. SAFS employs
a Yule’s-Y coefﬁcient as objective measure of effect between each feature value and an outcome; and
then encodes the variations across values in a given feature using the Gini-index sparsity metric. Both
Yule’s-Y and Gini-index are chosen as they were proven to satisfy the fundamental requirements
of good objective measures and sparsity metrics, respectively. We validated our feature selection
11"
2302.07248,D:\Database\arxiv\papers\2302.07248.pdf,How does the paper's exploration of uncertainty highlighting in AI-powered code completion relate to the broader field of human-computer interaction (HCI) research on error detection and correction?,"The paper builds upon HCI research that demonstrates the importance of helping users detect errors in AI predictions, particularly in high-stakes domains. It specifically focuses on the use of inline highlights to communicate uncertainty in the context of code generation, drawing parallels to existing work on error highlighting in handwriting recognition, static code analysis tools, and spell checkers.","highlighting across several diﬀerent metrics, while highlighting tokens with the highest
predicted likelihood of being edited leads to faster task completion and more targeted
edits, and is subjectively preferred by study participants.
•We leverage design probes to further explore the design space of uncertainty highlighting,
and ﬁnd that participants prefer highlights that are granular, informative, interpretable,
and not overwhelming.
2 Related Work
AI systems can make mistakes for various reasons, and users may not always be able to detect
their errors. This can lead to overreliance on AI, which prior work has shown can decrease
task performance [ 3,24], reduce trust [ 3,56], and worsen user experience [ 28]. It is therefore
importanttohelpusersdetecterrorsinAIpredictions, especiallyinhigh-stakesdomains(suchas
healthcare and criminal justice) where erroneous predictions can have serious consequences [ 18].
Previous research has shown that techniques such as communicating uncertainty [ 57], explaining
AI predictions [ 17], and providing appropriate onboarding and user training [ 9,29,39] can help
users detect errors in AI predictions. Communicating uncertainty, which is our focus, has been
shown to be an eﬀective method for improving task performance [ 3], detecting errors [ 3,17], and
increasing trust [ 6,57]. However, it is still an open question how to best present uncertainty,
how to compute it, and whether it helps for the increasingly important generative scenario of
AI-powered code completion.
2.1 Computing and Communicating Uncertainty
There are various techniques for computing and communicating uncertainty in AI predictions,
such as using gauges, percentages, highlights, discretization, and deferring to the user (e.g.,
withholding uncertain predictions) [ 14,53]. Post hoc calibration methods and auxiliary models
can also be used to compute and improve uncertainty scores [ 21,26,40]. However, it is unclear
how these techniques from classiﬁcation-like tasks can be applied to generative tasks or the
speciﬁc scenario of AI-powered code completion. This lack of generalization may be due
to diﬀerences in the nature of the tasks. In classiﬁcation, predictions are typically atomic,
structured, and simple, whereas for text or code generation, predictions are often tokens
of varying length and are unstructured. There is more to double-check and more to ﬁx in
an AI-generated code completion scenario, which motivates our research to explore how to
eﬀectively compute and present uncertainty in this context. We discuss speciﬁc methods to
compute uncertainty for large language models in Section 3.1.
2.2 Highlighting Uncertainty
A few prior works have studied the use of inline highlights to communicate uncertainty in
the generative scenario of AI-powered code completion. Sun et al . [48]conducted qualitative
studies to gain insight into the explainability-related needs of programmers, and found that
their design, which highlighted uncertainty in a line of code using a wavy line, revealed the
need for: alternative outputs for uncertain code suggestions, explanations for why the AI was
uncertain, and the ability to display diﬀerent levels of uncertainty (e.g., using more colors
or hues). Similarly, Vaithilingam et al . [49]conducted qualitative studies and found that
programmers found it challenging to read and debug suggestions from Copilot, particularly
when its suggestions were long. Their participants expressed a desire for highlighting uncertain
code completions. Our work builds on these ﬁndings by quantitatively measuring the eﬀect of
highlighting uncertainty.
4
Inline highlights have been used to communicate uncertainty in many other related domains.
Lank et al . [30]studied the task of handwriting recognition and observed that errors in
recognition can pass undetected by the user [ 30], but that highlighting the errors can help slow
down the user and decrease the error rate. The use of static code analysis tools, such as pylint,1
have also been popular in software development to improve programmer productivity. These
tools are deployed at scale inside IDEs and can, for example, highlight errors in code, such
as violations of syntax, coding conventions, or undeﬁned variables. However, they may not
catch errors in the logic of the program (e.g., an incorrect regex). Similarly, spell checkers in
word processors like MS Word and Google Docs can detect and highlight spelling and common
grammatical errors, which can improve incidental learning [ 33]. AI-based services such as
Grammarly can also be used to highlight grammatical and spelling errors in written text. Our
work builds on this ﬁnding by incorporating highlighting in our AI-powered code completion
tool to improve accuracy and user experience.
2.3 Generative Models and Programmer Performance
There is a growing body of research on the use of generative models to support people in a
variety of tasks. In this paper, we focus on the use of generative models for code completion, but
it is worth noting that there are many other generative tasks in which these models have been
shown to be eﬀective. These tasks include, but are not limited to, machine translation [ 19,20],
co-creative writing [ 10,12], and the generation of art or music with AI [ 34,35]. In all of these
domains, there is evidence that generative models can help improve human performance.
Measuring performance in these domains can be challenging, as it is often multi-dimensional
and may include subjective elements [ 15]. For example, in the context of programming, it is
diﬃcult to capture programmer productivity in a single metric. However, there are dimensions
that are clearly important and should be captured. These include the correctness of the code
produced and the eﬀort put in by the programmer. Therefore, in this study, we focus on
capturing performance through metrics such as the number of unit tests passed and the time
taken. While these metrics may not capture the full complexity of programmer productivity,
they do provide valuable insights and can help inform future research on the use of generative
models in this context.
3 Two Notions of Uncertainty
In this section, we lay out the two notions of uncertainty that we consider in the study, and
the reasoning behind each. We explain how each is calculated and walk through examples
of highlights generated using the diﬀerent notions. We end with a list of the hypotheses we
explore in our study.
3.1 Uncertainty in Code Generation Models
Code generation models, like the ones we consider in this paper, are large language models that
have been trained or ﬁne-tuned for the task of generating computer code. A language model
is a system that predicts the conditional probability of a token (which may be a character,
word, or other string) given either the preceding context or, in some cases, its surrounding
context [ 4,5,47]. In the case of code generation models, the context may include, for example,
prompts, comments, and previously written code. Blocks of recommended code are generated
by sampling one token at a time. Language models range from simple n-gram models to vastly
1https://www.pylint.org
5
more sophisticated and expressive models based on modern self-attention architectures like the
transformer [52], but all share this common structure.
The conditional probability of the model producing a particular token in a given context—
what we refer to in this paper as the generation probability of the token—can be viewed as one
particular localized notion of the model’s uncertainty. Thus it is natural to consider revealing
the generation probability to end users in order to convey the uncertainty of generated code.
Indeed, Sun et al . [48]propose highlighting low probability lines of code, and OpenAI’s online
playground interface [ 42] includes an option to highlight individual tokens with either high or
low generation probability.
While past work has shown generation probabilities to be predictive of which code sug-
gestions programmers are likely to accept [ 38], this notion of uncertainty may not line up
with programmers’ intuition. For example, when it is time to introduce a new variable name,
the model will have many choices, all of which may have low generation probability simply
because none is signiﬁcantly more statistically probable than the rest. However, programmers
may incorrectly attribute the uncertainty as indicating a potential error. Likewise, if there
are multiple correct ways to implement a function, they cannot all simultaneously have high
generation probability, meaning that some will necessarily appear “uncertain.” The discon-
nect between the meaning of a generation probability and a programmer’s intuition may be
exacerbated by the fact that language models can be conﬁdently wrong, producing ﬂuently
inadequate [37] outputs that are merely statistically plausible linguistically, but that lose or
hallucinate information [e.g., 4, 25].
Given that the ultimate goal of a programmer working with a code generation tool is
generally to produce high quality code that does what it is intended to do, we hypothesize
that a more useful notion of the uncertainty of a generated token is the likelihood that a
programmer would need to modify or delete this token in order to arrive at code that satisﬁed
their needs. This is not something that can be obtained directly from the code generation model.
However, it could potentially be approximated by building a separate model based on logs of
programmers’ actions when presented with AI-generated code in similar contexts. Therefore,
we propose that learning an edit model , which predicts the aforementioned interactions, may
better capture a notion of uncertainty aligned with what programmers need to correct errors
and improve the generated output.
In this paper, we learn a closed-world edit model to achieve two goals: (1) to show a
proof-of-concept implementation of such an edit model, and (2) as a probe to determine
whether revealing such a notion of uncertainty to programmers would enable them to more
quickly and accurately produce code when collaborating with a code generation model. We
next describe how we built this model for the purposes of our study. In Section 7, we discuss
the feasibility of building a more general edit model, suitable for open-world settings.
3.2 Building the Closed-World Edit Model
For the purposes of our study, we did not build a general-purpose edit model, but rather a
model that predicts what code programmers would be most likely to edit for the three speciﬁc
coding tasks and code completions we use in the main study. These tasks are described in
detail in Section 4.1. For each of these tasks (as well as two additional tasks that we piloted; see
Section 4.1), we ran the task instructions through OpenAI’s Codex model to obtain completions
and generation probabilities. To generate data for the edit model, we then provided these
completions to participants who were given instructions to “change the completions to ensure
they have completed the task properly.” Participants were also provided with unit tests to
check their code.
We recruited nine participants. As with the main study, all participants were employees of
6
a large technology company located in the United States, and had experience coding in Python.
They were recruited through a mix of direct emails, posts on message boards, and word of
mouth, and were paid $50. All interviews were conducted over a video-conferencing platform
and lasted approximately one hour. Because of this time constraint, not all participants were
able to complete all coding tasks; each coding task was completed by six participants.
We created our closed-world edit model based on which tokens participants edited for each
coding task. We programmatically tracked which tokens from each completion were edited,
considering a token edited if at least one character had been changed, deleted, or commented
out. For example, if a token was “==” and the user changed it to “!=,” the token was considered
edited. For each token, we then set the probability of an edit to be the fraction of participants
who edited it.
3.3 Setting Thresholds
In order to highlight the most uncertain tokens, it was necessary to choose two thresholds. In
the condition using the generation probabilities as our notion of uncertainty, any token with
generation probability lowerthan a speciﬁed threshold would be highlighted. In the condition
using the edit model, any token with edit probability higherthan a speciﬁed threshold would be
highlighted. In order to make a fair comparison, we aimed to set these thresholds in such a way
that the number of characters highlighted across the three coding tasks would be comparable
between the two conditions – though the number of characters highlighted for any particular
task might vary.
For the edit model, we chose to highlight characters that were edited by at least 4 out of 6
of the participants, resulting in a total of 203 highlighted characters across the three coding
tasks. The amount of text highlighted was not too sensitive to this choice; moving to 3 out
of 6 would not have changed the highlights at all for two of the three coding tasks, while the
highlights in the third coding task would change for only one token.
To set the threshold for the generation probabilities, we aimed to highlight a similar number
of characters. This could be achieved by highlighting tokens with probability less than 0.694,
which would lead to 205 characters highlighted across the three conditions.2
3.4 Diﬀerences and Imperfections in Generation Probabilities and the Edit
Model
Figure 2 shows the code generations output by Codex for each of the three coding tasks, along
with the highlights generated using generation probabilities (left) and the edit model (right). As
noted earlier, an approximately equal number of characters are highlighted in both conditions.
However, in the edit model condition the highlights concentrate in the Ugly Number task,
reﬂecting that the nine participants changed this generation the most to get it working.
This distribution of highlights helps restate an important characteristic of the edit model: it
tends to correspond to coding errors, since highlighted tokens are those that participants most
frequently changed or deleted in pursuit of correcting or improving the initial code completion.
For example, in Ugly Number , the “end” keywords are extraneous, and are not required (or
valid) in Python, and thus were deleted by participants. Likewise, in Most Common Word ,
2Due to a bug in our code, our interface showed 2 tokens that should have been highlighted but were not
and 3 tokens that were highlighted but should not have been, highlighting a total of 204 characters across the
three tasks. These errors were spread among the three coding tasks. Four of those ﬁve tokens had generation
probability between 0.692 and 0.701, or within 0.007 of the 0.694 threshold; the ﬁfth had probability 0.624. We
do not believe this discrepancy meaningfully impacted the results, since this would imply extreme sensitivity to
minor shifts in the threshold used.
7
(a) Ugly Number
(b) Base 7
(c) Most Common Word
Figure 2: Highlights in the Generation Probability condition (left) and the Edit Model condition
(right) for the three coding tasks. The No Highlights condition displays the code completion
alone.
the “List” identiﬁer is not deﬁned, and most participants replaced it with the built-in “list”
type (Python is case-sensitive).
Conversely, when using generation probabilities, highlights seem to be triggered by that
start and end of important sequences (e.g., deﬁning a new class, initiating a new test or loop,
or returning a value). Likewise, comparator and Boolean operators are often highlighted (e.g.,
“<=,” “not,” “and”), perhaps reﬂecting some momentary uncertainty about the construction and
directionality of these expressions. Finally, in several cases, newly introduced variable names
are also highlighted (e.g., in the Ugly Number andMost Common Word coding tasks).
This pattern of highlights reﬂects our characterization of generation probabilities discussed in
Section 3.1: At various moments in the generation process, decision points are reached where
the model has several possible paths it can pursue. Since tokens are generated one at a time,
all the uncertainty of choosing a path is imbued upon the token generated in that moment
(yielding a low token generation probability). Once a path is decided, and the corresponding
8
token is output, the generation probabilities of subsequent tokens tend to recover.
Since the generation probabilities and the edit model have diﬀerent distributions and
characteristics, we posit that they will similarly have diﬀerent eﬀects on programmers.
3.5 Pre-registered Hypotheses
We posited and pre-registered nine hypotheses,3which we state informally here; the bolded
variables in each hypothesis are deﬁned more formally in Section 4.3.
First, we wanted to explore how much beneﬁt uncertainty highlighting provides to partic-
ipants in terms of their task performance and eﬃciency. Ideally, the highlights would point
participants to errors in the code and therefore speed up the process of accurately completing
the task. However, highlights may also distract participants, potentially increasing time spent
and mistakes made.
•[H1]Highlight condition will aﬀect the timeit takes to complete the coding task.
•[H2]Highlight condition will aﬀect accuracy on unit tests.
Secondly, because one goal of working with an AI-powered code completion tool is to reduce
the amount of work that programmers need to do, we wanted to explore whether highlighting
would aﬀect how much participants have to edit or add to the code. We also examine whether
the particular tokens edited vary with diﬀerent forms of highlighting to see whether highlighted
tokens are more likely to be edited.
•[H3]Highlight condition will aﬀect the number of characters added to the code.
•[H4]Highlight condition will aﬀect the overall survival rate of tokens in the code.
•[H5]The interaction between highlight condition and whether a given token is highlighted
will aﬀect the token-level survival rate .
Finally, we wanted to measure participants’ subjective preferences for the tools. Since
one of our goals is to reduce the eﬀort required to get to working code, we hypothesized that
highlights would aﬀect cognitive load. Additionally, we hypothesized that participants would
feel more favorably towards the completions, highlights, and the whole code completion tool in
some conditions.
•[H6]Highlight condition will aﬀect self-reported cognitive load .
•[H7]Highlight condition will aﬀect self-reported completion utility .
•[H8]Highlight condition will aﬀect self-reported highlight utility .
•[H9]Highlight condition will aﬀect self-reported rankings of the code completion tools.
4 Methods
We conducted a mixed-methods study with 30 participants consisting of coding tasks and a
post-task interview. All participants were employees of a large technology company located in
the United States with experience of coding in Python. None had previously been exposed to
our study. Participants were recruited through a mix of direct emails, posts on message boards,
3Pre-registration link: https://osf.io/tymah
9
Figure 3: Screenshot of the interface used in our study. The “Task Overview” section provides
general instructions for the task and interface. The “Coding Task” section provides a description
of the current coding task. The “Expected Format” section describes the code format (e.g.,
function names and deﬁnitions) expected by unit tests. The “Suggestions” section provides
some suggestions for debugging in our interface. Participants have the option to run their
code as a ﬁle (“Run your code”) or run our unit tests on their code (“Run unit tests”). When
satisﬁed with their code, they can select “Submit.”
and word of mouth. The study was IRB approved with voluntary participation and paid $50.
All interviews were conducted over a video-conferencing platform and lasted approximately
one hour.
Of the 30 participants, 15 reported having more than 5 years of experience writing Python
code, 11 participants reported having 1–5 years of experience, and 4 reported less than one
year of experience. Likewise, 15 reported that they worked with Python code at least once a
week, while 12 reported only using Python “when needed.” Comparatively, participants were
far less experienced with AI code completion tools: 16 participants never used such a system
before. A detailed listing of participant experience is provided in Appendix A.
Finally, among our participants, 5 identiﬁed as women, 22 identiﬁed as men, and 3 declined
to report. 16 were age 24–29, 8 were aged 30–39, 3 were aged 40–49, and 3 declined to report.
Additionally, 2 identiﬁed as Black, 2 identiﬁed as Hispanic/Latino, 5 identiﬁed as White, 13
identiﬁed as Asian, and 8 declined to report.
We used a within-subjects design to compare diﬀerent highlighting options. Each participant
was asked to complete three coding tasks with three “diﬀerent” AI-powered code completion
tools (in actuality, the code generation was the same, only diﬀering in the highlights shown).
The three highlight conditions we used are as follows:
•No highlights: Only the generated code completion was displayed.
•Generation probability: The code completion was displayed with highlights on tokens
with generation probability ≤69.4%.
10
•Editmodel: Thecodecompletionwasdisplayedwithhighlightsontokenswithlikelihood
≥66.7%of being edited or removed according to the edit model.
The order of tasks as well as assignments of highlight conditions to tasks were randomized.
4.1 Coding Tasks and OpenAI’s Completions
We aimed to select coding tasks that would satisfy the following criteria: (1) they could be
completed in about ten minutes by a non-beginner Python programmer, and (2) they had
reasonable but imperfect code completions given by the Codex model, ideally with a diversity
of error types and frequencies among the tasks. We initially selected ﬁfteen potential coding
tasks from Leetcode [ 31] using the “easy” setting. After piloting with three participants, we
narrowed these down to a set of ﬁve tasks and gave the completions of these ﬁve tasks to nine
additional participants in the edit model training phase, as described in Section 3.2. We ﬁnally
selected three tasks from these ﬁve that best satisﬁed our selection criteria. These were Ugly
Number ,Base 7, and Most Common Word .
Togeneratethecodecompletion, werantheinstructionsprovidedbyLeetcodeastheprompt
to Codex (retrieved in June 2022) with the following (mostly default) parameters: model:
Code Davinci 002, temperature: 0.5,maximum tokens: 4000, top p:1,frequency penalty:
0,presence penalty: 0, and logprobs: 5. We collected the log probabilities of each token
and converted these to generation probabilities by exponentiating. We trimmed the output
of Codex to exclude any strings before the ﬁrst Python keyword or comment that are not in
Python (e.g., starting the output with “.”), as productized versions of code completion tools,
such as GitHub’s Copilot, are likely to do this automatically. Below, we outline the three
coding tasks and their completions (shown in Figure 2).
Ugly Number4This coding task is described as follows: “ An ugly number is a positive
integer whose prime factors are limited to 2, 3, and 5. Given an integer n, return trueifn
is an ugly number. ” The output returned by Codex and provided to participants has many
errors, including some syntax that is not in Python (e.g., “end” as a keyword) and the incorrect
function name (i.e., “ is_ugly” instead of “ isUgly”). Despite these errors, it has the correct
logic—for example, the function correctly uses the idea of dividing by 2 until it no longer is
evenly divisible by 2. In other words, the completion provides useful conceptual help, but a
participant would need to correct the many syntax errors to arrive at correct code.
Base 75This task is described as follows: “ Given an integer num, return a string of its base
7 representation. ” The output returned by Codex and provided to participants has no syntax
errors and only minimal conceptual errors; speciﬁcally, it appends an extra “0” to every output.
Therefore, to arrive at correct code, a participant would need only to remove the extra “0.”
Most Common Word6This coding task is described in the following way: “ Given a
string paragraph and a string array of the banned words banned, return the most frequent
word that is not banned. It is guaranteed there is at least one word that is not banned, and
that the answer is unique. The words in paragraph are case-insensitive and the answer should
be returned in lowercase. ” The output returned by Codex and provided to participants has
minimal syntax errors (e.g., using “List” which has not been imported—this could be ﬁxed by
instead using “list” since the non-native Python version is unnecessary). It also has minimal
4https://leetcode.com/problems/ugly-number/
5https://leetcode.com/problems/base-7/
6https://leetcode.com/problems/most-common-word/
11"
2402.02299,D:\Database\arxiv\papers\2402.02299.pdf,"While the paper discusses the use of deep learning in side-channel analysis, it focuses on profiling attacks.  What are the potential advantages and disadvantages of using deep learning for other types of side-channel attacks, such as differential power analysis (DPA)?","Deep learning could potentially be advantageous for DPA by enabling the analysis of complex, non-linear relationships in power traces. However, it might be challenging to train deep learning models for DPA due to the need for large datasets and the difficulty in defining appropriate labels for the attack.","4 Panoff et al.
[24]. The more high voltage level bits (or changes in the the number of high level bits for HD) in a design, the more
power is consumed. Next, one version of the model is compared against the measurement for a single time point across
all traces. The Pearson Correlation between these two vectors is then calculated and recorded. This is then repeated for
each time point and potential key pair. Finally, the highest correlation is found and the potential key that was used to
create it is taken as the key guess [6].
2.1.3 Template and Profiling Attacks. Template attacks represent a fundamentally different approach to SCA from
Differential and Correlational Analysis [ 9]. In a profiling attack, a type of Template attack, an adversary has full control
over a profiling device which is identical to the victim device. The attacker then builds a model representing intermediate
values using the profiling device, and uses that model to recover confidential information from the victim device. The
structure of this model depends on a number of factors. The adversary analyzes traces collected from the profiling
device to identify components of the traces that distinguish the intermediate values. Traces from the victim device are
then examined for these components and intermediate values predicted. For SCA on software implementations of AES,
it is often a prediction of the state of a certain byte following the first substitution box operation. These predictions are
then put through an inverse SBox and logical exclusive-or (XOR) operations with the plaintext to recover a key. Note
that different algorithms may have their templates and models built and attacked in different manners. Templates can
be built using either conventional [7] or DL approaches [54].
2.2 Side Channel Mitigation
Due to how common side channels are and the threat they present, a number of defence techniques have been developed
to protect against them. These can be divided into two main categories, Hiding and Masking.
2.2.1 Hiding. Hiding defences directly mitigate side channel leakage. This can be done through attenuating the signal
and adding additional background noise [ 14,15] or by randomizing time operations occur [ 5,48]. While this cannot
entirely prevent leakage, these defences often increase the number of measurements required to perform a successful
attack, thus restricting attackers. These approaches often present a trade-off between design complexity and side
channel effectiveness. An example of this can be found in “STELLAR"" [ 15] where a method to almost completely
mitigate electromagnetic (EM) side channel leakage is presented. However, the defence requires extremely accurate
modeling of the EM behavior of a device, which is computationally complex. Circuitry to cancel out the EM emanations
must then be designed, and placed above the protected design. On top of the complexity of this task, the protected design
is restricted from using the top layers of the full design. These requirements greatly impede the design complexity and
delay the time-to-market (TTM), although they do present significant security gains [15].
Another type of hiding technique is Dual-Rail Logic [ 10]. In this, every gate has a partner gate with an inverse state
(i.e. if the original gate is charged, the partner gate is discharged). This obviously incurred a heavy area and power
overhead, as the number of gates in a design is doubled and power draw is kept at a steady elevated level. Dual-rail
logic is highly effective at keeping power consumption steady and thus prevents accurate analysis of that side channel.
However, other side channels, such as EM, are less effected [23].
It is also possible to mitigate side channel leakage through a process known as timing disarrangement [ 3]. In this
approach, designers purposefully add small amounts of clock jitter to different components. This jitter is not enough to
affect the logical performance of the device, but by having the various components switch logic levels at different times,
the side channel signal as a whole becomes distorted. This defence can easily be implemented through CAD and EDA
tools automatically, though they have some performance overheads
Manuscript submitted to ACM
A Review and Comparison of AI Enhanced Side Channel Analysis 5
Fig. 1. An example neuron in an artificial neural network.
2.2.2 Masking. The other approach to side channel mitigation is to prevent a device from ever handling confiden-
tial information directly. This approach, known as masking, concedes that an attacker will be able to get accurate
measurements from a device, and so instead prevents those measurements from being useful. Byte masking has each
operation split into subsections, combined with some other information (often called a mask), operated on, unmasked
and recombined to restore the original data. The idea is to prevent adversaries from truly understanding what each
operation involves, which significantly complicates side channel attacks. As the operations no longer match the adver-
saries models, they can no longer be easily recovered. However, it is possible to defeat masking with more complicated
analysis [ 34] and types of Machine Learning [ 19] to recover the mask used, after which the attacks can proceed as
normal. Many types of machine learning do not use predicted models, and as such are mostly unaffected by masking. In
fact, the most popular publicly available SCA dataset, ASCAD, includes traces from a byte-masked AES implementation
on microprocessors [ 36]. More machine learning based side channel analysis methods will be discussed later as the
main focus of this survey paper.
2.3 Deep Learning
Recent advances in machine learning, especially in deep learning, have made these techniques popular in other
applications. Deep neural networks (DNN) are leading examples among all deep learning techniques. DNNs consist of
many neurons that “activate” under certain stimuli. In practice, this is often achieved by passing the weighted sum of
inputs to an activation function that scales the result, as shown in Figure 1. Deep Learning is so named as it includes
artificial neural network using at least one “hidden layer” between input and outputs as seen in Figure 2. Supervised
learning is a type of deep learning where true outputs are known for all training inputs. Specifically, The network
takes in the input, calculates an output, and the output is then compared to the true output or label . Aloss function
quantifies the difference and the network adjusts its internal parameters based on the calculated loss, often through
backpropagation . This process is then repeated until the loss has been minimized and/or meet the requirements.
Manuscript submitted to ACM
6 Panoff et al.
Fig. 2. An example of a neural network with a hidden layer i.e. a deep learning network.
2.3.1 Regression vs Classification Tasks. Machine learning can either perform regression or classification tasks. In a
regression task, the machine learning algorithm can take in outputs and extrapolate a numerical response to those
inputs. An example of this may be an algorithm that takes in a car’s mileage, age, model, and manufacturer and predicts
the price it may sell for. Classification tasks on the other hand attempt to place a given input into one of several
predetermined classes. Determining if a picture contains a cat or a dog would be a classification task.
SCA with DL often falls into this category. There are a certain set range of possible byte values, which must be
determined independently. For these values, known as “labels”, the numeric distance between predicted and true labels
is not indicative of correctness. This is to say if the true value for a certain key byte is 128, predictions of 129 and 34 are
equally wrong. Thus, SCA for DL is often treated as a classification task.
2.3.2 Multi-layer Perceptrons. Multi-layer Perceptrons are the simplest form of DL. In an MLP architecture, many
neurons or perceptrons are layered and connected. An example of an MLP can be found in Figure 1. Perceptrons that
share inputs are called a layer . There are at least two layers in a Deep Learning MLP, so that one layer is hidden , or
not exposed through the inputs or outputs of the model. It is possible for an MLP to approximately recreate any given
function with enough perceptrons per layer [ 12]. However, instead of making wider MLPs (i.e. with more neurons
per layer) to model complex functions, a more common approach is to make deeper MLPs (i.e. with more layers). It is
important to note that in an MLP, layers can have different activation functions.
For classification tasks, the final layer often has a softmax function as shown in Equation 1. ®𝑍denotes all the inputs
to the neuron, which has 𝐾outputs.𝑍𝑖is the value of the 𝑖th element of®𝑍. The denominator scales the final outputs
such that sum of them becomes one and thus each output represents a probability of that class.
𝜎(®𝑍)𝑖=𝑒𝑍𝑖
Í𝐾
𝑗=1𝑒𝑍𝑗(1)
Rectified Linear Units (ReLUs) are a popular choice for layers other than the final layer. They prevent negative
activations but allow positive ones, and have been found to be effective in a number of applications. Equation 2
demonstrates a ReLU. Y is the output and X is the input.
𝑌=𝑚𝑎𝑥(0,𝑋) (2)
Manuscript submitted to ACM
A Review and Comparison of AI Enhanced Side Channel Analysis 7
Fig. 3. A simple example of kernel convolution.
2.3.3 Convolutional Neural Networks. and Convolutional Neural Networks (CNNs) are often a combination of con-
volutional layers followed by an MLP when applied to classifications tasks such as in SCA. While others exist, such
as Auto-Encoders, they tend to be auxiliary techniques. Inputs to a CNN are first analyzed by convolutional layers,
which attempt to identify patterns in the input data. The layer performs a convolution of a certain matrix, or kernel ,
and the input to the layer. The kernel has a predetermined shape, and it moves across the input with a certain number
of elements per step, or stride . An element wise multiplication of the kernel and the overlapping portion of the input is
performed, the results summed, and optionally fed through an activation function such as a ReLU. An example of this
operation can be found in Figure 3. Convolutional layers can have multiple kernels, and are often used in combination
to find different sized patterns, or patterns of patterns, which is a popular approach to computer vision [27]. This can
be done by passing the outputs of one layer as the inputs to another, though without additionally precautions, this
can quickly deplete the amount of information recovered in each layer [ 44]. The results of these convolutions are
then passed into an MLP which results in the final output. Each kernel learns the weights/values of the elements that
minimize the loss function, similar to a MLP.
2.3.4 Auxiliary Layers. In addition to perceptron (also known as fully connected) and convolutional layers, there are a
few other types of layers that are important to deep learning. These are dropout, pooling, and batch normalization
layers. Each fulfills an important role in a DL model. We will start by discussing dropout layers. These layers help
prevent over-reliance on certain inputs or features, by randomly setting different features to zero during training. This
obviously impedes the speed at which a model can learn, but it also tends to improve how well a model generalizes. If
the model only needs a portion of the available information to reach the correct conclusion, then it tends to perform
better when seeing new data that differs slightly from the training data. Pooling layers are often an integral component
of CNNs. These layers are similar to convolutional layers in that they operate by passing a kernel along their input, but
perform a pooling operation instead of a convolution. Pooling operations distill several data points into a single one,
Manuscript submitted to ACM
8 Panoff et al.
often by finding the maximum value or taking the average of the set. Finally, batch normalization or “batch norm” layers
are essential parts of how modern DL models are structured, or their architecture . Batch norm layers simply re-scale
and re-center their inputs with each batch or set of inputs, to minimize artifacts that may affect model performance. All
three are often used in state-of-the-art deep learning models [27, 44].
2.3.5 DL Architectures and Hyper-parameters. DL models are more than just the weights of their parameters. The
number and contents of each layer, how quickly weights update, and the way weight to change are identified are
additional settings and known as hyper-parameters . The number and contents of layers are known collectively as an
architecture . As mentioned in Section 2, DL enhanced SCA often uses one of two architectures, an MLP or a CNN. MLPs
tend to require fewer training traces and are easier to train, but are far more sensitive to synchronization issues than
CNNs. There also more published guidance on creating CNN architectures for SCA [56].
Ideal hyper-parameters can also be found through automated optimization or meta-learning. Automated optimization
searches for ideal hyper-parameters through statistical analysis. “Sherpa” is one such example of this [ 20]. In Sherpa,
ideal aspects of the model, often hyperparmeters are found by building a statistical model of the system’s performance.
This model can take on many forms, from relatively simple Bayesian analysis to Markov chains [ 2]. Most model-focused
meta-learning follows a similar approach, though the statistical models are replaced with machine learned ones. This
is the source of the name meta-learning as a model learns how to best learn. Meta-learning can identify optimal
architectures [ 30] in additional to starting parameters [ 17]. Hyper-parameter tuning methods for DL SCA have been
proposed earlier including standard techniques such as Bayesian Optimization [ 29] [51]. However, novel approaches
exist as well, as seen in [ 38] where the authors use a specific type of Deep Learning, known as Reinforcement Learning,
to identify ideal model hyper-parameters. In Reinforcement Learning, an agent acts, evaluates the result of its actions,
and quantifies a reward. This differs from Supervised Learning in that the reward value may not be known prior to the
generation of the output (i.e. there is no ground truth label) but is only calculated after the agent completes its action.
The agent then attempts to identify actions that maximize the reward.
3 CONTEMPORARY DEEP LEARNING ENHANCED SCA
As mentioned in Section 2, profiling SCA naturally integrates well with supervised Deep Learning [ 29]. Each measure-
ment, or trace, often belongs to a distinct operation, which can be used as a label. There is rarely any interdependency
between measurements (i.e. each encryption is a stand-alone operation). Finally, the traces as captured are often time
series, which DL historically works well on [ 26]. This is not to say that this is the only way to perform DL SCA, in fact
we cover several other approaches in Section 5, but the majority of existing work on this topic follows this approach.
3.1 Threat Model
In a profiled DL SCA attack, an adversary needs a target (i.e. confidential information to recover) and knowledge of
a victim device (i.e. implementation or model information). We will take an adversary targeting the secret key used
in AES as an example although DL SCA is not limited to AES [ 32]. In this case, an adversary knows that their victim
device and knows the specific AES implementations, e.g., TinyAES running on an an ATMEGA microprocessor. Using
this information, the adversary would then acquire an identical device, program it with identical code, and then collect
traces from it under known inputs. The Substitution Box or SBox , of the first round of AES is a common target. As the
SBox operation is well defined and common to all AES implementations the adversary can easily reconstruct it. The
inputs to the SBox of the first round is the result of XORing a byte of the secret key with the equivalent byte of plaintext
Manuscript submitted to ACM
A Review and Comparison of AI Enhanced Side Channel Analysis 9
Fig. 4. The two stages of a Profiling Attack. In Step 1, the adversary trains a neural network (NN) using a device with know internal
states or labels. In step two, the attack uses the NN to recover the internal states or labels of an uncontrolled device.
(although it is possible to improve results by combining both plaintext and ciphertext [ 21]). Thus, if the adversary has
access to the plaintext and knowledge of the SBox output, they can recover the secret key. As such, the SBox output is
often used as the label that a model attempts to learn during a profiled DL SCA attack. Said model would learn how to
identify what SBox output a given trace contains. The adversary then needs to collect traces and plaintexts from the
victim device, and uses the DL model to analyze those traces, thereby recovering the key. The general process can be
found in Figure 4
In more detail, two data sets must be collected from the device, one to use for profiling (the training dataset) and
another to evaluate performance (the attack dataset). The attack dataset must have traces collected under the same
key, but the profiling set can use random key-text combinations. The profiling data set is used to directly train the
deep learning model as a training set, though some portion of it may be reserved as a validation set. This validation set
is used to evaluate model performance while tuning parameters. The distinction between validation and test data is
important to ensure unbiased analysis of the model’s performance [ 52]. Once optimal parameters are found, the model
is no longer updates it’s internal parameters, and then is supplied the attack or testset for final analysis. A model’s
performance is evaluated thorough Guessing Entropy (GE). GE can be calculated by following Algorithm 1 [43].
3.2 Intra-Device Profiled DL SCA
In intra-device profiled DL SCA, the profiling and victim devices are the same physical device [ 13]. While there are
legitimate threat models based around this methodology, intra-device attacks tend to be more useful as proofs-of-concept
of a novel analysis algorithm rather than an implementable attack. This is not to say that such algorithms or the works
using this are without merit. In fact, in many cases, only a few additional steps are required to adjust methodologies
using intra-device data to work cross-device, which is the next topic we will discuss.
Additionally, many of the publicly available databases for SCA use this approach. For example ASCAD, a popular
public SCA database for AES implementations on microprocessors, consists of traces taken from a single microprocessor
Manuscript submitted to ACM
10 Panoff et al.
Algorithm 1 Calculation of Guessing Entropy (GE) for a byte in the first round of AES. The set of traces to evaluate is
T. The set of plaintexts used to generate those traces is P. These must be supplied in the same order as T. The possible
key values areK. The number of evaluations to complete is 𝑁𝐸while the number of traces per evaluation is 𝑁𝑇.𝐷𝐿is
a function representing the deep learning model. The true key, 𝐾, must also be supplied.
Require:T,P,K,𝐷𝐿,𝐾,𝑁𝐸, and𝑁𝑇
Ensure: Ranking of the correct key guess 𝑅𝐾
// First identify what key each SBox output relates to for a given plaintext
1:Mis a|P|𝑥|K|matrix
2:for𝑝in range(|P|)do
3:for𝑘in range(|K|)do
4:M[𝑝][𝑘]=𝑆𝐵𝑜𝑥(P[𝑝]𝑥𝑜𝑟K[𝑘])
5:end for
6:end for
// UseMto find likelihood of 𝐾
7:𝑅𝐸= matrix of size 𝑁𝐸𝑥 𝑁𝑇
8:for𝑒in range(𝑁𝐸)do
9:for𝑡in range(𝑁𝑇)do
10:𝑖= random selection from range( |T|)
11: for𝑘in range(|K|)do
12:K[𝑘]+=𝐷𝐿(T[𝑖])[𝑘]
13: end for
14:𝑅𝐸[𝑒][𝑡]= index of𝐾in reverse(sort(K))
15: end for
16:end for
17:for𝑡in range(𝑁𝑇)do
18:𝑅𝐾[𝑡]= mean(𝑅𝐸[𝑎𝑙𝑙][𝑡])
19:end for
running a masked implementation of AES under varying conditions. This makes it an intra-device dataset. ASCAD
has several subsidiary data sets, which allows it to account for a wide range of attack scenarios. Adversaries can
build models for situations where a single fixed key is used during collection, or when both the key and plaintext
change randomly. Additionally, while all traces in ASCAD are synchronized, the authors provide support for artificially
de-synchronizing them, to simulate clock-jitter or poor testing conditions [ 36]. These features have led to ASCAD
being a popular publicly available database for SCA on microprocessors.
A few examples of intra-device profiling being useful include Ranking Loss [ 55] and Multi-Leak Deep-Learning
Side-Channel Analysis [ 22]. These are two of the latest techiniques for DL SCA and they focus only on Intra-Device
data. In Ranking Loss, the authors explore how changing the standard cross-entropy loss for one focusing on the rank
of the true key in a Guessing Entropy attack (the true objective of a profiling attack) improve the model’s performance.
Multi-Leak takes a different approach, in that they look to combine the results of analyzing multiple leakage points
simultaneously to recover the target key faster than other approaches which look at only a single source.
3.3 Cross-Device DL SCA
While the above approach works well when analyzing the performance of a new type of analysis, methods using it
often fail when the attacking and profiling devices are different instances [ 4,13]. This is likely due to slight differences
in the side channel characteristics between the devices. This differences are best explained by the effects of process
Manuscript submitted to ACM
A Review and Comparison of AI Enhanced Side Channel Analysis 11
variation. As the distance between components affects resistance, capacitance, and other electrical characteristics,
different physical instances of the same design may have slightly different side channel behaviors. There are two similar
but distinct solutions to resolve the issue this causes for SCA. Firstly, data can be collected from multiple profiling
devices, and a DL model trained using the combination of all the data [ 4,13]. This has the benefit that models trained in
this way are more likely to generalize to a new victim device, because the model already works for multiple distinct
devices. However, there is no guarantee that it will work for the victim device, and a much large amount of training
data is required.
“X-DeepSCA"" by Das et al. is one such approach [ 13]. In this work, the authors use multiple identical devices to
create the profiling and attack datasets. They show that a DL model training on a single device fails to accurately
predict a second device, but a similar model trained on traces from multiple devices (4 devices, 10k traces from each)
can accurately (99.9%) predict on an unseen device. In “Mind the Portability” Bhasin et al. take a similar approach, but
also test different secret keys in addition to different devices [4].
The other approach is Transfer Learning (TL). In TL, a model is trained on a single source task normally. Then the
model is fine-tuned on the target task. Fine-tuning needs far fewer traces and training than training a model from
scratch [ 45]. In Cross Device TL for SCA, the adversary builds a model using a single profiling device. It is further
assumed that the adversary has the access to the victim device and can obtain a limited number of traces (typically
orders of magnitude fewer than needed to build a model) from it for known operations. The adversary then fine-tunes
their model using these traces from the victim device. The resulting model can then quickly and accurately analyze new
traces from the victim device [45].
Thapar et al. demonstrate TL for SCA in “TranSCA” [ 45]. They evaluate how many traces are needed to build a model
from scratch as opposed to Transfer learning. They consistently find that far fewer traces are required for transfer
learning than building a model [ 45]. In fact, in one situation where a model needed 100,000 traces to perform well,
less than one third of that was needed to transfer from another model. Obviously this requires a much more capable
adversary than in in “X-DeepSCA,” but it also is significantly more efficient.
4 COMPARISONS AMONG DEEP LEARNING SCA
As many works use or at least evaluate their performance on the same database, ASCAD, we believe that is is fair to
compare the performance reported between these methods. Through this comparison, we will evaluate how different
approaches affect the performance of DL SCA. We will begin by comparing the Measurements to Disclose as reported in
each work we have cited that obtained and reported conclusive results against synchronized traces from ASCAD dataset.
These are the number of samples needed to recover the key, and as such lower numbers indicate a more successful or
powerful attack method.
The results of this analysis can be found in Figure 5. It should be readily apparent that some methods are more
successful than others, with the single layer perceptron from [ 58] significantly under performing all other compared
methods. Additionally we can see that the CNN monobit model from [ 58] has the best attack performance, needling
only∼70 attack traces to recover a key. We would also like to point out the PCA_QDA method from [ 36], which is a
statistical model (i.e. not a deep learning model) included as a baseline comparison.
Through the summary of how a created model functions, we believe that it would be useful to evaluate not just the
performance of an attack, but also the difficulty of training a model. As such, we will perform an additional analysis
using not only the Measurements to disclose (MTD) the correct as found through Guessing Entropy (GE, Algorithm 19),
but a custom metric we call Key Recovery Difficulty (KRD). Equation 3 showcases KRD, which will more fully represent
Manuscript submitted to ACM"
2106.08283,D:\Database\arxiv\papers\2106.08283.pdf,"In the context of federated learning, what are the key differences between the proposed method and differentially private federated learning (DPFL) in terms of their mechanisms, certification goals, and technical contributions?","While both methods aim to enhance the security of federated learning, they differ in their mechanisms, certification goals, and technical contributions. CRFL focuses on certifiably robust predictions against data poisoning attacks, while DPFL prioritizes client-level privacy guarantees for the learned model parameters.","CRFL: Certiﬁably Robust Federated Learning against Backdoor Attacks
ability distribution µ, i.e., the smoothing measure, to obtain
the “votes” Hc
s(w;xtest)for each class c∈Y. Then the
label returned by the smoothed classiﬁer hsis the mostly
probable label among all classes (the majority vote winner).
Formally,
hs(w;xtest) = arg max
c∈YHc
s(w;xtest),
whereHc
s(w;xtest) =PW∼µ(w)[h(W;xtest) =c].(1)
To be aligned with the training time Gaussian noise (perturb-
ing), we also adopt Gaussian smoothing measures µ(w) =
N(w,σT2I)during testing time. In practice, the exact
value of the probability pc=PW∼µ(w)[h(W;xtest) =c]
for labelcis difﬁcult to obtain for neural networks, and
hence we resort to Monte Carlo estimation (Cohen et al.,
2019; Lecuyer et al., 2019) to get its approximation ˆpc. At
roundt=T, given the clipped aggregated global model
ClipρT(wT), we add Gaussian noise ϵk
T∼N (0,σ2
TI)for
Mtimes to get Msets of noisy model parameters ( M
Monte Carlo samples for estimation), such that ˜wk
T←
ClipρT(wT) +ϵk
T, k= 1,2,...,M.
In Algorithm 2, The function GetCounts runs the classiﬁer
with each set of noisy model parameters wk
Tfor one test
samplextest, and returns a vector of class counts. Then
we take the most probable class ˆcAand the runner-up
class ˆcBto calculate the corresponding ˆpAandˆpB. The
function CalculateBound calibrates the empirical estima-
tion to bound the probability αofhsreturning an incor-
rect label. Given the error tolerance α, we use Hoeffd-
ing’s inequality (Hoeffding, 1994) to compute a lower
boundpAon the probability HcAs(w;xtest)and a upper
boundpBon the probability HcBs(w;xtest)according to
pA= ˆpA−√
log(1/α)
2N,pB= ˆpB+√
log(1/α)
2N. We leave
the function CalculateRadius to be deﬁned with our main
results in later sections and we will analyze the robustness
properties of the model trained and tested under our frame-
work CRFL.
Comparison with Certiﬁably Robust Models in Central-
ized Setting Our method is different from previous certi-
ﬁably robust models in centralized learning against evasion
attacks (Cohen et al., 2019) and backdoors (Weber et al.,
2020). Once the Mnoisy models (at round T, withσT)
are generated, they are ﬁxed and used for every test sam-
ple during test time, just like RAB (Weber et al., 2020) in
the centralized setting. However, RAB actually trains M
models using Mnoise-corrupted datasets, while we just
train one model through FL and ﬁnally generated Mnoise-
corrupted copies of it. For every test sample, randomized
smoothing (Cohen et al., 2019) generates Mnoisy sam-
ples. Suppose the test set size is m. Then during testing,
there arem·Mtimes noise addition on test samples for
randomized smoothing, and Mtimes noises addition onAlgorithm 2 Certiﬁcation of parameters smoothing
Input: a test sample xtestwith true label ytest, the global
model parameters ClipρT(wT), the classiﬁer h(·,·)
fork= 0,1,...M do
ϵk
T←a sample drawn from N(0,σ2
TI)
˜wk
T= ClipρT(wT) +ϵk
T
end for
Calculate empirical estimation of pA,pBforxtest
counts←GetCounts( xtest,{˜w1
T,...,˜wM
T})
ˆcA,ˆcB←top two indices in counts
ˆpA,ˆpB←counts [ˆcA]/M,counts [ˆcB]/M
Calculate lower and upper bounds of pA,pB
pA,pB←CalculateBound (ˆpA,ˆpB,N,α )
ifpA>pBthen
RAD = CalculateRadius( pA,pB)
Output: Prediction ˆcAand certiﬁed radius RAD
else
Output: ABSTAIN and 0
end if
trained model for CRFL. To our best knowledge, this is the
ﬁrst work to study parameter smoothing rather than input
smoothing, which is an open problem motivated by the FL
scenario, since the sever directly aggregates over the model
parameters.
5. Certiﬁed Robustness of CRFL
5.1. Pointwise Certiﬁed Robustness
Goal of Certiﬁcation In the context of data poisoning in
federated learning, the goal is to protect the global model
against adversarial data modiﬁcation made to the local train-
ing sets of distributed clients. Thus, the goal of certiﬁable
robustness in federated learning is for each test point, to
return a prediction as well as a certiﬁcate that the predic-
tion would not change had some features in (part of) local
training data of certain clients been modiﬁed.
Following our threat model in Section 3.2 and our training
protocol in Algorithm 1, we deﬁne the trained global model
M(D′) := ClipρT(w′T).For the FL training process that
is exposed to model replacement attack, when the distance
betweenD′(backdoored dataset) and D(clean dataset) is
under certain threshold (i.e., the magnitude of {{δi}qi
j=1}R
i=1
is bounded), we can certify that M(D′)is “close” toM(D)
and thus is robust to backdoors. The rationale lies in the
fact that we perform clipping and noise perturbation on
the model parameters to control the global model deviation
during training. During testing, intuitively, under the Gaus-
sian smoothing measures µas described in Algorithm 2, for
two close distribution µ(M(D′))andµ(M(D)), we would
expect that even though the probabilities for each class c,
i.e.,Hc
s(M(D′);xtest)andHc
s(M(D);xtest), may not be
equal, the returned most likely label hs(M(D′);xtest)and
hs(M(D);xtest)should be consistent.
CRFL: Certiﬁably Robust Federated Learning against Backdoor Attacks
In summary, we aim to develop a robustness certiﬁcate
by studying under what condition for {{δi}qi
j=1}R
i=1that
the prediction for a test sample is consistent between the
smoothed FL models trained from DandD′separately,
i.e.,hs(M(D′);xtest) =hs(M(D);xtest). To put forth
our certiﬁed robustness analysis, we make the following
assumptions on the loss function of all clients. Then we
present our main theorem and explain its derivation through
model closeness andparameter smoothing . Throughout this
paper, we denote∇wℓ(w;z)as∇ℓ(w;z)for simplicity.
Assumption 1 (Convexity and Smoothness) .The loss func-
tionℓ(w;z)isβ-smoothness, i.e,∀w1,w2,
∥∇ℓ(w1;z)−∇ℓ(w2;z)∥≤β∥w1−w2∥.
In addition, the loss function ℓ(w;z)is convex. Then co-
coercivity of the gradient states:
∥∇ℓ(w1;z)−∇ℓ(w2;z)∥2
≤β⟨w1−w2,∇ℓ(w1;z)−∇ℓ(w2;z)⟩.
Assumption 2 (Lipschitz Gradient w.r.t. Data) .The gradi-
ent∇wl(z;w)isLZLipschitz with respect to the argument
zand norm distance ∥·∥, i.e,∀z1,z2,
∥∇ℓ(w;z1)−∇ℓ(w;z2)∥≤LZ∥z1−z2∥.
Assumption 3. The whole FL system follows Algorithm 1
to train and Algorithm 2 to test.
The assumptions on convexity and smoothness are common
in the analysis of distributed SGD (Li et al., 2020; Wang
& Joshi, 2019). We also make assumption on the Lipschitz
gradient w.r.t. data, which is used in (Fallah et al., 2020;
Reisizadeh et al., 2020) for analyzing the heterogeneous
data distribution across clients.
Main Results
Theorem 1 (General Robustness Condition) .Lethsbe de-
ﬁned as in Eq. 1. When ηi≤1
βand Assumptions 1, 2, and 3
hold, suppose cA∈Y andpA,pB∈[0,1]satisfy
HcAs(M(D′);xtest)≥pA≥pB≥max
c̸=cAHc
s(M(D′);xtest),
then if
RR∑
i=1(piγiτiηiqBi
nBi∥δi∥)2≤−log(
1−(√pA−√pB)2)
σ2
tadv
2L2
ZT∏
t=tadv+1(
2Φ(
ρt
σt)
−1),
it is guaranteed that
hs(M(D′);xtest) =hs(M(D);xtest) =cA,
where Φis standard Gaussian’s cumulative density function
(CDF) and the other parameters are deﬁned in Section 3.In practice, since the server does not know the global model
in the current FL system is poisoned or not, we assume
the model is already backdoored and derive the condition
when its prediction will be certiﬁably consistent with the
prediction of the clean model. Our certiﬁcation is on three
levels: feature ,sample , and client . If the magnitude of the
backdoor is upper bounded for every attackers, then we can
re-write the Theorem 1 as the following corollary.
Corollary 1 (Robustness Condition in Feature Level) .Us-
ing the same setting as in Theorem 1 but further assume
identical backdoor magnitude ∥δ∥=∥δi∥fori= 1,...,R .
SupposecA∈Y andpA,pB∈[0,1]satisfy
HcAs(M(D′);xtest)≥pA≥pB≥max
c̸=cAHc
s(M(D′);xtest),
thenhs(M(D′);xtest) =hs(M(D);xtest) =cAfor all
∥δ∥<RAD , where
RAD =√−log(
1−(√pA−√pB)2)
σ2
tadv
2RL2
ZR∑
i=1(piγiτiηiqBi
nBi)2T∏
t=tadv+1(
2Φ(
ρt
σt)
−1)
(2)
The function CalculateRadius in our Algorithm 2 can cal-
culate the certiﬁed radius RAD according to Corollary 1.
We now make several remarks about Corollary 1 and will
verify them in our experiments: 1) The noise level σtand the
parameter norm clipping threshold ρtare hyper-parameters
that can be adjusted to control the robustness-accuracy trade-
off. For instance, the certiﬁed radius RAD would be large
when:σtis high;ρtis small; the margin between pAand
pBis large; the number of attackers Ris small; the poison
ratioqBi
nBiis small; the scale factor γiis small; the aggre-
gation weights for attackers piis small; the local iteration
τiis small; and the local learning rate ηismall. 2) Since
0≤2Φ(·)−1≤1, the certiﬁed radius RAD goes to∞
asT→∞ when Φ(·)<1. Ituitively, the benign ﬁne-
tuning after backdoor injection round tadvwould mitegate
the poisoning effect. Thus, with inﬁnite rounds of such
ﬁne-tuning, the model is able to tolerate backdoors with
arbitrarily large magnitude. In practice, we note that the
continued multiplication in the denominator may not ap-
proach 0 due to numerical issues, which we will verify in
the experiments section. 4) Large number of clients Nwill
decrease the aggregation weights piof attackers, thus it can
tolerate backdoors with large magnitude, resulting in higher
RAD . 5) For general neural networks, efﬁcient computation
of Lipschitz gradient constant (w.r.t. data input) is an open
question, especially when the data dimension is high. We
will provide a closed-form expression for LZunder some
constraints next.
As mentioned in Section 3.2, the backdoor for data sample
zi
jincludes both the backdoor pattern δixand adversarial
CRFL: Certiﬁably Robust Federated Learning against Backdoor Attacks
target label ﬂipping δiy. In Assumption 2 we deﬁne LZwith
z={x,y}(concatenation of xandy) to certify against both
backdoor patterns and label-ﬂipping effects. Without loss of
generality, here we focus on backdoor patterns considering
bounded model parameters in Lemma 1, which provides a
closed-form expression for LZin the case of multi-class
logistic regression. By applying LZfrom Lemma 1 to
Theorem 1, it indicates that the prediction for a test sample
is independent with the backdoor pattern so the backdoor
pattern is disentangled from the adversarial target label.
Lemma 1. Given the upper bound on model parameters
norm, i.e.,∥w∥≤ρ, and two data samples z1andz2with
x1̸=x2(y1=y2), for multi-class logistic regression (i.e.,
one linear layer followed by a softmax function and trained
by cross-entropy loss), its Lipschitz gradient constant w.r.t
data isLZ=√
2 + 2ρ+ρ2. That is,
∥∇ℓ(w;z1)−∇ℓ(w;z2)∥≤√
2 + 2ρ+ρ2∥z1−z2∥.
Proof for Lemma 1 is provided in the Appendix B.6.
In order to formally derive the main theorem, there are two
key results. We ﬁrst quantify the closeness between the
FL trained models M(D′)andM(D))using Markov Ker-
nel, and then connect the model closeness to the prediction
consistency through parameter smoothing.
5.2. Model Closeness
As described in Algorithm 1, owing to the Gaussian noise
perturbation mechanism, in each iteration the global model
can be viewed as a random vector with the Gaussian smooth-
ing measure µ. We use the f-divergence between µ(M(D′))
andµ(M(D))as a statistical distance for measuring model
closeness of the ﬁnal FL model. Based on the data post-
processing inequality, when we interpret each round of
CRFL as a probability transition kernel, i.e., a Markov Ker-
nel, the contraction coefﬁcient of Markov Kernel can help
bound the divergence over multiple training rounds of FL.
Letf: (0,∞)→Rbe a convex function with f(1) = 0 ,
µandνbe two probability distributions. Then the f-
divergence is deﬁned as Df(µ||ν) =EW∼ν[f(µ(W)
ν(W))].
Common choices of f-divergence include total variation
(f(x) =1
2∥x−1∥) and Kullback-Leibler (KL) diver-
gence (f(x) =xlogx). The data processing inequal-
ity (Raginsky, 2016; Polyanskiy & Wu, 2015; 2017) for
the relative entropy states that, for any convex function
fand any probability transition kernel (Markov Kernel),
Df(µK||νK)≤Df(µ||ν), whereµKdenotes the push-
forward of µbyK, i.e.,µK =∫
µ(dW)K(W). In
other words, Df(µ||ν)decreases by post-processing via
K. (Asoodeh & Calmon, 2020) extend it to analyze SGD.
In our setting, all the operations in one round of our
CRFL, including SGD, clipping and noise perturbations,are incorporated as a Markov Kernel. We note that in the
single-round attack setting, the adversarial clients use clean
datasets to train the local models after tadv, so the Markov
operator is the same as the one in the benign training process.
Therefore the f-divergence of the two global models (back-
doored and benign) of interest decreases over rounds, which
is characterized by a contraction coefﬁcient deﬁned in Ap-
pendix B. We quantify such contraction property of Markov
Kernel for each round with the help of two hyperparameters
in the server side: model parameter norm clipping threshold
ρtand the noise level σt, and ﬁnally bound f-divergence of
global models in round T. Although our analysis can be
adopted to general f-divergence, we here use KL divergence
as an instantiation to measure the model closeness.
Theorem 2. Whenηi≤1
βand Assumptions 1, 2, and 3
hold, the KL divergence between µ(M(D))andµ(M(D′))
withµ(w) =N(w,σT2I)is bounded as:
DKL(µ(M(D))||µ(M(D′)))
≤2R∑R
i=1(
piγiτiηiqBi
nBiLZ∥δi∥)2
σ2
tadvT∏
t=tadv+1(
2Φ(ρt
σt)
−1)
The proof is provided in the Appendix B.
5.3. Parameter Smoothing
We connect the model closeness to the prediction consis-
tency by the following theorem. The smoothed classiﬁer hs
is robustly certiﬁed at µ(w′)with respect to the bounded
KL divergence, DKL(µ(w),µ(w′))≤ϵ.
Theorem 3. Lethsbe deﬁned as in Eq. 1. Suppose cA∈Y
andpA,pB∈[0,1]satisfy
HcAs(w′;xtest)≥pA≥pB≥max
c̸=cAHc
s(w′;xtest),
thenhs(w′;xtest) =hs(w;xtest) =cAfor allwsuch that
DKL(µ(w),µ(w′))≤ϵ, where
ϵ=−log(
1−(√pA−√
pB)2)
The proof is provided in the Appendix C.
Finally, combining Theorem 2 and 3 leads to our
main Theorem 1. In detail, Theorem 2 states that
DKL(µ(M(D))||µ(M(D′)))under our CRFL framework
is bounded by certain value that depends on the difference
betweenDandD′. Theorem 3 states that for a test sam-
plextest, as long as the KL divergence is smaller than
−log(1−(√pA−√pB)2), the prediction from the poi-
soned smoothed classiﬁer hsthat is built upon the base
classiﬁer with model parameter M(D′)will be consistent
with the prediction from hsthat is built uponM(D). There-
fore, we derive the condition for DandD′in Theorem 1,
CRFL: Certiﬁably Robust Federated Learning against Backdoor Attacks
under which DKL(µ(M(D))||µ(M(D′)))≤− log(1−
(√pA−√pB)2). This condition also indicates that hsbuilt
upon the model parameter M(D′)is certiﬁably robust.
Defend against Other Potential Attack Here we dis-
cuss the potentials to generalize our method against other
training-time attacks. 1) Our method can naturally extend
toﬁxed-frequency attack by applying our analysis for each
attack period. In particular, we can repeatedly apply our
Theorem 2 to analyze model closeness for each attack pe-
riod, and the different initializations of each period can be
bounded based on its last period. Then Theorem 3 can be
applied to connect model closeness to certify the prediction
consistency. 2) (Wang et al., 2020) introduce edge-case
adversarial training samples to enforce the model to misclas-
sify inputs on the tail of input distribution. The edge-case
attack essentially conducts a special semantic attack (Bag-
dasaryan et al., 2020) by selecting rare images instead of
directly adding backdoor patterns. It is possible to apply our
framework against such attack by viewing it as the whole
sample manipulation.
Comparison with Differentially Private Federated
Learning In order to protect the privacy of each client, dif-
ferentially private federated learning (DPFL) mechanisms
are proposed (Geyer et al., 2017; McMahan et al., 2018;
Agarwal et al., 2018) to ensure that the learned FL model
is essentially unchanged when one individual client is mod-
iﬁed. Compared with DPFL, our method has several fun-
damental differences and addresses additional challenges:
1) Mechanisms: DPFL approaches add training-time noise
to provide privacy guarantee, while ours add smoothing
noise during training and testing to provide certiﬁed robust-
ness against data poisoning. In general, the added noise in
CRFL does not need to be as large as that in DPFL to pro-
vide strong privacy guarantee, and therefore preserve higher
model utility. 2) Certiﬁcation goals: DPFL approaches pro-
vide client-level privacy guarantee for the learned model
parameters, while in CRFL the robustness guarantee is de-
rived for certiﬁed pointwise prediction which could be on
the feature, samples and clients levels. 3) Technical con-
tributions: DPFL approaches derive DP guarantee via DP
composition theorems (Dwork et al., 2014; Abadi et al.,
2016), while we quantify the global model deviation via
Markov Kernel and verify the robustness properties of the
smoothed model via parameter smoothing.
6. Experiments
In our experiments, the attackers perform the model replace-
ment attack at round tadvduring our CRFL training, and the
server performs parameter smoothing on a possibly back-
doored FL model at round Tto calculate the certiﬁed radius
RAD for each test sample based on Corollary 1. Speciﬁ-cally, we evaluate the effect of the training time noise σt,
the attacker’s ability which includes the number of attack-
ersR, the poison ratioqBi
nBiand the scale factor γi, robust
aggregation protocol, the number of total clients Nand the
number of training rounds T. Moreover, we evaluate the
model closeness empirically to justify Theorem 2.
6.1. Experiment Setup
We focus on multi-class logistic regression (one linear layer
with softmax function and cross-entropy loss), which is a
convex classiﬁcation problem. We train the FL system fol-
lowing our CRFL framework with three datasets: Lending
Club Loan Data (LOAN) (Kan, 2019), MNIST (LeCun &
Cortes, 2010), and EMNIST (Cohen et al., 2017). We re-
fer the readers to Appendix A for more details about the
datasets, parameter setups and attack setting. We train the
FL global model until convergence and then use our certiﬁ-
cation in Algorithm 2 for robustness evaluation.
The metrics of interest are certiﬁed rate andcertiﬁed ac-
curacy . Given a test set of size m, fori-th test sample,
the ground truth label is yi, and the output prediction is
eitherciwith the certiﬁed radius RADiorci=ABSTAIN
with RADi= 0. Then we calculate certiﬁed rate atras
1
m∑m
i=11{RADi≥r}, and certiﬁed accuracy atras
1
m∑m
i=11{ci=yiandRADi≥r}. The certiﬁed rate
is the fraction of the test set that can be certiﬁed at ra-
diusRAD≥r, which reveals how consistent the possibly
backdoored classiﬁer’s prediction with the clean classiﬁer’s
prediction. The certiﬁed accuracy is the fraction of the test
set for which the possibly backdoored classiﬁer makes cor-
rect and consistent predictions with the clean model. In the
displayed ﬁgures, there is a critical radius beyond which
the certiﬁed accuracy and certiﬁed rate are dropped to zero.
Since each test sample has its own calculated certiﬁed ra-
dius RADi, this critical value is a threshold that none of
them have a larger radius than it, similar to the ﬁndings
in (Cohen et al., 2019). We certiﬁed 10000/5000/10000
samples from the LOAN/MNIST/EMNIST test sets. In all
experiments, unless otherwise stated, we use σT= 0.01to
generateM= 1000 noisy models in parameter smoothing
procedure, and use the error tolerance α= 0.001. In our
experiments, we adopt the expression of LZin Lemma 1.
LZcan be generalized to other poisoning settings by spec-
ifyingz1,z2in Assumption 2 under the case of “ x1̸=x2
andy1̸=y2” or “x1=x2andy1̸=y2”.
6.2. Experiment Results
We only change one factor in each experiment and keep oth-
ers the same as the experiment setup. We plot the certiﬁed
accuracy and certiﬁed rate on the clean test set, and report
the results on the backdoored test set in Appendix A.
CRFL: Certiﬁably Robust Federated Learning against Backdoor Attacks
0 1 2 3 4 5
radius0.00.20.40.60.81.0certified rate  = 0.005
  = 0.010
  = 0.015
  = 0.020
  = 0.025
(a) Certiﬁed rate on MNIST
0 1 2 3 4 5
radius0.00.20.40.60.81.0certified accuracy = 0, uncertified
  = 0.005
  = 0.010
  = 0.015
  = 0.020
  = 0.025
 (b) Certiﬁed acc. on MNIST
0 1 2 3 4 5 6
radius0.40.50.60.70.80.9certified accuracy = 0, uncertified
  = 0.005
  = 0.010
  = 0.015
  = 0.020
  = 0.025
(c) Certiﬁed acc. on LOAN
0 2 4 6 8 10
radius0.00.10.20.30.40.50.60.70.8certified accuracy = 0, uncertified
  = 0.005
  = 0.010
  = 0.015
  = 0.020
  = 0.025
 (d) Certiﬁed acc. on EMNIST
Figure 2. Certiﬁed accuracy and certiﬁed rate on MNIST, LOAN,
and EMNIST with different training-time noise σ. Solid lines
represent certiﬁed accuracy; dashed lines of the same color show
the accuracy of base classiﬁer trained with σ; black dashed line
presents the accuracy of the classiﬁer trained without noise.
Effect of Training Time Noise Since we aim to de-
fend against backdoor attack, the training time noise σ
(σ=σt,t<T ) in our Algorithm 1 is more essential than
σTin parameter smoothing (Algorithm 2). The reason is
thatσcan nullify the malicious model updates at early stage.
Figure 2 plots the certiﬁed accuracy and certiﬁed rate at-
tained by training FL system with different σ. In Figure 2(a),
whenσis high, certiﬁed rate is high at every rand large
radius can be certiﬁed. Figure 2(b)(c)(d) show that large
radius is certiﬁed but at a low accuracy, so the parame-
ter noiseσcontrols the trade-off between certiﬁability and
accuracy, which echoes the property of evasion-attack certi-
ﬁcation (Cohen et al., 2019). Comparing the solid line with
the dashed line for each color, we can see that the parameter
smoothing with σTdoes not hurt the accuracy much.
Effect of Attacker Ability From the perspective of at-
tackers, the larger number of attackers R, the larger poison
ratioqBi
nBiand the larger scale factor γiresult in the stronger
attack. Figure 3, Figure 4, and Figure 5 show that in the
three datasets, the stronger the attack, the smaller radius can
be certiﬁed. After training sufﬁcient number of rounds with
clean datasets after tadv, we show that the certiﬁed radius is
not sensitive to the attack timing tadvin Appendix A.2.
Effect of Robust Aggregation Our CRFL can be used to
assess different robust aggregation rules. Figure 6 presents
the certiﬁed accuracy on MNIST and EMNIST as Ris
varied, when our CRFL adopts the robust aggregation al-
0.0 0.5 1.0 1.5 2.0 2.5
radius0.00.20.40.60.81.0certified accuracy qBi/nBi = 5%
 qBi/nBi = 10%
 qBi/nBi = 20%
 qBi/nBi = 30%
 qBi/nBi = 50%
0 1 2 3 4 5
radius0.00.20.40.60.81.0certified accuracy qBi/nBi = 2.5%
 qBi/nBi = 5%
 qBi/nBi = 10%
 qBi/nBi = 15%
 qBi/nBi = 20%Figure 3. MNIST (left) and LOAN (right) test set certiﬁed accuracy
as the poison ratio qBi/nBiis varied.
0.0 0.5 1.0 1.5 2.0 2.5
radius0.00.20.40.60.81.0certified accuracy = 10
 = 20
 = 30
 = 50
 = 100
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
radius0.00.10.20.30.40.50.60.70.8certified accuracy = 10
 = 20
 = 30
 = 50
 = 100
Figure 4. Certiﬁed accuracy with different scaling factor γon
LOAN (left) and EMNIST (right).
gorithm RFA (Pillutla et al., 2019), which detects outliers
and down-weights the malicious updates during aggregation.
Comparing FedAvg in Figure 5 with RFA in Figure 6 (the
magnitude of x-axis is different), we observe that very large
radius can be certiﬁed under RFA. This is because that the
attacker is assigned with very low aggregation weights pi,
which is part of our bound in Eq. 2. Our certiﬁed radius
reveals that RFA is much robust than FedAvg, which shows
the potential usage of our certiﬁed radius as an evaluation
metric for the robustness of other robust aggregation rules.
Effect of Client Number Distributed learning across a
large number of clients is an important property of FL. Fig-
ure 7 shows that large radius can be certiﬁed when Nis
large (i.e., more clients can tolerant larger backdoor mag-
nitude), because it decreases the aggregation weights piof
attackers. Moreover, the backdoor effect could be mitigated
by more benign model updates during training.
Effect of Training Rounds According to Figure 8, the
certiﬁed accuracy is higher when Tis larger. However, the
largest radius that can be certiﬁed for the test set does not
increase. We note that this is due to numerical issues of the
standard Gaussian CDF Φ(·). As we mentioned in Section
5.1, the continued multiplication in the denominator of Eq. 2
will not achieve 0 in practice. Otherwise the certiﬁed radius
RAD goes to∞asT→∞ since 0≤2Φ(ρ/σ)−1≤1.
To verify our argument, we ﬁx pAandpBto be 0.7 and
0.1, use default values for other parameters, and study the
relationship between ρ/σ,TandRAD in Figure 9(b). When
ρ/σis larger than certain threshold, the certiﬁed radius RAD
CRFL: Certiﬁably Robust Federated Learning against Backdoor Attacks
0.0 0.5 1.0 1.5 2.0 2.5
radius0.00.20.40.60.81.0certified accuracy R = 1, FedAvg
 R = 2, FedAvg
 R = 3, FedAvg
 R = 4, FedAvg
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
radius0.00.10.20.30.40.50.60.70.8certified accuracy R = 1, FedAvg
 R = 2, FedAvg
 R = 3, FedAvg
 R = 4, FedAvg
Figure 5. MNIST (left) and EMNIST (right) test set certiﬁed accu-
racy as the number of adversarial clients Ris varied.
0 20 40 60 80 100 120
radius0.00.20.40.60.81.0certified accuracy R = 1, RFA
 R = 2, RFA
 R = 3, RFA
 R = 4, RFA
0 50 100 150 200 250 300 350 400
radius0.00.10.20.30.40.50.60.70.8certified accuracy R = 1, RFA
 R = 2, RFA
 R = 3, RFA
 R = 4, RFA
Figure 6. Certiﬁed accuracy on MNIST (left) and EMNIST (right)
with different Rwhen FL is trained under the robust aggregation
RFA (Pillutla et al., 2019).
does not change much when Tincreases. If one wishes to
increaseTfor improving certiﬁed radius, then we suggest
to keepρ/σsmaller than the threshold to make effect. The
increased certiﬁed accuracy when Tis large in Figure 8
could be attributed to improved model performance up to
convergence, so the margin between pA−pBis widened.
We also study the error tolerance αand the number of noisy
modelsMin Appendix A.2. Larger Myields larger certi-
ﬁed radius, and the certiﬁed radius is not very sensitive to
α.
Empirical Evaluation on Model Closeness Our theo-
rems are derived based on the analysis in comparison to
a “virtual” benign training process. Empirically, we train
such FL global model under the benign training process and
compare the ℓ2distance between the clean global model and
the backdoored global model at every round. In Figure 9(a),
one attacker performs model replacement attack on MNIST
at round tadv={20,40,60}respectively. We can observe
that the plotted ℓ2distance over the FL training rounds after
tadvis decreasing, which echos our assumption that because
all clients behave normal and use their clean local datasets
to purify the global model after tadv, the global models be-
tween two training process become close. This observation
also can justify the model closeness statement in Theorem
2.
0 2 4 6 8 10
radius0.00.20.40.60.81.0certified accuracyN = 20
N = 40
N = 60
N = 80
N = 100
0 1 2 3 4 5 6
radius0.00.20.40.60.81.0certified accuracyN = 10
N = 20
N = 30
N = 40
N = 50Figure 7. Certiﬁed accuracy on MNIST (left) and LOAN (right) as
the number of total clients Nis varied.
0.0 0.5 1.0 1.5 2.0 2.5
radius0.00.20.40.60.81.0certified accuracyT = 30
T = 50
T = 60
T = 80
0 1 2 3 4 5
radius0.00.10.20.30.40.50.6certified accuracyT = 20
T = 50
T = 70
T = 100
T = 130
T = 150
Figure 8. LOAN (left) and EMNIST (right) test set certiﬁed accu-
racy asTis varied.
0 20 40 60 80 100 120 140
T0.000.020.040.060.080.100.12L2 distance
tadv=20
tadv=40
tadv=60
3.0 3.2 3.4 3.6 3.8
 / 
140160180200220radiusT=50
T=100
T=150
T=200
T=250
T=300
T=350
T=400
T=450
Figure 9. (a) Theℓ2distance of the global models between the
backdoored training process and the benign training process. (b)
Numerical analysis of the standard Gaussian CDF Φ(·).
7. Conclusion
This paper establishes the ﬁrst framework (CRFL) on cer-
tiﬁably robust federated learning against backdoor attacks.
CRFL employs model parameter clipping and perturbing
during training, and uses model parameter smoothing dur-
ing testing, to certify conditions under which a backdoored
model will give consistent predictions with an oracle clean
model. Our theoretical analysis characterizes the relation
between certiﬁed robustness and federated learning pa-
rameters, which are empirically veriﬁed on three different
datasets.
Acknowledgements
This work is partially supported by NSF grant No.1910100,
NSF CNS 20-46726 CAR, Amazon Research Award, IBM-
ILLINOIS Center for Cognitive Computing Systems Re-
search (C3SR) – a research collaboration as part of the IBM
AI Horizons Network.
CRFL: Certiﬁably Robust Federated Learning against Backdoor Attacks
References
Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B.,
Mironov, I., Talwar, K., and Zhang, L. Deep learning
with differential privacy. In Proceedings of the 2016 ACM
SIGSAC conference on computer and communications
security , pp. 308–318, 2016.
Agarwal, N., Suresh, A. T., Yu, F., Kumar, S., and McMahan,
H. B. cpsgd: communication-efﬁcient and differentially-
private distributed sgd. In Proceedings of the 32nd Inter-
national Conference on Neural Information Processing
Systems , pp. 7575–7586, 2018.
Andreina, S., Marson, G. A., M ¨ollering, H., and Karame, G.
Bafﬂe: Backdoor detection via feedback-based federated
learning. arXiv preprint arXiv:2011.02167 , 2020.
Asoodeh, S. and Calmon, F. Differentially private federated
learning: An information-theoretic perspective. In ICML
Workshop on Federated Learning for User Privacy and
Data Conﬁdentiality , 2020.
Bagdasaryan, E., Veit, A., Hua, Y ., Estrin, D., and
Shmatikov, V . How to backdoor federated learning. In
International Conference on Artiﬁcial Intelligence and
Statistics , pp. 2938–2948. PMLR, 2020.
Bhagoji, A. N., Chakraborty, S., Mittal, P., and Calo, S.
Analyzing federated learning through an adversarial lens.
InInternational Conference on Machine Learning , pp.
634–643, 2019.
Blanchard, P., El Mhamdi, E. M., Guerraoui, R., and Stainer,
J. Machine learning with adversaries: Byzantine tolerant
gradient descent. In Proceedings of the 31st International
Conference on Neural Information Processing Systems ,
pp. 118–128, 2017.
Cao, X., Jia, J., and Gong, N. Z. Provably secure federated
learning against malicious clients. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence , volume 35,
pp. 6885–6893, 2021.
Chen, Y ., Su, L., and Xu, J. Distributed statistical machine
learning in adversarial settings: Byzantine gradient de-
scent. Proceedings of the ACM on Measurement and
Analysis of Computing Systems , 1(2):1–25, 2017.
Cohen, G., Afshar, S., Tapson, J., and Van Schaik, A. Em-
nist: Extending mnist to handwritten letters. In 2017 Inter-
national Joint Conference on Neural Networks (IJCNN) ,
pp. 2921–2926. IEEE, 2017.
Cohen, J., Rosenfeld, E., and Kolter, Z. Certiﬁed adver-
sarial robustness via randomized smoothing. In Interna-
tional Conference on Machine Learning , pp. 1310–1320.
PMLR, 2019.Dobrushin, R. L. Central limit theorem for nonstationary
markov chains. i. Theory of Probability & Its Applica-
tions , 1(1):65–80, 1956.
Dvijotham, K. D., Hayes, J., Balle, B., Kolter, Z., Qin, C.,
Gy¨orgy, A., Xiao, K., Gowal, S., and Kohli, P. A frame-
work for robustness certiﬁcation of smoothed classiﬁers
using f-divergences. In ICLR , 2020.
Dwork, C., Roth, A., et al. The algorithmic foundations of
differential privacy. Foundations and Trends in Theoreti-
cal Computer Science , 9(3-4):211–407, 2014.
El Mhamdi, E. M., Guerraoui, R., and Rouault, S. L. A.
The hidden vulnerability of distributed learning in byzan-
tium. In International Conference on Machine Learning ,
number CONF, 2018.
Fallah, A., Mokhtari, A., and Ozdaglar, A. Personalized
federated learning: A meta-learning approach. NeurIPS ,
2020.
Fu, S., Xie, C., Li, B., and Chen, Q. Attack-resistant fed-
erated learning with residual-based reweighting. arXiv
preprint arXiv:1912.11464 , 2019.
Fung, C., Yoon, C. J., and Beschastnikh, I. The limitations
of federated learning in sybil settings. In 23rd Interna-
tional Symposium on Research in Attacks, Intrusions and
Defenses ({RAID}2020) , pp. 301–316, 2020.
Geyer, R. C., Klein, T., and Nabi, M. Differentially private
federated learning: A client level perspective. arXiv
preprint arXiv:1712.07557 , 2017.
Gu, T., Liu, K., Dolan-Gavitt, B., and Garg, S. Badnets:
Evaluating backdooring attacks on deep neural networks.
IEEE Access , 7:47230–47244, 2019.
Hoeffding, W. Probability inequalities for sums of bounded
random variables. In The Collected Works of Wassily
Hoeffding , pp. 409–426. Springer, 1994.
Kan, W. Lending club loan data, Mar 2019. URL
https://www.kaggle.com/wendykan/
lending-club-loan-data .
LeCun, Y . and Cortes, C. MNIST handwritten digit
database. 2010. URL http://yann.lecun.com/
exdb/mnist/ .
Lecuyer, M., Atlidakis, V ., Geambasu, R., Hsu, D., and
Jana, S. Certiﬁed robustness to adversarial examples with
differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP) , pp. 656–672. IEEE, 2019.
Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z. On the
convergence of fedavg on non-iid data. In International
Conference on Learning Representations , 2020.
CRFL: Certiﬁably Robust Federated Learning against Backdoor Attacks
Makur, A. Information contraction and decomposition . PhD
thesis, Massachusetts Institute of Technology, 2019.
McMahan, B., Moore, E., Ramage, D., Hampson, S., and
y Arcas, B. A. Communication-Efﬁcient Learning of
Deep Networks from Decentralized Data. In Proceed-
ings of the 20th International Conference on Artiﬁcial
Intelligence and Statistics , volume 54 of Proceedings
of Machine Learning Research , pp. 1273–1282. PMLR,
20–22 Apr 2017a.
McMahan, B., Moore, E., Ramage, D., Hampson, S., and
y Arcas, B. A. Communication-efﬁcient learning of deep
networks from decentralized data. In Artiﬁcial Intelli-
gence and Statistics , pp. 1273–1282. PMLR, 2017b.
McMahan, H. B., Ramage, D., Talwar, K., and Zhang, L.
Learning differentially private recurrent language models.
InInternational Conference on Learning Representations ,
2018.
Pillutla, K., Kakade, S. M., and Harchaoui, Z. Ro-
bust aggregation for federated learning. arXiv preprint
arXiv:1912.13445 , 2019.
Polyanskiy, Y . and Wu, Y . Dissipation of information in
channels with input constraints. IEEE Transactions on
Information Theory , 62(1):35–55, 2015.
Polyanskiy, Y . and Wu, Y . Strong data-processing inequal-
ities for channels and bayesian networks. In Convexity
and Concentration , pp. 211–249. Springer, 2017.
Raginsky, M. Strong data processing inequalities and phi-
sobolev inequalities for discrete channels. IEEE Transac-
tions on Information Theory , 62(6):3355–3389, 2016.
Reisizadeh, A., Farnia, F., Pedarsani, R., and Jadbabaie, A.
Robust federated learning: The case of afﬁne distribution
shifts. NeurIPS , 2020.
Rudin, W. Principles of Mathematical Analysis . In-
ternational series in pure and applied mathematics.
McGraw-Hill, 1976. ISBN 9780070856134. URL
https://books.google.com.hk/books?id=
kwqzPAAACAAJ .
Smith, V ., Chiang, C.-K., Sanjabi, M., and Talwalkar, A. S.
Federated multi-task learning. In Advances in Neural
Information Processing Systems , pp. 4424–4434, 2017.
Sun, Z., Kairouz, P., Suresh, A. T., and McMahan, H. B. Can
you really backdoor federated learning? arXiv preprint
arXiv:1911.07963 , 2019.
Wang, H., Sreenivasan, K., Rajput, S., Vishwakarma, H.,
Agarwal, S., Sohn, J.-y., Lee, K., and Papailiopoulos, D.
Attack of the tails: Yes, you really can backdoor federated
learning. NeurIPS , 2020.Wang, J. and Joshi, G. Cooperative sgd: A uniﬁed frame-
work for the design and analysis of communication-
efﬁcient sgd algorithms. In ICML Workshop on Coding
Theory for Machine Learning , 2019.
Weber, M., Xu, X., Karlas, B., Zhang, C., and Li, B. Rab:
Provable robustness against backdoor attacks. arXiv
preprint arXiv:2003.08904 , 2020.
Xie, C., Huang, K., Chen, P.-Y ., and Li, B. Dba: Distributed
backdoor attacks against federated learning. In Interna-
tional Conference on Learning Representations , 2019.
Yin, D., Chen, Y ., Kannan, R., and Bartlett, P. Byzantine-
robust distributed learning: Towards optimal statistical
rates. In International Conference on Machine Learning ,
pp. 5650–5659. PMLR, 2018.
Zhang, J., Zheng, K., Mou, W., and Wang, L. Efﬁcient
private erm for smooth objectives. In Proceedings of the
26th International Joint Conference on Artiﬁcial Intelli-
gence , pp. 3922–3928, 2017.
Zhao, Y ., Li, M., Lai, L., Suda, N., Civin, D., and Chandra,
V . Federated learning with non-iid data. arXiv preprint
arXiv:1806.00582 , 2018."
2305.15333,D:\Database\arxiv\papers\2305.15333.pdf,"In the context of large-scale recommendation systems, what are the key challenges associated with maintaining and updating user engagement history, particularly when dealing with a rapidly growing number of items?","The primary challenge is the potential for memory overload and computational inefficiency as the number of unique items grows, leading to increasingly large and complex user engagement histories. This can be addressed through various strategies like sampling, aggregation, and retrieval techniques.","Zhao et al.
as keys for embedding look-ups (i.e, the ‘sparse-id’ and ‘target-id’
in the figure).
3.3 Hybrid Models
It is also possible and actually straightforward to have a hybrid
formulation, i.e., to implement models that include both a user-
centric and an item-centric attentive pooling components. Fig-
ure 1(b) shows how the example architecture in Figure 1(a) looks
like in the hybrid formulation. Such hybrid models will have simi-
lar “parameter explosion” problem as item-centric models. We will
compare all these different model formulations in our experiments.
4 IMPLEMENTATION
4.1 Item-Centric Ranking
Item-centric id-lists represent the engagement history of each user.
Although the number of items that one user can interact within
one day is hardly over a few hundreds, the list of distinctive items
and their embeddings gets accumulated very quickly over time,
especially considering that the same item is rarely recommended to
the same user again. A sampling strategy is needed in order for each
engagement list to not exceed certain length. In our implementation,
we limit the length to 1024 at max, by only including the most
recent engagements. In our experiment, this method is referred to
as “IC-Sampling ”.
4.2 User-Centric Ranking
One of the challenges for implementing UCR is to handle the distri-
bution skewness. In an item-centric setting, the number of items
one user can interact with tends to be evenly distributed (e.g., daily
engagements range from a few to a few hundred), whereas in the
new setting, the distribution is more irregular, e.g., some items can
attract millions of users to engage with while others can get only a
few. This means that for some items it is no longer feasible to fit the
entire list of engaged users in memory during training/inference.
We explore three different approaches:
•Sampling . In this implementation, we simply down-sample
the list of engaged users of an item to a fixed-size sub-list
uniformly using reservoir sampling. Note that in practice, if
we sample for each item only once, instead of resampling for
each user-item interaction, this will introduce an artificial
bias. This method is referred to as “ UC-Sampling .""
•Aggregation . Another approach is to summarize a long
sequence of engaged users to a shorter list, e.g., by cluster-
ing the users and using cluster-id in replacement of user-id.
In our implementation, the clusters are obtained by apply-
ing the Louvain algorithm [ 1] to the user-item interaction
graph. Our in-house implementation provides the function-
ality to incrementally update the clustering structure over
time with constraints on cluster size and re-mapping ratio.
This method is referred to as “ UC-Clustering "".
•Retrieval . Alternatively, we can pre-index the engagement
history and use retrieval (e.g., max inner-product search)
to identify the subset of most relevant users (w.r.t. the tar-
get user), on which attentive pooling is then applied. Sinceattention is of quadratic complexity, the overhead of re-
trieval can be compensated by the speedup due to a shorter
and more selective attention window. A sparsified atten-
tion distribution also means an improved signal-to-noise
ratio (i.e., long-tail less relevant candidates are pruned and
excluded from the attentive aggregation) and can further
improve model quality. We leave this method for future
investigation.
Note that this problem is only a concern for a very small subset
of the most popular items, for which most ranking models already
have good prediction accuracy. For the vast majority of items in
our case, the engagement users are below the 1024 length limit.
4.3 Parameter Hashing
Another technical challenge is memory management when work-
ing with large-scale ID spaces such as user-ids Uand item-idsI.
Considering that we are learning embedding vectors, one for each
distinctive ID, the extremely large cardinalities (i.e., in the order
of billions) of these ID spaces imply that the memory requirement
as well as the index to map IDs to their address can be quite a
challenge. Especially for item-centric ranking, the number of item
IDs can grow unboundedly to infinity.
One common approach to address this problem is to implement
feature hashing, i.e., to maintain a constant hash space for these IDs
and allocate one embedding vector for each distinctive “hashed ID"".
This is of course not ideal. The existence of hash collisions means
that we are forcing certain random IDs to share the same embedding
vectors. This is not necessarily a bad thing when the collision rate
is at a reasonable level, because feature hashing provides a type
of regularization effect to the embedding parameters similar to
dropout. However, for unbounded ID spaces such as Iin user-
centric ranking, the collision rate is expected to grow linearly over
time (i.e,𝑂(𝑡)), and can be arbitrarily large and no longer negligible.
In contrast, in user-centric ranking, the ID space Uis bounded and
hence collision rate is under control.
4.4 Aggregation Operators
We implement two aggregation operators, sum-pooling and tar-
geted attentive pooling. The former aggregates the list of associated
IDs by the sum or mean of their corresponding embeddings. Sum-
pooling is computationally inexpensive and easy to implement.
However, it has very limited expressive capability (e.g., the oper-
ator itself is parameter-less) and needs to rely on the interaction
arch to encode complex interactions. Moreover, especially when the
list is long, using an unweighted sum could deteriorate the signal-
to-noise ratio and make the prediction less accurate. By attending
to the target user (item), attentive pooling can adaptively adjust
how much weight an embedding could get based on not only the
relevancy of the current item (user) at hand but also the relevance
of other competing entities. This aggregation is especially powerful
when the list contains entities of diverse topics (e.g., a user’s engage-
ment history could contain items in different categories), for which
the multiple distribution modes would be inevitably collapsed into
one if sum-pooling is used. Attentive pooling is also more robust
and tolerant to noises, outliers or corruptions in the ID list.
Breaking the Curse of Quality Saturation with User-Centric Ranking
Table 1: Evaluation results (AUC) on MovieLens data.
ICR UCR Hybrid
DIN with Attentive Pooling 0.712 0.731 0.737
5 EXPERIMENTS
5.1 On Public Data
A major goal of this paper is to improve the scaling capability of
ranking models due to the curse of quality saturation caused by
growing item inventories. To test our findings, data sets need to be
both (1) substantially large-scale and (2) based on dynamic inven-
tory as in real-world systems. Unfortunately, public data cannot
meet the requirement: they do not have the desired scale, nor do
they have the needed dynamics (matrix completion settings with
fixed users & items). We notice that this is a common issue in the
community. Notably, recent works on scaling, including those in
NLP and CV are based on dedicated data sets. The matter is even
worse in the area of ranking, because published data is not only
too small in scale but also lacks many vital characteristics that real-
world systems possess, making findings on such toy data sets less
reliable when being generalized to real world. However, to improve
the reproducibility of our results, we tested our methods on one
public data set for demonstration purposes.
5.1.1 Data. The MovieLens-20M data set is a popular benchmark
in recommendation systems [ 13]. It contains 20-million ratings from
138,493users on 27,278movies. In our experiments, we follow a
protocol similar to that of [ 30]: ratings of 4-star or above are treated
as positive and the rest as negative; for each user, the most recent 𝑁
(𝑁=512) positively-rated movies are used as item-centric channels
of that user; similarly, the 𝑀(𝑀=512) users who historically rated
a movie positively are used as user-centric channels of that movie.
As we mainly compare the difference between ICR and UCR, we
do not include other categorical features, such as genre.
5.1.2 Results. We tested the DIN [ 30] architecture (Figure 1(a))
in the three different formulations (i.e, ICR, UCR, hybrid) with
‘Attention-pooling’ as aggregation operator. A 4:1 split is used for
training and testing. The evaluation results in terms of AUC (i.e,
area under ROC curve) are reported in Table 1.
Note that MovieLens is a static data set. It does not have the
inventory dynamics that real-world systems have, and hence we
will not be able to see parameter explosion on this data set. From
Table 1, our observation is that UCR is at least on par with or slightly
better than ICR, while hybrid performs the best possibly because it
uses more signals than either of them.
5.2 On Real-World Production Data
5.2.1 Data. We further experiment on real-world production data.
For offline evaluation, we created a “lab data set” by sampling the
production log of a real-world short-form video recommendation
system. Our data set contains about 24million users and their
engagement activities in the time range of 60days (from late July to
early October of 2022). In total, the data set contains about 28billionexamples (engagement activities) involving 1type of negative and
5types of positive engagements.
5.2.2 Metric. We use Normalized Cross-Entropy (NCE) as the pri-
mary evaluation metric [ 16]. NCE is defined as the cross-entropy
loss of the model prediction 𝑝normalized by the entropy of the
label𝑦.
𝑁𝐶𝐸(𝑝,𝑦)=𝐶𝑟𝑜𝑠𝑠𝐸𝑛𝑡𝑟𝑜𝑝𝑦(𝑝,𝑦)
𝐸𝑛𝑡𝑟𝑜𝑝𝑦(𝑦)(1)
NCE is widely used as the gold standard offline metric for en-
gagement probability (e.g, CTR) prediction tasks because of its high
consistency with online engagement metrics.
5.2.3 Parameter Growth. In both ICR and UCR, the total number
of parameters that a model has can be expressed as 𝑐𝑜𝑛𝑠𝑡+𝑛×𝑑,
where the constant part is mostly related to model architectures,
while𝑛and𝑑denote the total number of distinctive sparse-ids and
the dimensionality of each embedding vector. In our data set, as
is common in most ranking systems, the cardinality of the user
set tends to be bigger than that of the item set for any given day,
|U|>|I𝑡+1|−|I 𝑡|, whereI𝑡is the accumulative item set on day
𝑡. However, that comparison is quickly reversed as time goes by
because|I𝑡|grows linearly in 𝑂(𝑡).
Figure 2 shows the model size growth over time for both ICR and
UCR models. We only plotted the curves for the case with sampling
and attentive pooling, but the trend is similar for all other variants.
While it is true that for the first few days the ICR model has fewer
parameters, it constantly adds parameters every day as new item
IDs emerge. As a result, the ICR model size grows almost linearly
over time. In contrast, the UCR model, although has a bit more
parameters initially, the model size stays relatively stable over time.
Considering these two models are trained using the same amount
of dyadic interaction data, the drastic contrast of the parameter
growth can have profound impacts on model quality. For example,
at the end of the 60-day window, the ICR model is 21x larger in
size than its UCR counterpart. This means that ICR consumes 21x
more memory, or when parameter hashing is used the collision
061218243036424854
Num of days1159131721Relative sizeICR
UCR
Figure 2: The growths of model size (the total number of
parameters) over time for ICR and UCR models.
Zhao et al.
04812162024283236404448525660
Num of days0.8250.8500.8750.9000.9250.9500.9751.000Relative NCE
IC Sampling Sum Pooling
UC Sampling Sum Pooling
IC Sampling Attentive Pooling
UC Sampling Attentive Pooling
Figure 3: Comparison of ICR and UCR models in offline
evaluation. Models are trained recurrently on a daily basis
and evaluated on future 10K activities using NCE. (lower is
better)
rate is 21x higher; at the same time, on average, each ID embedding
receives 21x less training data in ICR as compared to in UCR.
5.2.4 ICR vs. UCR. We compare IC-Sampling andUC-Sampling
with the two aggregation operator options. All the models are
trained recurrently and evaluated on a daily basis using the first
~10K examples of the next day. Because we have 6 tasks (and corre-
spondingly 6 engagement history channels) in our data set, each
task (and the engagement channel) is evaluated independently. The
results are reported in Figure 3, where only the results on ‘Task 1’
are shown (results on other tasks are very similar); all the NCE num-
bers are normalized by the NCE of the IC-Sampling sum pooling
model on day 1, and relative NCEs are used in the plot.
We can observe that UC-Sampling demonstrates a clear gain
over IC-Sampling , with the gap increasing rapidly from day 1 to
day 10, and then slowly converging till the end. The performance
matches our hypothesis that UCR accumulates and refines the un-
derstanding of each user, which helps with better recommendations
as the data scales up. However, we did not notice the gain increase
through the end of the experiments. We believe that this is because
UCR excels more on active users due to its nature of aggregating
user embeddings to profile engaged items, but falls short on less
active users. We will come back to address more about this issue in
Section 5.2.9.
5.2.5 Sum Pooling vs. Attentive Pooling. We also compare the im-
pact of the two aggregation operators in ICR and UCR. As shown in
Figure 3, attentive pooling consistently performs better than sum
pooling in UCR. With more data, the gap is also increasing. After 60
days of training, UCR attentive pooling get 0.44% gain over the sum
pooling alternative. In contrast, the advantage of attentive pooling
in ICR is very minimal.
This also proves our hypothesis in Section 4.4. In ICR, the item ID
is not well trained due to the linearly increased ID space . As a result
the attention score between history item and target item does not
learn useful signals, and attentive pooling falls back to mean (sum)
pooling. In UCR, user ID space is stable, and all ID embeddings
061218243036424854
Num of days0.8500.8750.9000.9250.9500.9751.000Relative NCE
UC Sampling Sum Pooling
UC Clustering Sum Pooling
UC Sampling Attentive Pooling
UC Clustering Attentive PoolingFigure 4: Comparison of the two implementation methods
for UCR: sampling vs clustering.
could be optimized. This finding verifies the potential to solve the
quality saturation problem using UCR with more training data.
5.2.6 Sampling vs. Clustering. In UCR, one of the key aspects to
ensure good performance is to construct better and more representa-
tive engaged user lists for each item, especially for those extremely
popular items that gain millions of user interactions. We imple-
mented two of the approaches presented in Section 4.2, namely
UC-Sampling andUC-Clustering . Figure 4 shows the comparison
between these two approaches. As can be seen, UC-Sampling seems
to dominate UC-Clustering in terms of NCE consistently across
the entire time span and all the tasks involved. We want to point
out that this may not be definite as the performance highly depends
on the choice of implementation, e.g., the incremental Louvain al-
gorithm [ 1] used in our experiments. If a better algorithm is used,
the result can be different. We leave such investigation for future
research.
5.2.7 Hybrid Method. We also compare the hybrid method with
UCR and ICR. Because the consistently superior performance of
08162432404856
Num of days0.8250.8500.8750.9000.9250.9500.9751.000Relative NCE
IC Sampling Sum Pooling
UC Sampling Sum Pooling
Hybrid Sum Pooling
IC Sampling Attentive Pooling
UC Sampling Attentive Pooling
Hybrid Attentive Pooling
Figure 5: Comparison of the hybrid model with its UCR and
ICR counterparts.
Breaking the Curse of Quality Saturation with User-Centric Ranking
Table 2: Multi-task relative NCE percentage (%) change between ICR (baseline), UCR and Hybrid models implemented with
attention pooling. Baseline setting is denoted as “-"".
TaskDay 7 Day 14 Day 30 Day 60
IC UC Hybrid IC UC Hybrid IC UC Hybrid IC UC Hybrid
1 --2.58 -2.88 -1.73 -4.01 -4.32 -3.01 -4.90 -5.21 -3.48 -5.18 -5.31
2 --2.71 -2.94 -0.46 -3.23 -3.42 -0.45 -3.24 -3.44 -0.44 -3.19 -3.32
3 -+1.84 -2.04 -7.52 -7.60 -10.38 -10.96 -12.64 -14.29 -11.98 -13.98 -12.78
4 --2.86 -3.08 -0.31 -3.23 -3.41 -0.08 -3.03 -3.23 -0.05 -2.96 -3.08
5 --2.88 -3.14 -0.66 -3.61 -3.88 -0.79 -3.77 -4.00 -0.81 -3.73 -3.88
6 --3.09 -3.28 -0.86 -3.97 -4.14 -1.19 -4.34 -4.53 -1.28 -4.38 -4.47
sampling over clustering as reported before, we only experimented
with the sampling implementation. The results are shown in Fig-
ure 5. It seems that the hybrid method has very similar performance
as the UCR counterpart, albeit slightly better. This phenomenon is
pretty consistent. We observe that the hybrid method achieves the
best NCE results across all the tasks. Considering that the hybrid
architecture, as shown in Figure 1, includes both an UCR sparse
sub-arch and an ICR sparse sub-arch, the results are partly as ex-
pected (i.e., it should have the advantages of both UCR and ICR) and
partly surprising (i.e., it has the same parameter explosion problem
as ICR).
5.2.8 Multi-Task Evaluation. In our previous evaluations, we use 1
single task and 1 single engagement history channel. In this sec-
tion, for both ICR and UCR, we use all the available engagement
signal channels (one for each engagement type) and jointly train
the model on all of the 6 tasks. This multi-channel and multi-task
setting allows the model to capture correlations among different
tasks as well as between the signal channel and the task loss corre-
sponding to different engagement types, which cannot be done in
the previous setting. The results are reported in Table 2, where the
NCE is calculated relative to the NCE of the ICR model at day 7.
We observe that overall UCR models show clear gains when com-
pared to ICR counterparts across all the tasks; moreover, the hybrid
model consistently performs the best at all of the tasks, although
the difference with the UCR models is very marginal.
5.2.9 Segment Analysis. We segment users into five buckets based
on their activeness (e.g., number of engagements within a given
< 10 < 20 < 100 <200 >= 200
User activity0.4
0.3
0.2
0.1
0.00.1Relative NCE difference (%)UC - IC
< 10 < 20 < 100 <200 >= 200
User activity0.6
0.5
0.4
0.3
0.2
0.1
0.0Relative NCE difference (%)Hybrid - IC
(a) UCR (b) Hybrid
Figure 6: Distribution of NCE gains over ICR on different
user activeness segments (negative means better).time window). In Figure 6(a), we show the NCE differences between
one UCR model ( UC-Sampling ) and one ICR model ( IC-Sampling )
for each user segment. We can see that, although UCR performs
better than ICR overall, the gain mostly come from more active
users. For less active users (e.g., engagement counts <10), UCR
actually performs worse than the ICR baseline. This explains why
the hybrid methods tend to perform the best because it leverages
both components to provide the better of the two worlds. As a
validation, Figure 6(b) shows the similar analysis of the Hybrid
model over ICR, and we can see it provides gains across all the user
segments.
5.2.10 Ablation Study. To better understand how different config-
urations impact model performance, we conduct a set of parameter
sweep experiments. For this analysis, we set the number of training
data to be 30 days for all the runs. In addition, we use IC-Sampling
andUC-Sampling with the same single-task setting in our experi-
ments.
Hash Size. Parameter hashing maps user IDs or item IDs to em-
bedding vectors by applying a hash function. Though being space-
efficient, it is essential to have a large enough hash space so that
a high collision rate between these IDs can be avoided. In this
experiment, we further examined how hash size affects model per-
formance by varying it from the default value of 20 million. As hash
size affects both IC and UC ranking, we test both IC-Sampling and
UC-Sampling as well as using both sum pooling and attentive pool-
ing model architectures. The results are reported in Table 3. Overall,
increasing the hash size leads to a better model performance. This
trend is more evident for UCR. For example, increasing the hash
size from 1M to 30M for UC-Attn results in a 1.71%reduction in
relative NCE. One reason why UCR benefits more than ICR is that
UCR has much fewer embedding vectors, the reduction in hash
collision is more dramatic for UCR when increasing hash size.
Table 3: Relative NCE percentage (%) change from different
models with varying hash sizes. Baseline setting is denoted
as “-"".
1M 5M 10M 20M 30M
IC Sum +0.08 +0.04 +0.01 - +0.02
IC Attn +0.12 +0.05 +0.05 +0.07 +0.06
UC Sum -0.04 -0.73 -1.07 -1.43 -1.53
UC Attn -0.24 -1.13 -1.48 -1.84 -1.95
Zhao et al.
Table 4: Relative NCE percentage (%) change from different
models with varying feature dimensions. Baseline setting is
denoted as “-"".
96 192 384
IC-Sampling -0.05 - +0.01
UC-Sampling -1.39 -1.91 -2.37
Embedding Dimensionality. We conduct another ablation study
on the dimensionality of the embedding vectors. Our default embed-
ding dimension is 192, and we tune it between 96 and 384. Results
are illustrated in Table 4. We can see that IC-Sampling is not able
to utilize a larger embedding dimension, and its performance is
worse when the largest dimensionality is used. On the other hand,
UC-Sampling shows consistent improvements when higher dimen-
sional embeddings are used.
5.3 Online Results
Based on the encouraging results on the sampled lab data, we took
the step forward to productionize the proposed techniques in our
recommendation system. On the full-scale production data, we
observed up to 0.6% NCE gains compared to the production ICR
model when UCR models were trained with the standard workflow
using a few days of training data without any architecture changes.
The best version was then tested live in the production system.
A number of infrastructure optimizations were done to make
this happen. For example, we optimize the batching algorithm to
put the same user’s data in one batch for ICR, so the sum (attention)
pooling of the item-centric features only needs to be computed
once and then could be shared within the batch. For UCR, we
do the similar operation to batch the same video’s data together.
With the improvement on data locality, we can lower down the
memory consumption, and in turn improve the throughput for both
training and serving. Also, by using full-precision for training and
lower-precision (e.g., FP16) for inference, we were able to improve
the inference performance (both throughput and latency) without
significant regression in prediction quality (e.g., NCE) and reduce
the number of GPUs required for serving by almost half. The online
A/B experiments showed that quite significant wins were achieved
across a wide range of topline metrics, in particular, one of the key
business metrics, video watch time was improved by 3.24% .
An important observation during our productionization pro-
cess is that the offline NCE gain can be further enlarged when
we increase the amount of training data. In addition, if we scale
up both training data and model complexity, we could potentially
obtain an outsized gain in terms of NCE in offline evaluation. This
investigation is currently in progress.
5.4 Open Questions and Discussions
We are motivated to address the quality saturation problem in rank-
ing. Our expectation is that the UCR formulation should provide
somewhat a remedy. However, from our experiment results, this is
only partially validated. In particular, we did see UCR models lead
0 2 4 6 8 10 12 14 16 18 20 22 24
Hours0.960.970.980.991.001.01Relative NCE
IC Sampling Sum Pooling
UC Sampling Sum Pooling
IC Sampling Attentive Pooling
UC Sampling Attentive PoolingFigure 7: Prediction quality (NCE) of pre-trained models over
the next 24 hours indicates there is a strong distribution drift
in the data.
to consistently better NCE than their ICR counterparts; we also saw
a tendency of improving NCE gain as we increase the training data.
Nonetheless, the NCE gap between UCR and ICR is not as big as
we expected, and also that gap is being enlarged at a much slower
speed, far too slow if we compare it with the model parameter or
collision rate growth curves. This is kind of surprising.
In an attempt to understand the discrepancies, we have a few
plausible explanations.
Firstly, we notice there’s a nontrivial discrepancy between the
full-scaled production data and our sampled lab data. The scaling
characteristics of UCR models are significantly better on production
data than what we observed. This is partly related to the sampling
algorithm we used to generate this data set, and partly related to
the nonlinearity between the complexity that the data manifests
and the scale at which the problem is examined.
Secondly, in the aforementioned areas where scaling has led to
tremendous success, including CV and NLP, the concepts we try
to model are often static. In other words, there’s usually a ground-
truth model in hindsight and the goal of training is to approach that
ground-truth. However, in ranking it is fundamentally different.
There is drastic and frequent distribution drift due to the highly
dynamic two-sided ecosystem and the interactive highly counterfac-
tual nature of the engagement process. Because of the distribution
drift, there is no ground-truth model (or you could say the optimal
model is a moving target instead of static). For example, Figure 7
shows how a pre-trained static model performs in the next 24 hours
after it was trained. We can see a very significant deterioration of
the prediction NCE as the model becomes increasingly outdated. In
a situation where the distribution is drifting dynamically, a model
that scales well and does not saturate quickly in a static context
may not always scale well. To fully combat the obstacles for scaling
ranking models, deep understanding of and the ability to control
such dynamics are critical.
Last but not the least, our current study is limited, without any
changes to the model architecture. We observed, especially for the
smaller-scale lab data set, the absolute NCE values are quite small
and may be close to their limits for the architecture we used. At
Breaking the Curse of Quality Saturation with User-Centric Ranking
the same time, we noticed that ranking model’s architectures are
significantly simpler than what are commonly used in NLP and CV,
which is of course a practical choice given the scales in ranking. We
believe that by using significantly more expressive architectures,
we will be able to improve the scaling property further.
We leave these investigations for future study.
6 SUMMARY
We suspected that the item-centric formulation of ranking mod-
els may be contributing to the quality saturation problems. We
introduced user-centric ranking as an alternative formulation. We
showed that in general, UCR models have a stable model size (i.e.,
total number of parameters) that will not grow as we increase train-
ing data. On a lab data set of sampled production data, we observed
that UCR models yield consistently better prediction quality and
have slightly better scaling property. We did not believe that this
fundamental problem in ranking has been fully solved. We listed a
number of open problems from our study and hope they can spark
further investigations.
ACKNOWLEDGMENTS
We would like to thank the following individuals from Meta for
the collaboration and support: Pei Yin, Hui Zhang, Jason Liu, Xian-
jie Chen, Mingze Gao, Jiyan Yang, Hitesh Kumar, Mert Terzihan,
Nathan Berrebbi, Liang Xiong, Jiaqi Zhai, Shilin Ding. Shuang Yang
is grateful to Jeff Zheng and Junhua Wang from Newsbreak for
many helpful discussions.
REFERENCES
[1] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefeb-
vre. 2008. Fast unfolding of communities in large networks. Journal of statistical
mechanics: theory and experiment 2008, 10 (2008), P10008.
[2]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al .2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877–1901.
[3] Vitor R Carvalho, Jonathan L Elsas, William W Cohen, and Jaime G Carbonell.
2008. A meta-learning approach for robust rank learning. In SIGIR 2008 workshop
on learning to rank for information retrieval , Vol. 1.
[4] Lei Chen, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. 2020. Revisiting
graph based collaborative filtering: A linear residual graph convolutional network
approach. In Proceedings of the AAAI conference on artificial intelligence , Vol. 34.
27–34.
[5] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .
2016. Wide & deep learning for recommender systems. In Proceedings of the 1st
workshop on deep learning for recommender systems . 7–10.
[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-
bastian Gehrmann, et al .2022. Palm: Scaling language modeling with pathways.
arXiv preprint arXiv:2204.02311 (2022).
[7]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Im-
agenet: A large-scale hierarchical image database. In 2009 IEEE conference on
computer vision and pattern recognition . Ieee, 248–255.
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding.
arXiv preprint arXiv:1810.04805 (2018).
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al .2021. An image is worth 16x16 words: Transformers
for image recognition at scale. arXiv preprint arXiv:2010.11929 (2021).
[10] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin,
Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al .
2022. Glam: Efficient scaling of language models with mixture-of-experts. In
International Conference on Machine Learning . PMLR, 5547–5569.[11] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He. 2022. Masked
Autoencoders As Spatiotemporal Learners. arXiv preprint arXiv:2205.09113
(2022).
[12] Maurizio Ferrari Dacrema, Paolo Cremonesi, and Dietmar Jannach. 2019. Are
We Really Making Much Progress? A Worrying Analysis of Recent Neural Rec-
ommendation Approaches. In Proceedings of the 13th ACM Conference on Recom-
mender Systems (Copenhagen, Denmark) (RecSys ’19) . Association for Comput-
ing Machinery, New York, NY, USA, 101–109. https://doi.org/10.1145/3298689.
3347058
[13] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets:
History and Context. ACM Trans. Interact. Intell. Syst. 5, 4, Article 19 (dec 2015),
19 pages. https://doi.org/10.1145/2827872
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition . 770–778.
[15] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng
Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international
conference on world wide web . 173–182.
[16] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine
Atallah, Ralf Herbrich, Stuart Bowers, and Joaquin Quiñonero Candela. 2014.
Practical Lessons from Predicting Clicks on Ads at Facebook. In ADKDD’14 .
[17] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.
2015. Session-based recommendations with recurrent neural networks. arXiv
preprint arXiv:1511.06939 (2015).
[18] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry
Heck. 2013. Learning deep structured semantic models for web search using
clickthrough data. In Proceedings of the 22nd ACM international conference on
Information & Knowledge Management . 2333–2338.
[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Clas-
sification with Deep Convolutional Neural Networks. In Advances in Neural
Information Processing Systems , F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein-
berger (Eds.), Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/
paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
[20] Lam Nguyen, Phuong Ha Nguyen, Marten Dijk, Peter Richtárik, Katya Schein-
berg, and Martin Takác. 2018. SGD and Hogwild! convergence without the
bounded gradients assumption. In International Conference on Machine Learning .
PMLR, 3750–3758.
[21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-
hini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al .
2021. Learning transferable visual models from natural language supervision. In
International Conference on Machine Learning . PMLR, 8748–8763.
[22] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Rad-
ford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation.
InInternational Conference on Machine Learning . PMLR, 8821–8831.
[23] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.
2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep-
resentations from transformer. In Proceedings of the 28th ACM international
conference on information and knowledge management . 1441–1450.
[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[25] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan,
Zicheng Liu, Ce Liu, and Lijuan Wang. 2022. GIT: A Generative Image-to-text
Transformer for Vision and Language. arXiv preprint arXiv:2205.14100 (2022).
[26] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong,
and Ed Chi. 2021. Dcn v2: Improved deep & cross network and practical lessons
for web-scale learning to rank systems. In Proceedings of the Web Conference
2021. 1785–1797.
[27] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.
Neural graph collaborative filtering. In Proceedings of the 42nd international ACM
SIGIR conference on Research and development in Information Retrieval . 165–174.
[28] Yuan Wang, Zhiqiang Tao, and Yi Fang. 2022. A Meta-learning Approach to
Fair Ranking. In Proceedings of the 45th International ACM SIGIR Conference on
Research and Development in Information Retrieval . 2539–2544.
[29] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini,
and Yonghui Wu. 2022. Coca: Contrastive captioners are image-text foundation
models. arXiv preprint arXiv:2205.01917 (2022).
[30] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui
Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through
rate prediction. In Proceedings of the 24th ACM SIGKDD international conference
on knowledge discovery & data mining . 1059–1068."
2201.05236,D:\Database\arxiv\papers\2201.05236.pdf,"In the context of high-dimensional data analysis, where the number of variables exceeds the number of observations, what challenges arise in estimating the covariance matrix and how are these challenges addressed in the proposed method?","Estimating the covariance matrix in high-dimensional settings is problematic due to the limited data available. The proposed method utilizes a regularized covariance matrix estimator that shrinks the full sample covariance matrix towards a diagonal target matrix, ensuring positive definiteness and improving accuracy when the number of variables is significantly larger than the number of observations.","squares linear models on a real world example. Section 4.2 demonstrates the extrapolation control
workﬂow for general machine learning models on a real world example.
2 Methods
2.1 Least squares models
For least squares linear models we chose leverage, a well known metric used to identify outliers
in linear models. Let H=X(XTX)−1XTbe the hat matrix for a linear model with n×pdesign
matrix X.hii, which is the ith diagonal element of H, is the leverage for observation iin the
training data. A typical point will have average leverage, which is1
n∑n
i=1hii=p
n. The leverage for
a new prediction point is computed as hpred=xT
pred(XTX)−1xpred, wherexpredis ap-dimensional
new prediction observation.
One interpretation of hpredis that is the multivariate distance from the center of the training
data in the factor space. Another interpretation is that it is the scaled prediction variance. That
is, as the prediction point moves further away from the center of the data (in a multivariate sense)
the uncertainty of the prediction increases.
There are two criteria commonly used in the statistical literature (Cook, 1977; Bartley et al.,
2019) for determining if a prediction point with leverage hpredshould be considered extrapolation:
•hpred> k·max(hii), wherekis a customizable multiplier. A typical value of kis 1. The
max(hii) is the leverage of the furthest point on the convex hull of the training data in factor
space. When k= 1, prediction points beyond the threshold are outside of the convex hull.
•hpred> l·p
n, wherelis a customizable multiplier. Typical values of lare 2 and 3. Recall
thatp
nis the average leverage for the training data.
2.2 General machine learning models
When selecting a generalized extrapolation control metric for models other than linear models,
we had a number of criteria. First, we needed a metric that was computationally eﬃcient for a
large number of factors and observations. This was necessary to maintain the interactivity of the
proﬁler traces and to preform optimization eﬃciently. Next, we wanted to be able to support
4
Figure 1: Example of a prediction proﬁler for a least squares model ﬁt to the diabetes data
discussed in Section 4.1. Predictions are shown for the response variable on the y-axis. The
model factors are shown on the x-axis. Only a subset of the factors are shown for simplicity.
95% onﬁdence intervals are shown in grey. The desirability function has been selected to increase
linearly with the response. A) Continuous factors are initialized to their mean values in the training
data and categorical factors are initialized to the highest frequency level. B) Factor settings when
the response variable (Y) is maximized. C) Scatterplot for three factor variables. The training
data (grey) and the prediction point where the response is maximized (red). Since total cholesterol
is a function of the sum of HDL and LDL, the high LDL and low total cholesterol observed in the
prediction point is not physically realizable.5
continuous, categorical and ordinal factors. We also wanted to utilize observations with missing
cells, because some modeling methods include these observations in the ﬁt. We wanted a method
that was robust to linear dependencies in the data, these occur when the number of variables is
larger than the number of observations, for example. We also wanted something that was easy to
automate, without much user input. And ﬁnally, we wanted something that was easy to explain
to non-statisticians.
We restricted our focus to extrapolation control methods that were unsupervised. That is, we
only ﬂag a prediction point as extrapolation if it is far outside the distribution of the training data
in factor space. We do this so that our extrapolation control metric will generalize to wide variety
of machine learning models. This was also necessary to be consistent across model proﬁlers, so
that proﬁlers can be linked and models can be ensembled.
We also restrict our consideration to extrapolation control methods that protect against viola-
tions of correlation structure in the training data. This type of extrapolation is the major concern
in the use cases we focused on, where popular methodology is based on T2and square prediction
error (SPE) metrics for principal components analysis and partial least squares models (Eriksson
et al., 2003; Rousseeuw et al., 2006). We discuss future generalizations of this approach to protect
against extrapolations between clusters of data in the discussion.
The multivariate distance interpretation of leverage suggested Hotelling’s T2as a distance met-
ric for general extrapolation control. In fact, there is a monotonic relationship between Hotelling’s
T2and leverage. Since we are no longer in linear models, this metric does not have the same
connection to prediction variance. So instead of relying on the extrapolation thresholds used for
linear models, we make distributional assumptions about T2to determine an upper control limit
to be used as the extrapolation threshold.
The formula for Hotelling’s T2isT2= (x−¯x)TˆΣ−1(x−¯x). The mean and covariance matrix
for the factors are estimated using the training data for the model. If p < n and the factors
are multivariate normal (MVN), then T2
predfor a prediction point has an F distribution: T2
pred∼
(n+1)(n−1)p
n(n−p)F(p,n−p). However, we wanted the method to generalize to data with complex data
types such as a mix of continuous and categorical variables, data sets where p>n , and data sets
where there are a large fraction of missing values. Instead of determining the distribution of T2
pred
in each of these scenarios, we use a simpler and more conservative control limit that we found
6
works well in practice: a 3-sigma control limit that uses the empirical distribution of T2based on
the training data. The control limit, that we use as our extrapolation threshold, can be calculated
by the following equation: UCL =¯T2+ 3ˆσT2where ¯T2+ is the mean of training data T2and ˆσT2
is the standard deviation of the training data T2.
Next, we describe a novel approach to computing the Hotelling’s T2, which we refer to as
Regularized T2. One complication with the standard Hotelling’s T2is that it is undeﬁned when p>
n, because there are too many parameters in the covariance matrix to estimate with the available
data. To address this we use a regularized covariance matrix estimator developed by Sch¨ afer
and Strimmer (2005). This estimator was originally developed for the estimation of covariance
matrices for high-dimensional genomics data. The estimator has been shown to produce a more
accurate covariance matrix estimate when pis much larger than n. The estimator is:
ˆΣ = (1−ˆλ)ˆU+ˆλˆD.
This is simply a weighted combination of the full sample covariance matrix ( ˆU) and a constrained
target covariance matrix ( ˆD). The target matrix we used is a diagonal matrix, with the sample
variances of the factor variables on the diagonal and zeroes on the oﬀ-diagonal. This shrinkage
estimator is guaranteed to be positive deﬁnite when p>n . This is necessary to compute Hotelling’s
T2whenp>n . For the ˆλweight parameter, Schafer and Strimmer derived an analytical expression
that minimizes the mean squared error of the estimator asymptotically.
Schaﬀer and Strimmer proposed several possible target matrices. The diagonal target matrix ˆD
assumes that variables are uncorrelated as the prior. This works well for a general extrapolation
control method, as we do not assume any correlation structure between the variables without
prior knowledge of the data. Also, when there is little data to estimate the covariance matrix,
the elliptical extrapolation control constraints are expanded by the shrinkage estimator with the
diagonal target matrix. This results in a more conservative test statistic for extrapolation control.
That is, when there is little data available to estimate the covariances, tests based on Regularized
T2are less likely to label prediction points as extrapolation. This is sensible, as covariances may
be observed by chance when training data is limited.
Another complication that needed to be addressed was how to compute Hotelling’s T2when
there is missing data. Many predictive modeling methods can utilize observations with missing
7
values for prediction, such as random forests and boosted trees. When the sample size of the
training data is small and there is missing data, it would be ideal to do pair-wise deletion instead
of row-wise deletion to estimate the covariance matrix. This increases the amount of data that
is available to estimate the entries. Let ˆU= ((ukl)) be the covariance matrix estimator. The
following formulas show the pairwise deletion method:
¯xk=∑n
i=1xk
i 1(xk
i̸=NA)
1(xk
i̸=NA)
ukl=∑n
i=1(xk
i−¯xk)(xl
i−¯xl) 1(xk
i̸=NA,xl
i̸=NA)
1(xk
i̸=NA,xl
i̸=NA)
Previously, the proﬁler allowed constrained optimization with linear constraints. Since extrap-
olation control is a non-linear constraint, the optimization problem is more challenging. A genetic
algorithm has been implemented to perform the optimization.
3 Simulation Studies
To evaluate the extrapolation control performance of Regularized T2, we performed a simulation
study. First, we simulate a factor matrix with a low rank approximation. We do this to evaluate
our ability to detect violations of the correlation structure in the data:
Xn×p=Un×rDr×p+en×1
whereris the desired the rank, and each element of U,D, and eis i.i.d. standard normal ( Figure
2).
For each simulated data set, we chose the pair of most correlated variables. We then extend
a grid of equally spaced points from the center of the data to the corner of the box constraint
in the proﬁler, which is the range of the factors according to the training data. Since the factor
variables are multivariate normal (MVN) with known covariance matrix, T2isχ2-distributed. We
then use the probability that T2
predfor a grid point is from the same distribution as the data to
classify points as extrapolation or not extrapolation:


“extrapolation” , ifP(T2≥T2
pred)<.05
“not extrapolation” ,ifP(T2≥T2
pred)≥.05
8
Figure 2: Example grid of prediction points classiﬁed as extrapolation (red) or not extrapolation
(green).
9
To evaluate Regularized T2extrapolation control in terms of both false positive rate (FPR)
and true positive rate (TPR), we simulated data with increasing sample sizes using the true MVN
distribution. We then used the simulated data to compute a 3-sigma limit, and we determined
how well we were able to classify the new prediction points as extrapolated or not. For all of the
scenarios shown in Figure 3 the FPR is less than .05, so only the TPR for extrapolated points
are shown. On the x-axis, the new prediction points along the grid are ranked by the extent of
extrapolation. Along the panels, the number of variables and the rank of the factor matrix is
varied. Figure 3A shows a case when there are 20 factor variables with rank 10. Because the
threshold is conservative, notice that the TPR is low when the extrapolation is mild. When there
is limited training data, the Regularized T2is less likely to label predictions as extrapolation.
This is desirable because covariances may be observed by chance when training data is limited.
However, the TPR does improve the larger the sample size.
Also note that the TPRs are high in all cases when p= 20 andn= 20. This is when p=n,
and when the rank of the covariance matrix, rcov, is equal to n−1. In these cases, Hotelling’s
T2is undeﬁned, since the covariance is not full rank. If a generalized inverse, such as the Moore-
Penrose inverse, is used to invert the covariance matrix, then the Hotelling’s T2for all training set
observations will be a constant, (2 rcov)1/2. Our 3-sigma control limit for Hotelling’s T2will be too
large are too small depending on rcov.Figure S.1 shows a case when the Hotelling’s T2control
limit is too small, leading to extrapoltion control with high FPRs, and a case when the limit
is too large, leading to extrapolation control with low TPRs. Regularized T2achieves improved
extrapolation control in both cases.
Whenp= 100 andr= 25 ( Figure 3B ), the TPRs are low when pis much larger than n. This
is a hard problem for distance based methods due to the curse of dimensionality, but the TPRs
do improve as the sample size increases. Figure 3C-D shows how TPR is aﬀected by varying
rank with ﬁxed p. The TPR is better when the intrinsic dimensionality of the data is low.
We also evaluated Regularized T2extrapolation control on simulated data with a mix of con-
tinuous and categorical variables ( Figure 4 ). We simulated an entirely continuous factor matrix
using the same procedure as previously described. We then selected a number of variables ( pcat) to
transform into categorical variables by randomly selecting the number of categories from 2 to 4 for
each variable and using equally spaced quantiles to discretize each variable. Figure 4 shows two
10
0.00.20.40.60.81.0
12345678
DistanceTrue Positive Rate20 continuous, rank = 10 A
0.00.20.40.60.81.0
1234567
DistanceTrue Positive Rate100 continuous, rank = 25 B
0.20.40.60.81.0
12345678
DistanceTrue Positive Rate20 continuous, rank = 5 C
0.00.20.40.60.81.0
12345678
DistanceTrue Positive Rate20 continuous, rank = 15 D
n = 20 n = 200 n = 2000Figure 3: True positive rates of classiﬁcation of extrapolated points in simulated data consisting
of entirely continuous factors. Various sample sizes, number of factors and rank of factor matrix
are shown. The new prediction points are ranked by the extent of extrapolation. 100 simulation
replicates were performed. Error bars show 95% conﬁdence intervals.
11"
2303.13592,D:\Database\arxiv\papers\2303.13592.pdf,"The paper discusses the ability of large language models to generate code-mixed text, highlighting the challenges and potential benefits of this technology.  Considering the diverse linguistic landscape of Southeast Asia, what are the potential implications of this technology for language preservation and revitalization efforts in the region?","The ability of large language models to generate code-mixed text could be a valuable tool for language preservation and revitalization efforts in Southeast Asia, particularly for under-resourced languages.  By providing access to synthetic data and resources, these models could help to create new learning materials and tools, fostering greater awareness and engagement with endangered languages.","1
Loanwords2
Topic-Related
Nouns3
Linguistic
Elements020406080100Percent of Prompts(a) Languages
Chinese
Indonesian
Malay
Tagalog
Tamil
Vietnamese
1
Loanwords2
Topic-Related
Nouns3
Linguistic
Elements(b) Topics
AI
family
food
traffic
weather
1
Loanwords2
Topic-Related
Nouns3
Linguistic
Elements(c) Templates
assume as bilingual speaker
explicitly define CM
imitate speaking style
native speaker
two bilingual speakers
write a CM sentenceFigure 4: Analysis of code-mixed data generated by ChatGPT.
才到了办公室.” (Chinese: “The traffic today
is really terrible. I spent an hour driving to get
to the office.”)
•3 - Linguistic Elements: The generated
text mixes linguistic elements beyond loan-
words and topic-related nouns at the phrasal
or clausal level. One example is verb phrases:
“My family ay nagplano ng isang malaking
family reunion sa park this coming weekend.”
(Tagalog: “My family has planned a big fam-
ily reunion at the park this coming weekend.”)
This category also includes intraword code-
mixing, e.g., “Kapag busy ang trapiko, mag-
ingat ka sa pagda-drive4para maiwasan mo
ang mga masamang pangyayari.” (Tagalog:
“When traffic is busy, be careful while driving
to avoid accidents.”)
We use this scale instead of popular word-level
metrics such as CMI (Gamb ¨ack and Das, 2014)
because our scale more holistically evaluates the
ability of LLMs to code-mix. The lower end of this
scale reflects a lower complexity of code-mixing.
Code-mixing with loanwords is arguably less chal-
lenging, as they are often used in a monolingual
context to begin with. Likewise, code-mixing
topic-related nouns is not as complex as there is
presumably a correspondence between the nouns
in the two languages and is primed by the prompts.
On the other hand, code-mixing prefixes/suf-
fixes, phrases and clauses requires a good grasp
of the intricate morphosyntactic structures of both
languages and can produce syntactically diverse
code-mixed data. Therefore, we consider the LLM
to have successfully generated code-mixed text only
if the text belongs to this category.
4The prefix “pag-” in Tagalog is affixed to the English
word “drive”, resulting in the word “pagda-drive” (the act
of driving). This example demonstrates the application of
Tagalog inflection rules to English words.Naturalness
We asked native speakers to annotate the natural-
ness of the generated text on a rating scale of 1—
not natural at all, 2—an annotator may see someone
else from other regions/cultures speaking it, and
3—an annotator may see themselves speaking it. If
the text is not considered natural (rating of 1), we
requested the annotators to document their reasons,
including fluency issues, syntactical errors, and
semantic incorrectness.
Code-mixing practices vary across race, gender,
class, and geography, among other axes. Therefore,
in line with sociolinguistic research, we emphasize
that the “naturalness” of code-mixed text (as per-
ceived by an annotator) is subjective, and depends
on the background and social location of the anno-
tator. All annotators are AI researchers and native
speakers of their respective SEA languages. We
refer to Appendix G for more information about
our annotators and their inter-annotator agreement
scores.
Accurateness
We additionally annotate the accurateness of the
generations to account for task failure or incorrect
explanations. Inaccurate outputs were defined by:
•Failure to follow instructions: Some LLMs
fail to accurately carry out the prompt instruc-
tion (for example, generating monolingual
sentences). We also observe some code-mixed
outputs that include additional languages un-
specified in the prompts.
•Inaccurate explanations: ChatGPT, among
other LLMs, tends to explain the code-mixed
phrases, but occasionally provides incorrect
explanations (see Figure 10).
Imagine there are two English-Vietnamese bilingual
speakers talking to each other, can you imagine a conver-
sation between them talking about family?
(ChatGPT) Person A: Hi, how’s your family doing?
Person B: Chào bạn, gia đình tôi đang khỏe. Còn gia đình
bạn thì sao?
Person A: They’re good, thanks for asking. How about
yours?
Person B: Gia đình tôi cũng khỏe, cảm ơn bạn. Mới đây,
tôi đã đi chơi với ông bà và các em họ của mình.
Imagine there are two English-Chinese bilingual speakers
talking to each other, can you imagine a conversation
between them talking about weather?
(ChatGPT) English Speaker 1: Hey, have you been out-
side today? It’s really hot!
Chinese Speaker 1: 是啊，今天天气真的很热。
English Speaker 2: Yeah, it’s been pretty sunny all day.
Chinese Speaker 2: 是啊，今天一整天都很晴朗。
Figure 5: Failure cases when we prompt ChatGPT to
imagine two bilingual speakers conversing about a spe-
cific topic.
3 Results
3.1 English-SEA Languages
Figure 3 demonstrates that ChatGPT outperforms
other language models in generating code-mixed
data across six different language pairs, and is par-
ticularly proficient at code-mixing linguistic ex-
pressions. For InstructGPT, the davinci-003 model
demonstrates some ability to generate syntacti-
cally varied code-mixed texts, whereas davinci-002
mostly code-mixes with loanwords. Conversely,
the other two publicly available multilingual LLMs
have extremely limited code-mixing capabilities.
More specifically, Flan-T5-XXL can only code-
mix with loanwords for the Indonesian-English lan-
guage pair, and most of its non-English monolin-
gual outputs suffer from severe fluency issues (see
Appendix D). BLOOMZ is only capable of code-
mixing topic-related nouns for Tamil-English even
though its multilingual pretraining data ROOTS
(Lauren c ¸on et al., 2022) and instruction-tuning data
xP3 (Muennighoff et al., 2022) cover Indonesian,
Chinese, Tamil, and Vietnamese. We observe no
direct effects of the proportions of these languages
in the training sets on BLOOMZ’s ability to code-
mix (Appendix E).
We further break down the performance of Chat-
GPT in Figure 45. In Figure 4(a), we see that
5Detailed analysis for davinci-002, davinci-003, Flan-T5-
XXL and BLOOMZ can be found in the Appendix (Figure 11,
Figure 12, Figure 13 and Figure 14).ChatGPT is least proficient at mixing linguistic
elements for English-Tagalog. This may be due to
syntactic differences between the two languages;
for example, English exhibits Subject-Verb-Object
(SVO) word order, whereas Tagalog exhibits a
verb-initial structure. Moreover, English demon-
strates nominative-accusative alignment, whereas
Tagalog, being a symmetrical-voice language, uti-
lizes a case system with a typological classifica-
tion that “remains controversial among Austrone-
sian linguists” (Aldridge, 2012, 192). In contrast,
ChatGPT performs the best for English-Indonesian
code-mixing, which may be due to training data
distribution and similarities between the two lan-
guages regarding word order and morphosyntactic
alignment. We also find that ChatGPT is capable
of using either English or a SEA language as the
matrix language, i.e., as the main language of a
sentence as per the Matrix Language Frame model
(Myers-Scotton, 1997).
Figure 4(b) shows ChatGPT’s code-mixing pro-
ficiency based on topics. ChatGPT tends to code-
mix with loanwords when the topic is about “AI”
by mixing the English loanwords “Artificial In-
telligence,” or its short form “AI.” For food, it
tends to code-mix with food-related terms—which
are topic-related nouns—in SEA languages such
as “bánh mì” (Vietnamese sandwich). We also
observe some representation biases in specific
language-topic pairs. For instance, when it comes
to food, ChatGPT uses the word “nasi goreng”
(fried rice) for all English-Indonesian responses.
For other topics, such as traffic and weather, it tends
to code-mix phrases related to traffic congestion
and hot weather.
In Figure 4(c), we find the prompt template with
the highest quality results is the one where the
term code-mixing is explicitly defined. In contrast,
the worst-performing template consists of asking
the model to generate conversations between two
bilingual speakers, where the term code-mixing is
unmentioned. In Figure 5, we see that ChatGPT
generates an uncommon pattern of conversations
where one interlocutor speaks in English and the
other speaks in another language entirely (top ex-
ample). Furthermore, ChatGPT may assume there
are four speakers though the prompt asks for a con-
versation between two speakers (bottom example).
In terms of naturalness, we observe a consider-
able variance in ChatGPT’s ouputs, with English-
Tamil being the least natural (Figure 6). Further
Chinese
IndonesianMalayTagalogTamil
VietnameseSinglish0.00.20.40.60.81.0Percent of PromptsNon-CM
Not NaturalSemi-Natural
NaturalFigure 6: Naturalness of code-mixed (CM) text gener-
ated from ChatGPT. “Semi-Natural” indicates that the
annotators see someone else from other region/culture
speaking it, whereas “Natural” indicates that the anno-
tators see themselves speaking it.
ChatGPT InstructGPT
(davinci-003)InstructGPT
(davinci-002)Flan-T5-XXL BLOOMZ020406080100Percent of Prompts
Figure 7: Models’ performance in generating Singlish
expressions through zero-shot prompting.
analysis shows that ChatGPT either commits gram-
matical mistakes (such as comma splice and redun-
dancy) or generates semantically confusing sen-
tences. We also observe unnatural text patterns
that mix two different script systems of the same
language in the single sentence (for example, the
Tamil script and its transliterated Latin script). We
document these naturalness and fluency issues in
Table 4 (Appendix F) and report the interannotator
agreement scores in Appendix G.
3.2 Singlish
Imagine that you are a Singaporean speaker. How would
you talk about food using Singlish in a single sentence?
(ChatGPT) “Wah, the hawker center got so many sedap
food, like chicken rice, laksa, and rojak, can die for sia!”
(Translation: “Wow, the hawker center has so many deli-
cious foods, like chicken rice, laksa, and rojak, they are to
die for!”)
Figure 8: Singlish sentence and its translation to Stan-
dard English, both generated by ChatGPT.Given that Singlish is an English-based creole
language that has lexicalized a lot of words from
many languages such as Malay, Chinese, Tamil,
Cantonese, and Hokkien (e.g., “lah,” “leh,” and
“loh,” etc.), it is difficult to disambiguate loan-
words and linguistic elements. Therefore, we clas-
sify the model success based on a binary scale of
whether the generated sentences contain Singlish
expressions (e.g., interjections, lexicalized phrases
of languages without a standardized writing sys-
tem)6. In Figure 7, we see that ChatGPT and In-
structGPT (davinci-003) have up to a 96% suc-
cess rate in generating Singlish sentences, whereas
Flan-T5-XXL and BLOOMZ have a near-zero suc-
cess rate. Furthermore, we find that ChatGPT is
also capable of translating Singlish expressions into
Standard American English (Figure 8).
While ChatGPT is capable of generating flu-
ent and natural Singlish (Figure 6), we observe
semantic inaccuracies in their word choices that
are discernible by native speakers. In Figure 2(b),
ChatGPT generates an English-Malay code-mixed
phrase “sotong and chilli sauce” where “sotong” is
a Malay word for “squid.” Upon first inspection,
the sentence appears syntactically sound but native
speakers found the analogy between familial con-
nections and the dish (squid with chili sauce) to be
semantically confusing.
OpenAI’s documentation of model differ-
ences7—more specifically, that davinci-003 can
produce higher-quality writing, process more com-
plex instructions, and generate longer content than
davinci-002—does not adequately explain why
ChatGPT and davinci-003 significantly outperform
davinci-002 and other multilingual LLMs in gen-
erating Singlish text. We hypothesize that the per-
formance gap may be due to the larger presence of
Singlish in OpenAI’s training data. Singlish is not
only one of the most well-researched dialects of
English (Sin, 2017), but it also uses the English al-
phabet and Latin script systems (unlike many other
SEA languages), which potentially makes it easier
for LLMs to scrape and parse data in this language.
3.3 Failures of ChatGPT
We notice that ChatGPT may fail to produce cor-
rectly code-mixed text when the nationality of the
speaker is mentioned. Invoking nationality may
6A reference list can be found at https://en.wikipedia.o
rg/wiki/Singlish_vocabulary
7https://help.openai.com/en/articles/6779149-how
-do-text-davinci-002-and-text-davinci-003-differ
Imagine that you are a Malaysian speaker. How would
you talk about traffic using both English and Chinese in a
single sentence?
(ChatGPT) Assuming that you are asking for a sentence
that combines both English and Chinese in a natural and
commonly used way by Malaysian speakers, here is an
example:
“今天的traffic很teruk，我要赶紧走去KLCC for meet-
ing。”
This sentence incorporates the English word “traffic” and
theChinese word “teruk” (which means “bad” or “terrible”
in English) to describe the situation of the traffic being
very bad. The speaker then switches back to English to
express their intention to hurry to KLCC for a meeting.
This kind of code-switching is common in Malaysia, where
speakers often mix languages in their daily conversations.
Figure 9: ChatGPT’s failure to follow the prompt. Un-
derlined texts show its language misclassification.
induce ChatGPT to generate code-mixed texts with
anadditional national language of the country. In
the Figure 9 example, because the national lan-
guage of Malaysia is Malay8, ChatGPT not only
returns a code-mixed response that includes the
Malay word “teruk” when prompted for English-
Chinese utterance by a Malaysian, but also mis-
classifies the word “teruk” as Chinese.
While ChatGPT can explain how its generated
text is code-mixed (as illustrated in Figure 2(a)),
we find its justifications unreliable for two reasons.
First, it may hallucinate words that do not exist in
the code-mixed text. For instance, in the top exam-
ple in Figure 10, the English word “cold” does not
exist in the code-mixed text. Second, it may assign
words or phrases to the wrong language. In the bot-
tom example in Figure 10, the phrase “nak minum
air sejuk je” is in Malay, not English. This obser-
vation corroborates Bang et al.’s (2023) finding
suggesting ChatGPT performs poorly on language
identification tasks for low-resource languages.
4 Implications for LLMs and Ways
Forward
In this section, we walk through various implica-
tions of our work for understanding the deploy-
ment of LLMs for synthetic data generation in low-
resource settings, multilinguality in LLMs, impor-
tance of code-mixing ability in generative LLMs,
and research transparency.
Deploying LLMs for Low-Resourced Data Gen-
eration By putting LLMs’ generative capabilities
to the test, we ask in this work if they can gener-
ate high-quality and low-cost code-mixed texts for
8Mandarin Chinese is widely spoken in the country.Imagine that you are an Indonesian speaker. How would
you talk about weather using both English and Indonesian
in a single sentence?
(ChatGPT) If I were to talk about weather using both
English and Indonesian in a single sentence, I might say
something like: “ Hari ini sangat hot dan panas sekali, saya
rasa aku butuh minum air dingin. ”This sentence includes
theEnglish words “hot” and“cold” to describe the weather,
and the Indonesian words “panas” and “dingin” which have
similar meanings. Additionally, the sentence also includes
a personal statement in Indonesian, “saya rasa aku butuh
minum air dingin,” which means “I feel like I need to drink
cold water.”
Code-mixing refers to a phenomenon of combining two or
more languages in a single sentence. Can you generate
a code-mixed English-Malay sentence about weather?
(ChatGPT) “The weather hari ni sangat panas, I rasa
macam nak minum air sejuk je. ” (Translation: “The
weather today is very hot, I feel like drinking cold wa-
ter.”)
In this sentence, “hari ni” and “sangat panas” are in Malay,
while “I” and “nak minum airsejukje”areinEnglish.
Figure 10: Underlined texts show wrong explanations
about the code-mixed text. We italicize the code-mixed
sentences to make it explicit to the reader.
researchers working on a topic plagued by limited
data availability. While we conclude that Chat-
GPT has shown relative success in generating code-
mixed texts for some SEA languages, we advise re-
searchers to exercise heavy caution when using this
data generation technique. Even for Singlish, which
outperforms the other languages examined, we find
that syntactically-sound responses may contain se-
mantic inaccuracies that are difficult for non-native
speakers to detect. Furthermore, its explanations
may be misleading. Due to the lack of reliability,
we strongly suggest researchers to implement ex-
tensive human checks with native speakers if they
wish to pursue this method of data generation.
Multilingual ̸=Code-Mix Compatible Our re-
sults with BLOOMZ and Flan-T5-XXL show that
the ability to code-mix is not acquired by LLMs af-
ter pretraining and/or finetuning with multilingual
data (Lauren c ¸on et al., 2022; Muennighoff et al.,
2022; Chung et al., 2022). In other words, for most
NLP models, multilinguality simply means that the
same system can process tasks and generate out-
puts in multiple languages, but not necessarily in
the same sentence. By highlighting this limitation,
we echo previous research motivating the inclusion
of code-mixing abilities in NLP models. Doing so
requires NLP models to capture the dynamics of
combining languages that have different degrees
of typological affinities, as well as pragmatic and
contextual features such as tone, formality, and
other cultural nuances (Winata et al., 2020; Lai and
Nissim, 2022; Kabra et al., 2023).
Towards More Inclusive Language Technology
Recognizing that generative LLMs are the primary
driving force behind the advancement of AI conver-
sational agents and speech technology (Thoppilan
et al., 2022; SambaNova Systems, 2023; Pratap
et al., 2023), we emphasize the significance of
incorporating code-mixed output recognition and
generation capabilities in LLMs in order to en-
hance the inclusivity and humaneness of language
technology. By enabling conversational agents to
reflect the language-mixing patterns of the users,
people can communicate in ways that are more
comfortable and authentic to their linguistic iden-
tities. In fact, a recent study by Bawa et al. (2020)
has shown that multilingual users strongly prefer
chatbots that can code-mix. Removing the need
for people to adjust their speech patterns to be-
come legible to machines would not only mitigate
the effects of linguistics profiling (Baugh, 2005;
Dingemanse and Liesenfeld, 2022) and hegemonic,
Western-centric technological designs, but also en-
able users to develop more trust with language tech-
nology through naturalistic dialogue interactions.
Research Transparency Aside from showing
that ChatGPT and InstructGPT cancode-mix, we
cannot confidently identify howthe models do so
due to the lack of transparency in how these sys-
tems are developed. Without a window into train-
ing data and engineering processes that went into
models like ChatGPT, we can only speculate that
their training data includes a substantial amount of
code-mixed texts. To help facilitate greater levels of
transparency and accountability, we urge forthcom-
ing LLMs to be more open about how the models
were developed and to document accurately and
comprehensively the training data used.
5 Related Work
Code-Mixed Data in SEA Unlike monolin-
gual data, there is only a limited number of
human-curated code-mixed datasets. This re-
source limitation is more severe in SEA due to
its marginalization in NLP research (Winata et al.,
2022). Popular current code-mixing evaluation
benchmarks (Aguilar et al., 2020; Khanuja et al.,
2020) do not include SEA languages, and ex-isting code-mixing studies in SEA only cover a
limited number of language pairs and creoles,
e.g., English-Tagalog (Oco and Roxas, 2012),
English-Indonesian (Barik et al., 2019; Yulianti
et al., 2021), Javanese-Indonesian (Tho et al.,
2021), Chinese-English (Lyu et al., 2010; Love-
nia et al., 2022; Zhang and Eickhoff, 2023) and
Singlish (Chen and Min-Yen, 2015; Lent et al.,
2021)9. The current corpus does not even scratch
the surface of the sheer amount of code-mixedness
in SEA (Redmond et al., 2009), where deployable
data is practically non-existent. In this work, we
try to close this gap by exploring the potential of
generating synthetic code-mixed data for the SEA
region by prompting LLMs.
Synthetic Code-Mixing Generation of synthetic
code-mixed data to address data scarcity problem
has been previously explored. Solorio and Liu
(2008), Winata et al. (2019), and Tan and Joty
(2021) have attempted to generate synthetic code-
mixed sentences through word alignment and can-
didate selection from a parallel corpus. Liu et al.
(2020) and Adilazuarda et al. (2022) have similarly
generated synthetic code-mixed sentences by re-
placing words in monolingual sentences with their
machine-translated counterparts, whereas Pratapa
et al. (2018), Rizvi et al. (2021) and Santy et al.
(2021) leveraged parse tree structure for such re-
placements. Another approach is to perform neural
machine translation to translate monolingual sen-
tences to code-mixed ones (Appicharla et al., 2021;
Gautam et al., 2021; Jawahar et al., 2021; Dowlagar
and Mamidi, 2021). In this work, we assess a novel
way of generating synthetic code-mixed sentences
through prompting multilingual LLMs.
6 Conclusion
To ameliorate the scarcity of code-mixed data for
South East Asian languages, we explore generat-
ing synthetic code-mixed data using state-of-the-art
multilingual Large Language Models (LLMs). On
one hand, we find that publicly available LLMs
such as BLOOMZ and Flan-T5-XXL have lim-
ited capability in generating syntactically diverse
code-mixed data. On the other hand, closed-source
models such as ChatGPT and InstructGPT are bet-
ter at generating natural code-mixed text, but their
performance varies substantially depending on the
9To exacerbate the situation, some of the SEA code-mixed
datasets are no longer publicly available.
prompt template and language pairing. Further-
more, many outputs suffer from syntactic, seman-
tic, and reliability issues. Therefore, we caution
against using LLM-generated synthetic code-mixed
data without the involvement of native speakers for
annotating and editing.
7 Limitations
7.1 Effectiveness of Synthetic Code-Mixed
Data on Downstream Tasks
In our study, we did not evaluate how much our syn-
thetically generated code-mixed data improve the
ability of language models to handle code-mixed
text in downstream NLP tasks. While previous
findings have shown that finetuning models with
synthetic code-mixed data yields less performance
gains than with naturally occurring code-mixed
data (Santy et al., 2021), we believe that this perfor-
mance gap will diminish as the quality of synthetic
data generation gets better with future multilingual
LLMs.
7.2 Lack of Human-Generated Data
While we annotated the degree of code-mixedness
and naturalness, we did not have human-generated,
naturally occurring, code-mixed sentences in re-
sponse to the prompt topics. Therefore, we could
not systematically compare the data distribution
of our synthetic data against the human-generated
data. However, since there are multiple ways in
which a sentence can be code-mixed, our focus in
this work is on how human-like are the sentences,
and this, we believe, was adequately captured by
our evaluation.
7.3 Monolingual Zero-Shot Prompting
Our study only uses prompt templates written in
English to prompt language models in a zero-shot
manner. In future follow-ups, we will (1) use
code-mixed prompt templates such as “Generate
an English-Bahasa sentence” instead of “Gener-
ate an English-Malay sentence” and (2) investigate
LLMs’ capabilities in generating code-mixed data
with in-context few-shot examples.
7.4 Instruction-Tuned Language Models
Our work only covers instruction-tuned language
models. In future work, we will include a com-
parison between multilingual models that are not
finetuned with instructions—for example, GPT3
(davinci) (Brown et al., 2020) and BLOOM (Scaoet al., 2022)—to explore the effects of instruction
tuning in generating code-mixed data.
7.5 English-Centric Code-Mixing
Our study focuses on generating code-mixed data
only for English-SEA language pairs. For future
studies, we plan to investigate generating code-
mixed data for non-English language pairs com-
monly spoken in SEA countries (such as Malay-
Chinese and Indonesian-Javanese).
7.6 Failures of BLOOM and Flan-T5-XXL
Given the lack of research transparency on why
ChatGPT performs better at code-mixed text gener-
ation, we assume that the publicly available models
such as BLOOM and Flan-T5-XXL are unable to
code-mix due to the lack of code-mixed texts in the
pretraining corpora and code-mixing tasks in the
instruction-tuning datasets. Further investigation is
warranted to understand the effects of code-mixed
text in pretraining and instruction-tuning data on
code-mixed text generation.
7.7 Presence of Synthetic Code-Mixed Data in
Future Pretraining Data
As we advocate for the code-mixing ability in fu-
ture generations of LLMs, we are aware of the
potential risks of data feedback , where genera-
tive models that recursively train on data gener-
ated by previous generations may amplify biases
and lose information about the tails of the origi-
nal distribution (Shumailov et al., 2023; Taori and
Hashimoto, 2022). Since these negative effects
can be mitigated through human-generated content
(Shumailov et al., 2023), it becomes imperative for
the NLP community to collect natural code-mixed
data for low-resource languages.
8 Ethical Considerations
Code-mixing reflects the linguistic, social, and cul-
tural identity of a multilingual community. Re-
searchers and practitioners should approach syn-
thetic code-mixing with sensitivity and respect, and
be cognizant of the potential risks of cultural ap-
propriation or misrepresentation when generating
code-mixed data using LLMs. Since LLMs are
trained on web data, they may encode biases per-
petuating stereotypes, discrimination, or marginal-
ization of specific languages or communities. Prior
work has also documented how synthetic data may
play a role in feedback loops that amplify the
presence of biased language generation (Taori and
Hashimoto, 2022). Therefore, collaboration with
linguists, language experts, and community repre-
sentatives is necessary to avoid the unintentional
perpetuation of stereotypes and cultural insensitiv-
ity.
References
Muhammad Farid Adilazuarda, Samuel Cahyawijaya,
Genta Indra Winata, Pascale Fung, and Ayu Purwari-
anti. 2022. IndoRobusta: Towards robustness against
diverse code-mixed Indonesian local languages. In
Proceedings of the First Workshop on Scaling Up
Multilingual Evaluation , pages 25–34, Online. Asso-
ciation for Computational Linguistics.
Gustavo Aguilar, Sudipta Kar, and Thamar Solorio.
2020. LinCE: A centralized benchmark for linguis-
tic code-switching evaluation. In Proceedings of the
Twelfth Language Resources and Evaluation Confer-
ence, pages 1803–1813, Marseille, France. European
Language Resources Association.
Alham Fikri Aji, Genta Indra Winata, Fajri Koto,
Samuel Cahyawijaya, Ade Romadhony, Rahmad Ma-
hendra, Kemal Kurniawan, David Moeljadi, Radi-
tyo Eko Prasojo, Timothy Baldwin, Jey Han Lau,
and Sebastian Ruder. 2022. One country, 700+ lan-
guages: NLP challenges for underrepresented lan-
guages and dialects in Indonesia. In Proceedings
of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 7226–7249, Dublin, Ireland. Association for
Computational Linguistics.
Edith Aldridge. 2012. Antipassive and ergativity in
tagalog. Lingua , 122:192–203.
Douglas G Altman. 1990. Practical statistics for medi-
cal research . CRC press.
Ramakrishna Appicharla, Kamal Kumar Gupta, Asif
Ekbal, and Pushpak Bhattacharyya. 2021. IITP-MT
at CALCS2021: English to Hinglish neural machine
translation using unsupervised synthetic code-mixed
parallel corpus. In Proceedings of the Fifth Work-
shop on Computational Approaches to Linguistic
Code-Switching , pages 31–35, Online. Association
for Computational Linguistics.
Eric Armstrong. Tamil and tamil-english accent.
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-
liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei
Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multi-
task, multilingual, multimodal evaluation of chatgpt
on reasoning, hallucination, and interactivity. arXiv
preprint arXiv:2302.04023 .
Anab Maulana Barik, Rahmad Mahendra, and Mirna
Adriani. 2019. Normalization of Indonesian-English
code-mixed Twitter data. In Proceedings of the 5thWorkshop on Noisy User-generated Text (W-NUT
2019) , pages 417–424, Hong Kong, China. Asso-
ciation for Computational Linguistics.
John Baugh. 2005. Linguistic profiling. In Black lin-
guistics , pages 167–180. Routledge.
Maria Lourdes S Bautista and Andrew B Gonzalez.
2006. Southeast asian englishes. The handbook of
world Englishes , pages 130–144.
Anshul Bawa, Pranav Khadpe, Pratik Joshi, Kalika Bali,
and Monojit Choudhury. 2020. Do multilingual users
prefer chat-bots that code-mix? let’s nudge and find
out! Proceedings of the ACM on Human-Computer
Interaction , 4(CSCW1):1–23.
Susan Berk-Seligson. 1986. Linguistic constraints on
intrasentential code-switching: A study of span-
ish/hebrew bilingualism. Language in society ,
15(3):313–348.
T. K. Bhatia and W. C. Ritchie. 2004. Social and psy-
chological factors in language mixing. In W. C.
Ritchie and T. K. Bhatia, editors, Handbook of bilin-
gualism , pages 336–352. Blackwell Publishing.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Samuel Cahyawijaya, Holy Lovenia, Alham Fikri
Aji, Genta Indra Winata, Bryan Wilie, Rahmad
Mahendra, Christian Wibisono, Ade Romadhony,
Karissa Vincentio, Fajri Koto, Jennifer Santoso,
David Moeljadi, Cahya Wirawan, Frederikus Hudi,
Ivan Halim Parmonangan, Ika Alfina, Muham-
mad Satrio Wicaksono, Ilham Firdausi Putra, Samsul
Rahmadani, Yulianti Oenang, Ali Akbar Septian-
dri, James Jaya, Kaustubh D. Dhole, Arie Ardiyanti
Suryani, Rifki Afina Putri, Dan Su, Keith Stevens,
Made Nindyatama Nityasya, Muhammad Farid
Adilazuarda, Ryan Ignatius, Ryandito Diandaru,
Tiezheng Yu, Vito Ghifari, Wenliang Dai, Yan Xu,
Dyah Damapuspita, Cuk Tho, Ichwanul Muslim Karo
Karo, Tirana Noor Fatyanosa, Ziwei Ji, Pascale Fung,
Graham Neubig, Timothy Baldwin, Sebastian Ruder,
Herry Sujaini, Sakriani Sakti, and Ayu Purwarianti.
2022. Nusacrowd: Open source initiative for indone-
sian nlp resources.
Joyce YC Chan, Houwei Cao, PC Ching, and Tan Lee.
2009. Automatic recognition of cantonese-english
code-mixing speech. In International Journal of
Computational Linguistics & Chinese Language Pro-
cessing, Volume 14, Number 3, September 2009 .
Su-Chiao Chen. 1996. Code-switching as a verbal strat-
egy among chinese in a campus setting in taiwan.
World Englishes , 15(3):267–280.
T Chen and Kan Min-Yen. 2015. The national university
of singapore sms corpus.
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language mod-
els.arXiv preprint arXiv:2210.11416 .
Commonwealth of the Philippines. 1936. Common-
wealth act no. 184: Govph.
Mona Diab, Julia Hirschberg, Pascale Fung, and Thamar
Solorio, editors. 2014. Proceedings of the First
Workshop on Computational Approaches to Code
Switching . Association for Computational Linguis-
tics, Doha, Qatar.
Mark Dingemanse and Andreas Liesenfeld. 2022. From
text to talk: Harnessing conversational corpora for
humane and diversity-aware language technology.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 5614–5633, Dublin, Ireland.
Association for Computational Linguistics.
A. Seza Do ˘gru¨oz, Sunayana Sitaram, Barbara E. Bul-
lock, and Almeida Jacqueline Toribio. 2021. A sur-
vey of code-switching: Linguistic and social per-
spectives for language technologies. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 1654–1666, Online.
Association for Computational Linguistics.
Suman Dowlagar and Radhika Mamidi. 2021. Gated
convolutional sequence to sequence based learning
for English-hingilsh code-switched machine transla-
tion. In Proceedings of the Fifth Workshop on Com-
putational Approaches to Linguistic Code-Switching ,
pages 26–30, Online. Association for Computational
Linguistics.
Joseph L Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological bulletin ,
76(5):378.
Bj¨orn Gamb ¨ack and Amitava Das. 2014. On measuring
the complexity of code-mixing. In Proceedings of the
11th international conference on natural language
processing, Goa, India , pages 1–7.
Devansh Gautam, Prashant Kodali, Kshitij Gupta, An-
mol Goel, Manish Shrivastava, and Ponnurangam
Kumaraguru. 2021. CoMeT: Towards code-mixed
translation using parallel monolingual sentences. In
Proceedings of the Fifth Workshop on Computational
Approaches to Linguistic Code-Switching , pages 47–
55, Online. Association for Computational Linguis-
tics.
Cliff Goddard. 2005. The languages of East and South-
east Asia: an introduction . Oxford University Press
on Demand.
F. Grosjean. 1982. Life with two languages: An intro-
duction to bilingualism . Harvard University Press.Xingwei He, Zhenghao Lin, Yeyun Gong, A Jin, Hang
Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan,
Weizhu Chen, et al. 2023. Annollm: Making large
language models to be better crowdsourced annota-
tors. arXiv preprint arXiv:2303.16854 .
Republik Indonesia. 2002. Undang-Undang Dasar Ne-
gara Republik Indonesia Tahun 1945 . Sekretariat
Jenderal MPR RI.
Ganesh Jawahar, El Moatez Billah Nagoudi, Muham-
mad Abdul-Mageed, and Laks Lakshmanan, V.S.
2021. Exploring text-to-text transformers for En-
glish to Hinglish machine translation with synthetic
code-mixing. In Proceedings of the Fifth Work-
shop on Computational Approaches to Linguistic
Code-Switching , pages 36–46, Online. Association
for Computational Linguistics.
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika
Bali, and Monojit Choudhury. 2020. The state and
fate of linguistic diversity and inclusion in the NLP
world. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
6282–6293, Online. Association for Computational
Linguistics.
Anubha Kabra, Emmy Liu, Simran Khanuja, Al-
ham Fikri Aji, Genta Indra Winata, Samuel Cahyawi-
jaya, Anuoluwapo Aremu, Perez Ogayo, and Graham
Neubig. 2023. Multi-lingual and multi-cultural figu-
rative language understanding.
Simran Khanuja, Sandipan Dandapat, Anirudh Srini-
vasan, Sunayana Sitaram, and Monojit Choudhury.
2020. GLUECoS: An evaluation benchmark for
code-switched NLP. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics , pages 3575–3585, Online. Association
for Computational Linguistics.
Andy Kirkpatrick. 2014. English in southeast asia: Ped-
agogical and policy implications. World Englishes ,
33(4):426–438.
Paul Kroeger. 1993. Phrase structure and grammat-
ical relations in Tagalog . Center for the Study of
Language (CSLI).
Huiyuan Lai and Malvina Nissim. 2022. Multi-
figurative language generation. In Proceedings of
the 29th International Conference on Computational
Linguistics , pages 5939–5954, Gyeongju, Republic
of Korea. International Committee on Computational
Linguistics.
J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
biometrics , pages 159–174.
Hugo Lauren c ¸on, Lucile Saulnier, Thomas Wang,
Christopher Akiki, Albert Villanova del Moral,
Teven Le Scao, Leandro Von Werra, Chenghao Mou,
Eduardo González Ponferrada, Huu Nguyen, J ¨org
Frohberg, Mario ˇSaˇsko, Quentin Lhoest, Angelina"
1709.06533,D:\Database\arxiv\papers\1709.06533.pdf,"In the context of generative adversarial networks (GANs), how does the choice of function class for the critic impact the stability and performance of the training process, and what are the trade-offs involved in selecting a particular function class?","The choice of function class for the critic in GANs significantly affects the stability and performance of the training process. While a rich function class can potentially capture complex relationships between the real and generated data, it can also lead to instability and difficulty in finding the optimal critic. Conversely, a simpler function class may be more stable but might not be expressive enough to accurately distinguish between the distributions.","As a result, we can frame the optimization of the critic as
a convex optimization problem, where all local maxima are
global maxima.
For appropriate settings of ξn,m, we can use the Fourier
or Taylor bases giving
WT(Pr,Pg) (14)
= sup∑∞
n=1|nAn,1|≤1∞∑
n=1An,1(
EPr[xn]−EPg[xn])
and
WF(Pr,Pg) (15)
= sup∑∞
n=1|πnAn,1|+|πnAn,2|≤1∞∑
n=0An,1(EPr[cos(nπx)]
−EPg[cos(nπx)]) +
An,2(
EPr[sin(nπx)]−EPg[sin(nπx)])
,
respectively.
Thus, by representing the class of critic functions as Tay-
lor or Fourier expansions, we obtain a clean way to enforce
the1-Lipschitz constraint over the entire domain, while en-
suring that gradient-based optimization schemes can ﬁnd the
globally optimal critic. We note that by enforcing an upper
bound on the 1-Lipschitz constraint, we are optimizing over
a smaller set of functions. However, we have not noticed this
additional constriction to affect performance empirically.
When minimizing the expresions above, slight modiﬁca-
tions must be made for computational tractability. First, we
must choose some N <∞and cut off the remaining terms
in the outer sum. Second, we must enforce our constraint as
a penalty term in the loss function. Fortunately, neither of
these practical considerations change the theoretical guaran-
tees proved above. Particularly, limiting the number of terms
in the outer sum to Ndoes not affect the our convexity-based
arguments and embedding the constraint into the loss func-
tion as a penalty still results in a convex optimization prob-
lem.
Connection with Moment Matching and
Maximum Mean Discrepancy
In this section, we review two methods in statistics that ex-
hibit similar characteristics to the Wasserstein distance met-
ric and the summable parameterization we presented in this
work.
Maximum Mean Discrepancy
Recent work by Gretton et al. (2012) explores the “Maxi-
mum Mean Discrepancy” technique to distinguish between
samples drawn from different data sources. The Maximum
Mean Discrepancy (also known as an Integral Probability
Metric) between two data sources is deﬁned as
MMD (F,PX,PY) = sup
f∈FEPX[f(X)]−EPY[f(Y)],
(16)
wherePXandPYare the distributions of the data sources
andFis some function class that is sufﬁciently rich that
PX=PYwhen MMD (F,PX,PY) = 0 .Notice how the dual-form of Wasserstein distance is a
specical case of the above Integral Probability Metric when
Fis the set of 1-Lipschitz functions.
In their work, Gretton et al. (2012) explore using Repro-
ducing Kernel Hilbert Spaces (Berlinet and Thomas-Agnan
2011) as the function class to perform their maximum mean
discrepancy tests. This kernel-based approach is adopted by
Li, Swersky, and Zemel (2015) in their work on Genera-
tive Moment Matching Networks. This work offers a method
that competes directly with GANs. Rather than a mini-max
game between a generator and discriminator, Generative
Moment Matching Networks boast only needing a genera-
tor network that is trained to minimize the Maximum Mean
Discrepancy between the real and generated sources. The
authors note that their use of kernels approximates match-
ing the moments of the sampled and generated random vari-
ables.
Moment Matching
Moment matching, also known as the “method-of-moments”
is the process of ﬁtting a model to a distribution by sampling
from that distribution and setting the model’s parameters to
be the distribution’s sampled moments. In general, moments
can refer to any set of functions that characterize the behav-
ior of a random variable, but they are most commonly repre-
sented as the random variable raised to different powers. For
anyn≥1, we denote the nth moment of a random variable
Xas
mn(X) =EPx[Xn]. (17)
Particularly notice that for critics represented by the Tay-
lor series parameterization, the Wasserstein distance can be
expressed as a sum of weighted moments:
WT(Pr,Pg) = max∑∞
n=1|nAn,1|≤1∞∑
n=1An,1(
EPr[Xn]−EPg[Xn])
.
(18)
Experiments
In the following subsections, we describe our experimental
procedure.
Domains
We evaluated our method against three different synthetic
data sources and one real-world data source. Our synthetic
data sources consisted of a “sawtooth” distribution, a dis-
crete distribution with three possible values, and a mixture
of two Gaussian distributions. Each distribution was sam-
pled10,000times to construct a dataset that was then used
across all experiments and models. These distributions cor-
repond to the following random variables deﬁned below:
Xsawtooth =√
Y1−1
Xdiscrete =1
2(−1{Y2<0.25}+1{Y2>0.75})
Xmixture =BN1+ (1−B)N2,(19)
whereY1,Y2∼Uniform (0,1),B∼Bernoulli (0.5),N1∼
Normal (0.5,0.05)andN2∼Normal (−0.5,0.05).
Our real-world data source is a collection of city
populations from the Free World Cities Database
(https://www.maxmind.com/en/free-world-cities-database).
We pre-processed this data by applying a logarithmic
scaling to the population numbers and normalizing the
resulting log-populations to be between −1and1. We
denote the random variable associated with this data source
asXcities.
Figure 1 characterizes each of these data sources by sam-
pling a million points from each and plotting their his-
tograms.
Network Architectures
To maintain consistency between experiments, we used the
same generator network architecture for both the wGAN-GP
experiments and for our method. This generator network ar-
chitecture consists of 3batch-normalized, fully connected
layers with 500 neurons each and leaky ReLU activation,
followed by a single fully connected output layer with 1neu-
ron and a tanh activation. The wGAN-GP experiment used
a discriminator with 2fully connected layers with 100neu-
rons each and leaky ReLU activations, followed by a single
fully connected output layer with 1neuron and linear acti-
vation. Following Gulrajani et al. (2017), we used λ= 10
to enforce constraints across all experiments. Additionally,
we used the AdamOptimizer (Kingma and Ba 2014) with
β1= 0,β2= 0.9and a learning rate of 0.0001 . For all repa-
rameterized critic models we clipped the inﬁnite sums in the
expansions at N= 20 . Additionally, batch normalization
(Ioffe and Szegedy 2015) is used for all generator networks
in our experiments.
Evaluation Procedure
All of the comparison algorithms attempt to learn a represen-
tation of the target 1-dimensional probability distribution.
For each model, we measure its accuracy by computing the
sample Earth-Mover’s distance. This quantity is computed
by sampling the model 10,000times and constructing a his-
togram out of its sampled outputs. The entries in these his-
tograms are then normalized so that the sum of the bin values
is1. A similar histogram is then constructed using the true
data source, and the Earth-Mover’s distance is computed be-
tween them. For computing the Earth-Mover’s distance, we
used the publicly available Python library pyEMD. For each
of the GAN methods, training was conducted over 100,000
iterations, with an estimate of the Earth-Mover’s distance
being computed with the training data every 1000 iterations.
At the end of training, the lowest estimate over the course of
training is reported as the model’s Earth-Mover’s distance
(EMD).
Results
We present the results of running 4trials for each of the
GAN-based models. We denote our runs with reparame-
terized critics as “Taylor Critic” and “Fourier Critic” for
the Fourier Series and Taylor Series reparameterizations, re-
spectively. The best obtained Earth-Mover’s distances for
each run and model are reported in Tables 2, 3, 4 and 5.
(a)
(d)(b)
(c)Figure 1: Histograms generated by drawing 1,000,000 sam-
ples of random variables (a) Xsawtooth , (b)Xmixture , (c)
Xdiscrete and (d)Xcities.
We additionally report the average Earth-Mover’s distances
across the 4 trials and compare these numbers to the per-
formance of a Kernel Density Estimator as a nonparametric
baseline. These results are posted in Table 1.
XmixtureXdiscreteXsawtoothXcities
KDE 0.0073 0.01002 0.0040 0.0027
wGAN-GP 0.0822 0.1318 0.26055 0.0188
Taylor Critic 0.0216 0.0106 0.0151 0.0096
Fourier Critic 0.0186 0.0193 0.0109 0.0103
Table 1: Table containing the average Earth-Mover’s Dis-
tances over the 4runs detailed in Tables 2, 3, 4 and 5 for
each GAN-based model.
Xmixture
1 2 3 4
wGAN-GP 0.0206 0.0279 0.2578 0.0226
Taylor Critic 0.0179 0.0216 0.0217 0.0250
Fourier Critic 0.0204 0.0164 0.0201 0.0175
Table 2: Table of Earth-Mover’s distances for 4runs of the
wGAN-GP, Taylor Critic and Fourier Critic on the Gaussian
Mixture dataset. Run 3 of the wGAN-GP illustrates its in-
stability.
Xdiscrete
1 2 3 4
wGAN-GP 0.1381 0.1333 0.1314 0.1242
Taylor Critic 0.0091 0.0121 0.0103 0.0110
Fourier Critic 0.0129 0.0129 0.0287 0.0226
Table 3: Table of Earth-Mover’s Distances for 4runs of the
wGAN-GP, Taylor Critic and Fourier Critic on the Discrete
dataset.
Xsawtooth
1 2 3 4
wGAN-GP 0.4891 0.0226 0.4653 0.0652
Taylor Critic 0.0157 0.0133 0.0152 0.0161
Fourier Critic 0.0081 0.0132 0.0132 0.0091
Table 4: Table of Earth-Mover’s distances for 4runs of the
wGAN-GP, Taylor Critic and Fourier Critic on the Sawtooth
dataset. Runs 1 and 3 of the wGAN-GP illustrate its insta-
bility.
We observe that both models with reparameterized crit-
ics signiﬁcantly outperform wGAN-GP and are frequently
competitive with Kernel Density Estimation. From Tables 2,
3, 4 and 5 we observe that the reparameterized critic models’
worst runs are generally better than the wGAN-GP model’s
best runs, and the reparameterized critic models have signif-
icantly lower variance across runs than wGAN-GP.Xcities
1 2 3 4
wGAN-GP 0.0225 0.0189 0.0182 0.0157
Taylor Critic 0.0120 0.0103 0.0066 0.0094
Fourier Critic 0.0108 0.0069 0.0086 0.0150
Table 5: Table of Earth-Mover’s distances for 4runs of the
wGAN-GP, Taylor Critic and Fourier Critic on the City Pop-
ulation dataset.
Since all GAN-based methods in this paper have the same
network architecture for their generators, it is reasonable to
attribute this difference to the forms of the critics. As we
showed in Theorem 1, the process of optimizing the critic
with respect to a given generator cannot “get stuck” in some
locally maximal region of the space of critics. Thus, as long
as the set of critics satisfying Lξ(·)≤1is sufﬁciently close
to the set of critics satisfying ∥·∥L≤1, then the generator
should always have a clean gradient to follow during its op-
timization as shown in Lemma 1 of Gulrajani et al. (2017).
While this does not preclude the possibility that the gen-
erator itself could “get stuck” during its own optimization
against the critic, the difference in consistency across runs
between the reparameterized critic models and the wGAN-
GP models is evidence that the additional guarantees on
reparameterized critics helps empirically. Note that we made
every effort to set the hyperparameters of GP-wGAN to re-
duce or eliminate its instability. It is possible that it would
perform better with some other parameter setting, but we
were not able to ﬁnd such a setting. That being said the per-
formance of the reparameterized critic models was relatively
unchanged across the parameter settings we explored.
Conclusion and Future Work
In this work, we illustrated an alternate parameterization
of the critic networks that has ideal theoretical properties
for gradient-based optimization. We demonstrated that, in
the one-dimensional setting, our summable critic models
categorically outperform Wasserstein GAN with gradient
penalty and are competitive with Kernel Density Estimation
on a variety of synthetic and real-world domains.
While our work on this paper focuses on the one-
dimensional setting, there is considerable room to explore
extending the approach to higher dimensions. For both
the Taylor and Fourier series expansions, there are high-
dimensional analogues. These higher-dimensional decom-
positions generally require exponentially many terms in the
number of input dimensions. It may be possible to allevi-
ate this computational cost by exploiting recent techniques
to learn sparse polynomials or Fourier series (Andoni et al.
2014; Hassanieh et al. 2012). Particularly, while all expo-
nentially many terms of these series may be necessary to
model arbitrarily messy functions, it is unlikely that all or
even most of them will be required to reasonably approxi-
mate the space of 1-Lipschitz functions.
References
[Andoni et al. 2014] Andoni, A.; Panigrahy, R.; Valiant, G.;
and Zhang, L. 2014. Learning sparse polynomial functions.
InSODA .
[Arjovsky, Chintala, and Bottou 2017] Arjovsky, M.; Chin-
tala, S.; and Bottou, L. 2017. Wasserstein GAN. CoRR
abs/1701.07875.
[Berlinet and Thomas-Agnan 2011] Berlinet, A., and
Thomas-Agnan, C. 2011. Reproducing kernel Hilbert
spaces in probability and statistics . Springer Science &
Business Media.
[Goodfellow et al. 2014] Goodfellow, I.; Pouget-Abadie, J.;
Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville,
A.; and Bengio, Y . 2014. Generative adversarial nets. In
Advances in Neural Information Processing Systems , 2672–
2680.
[Gretton et al. 2012] Gretton, A.; Borgwardt, K. M.; Rasch,
M. J.; Sch ¨olkopf, B.; and Smola, A. J. 2012. A kernel two-
sample test. Journal of Machine Learning Research 13:723–
773.
[Gulrajani et al. 2017] Gulrajani, I.; Ahmed, F.; Arjovsky,
M.; Dumoulin, V .; and Courville, A. C. 2017. Improved
training of Wasserstein GANs. CoRR abs/1704.00028.
[Hassanieh et al. 2012] Hassanieh, H.; Indyk, P.; Katabi, D.;
and Price, E. 2012. Nearly optimal sparse fourier transform.
InSTOC .
[Ioffe and Szegedy 2015] Ioffe, S., and Szegedy, C. 2015.
Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In ICML .
[Kingma and Ba 2014] Kingma, D. P., and Ba, J. 2014.
Adam: A method for stochastic optimization. CoRR
abs/1412.6980.
[Li, Swersky, and Zemel 2015] Li, Y .; Swersky, K.; and
Zemel, R. S. 2015. Generative moment matching networks.
InICML .
[Villani 2008] Villani, C. 2008. Optimal Transport: Old and
New, volume 338. Springer Science & Business Media."
1908.11229,D:\Database\arxiv\papers\1908.11229.pdf,"In the context of machine learning, how can the concept of ""overfitting"" be leveraged to improve the accuracy of a membership inference attack?","Overfitting, where a model learns the training data too well, can be exploited by membership inference attacks.  The attack can identify data points that were used in training by observing whether the model predicts their labels with higher accuracy than it does for unseen data.","White-box vs Black-box: Bayes Optimal Strategies for Membership Inference
where we deﬁne the following score:
τp(z1) :=−Tlog(∫
te−1
Tℓ(t,z1)p(t)dt)
(14)
s(z1,θ,p) :=1
T(τp(z1)−ℓ(θ,z1)). (15)
Proof. Singling out m1in Equation (2) yields the following
expressions for αandβ:
α=λe−1
Tℓ(θ,z1)e−1
T∑n
i=2miℓ(θ,zi)
∫
te−1
Tℓ(t,z1)e−1
T∑n
i=2miℓ(t,zi)dt(16)
=λe−1
Tℓ(θ,z1)pT(θ)∫
te−1
Tℓ(t,z1)pT(t)dt, (17)
and
β= (1−λ)e−1
T∑n
i=2miℓ(θ,zi)
∫
te−1
T∑n
i=2miℓ(t,zi)dt= (1−λ)pT(θ).
(18)
Thus,
log(α
β)
=−ℓ(θ,z1)
T−log(∫
te−1
Tℓ(t,z1))pT(t)dt)
+tλ
=s(z1,θ,pT) +tλ. (19)
Then, Equation (8) yields the expected result.
The ﬁrst observation in Theorem 2 is that M(θ,z1)does
not depend on the parameters θbeyond the evaluation of
the lossℓ(θ,·): this strategy does not require, for instance,
internal parameters of the model that a white-box attack
could provide. This means that if we can compute τpor
approximate it well enough, then the optimal membership
inference depends only on the loss. In other terms, asymptot-
ically, the white-box setting does not provide any beneﬁt
compared to black-box membership inference.
Let us analyze qualitatively the terms in the expression:
SinceTis a training set, pTcorresponds to a posterior over
this training set, i.e., a typical distribution of trained param-
eters.τp(z1)is the softmin of loss terms ℓ(·,z1)over these
typical models, and corresponds therefore to the typical loss
of samplez1under models that have not seen z1.
The quantity τp(z1)can be seen as a threshold, to which
the lossℓ(θ,z1)is compared. Around this threshold, when
ℓ(θ,z1)≈τp(z1), thens≈0: sinceσ(tλ) =λ, the mem-
bership posterior probability M(θ,z1)is equal toλ, and
thus we have no information on m1beyond prior knowledge.
As the lossℓ(·,z1)gets lower than this threshold, sbecomes
positive. Since σis non decreasing, when s(z1,θ,pT)>0,
M(θ,z1)> λ and thus we gain non-trivial membership
information on z1.Another consequence of Theorem 2 is that a higher temper-
atureTdecreasess, and thus decreases P(m1= 1|θ,z1):
it corresponds to the intuition that more randomness in θ
protects the privacy of training data.
3.4. Differential privacy and guarantees
In this subsection we make the link with differential privacy.
Differential privacy (Dwork et al., 2006) is a framework that
allows to learn model parameters θwhile maintaining the
conﬁdentiality of data. It ensures that even if a malicious
attacker knows parameters θand samples zi,i≥2, for
whichmi= 1, the privacy of z1is not compromised.
Deﬁnition 2 (ϵ-differential privacy) .A machine learning
algorithm is ϵ-differentially private if, for any choice of z1
andT,
log(P(θ|m1= 1,z1,T)
P(θ|m1= 0,z1,T))
<ϵ. (20)
Note that this deﬁnition is slightly different from the one of
Dwork et al. (2006) in that we consider the removal of z1
rather than its substitution with z′. Additionally we consider
probability densities instead of probabilities of sets, without
loss of generality.
Property 1 (ϵ-differential privacy) .If the training is ϵ-
differentially private, then:
P(m1= 1|θ,z1)≤λ+ϵ
4. (21)
Proof. Combining Equation (20) and the fact that σ(u)≤
σ(v) + max(u−v,0)/4(Appendix A.3), we have:
σ(
log(P(θ|m1= 1,z1,T)
P(θ|m1= 0,z1,T))
+tλ)
≤σ(tλ) +ϵ
4
=λ+ϵ
4.(22)
Combining this expression with Theorem 1 yields the result.
Note that this bound gives a tangible sense of ϵ. In general,
decreasingϵincreases privacy, but there is no consensus
over “good” values of ϵ; this bound indicates for instance
thatϵ= 0.01would be sufﬁcient for membership privacy.
ϵ-differential privacy gives strong membership inference
guarantees, at the expense of a constrained training proce-
dure resulting generally in a loss of accuracy (Abadi et al.,
2016). However, if we assume that the attacker knows the
zi,i≥2for whichmi= 1,ϵ-differential privacy is required
to protect the privacy of z1. Depending on the information
we have onzi,i≥2, there is a continuum between differen-
tial privacy (all zi’s are known) and membership inference
(only prior knowledge on zi). In the case of membership
inference, it sufﬁces to have the following guarantee:
White-box vs Black-box: Bayes Optimal Strategies for Membership Inference
Deﬁnition 3 ((ϵ,δ)membership privacy) .The training is
(ϵ,δ)-membership private for some ϵ > 0,δ > 0if with
probability 1−δover the choice ofT:
∫
tℓ(t,z1)pT(t)dt−ℓ(θ,z1)≤ϵ. (23)
Property 2. If the training is (ϵ,δ)-membership private,
then:
P(m1= 1|θ,z1)≤λ+ϵ
4T+δ. (24)
Proof. Jensen’s inequality states that for any distribution p
and any function f:
∫
tf(t)p(t)dt≤log(∫
tef(t)p(t)dt)
, (25)
hence the score sfrom Equation (15) veriﬁes:
s(z1,θ,p)≤1
T(∫
tℓ(t,z1)p(t)dt−ℓ(θ,z1))
.(26)
Thus, distinguishing the cases δand1−δin the expectation
in Equation (13),
P(m1= 1|θ,z1)≤δ+ (1−δ)(
λ+ϵ
4T)
(27)
≤λ+ϵ
4T+δ, (28)
which gives the desired bound.
Membership privacy provides a post-hoc guarantee on θ,z1.
Guarantees in the form of Equation (23) can be obtained by
PAC (Probably Approximately Correct) bounds.
4. Approximations for membership inference
Estimating the probability of Equation (15) mainly requires
to compute the term τp. Since its expression is intractable,
we use approximations to derive concrete membership at-
tacks (MA). We now detail these approximations, referred
to as MAST (MA Sample Threshold), MALT (MA Loss
Threshold) and MATT (MA Taylor Threshold).
4.1. MAST: Approximation of τ(z1)
We ﬁrst make the mean-ﬁeld assumption that pT(t)does
not depend onT(we note itp), and deﬁne
τ(z1) := log(∫
te−1
Tℓ(t,z1)p(t)dt)
. (29)
The quantity τ(·)is a “calibrating” term that reﬂects the
difﬁculty of a sample. Intuitively, a low τ(z1)means that
the samplez1is easy to predict, and thus a low value of
ℓ(θ,z1)does not necessarily indicate that z1belongs to the
train set. Thus, Theorem 2 gives the optimal attack model:
sMAST (θ,z1) =−ℓ(θ,z1) +τ(z1). (30)4.2. MALT: Constant τ
If we further assume that τ(·)is constant, the optimal strat-
egy reduces to predicting that z1comes from the training set
if its lossℓ(θ,z1)is lower than this threshold τ, and from
the test set otherwise:
sMALT (θ,z1) =−ℓ(θ,z1) +τ. (31)
A similar strategy is proposed by Yeom et al. (2018) for
Gaussian models. Carlini et al. (2018) estimate a secret
token in text datasets by comparing probabilities of the sen-
tence “My SSN is X” with various values of X. Surprisingly,
to the best of our knowledge, it has not been proposed in
the literature to estimate the threshold τon public data, and
to apply it for membership inference. As we show in Sec-
tion 6, this simple strategy yields better results than shadow
models. Their attack models take as input the softmax acti-
vationφθ(x)and the class label y, and predict whether the
sample comes from the training or test set. For classiﬁcation
models,ℓ(θ,(x,y)) =−log(φθ(x)y). Hence the optimal
attack performs:
sMALT (θ,(x,y)) = log(φθ(x)y) +τ. (32)
In Shokri et al. (2017), we argue that the attack model
essentially performs such an estimation, albeit in a non-
explicit way. In particular, we believe that the gap between
Shokri et al. (2017)’s method and ours is due to instabilities
in the estimation of τand the numerical computation of the
log, as the model is given only φθ(x). As a side note, the
expectation term in T=z2,...,zn,m2,...,mnis very
similar in spirit to the shadow models, and they can be
viewed as a Monte-Carlo estimation of this quantity.
An experiment with Gaussian data. We illustrate the
difference between a MALT (global τ) and MAST (per-
sampleτ(·)) on a simple toy example. Let’s assume we
estimate the mean µof Gaussian data with unit variance.
We samplenvaluesz1,...,znfromD=N(µ,I). The
estimate of the mean is θ=1
n′∑n
i=1miziwheren′=
|{i|mi= 1}|. We have (see Appendix A.2 for derivations):
ℓ(θ,zi) :=1
2∥zi−θ∥2(33)
τ(zi) =n′
2(n′+ 1)∥zi−µ∥2(34)
τ=n′
2(n′+ 1)E∥z−µ∥2=n′
2(n′+ 1)d. (35)
The expression of τ(zi)shows that the “difﬁculty” of sample
ziis its distance to µ,i.e., how untypical this sample is.
Figure 1 shows the results with a global τor a per-sample
τ: the per-sample τbetter separates the two distributions,
leading to an increased membership inference accuracy.
White-box vs Black-box: Bayes Optimal Strategies for Membership Inference
���������������������������������������
���������������
���������
��� ��� �� ��� ����������
���������������
���������
Figure 1. Comparison of MALT and MALT for membership inference on the mean estimator for Gaussian data ( n= 100 samples in
2000 dimensions). Distribution of scores sused to distinguish between samples seen or not at training. MALT : a single threshold is used
for all the dataset; MAST : each sample gets assigned a different threshold. MAST better separates training and non-training samples.
MATT: Estimation with Taylor expansion We as-
sume that the posterior induced by the loss L(θ) =∑n
i=1miℓ(θ,zi)is a Gaussian centered on θ∗, the mini-
mum of the loss, with covariance C. This corresponds to
the Laplace approximation of the posterior distribution. The
inverse covariance matrix C−1is asymptotically ntimes
the Fisher matrix (van der Vaart, 1998), which itself is the
Hessian of the loss (Kullback, 1997):
P(θ|z1,T) =1√
det (2πH−1)e−1
2(θ−θ∗)TH(θ−θ∗).
(36)
We denote by θ∗
0(resp.θ∗
1) the mode of the Gaussian corre-
sponding to{z2,...,zn}(resp.{z1,...,zn}), andH0(resp.
H1) the corresponding Hessian matrix. We assume that H
is not impacted by removing z1from the training set, and
thusH:=H0≈H1(cf. appendix A.4 for a more precise
justiﬁcation). The log-ratio is therefore
log(P(θ|m1= 1,z1,T)
P(θ|m1= 0,z1,T))
(37)
=−1
2(θ−θ∗
1)TH(θ−θ∗
1) +1
2(θ−θ∗
0)TH(θ−θ∗
0)
= (θ−θ∗
0)TH(θ∗
1−θ∗
0)−1
2(θ∗
1−θ∗
0)TH(θ∗
1−θ∗
0).
The difference θ∗
1−θ∗
0can be estimated using a Taylor
expansion of the loss gradient around θ∗
0(see e.g.Koh &
Liang (2017)):
θ∗
1−θ∗
0≈−H−1∇θℓ(θ∗
0,z1) (38)
Combining this with Equation (37) leads to
−(θ−θ∗
0)T∇θℓ(θ∗
0,z1)−1
2∇θℓ(θ∗
0,z1)TH−1∇θℓ(θ∗
0,z1).
(39)
We study the asymptotic behavior of this expression when
n→∞ . On the left-hand side, the parameters θandθ∗
0are
estimates of the optimal θ∗, and under mild conditions, theerror of the estimated parameters is of order 1/√n. There-
fore the difference θ−θ∗
0is of order 1/√n. On the right-
hand side, the matrix His the summation of nsample-wise
Hessian matrices. Therefore, asymptotically, the right-hand
side shrinks at a rate 1/n, which is negligible compared
to the other, which shrinks at 1/√n. In addition to the
asymptotic reasoning, we veriﬁed this approximation exper-
imentally. Thus, we approximate Equation (39) to give the
following score:
sMATT (θ,z1) =−(θ−θ∗
0)T∇θℓ(θ∗
0,z1). (40)
Equation (40) has an intuitive interpretation: parameters θ
were trained using z1if their difference with a set of parame-
ters trained without z1(i.e.θ∗
0) is aligned with the direction
of the update−∇θℓ(θ∗
0,z1).
5. Membership inference algorithms
In this section, we detail how the approximations of
s(θ,z1,p)are employed to perform membership inference.
We assume that a machine learning model has been trained,
yielding parameters θ. We assume also that similar models
can be re-trained with different training sets. Given a sample
z1, we want to decide whether z1belongs to the training set.
5.1. The 0-1 baseline
We consider as a baseline the “0-1” heuristic, which predicts
thatz1comes from the training set if the class is predicted
correctly, and from the test set if the class is predicted incor-
rectly. We note ptrain(resp.ptest) the classiﬁcation accuracy
on the training (resp. held-out) set. The accuracy of the
heuristic is (see Appendix A.1 for derivations):
pbayes=λptrain+ (1−λ)(1−ptest). (41)
For example when λ= 1/2, sinceptrain≥ptestthis heuristic
White-box vs Black-box: Bayes Optimal Strategies for Membership Inference
is better than random guessing (accuracy 1/2) and the im-
provement is proportional to the overﬁtting gap ptrain−ptest.
5.2. Making hard decisions from scores
Variants of our method provide different estimates of
s(θ,z1,p). Theorem 2 shows that this score has to be passed
through a sigmoid function, but since it is an increasing func-
tion, the threshold can be chosen directly on these scores.
Estimation of this threshold has to be conducted on simu-
lated sets, for which membership information is known. We
observed that there is almost no difference between chosing
the threshold on the set to be tested and cross-validating
it. This is expected, as a one-dimensional parameter the
threshold is not prone to overﬁtting.
5.3. Membership algorithms
MALT: Threshold on the loss. Sinceτin Equation (31) is
constant, and using the invariance to increasing functions,
we need only to use loss value for the sample, ℓ(θ,z1).
MAST: Estimating τ(z1).To estimate τ(z1)in Equa-
tion (29), we train several models with different subsamples
of the training set. This yields a set of per-sample losses for
z1that are averaged into an estimate of τ(z1).
MATT: the Taylor approximation. We run the training
on a separate set to obtain θ∗
0. Then we take a gradient step
over the loss to estimate the approximation in Equation (40).
Note that this strategy is not compatible with neural net-
works because the assumption that parameters lie around
a unique global minimum does not hold. In addition, pa-
rameters from two different networks θandθ∗
0cannot be
compared directly as neural networks that express the same
function can have very different parameters ( e.g.because
channels can be permuted arbitrarily).
6. Experiments
In this section we evaluate the membership inference meth-
ods on machine-learning tasks of increasing complexity.
6.1. Evaluation
We evaluate three metrics: the accuracy of the attack, and
the mean average precision when detecting either from train
(mAP train) or test (mAP test) images. For the mean average
precision, the scores need not to be thresholded, the metric
is invariant to mapping by any increasing function.
6.2. Logistic regression
CIFAR-10 is a dataset of 32×32pixel images grouped in
10 classes. In this subsection, we consider two of the classes
(truck andboat) and vary the number of training imagesTable 1. Accuracy (top) and mAP (bottom) of membership infer-
ence on the 2-class logistic regression with simple CNN features,
for different types of attacks. Note that 0-1 corresponds to the
baseline (Yeom et al., 2018). We do not report Shokri et al. (2017)
since Table 2 shows MALT performs better. Results are averaged
over100different random seeds.
Model accuracy Attack accuracy
n train validation 0−1MALT MATT
400 97.9 93.8 52.1 54.4 57.0
1000 97.3 94.5 51.4 52.6 54.5
2000 96.8 95.2 50.8 51.7 53.0
4000 97.7 95.6 51.0 51.4 52.1
6000 97.5 96.0 50.7 51.0 51.8
mAP test mAP train
n MALT MATT MALT MATT
400 55.8 60.1 51.9 57.1
1000 53.2 56.6 50.5 54.8
2000 51.8 54.4 50.4 53.4
4000 51.9 53.7 50.1 52.6
6000 51.4 53.0 50.2 52.2
fromn= 400 to6,000.
We train a logistic regression to separate the two classes.
The logistic regression takes as input features extracted from
a pretrained Resnet18 on CIFAR-100 (a disjoint dataset).
The regularization parameter Cof the logistic regression is
cross-validated on held-out data.
We assume that λ= 1/2(n/2training images and n/2test
images). We also reserve n/2images to estimate θ∗
0for the
MATT method. In both experiments, we report the peak
accuracy obtained for the best threshold (cf. Section 5.2).
Table 1 shows the results of our experiments, in terms of
accuracy and mean average precision. In accuracy, the
Taylor expansion method MATT outperforms the MALT
method, for any number of training instances n, which itself
obtains much better results than the naive 0-1 attack.
Interestingly, it shows a difference between MALT and
MATT: both perform similarly in terms of mAP test, but
MATT slightly outperforms MALT in mAP train. The main
reason for this difference is that the MALT attack is asym-
metric: it is relatively easy to predict that elements come
from the test set, as they have a high loss, but elements with
a low loss can come either from the train set or the test set.
White-box vs Black-box: Bayes Optimal Strategies for Membership Inference
Table 2. Accuracy of membership attacks on the CIFAR-10 classi-
ﬁcation with a simple neural network. The numbers for the related
works are from the respective papers.
Method Accuracy
0-1 (Yeom et al., 2018) 69.4
Shadow models (Shokri et al., 2017) 73.9
MALT 77.1
MAST 77.6
6.3. Small convolutional network
In this section we train a small convolutional network1with
the same architecture as Shokri et al. (2017) on the CIFAR-
10 dataset, using a training set of 15,000images. Our model
is trained for 50epochs with a learning rate of 0.001. We
assume a balanced prior on membership ( λ= 1/2).
We run the MALT and MAST attacks on the classiﬁers.
As stated before, the MATT attack cannot be carried out
on convolutional networks. For MAST, the threshold is
estimated from 30 shadow models: we train these 30 shadow
models on 15,000images chosen among the train+held-out
set (30,000images). Thus, for each image, we have on
average 15 models trained on it and 15 models not trained
on it: we estimate the threshold for this image by taking the
valueτ(z)that separates the best the two distributions: this
corresponds to a non-parametric estimation of τ(z).
Table 2 shows that our estimations outperform the related
works. Note that this setup gives a slight advantage to
MAST as the threshold is estimated directly for each sample
under investigation, whereas MALT ﬁrst estimates a thresh-
old, and then applies it to never-seen data. Yet, in contrast
with the experiment on Gaussian data, MAST performs only
slightly better than MALT. Our interpretation for this is that
the images in the training set have a high variability, so it
is difﬁcult to obtain a good estimate of τ(z1). Furthermore,
our analysis of the estimated thresholds τ(z1)show that
they are very concentrated around a central value τ, so their
impact when added to the scores is limited.
Therefore, in the following experiment we focus on the
MALT attack.
6.4. Evaluation on Imagenet
We evaluate a real-world dataset and tackle classiﬁcation
with large neural networks on the Imagenet dataset (Deng
et al., 2009; Russakovsky et al., 2015), which contains 1.2
million images partitioned into 1000 categories. We divide
Imagenet equally into two splits, use one for training and
hold out the rest of the data.
12 convolutional and 2 linear layers with Tanh non-linearity.Table 3. Imagenet classiﬁcation with deep convolutional networks:
Accuracy of membership inference attacks of the models.
Model Augmentation 0-1 MALT
Resnet101 None 76.3 90.4
Flip, Crop±569.5 77.4
Flip, Crop 65.4 68.0
VGG16 None 77.4 90.8
Flip, Crop±571.3 79.5
Flip, Crop 63.8 64.3
We experiment with the popular VGG-16 (Simonyan &
Zisserman, 2014) and Resnet-101 (He et al., 2016) archi-
tectures. The model is learned in 90epochs, with an initial
learning rate of 0.01, divided by 10every 30epochs. Param-
eter optimization is conducted with SGD with a momentum
of0.9, a weight decay of 10−4, and a batch size of 256.
We conduct the membership inference test by running the
0-1 attack and MALT. An important factor for the success of
the attacks is the amount of data augmentation. To assess the
effect of data augmentation, we train different networks with
varying data augmentation: None, Flip+Crop ±5, Flip+Crop
(by increasing intensity).
Table 3 shows that data augmentation reduces the gap be-
tween the training and the held-out accuracy. This decreases
the accuracy of the Bayes attack and the MALT attack. As
we can see, without data augmentation, it is possible to
guess with high accuracy if a given image was used to train
a model (about 90% with our approach, against 77% for
existing approaches). Stronger data augmentation reduces
the accuracy of the attacks, that still remain above 64%.
7. Conclusion
This paper has addressed the problem of membership infer-
ence by adopting a probabilistic point of view. This led us
to derive the optimal inference strategy. This strategy, while
not explicit and therefore not applicable in practice, does
not depend on the parameters of the classiﬁer if we have ac-
cess to the loss. Therefore, a main conclusion of this paper
is to show that, asymptotically, white-box inference does
not provide more information than an optimized black-box
setting.
We then proposed two approximations that lead to three
concrete strategies. They outperform competitive strategies
for a simple logistic problem, by a large margin for our
most sophisticated approach (MATT). Our simplest strat-
egy (MALT) is applied to the more complex problem of
membership inference from a deep convolutional network
on Imagenet, and signiﬁcantly outperforms the baseline.
White-box vs Black-box: Bayes Optimal Strategies for Membership Inference
References
Martin Abadi, Andy Chu, Ian Goodfellow, Brendan McMa-
han, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep
learning with differential privacy. In CCS, 2016.
Giuseppe Ateniese, Luigi V Mancini, Angelo Spognardi,
Antonio Villani, Domenico Vitali, and Giovanni Felici.
Hacking smart machines with smarter ones: How to ex-
tract meaningful data from machine learning classiﬁers.
IJSN , 2015.
Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke,
Uri Stemmer, and Jonathan Ullman. Algorithmic stability
for adaptive data analysis. In STOC , 2016.
Battista Biggio, Igino Corona, Blaine Nelson, Benjamin I. P.
Rubinstein, Davide Maiorca, Giorgio Fumera, Giorgio
Giacinto, and Fabio Roli. Security Evaluation of Support
Vector Machines in Adversarial Environments . Springer
International Publishing, 2014.
Piotr Bojanowski and Armand Joulin. Unsupervised learn-
ing by predicting noise. In ICML , 2017.
Nicholas Carlini, Chang Liu, Jernej Kos, ´Ulfar Erlingsson,
and Dawn Song. The secret sharer: Measuring unintended
neural network memorization & extracting secrets. arXiv
preprint arXiv:1802.08232 , 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR , 2009.
Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Ried-
miller, and Thomas Brox. Discriminative unsupervised
feature learning with convolutional neural networks. In
NIPS , 2014.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam
Smith. Calibrating noise to sensitivity in private data
analysis. In TCC , 2006.
Cynthia Dwork, Adam Smith, Thomas Steinke, Jonathan
Ullman, and Salil Vadhan. Robust traceability from trace
amounts. In Proceedings of the Symposium on the Foun-
dations of Computer Science , 2015.
Jamie Hayes, Luca Melis, George Danezis, and Emiliano
De Cristofaro. Logan: evaluating privacy leakage of
generative models using generative adversarial networks.
arXiv preprint arXiv:1705.07663 , 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
2016.
Pang Wei Koh and Percy Liang. Understanding black-box
predictions via inﬂuence functions. In ICML , 2017.David Krueger, Nicolas Ballas, Stanislaw Jastrzebski, De-
vansh Arpit, Maxinder S. Kanwal, Tegan Maharaj, Em-
manuel Bengio, Asja Fischer, Aaron Courville, Simon
Lacoste-Julien, and Yoshua Bengio. A closer look at
memorization in deep networks. In ICML , 2017.
S. Kullback. Information Theory And Statistics . Dover
Publications, 1997.
Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue
Bu, Xiaofeng Wang, Haixu Tang, Carl A Gunter,
and Kai Chen. Understanding membership inferences
on well-generalized learning models. arXiv preprint
arXiv:1802.04889 , 2018.
B. T. Polyak and A. B. Juditsky. Acceleration of stochastic
approximation by averaging. SIAM J. Control Optim. ,
1992.
Benjamin IP Rubinstein, Peter L Bartlett, Ling Huang,
and Nina Taft. Learning in a large function space:
Privacy-preserving mechanisms for SVM learning.
arXiv:0911.5708 , 2009.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, Alexan-
der C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual
Recognition Challenge. IJCV , 2015.
Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal
Berrang, Mario Fritz, and Michael Backes. Ml-leaks:
Model and data independent membership inference at-
tacks and defenses on machine learning models. In NCSS ,
2019.
Sriram Sankararaman, Guillaume Obozinski, Michael I. Jor-
dan, and Eran Halperin. Genomic privacy and limits of
individual detection in a pool. Nature Genetics , 2009.
Reza Shokri, Marco Stronati, and Vitaly Shmatikov. Mem-
bership inference attacks against machine learning mod-
els.IEEE Symp. Security and Privacy , 2017.
Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In
ICLR , 2014.
A. W. van der Vaart. Asymptotic statistics . Cambridge Series
in Statistical and Probabilistic Mathematics. Cambridge
University Press, 1998.
Yu-Xiang Wang, Stephen Fienberg, and Alex Smola. Pri-
vacy for free: Posterior sampling and stochastic gradient
monte carlo. In ICML , 2015.
Yu-Xiang Wang, Jing Lei, and Stephen E Fienberg. On-
average KL-privacy and its equivalence to generalization
for max-entropy mechanisms. In PSD. Springer, 2016.
White-box vs Black-box: Bayes Optimal Strategies for Membership Inference
Max Welling and Yee Whye Teh. Bayesian learning via
stochastic gradient langevin dynamics. In ICML , 2011.
Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and
Somesh Jha. Privacy risk in machine learning: Analyzing
the connection to overﬁtting. In CSF, 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin
Recht, and Oriol Vinyals. Understanding deep learning
requires rethinking generalization. In ICLR , 2017.
White-box vs Black-box: Bayes Optimal Strategies for Membership Inference
A. Derivations
A.1. Accuracy of the 0-1 attack
We noteg1the binary random variable that indicates whether
z1was classiﬁed correctly, and thus considered part of the
training set by the 0-1 attack. The attack is accurate if
g1= 1 on training images and g1= 0 on other images.
This happens with probability
pbayes=P(m1=g1)
=P(g1=1|m1=1)P(m1=1) +P(g1=0|m1=0)P(m1=0)
=λptrain+ (1−λ)(1−ptest). (42)
A.2. Gaussian data
Estimation of average distribution. We assume without
loss of generality that µ= 0.θis the mean of nGaussian
variables, centered on µwith covariance I. Thus,θfollows
a Gaussian distribution, of variance1
nI.
∫
te−ℓ(z,t)p(t)dt=1√
det(2π
nI)∫
te−∥z−t∥2−n∥t∥2
2dt
(43)
Denotingω:=z
n+1, we have
n∥t∥2+∥z−t∥2= (n+ 1)∥t−ω∥2+n
n+ 1∥z∥2,
(44)
hence
∫
te−∥z−t∥2−n∥t∥2
2dt=√
det(2π
n+ 1I)
e−n∥z∥2
2(n+1).(45)
We have:
log(∫
te−ℓ(z,t)p(t)dt)
=C−n
2(n+ 1)∥z∥2(46)
A.3. Bound on variations of a sigmoid
We show that
σ(u)≤σ(v) +|u−v|+/4∀u,v∈R. (47)
Sinceσis increasing, the relation is obvious for v>u .
Foru>v , we observe that
sup
u|σ′(u)|= sup
ue−u
(1 +e−u)2=1
4. (48)
Thus,σis Lipschitz-continuous with constant 1/4, which
entails Equation (47).A.4. Hessian approximations
We give here a rough justiﬁcation of the approximation
conducted in the MATT paragraph of Section 5.
Equation (37) writes:
log(P(θ|m1= 1,z1,T)
P(θ|m1= 0,z1,T))
(49)
≈−(θ−θ∗
1)TH(θ−θ∗
1) + (θ−θ∗
0)TH(θ−θ∗
0).
(50)
This approximation holds up to the following quantity:
δ=−1
2log(det (H1)
det (H0))

δ1+ (θ∗
1−θ∗
0)T(H1−H0)(θ∗
1−θ∗
0)  
δ2.
(51)
We reason qualitatively in orders of magnitude. θ∗
0−θ∗
1
has order of magnitude 1/n, andH1−H0has order of
magnitude 1, soδ2has order of magnitude 1/n2. As for
δ1, we observe that H−1
0(H1−H0)has order of magnitude
1/nand therefore
δ1=−1
2log(det (H1)
det (H0))
(52)
=−1
2log(
det(
I+H−1
0(H1−H0)))
(53)
≈−Tr(H−1
0(H1−H0)). (54)
Hence,δ1has order of magnitude 1/nas well. Since the
main term in Equation (37) is in the order of 1/√n,δ1and
δ2can be safely neglected."
2403.16439,D:\Database\arxiv\papers\2403.16439.pdf,How does the incorporation of map uncertainty in trajectory prediction models affect the models' ability to understand and adapt to unreliable map estimations?,"Incorporating map uncertainty allows prediction models to recognize when map element estimations are unreliable and adjust their outputs accordingly, leading to significant accuracy improvements, particularly in endpoint prediction.","trajectory prediction models to incorporate upstream map
uncertainty, choosing these models as they implement the
two dominant paradigms of encoding map information:
GNNs and Transformers, respectively.
At a high-level, DenseTNT [13] leverages VectorNet [9]
to extract features from lanes and agents. It employs a hi-
erarchical GNN consisting of two stages: local informa-
tion from individual polylines is first aggregated and en-
coded, followed by global interactions between the result-
ing polyline node features. DenseTNT [13] then employs
a dense goal probability estimation technique to predict the
endpoints of trajectories and generates complete trajectories
based on the best goal candidates. To augment DenseTNT
to incorporate map uncertainty, we integrate map element
vertex uncertainty into the lane feature encoding, alongside
the vertex coordinates (as in Eq. (3)). These uncertainty-
enhanced vectors are then encoded with VectorNet [9].
HiVT [13] similarly encodes scene context in two hier-
archical stages: first encoding local context (relative to each
agent), followed by global interaction modeling between
the local neighborhoods to capture long-range dependencies
and scene-level dynamics. The resulting agent embeddings
are then decoded with an MLP to produce the parameters of
a multimodal trajectory distribution.
We augment HiVT [13] to incorporate map uncertianty
by inputting the estimated map as a point set , instead of a
vector set as in the original model, enabling the direct in-
corporation of vertex uncertainty in the encoder. Specifi-
cally, the uncertainty (scale parameter b) of each point is
directly concatenated with the mean values of the point set,
which are then encoded by the local neighborhood encoder
together with agent trajectory information.
As we will show in Sec. 5.3, incorporating polyline un-
certainty directly in this manner enables prediction models
to understand when map element estimations may be unre-
liable and adjust their outputs accordingly, yielding signifi-
cant accuracy improvements.
5. Experiments
5.1. Experiment Setup
Dataset. We evaluate our probabilistic map estimation
and prediction framework on the large-scale nuScenes
dataset [1], which provides ground truth (GT) HD maps,
sensor data (RGB images), as well as agent trajectories. It
consists of 1000 driving scenes with each scene sampled at
2Hz, and is split into training, validation, and test sets con-
taining 500, 200, and 150 scenes, respectively.
We leverage trajdata [17] to provide a unified inter-
face between vectorized map estimation models and down-
stream prediction models. To ensure compatibility across
prediction models, we upsample nuScenes’ data frequency
to 10Hz (from its original 2Hz) using trajdata’s time in-
terpolation utilities [17]. This modification provides adenser dataset, thereby facilitating finer-grained analyses
and aligning our data more closely with the real-time exe-
cution rates of onboard prediction models. Finally, we task
each prediction model to predict motion 3 seconds into the
future from 2 seconds of history.
Metrics. The Chamfer distance DChis employed to
measure the distance between two maps (represented as
point sets S1andS2). Formally,
DCh=X
x∈S1min
y∈S2∥x−y∥2
|S1|+X
y∈S2min
x∈S1∥y−x∥2
|S2|.(4)
In line with prior works [22, 23, 38], we adopt Average
Precision (AP) as the evaluation metric for our probabilis-
tic map construction of four map elements: road bound-
ary, pedestrian crossing, lane divider, and lane centerlines.
Mean AP (mAP) is further calculated as the mean AP under
three distinct DChthresholds: 0.5 m, 1.0 m, and 1.5 m.
For trajectory prediction, we evaluate our model on stan-
dard metrics adopted by numerous recent prediction chal-
lenges [3, 8, 36], specifically minimum Average Displace-
ment Error (minADE), minimum Final Displacement Er-
ror (minFDE), and Miss Rate (MR) [3]. For each agent,
6 potential trajectories are output for evaluation. The mi-
nADE metric computes the average Euclidean ( ℓ2) distance
in meters across all future time steps between the most ac-
curately predicted trajectory and the ground truth trajectory.
Similarly, minFDE calculates the error of only the final pre-
dicted time step. The most accurately predicted trajectory
is identified based on having the smallest FDE. MR quan-
tifies the proportion of scenarios where the endpoint of the
best-predicted trajectory deviates from the ground truth tra-
jectory’s endpoint by more than 2.0meters.
Data Preprocessing and Training. We standardize all
agent and lane features by transforming their coordinates
to be relative to ego-vehicle’s position, as well as rotating
the scene to make the A V’s heading point up. As a conse-
quence, we also transform the map uncertainty with
σx′=q
σ2xcos2(θ) +σ2ysin2(θ),
σy′=q
σ2xsin2(θ) +σ2ycos2(θ),(5)
where θis the rotated angle and σ=√
2·bis the Laplace
distribution’s standard deviation (derived from its scale pa-
rameter b). All models are trained using a single NVIDIA
GeForce RTX 4090 GPU. For full model hyperparameter
settings and training details, please refer to Appendix A.
5.2. Producing Map Uncertainty
Augmenting MapTR [22], MapTRv2 [23] (and its
centerline-producing version), and StreamMapNet [38] to
produce uncertainty does not substantially affect their orig-
inal mapping performance. We are able to reproduce
most models’ published performance within 2% mAP, with
some uncertainty-augmented versions even outperforming
4
(a) GT
 (b) MapTR
 (c) MapTRv2
 (d) MapTRv2-Center
 (e) StreamMapNet
Figure 3. Our proposed uncertainty formulation is able to capture uncertainty stemming from occlusions between the A V’s cameras and
surrounding map elements. Left: Images from the front and front-right cameras. Right: HD maps from our augmented online HD mapping
models. Ellipses show the std. dev. of distributions. Colors are road boundary, lane divider, pedestrian crossing, lane centerline.
(a) GT
 (b) MapTR
 (c) MapTRv2
 (d) MapTRv2-Center
 (e) StreamMapNet
Figure 4. In a dense parking lot, many models fail to produce accurate maps. Left: Images from the rear and rear-left cameras. Right: HD
maps from our augmented online HD mapping models. Ellipses show the std. dev. of distributions. Colors indicate road boundary, lane
divider, pedestrian crossing, lane centerline.
the original models. In the following, we analyze the uncer-
tainty output by these map estimation models and identify
various sources of uncertainty that our approach captures.
Uncertainty from Occlusion. Our proposed uncertainty
formulation is able to capture uncertainty stemming from
occlusions between the A V’s cameras and the surrounding
map elements. As can be seen in Fig. 3, the top right por-
tion of the map (forward and to the right of the A V) is oc-
cluded by a red callbox and a grey parked car. Importantly,
even though our work only modifies the final output heads,
Fig. 3 shows that it is still able to identify when certain map
elements are occluded in the input RGB images.
We also observe in Fig. 3e the benefits of StreamMap-
Net’s memory module: It outputs less uncertainty in the
same top-right portion of the map, owing to its incorpora-
tion of temporal information from past frames (when map
elements were visible). Conversely, MapTR and MapTRv2
are single-frame models and cannot enjoy such benefits.
Similarly, Fig. 4 visualizes a scenario where all models
struggle to delineate between driveable road and parking
spots in a parking lot (region at the bottom of the map, be-
hind the ego-vehicle). Additionally, in easily-observed parts
of the map (the region at the top of the map, in front of theego-vehicle), all models produce confident predictions with
very little uncertainties.
Uncertainty from Sensor Range. Another important
source of uncertainty in map estimation is the distance from
the onboard cameras to the map elements, stemming from
the 2D-to-BEV transformation in many mapping models.
As can be seen in Fig. 5, map uncertainty generally in-
creases with increasing distance between the vehicle and the
corresponding map elements. We can also see that Map-
TRv2 generally yields lower uncertainties than its prede-
cessor MapTR, matching the fact that MapTRv2 is gener-
ally more accurate than MapTR [23]. StreamMapNet’s un-
certainty remains relatively constant compared to the other
per-frame models, again highlighting the benefits of aggre-
gating temporal information.
Further, note the increase in pedestrian crosswalk uncer-
tainty when MapTRv2 is tasked with estimating lane cen-
terlines. One hypothesis is that lane centerlines frequently
pass through pedestrian crosswalks, causing confusion in
the model about which polyline to optimize during training.
Uncertainty from Lighting and Weather. Fig. 6 and
Fig. 12 in Appendix B show the effect of different lighting
and weather conditions, respectively, on map estimation un-
5
(0, 5](5, 10](10, 15](15, 20](20, 25](25, 30]
Distance (m)01234Uncertainty (m)
MapTR
(0, 5](5, 10](10, 15](15, 20](20, 25](25, 30]
Distance (m)
MapTRv2
(0, 5](5, 10](10, 15](15, 20](20, 25](25, 30]
Distance (m)
MapTRv2-Centerline
(0, 5](5, 10](10, 15](15, 20](20, 25](25, 30]
Distance (m)
StreamMapNet
Map Element
Boundary
Centerline
Divider
Ped. CrossingFigure 5. Our uncertainty formulation captures the fact that uncertainty generally increases with the distance between the predicted map
elements and the A V , owing to the difficulty of resolving the details of faraway objects in images. Error bars show 95% confidence intervals.
BoundaryDivider
Ped. CrossingCenterline012345Uncertainty (m)MapTR
BoundaryDivider
Ped. CrossingCenterlineMapTRv2
BoundaryDivider
Ped. CrossingCenterlineMapTRv2-Centerline
BoundaryDivider
Ped. CrossingCenterlineStreamMapNet
Time
Day
Night
Figure 6. Different time-of-day lighting can significantly affect the confidence with which map estimation models predict certain elements,
such as pedestrian crossings. Error bars show 95% confidence intervals.
certainty. In Fig. 6, we can see that map elements which are
typically lit by street lamps, vehicle headlights, and/or con-
tain reflective surfaces (i.e., lane boundaries and dividers)
retain the same level of uncertainty in day and nighttime.
Conversely, models predict pedestrian crossings with sig-
nificantly more uncertainty at night, potentially indicating
that they may not have the same consistent lighting at night
compared to other elements. Fig. 12 in Appendix B ad-
ditionally shows that StreamMapNet produces more uncer-
tainty in rainy conditions, indicating potential difficulties in
aggregating temporal information due to rain.
Uncertainty from Motion. Finally, Fig. 13 in Ap-
pendix B shows that current models do not have any partic-
ular lack of confidence across different A V driving speeds.
However, nuScenes [1] does not contain much high-speed
driving (shown in Figure 9 of [17]), leaving high-speed
analyses (e.g., about rolling shutter effects) to future work.
5.3. Incorporating Map Uncertainty in Prediction
To evaluate the effect of incorporating map uncertainty
in downstream autonomy stack components, we train
DenseTNT [13] and HiVT [41] on the outputs of the afore-
mentioned mapping models with and without our output un-
certainty formulation, yielding 16 total combinations.
Prediction Accuracy Improvements. As shown in
Tab. 1, for virtualy all mapping/prediction model combi-
nations, incorporating uncertainty yields better prediction
performance. In general, the improvements in MR are thegreatest, indicating that, by incorporating map uncertainty,
prediction models can effectively adjust their behaviors to
more closely match the ground truth future, especially at
the endpoints. Endpoint accuracy is particularly important
for trajectory prediction as many methods adopt a two-stage
pipeline where the first stage predicts possible endpoints.
Further, although MapTRv2 significantly outperforms
MapTR in map estimation [23], there is little resulting dif-
ference in prediction performance (in fact, MapTR yields
better prediction performance than MapTRv2, see the first
two sets of rows in Tab. 1). This indicates that accuracy
improvements in upstream map estimation models may not
directly improve downstream prediction accuracy.
The best prediction performance across all metrics (by
far, in some cases) is achieved when leveraging the lane cen-
terlines output by MapTRv2-Centerline. This confirms the
superiority of using centerlines to guide trajectory predic-
tion [4] and indicates where integrated systems can see the
most improvement from future map estimation research.
Most interestingly, the performance of DenseTNT
trained on maps from MapTRv2-Centerline exceeds the
performance of DenseTNT trained on GT lane center-
lines (Tab. 3). The reason for this stems from MapTRv2-
Centerline sometimes producing multiple centerlines for
one lane. For a target-based model such as DenseTNT, mul-
tiple centerlines in the same lane provides a richer set of
options for endpoint selection, focusing more closely the
resulting endpoints within lanes and yielding better predic-
6
Prediction Method HiVT [41] DenseTNT [13]
Online HD Map Method minADE ↓ minFDE ↓ MR↓ minADE ↓ minFDE ↓ MR↓
MapTR [22] 0.4015 0.8418 0.0981 1.091 2.058 0.3543
MapTR [22] + Ours 0.3854 (−4%)0.7909 (−6%)0.0834 (−15%)1.089 (0%) 2.006 (−3%)0.3499 (−1%)
MapTRv2 [23] 0.4057 0.8499 0.0992 1.214 2.312 0.4138
MapTRv2 [23] + Ours 0.3930 (−3%)0.8127 (−4%)0.0857 (−14%)1.262 (+4% ) 2.340 (+1% )0.3912 (−5%)
MapTRv2-Centerline [23] 0.3790 0.7822 0.0853 0.8466 1.345 0.1520
MapTRv2-Centerline [23] + Ours 0.3727 (−2%)0.7492 (−4%)0.0726 (−15%)0.8135 (−4%)1.311 (−3%)0.1593 (+5% )
StreamMapNet [38] 0.3972 0.8186 0.0926 0.9492 1.740 0.2569
StreamMapNet [38] + Ours 0.3848 (−3%)0.7954 (−3%)0.0861 (−7%) 0.9036 (−5%)1.645 (−5%)0.2359 (−8%)
Table 1. Quantitative prediction results for all 16 mapping/prediction model combinations on the nuScenes [1] dataset. In general, incor-
porating upstream map uncertainty improves the performance of prediction models, especially for endpoint prediction accuracy.
DenseTNT [13] Training Epochs to Convergence
Map Model Without Unc. With Unc.
MapTR [22] 8 4 (−50%)
MapTRv2 [23] 7 4 (−43%)
MapTRv2-Centerline [23] 9 7(−22%)
StreamMapNet [38] 6 4 (−33%)
Table 2. When trained with map uncertainty, DenseTNT [13] con-
sistently converges faster, arriving at equal or better validation per-
formance, irrespective of the upstream mapping model.
DenseTNT [13] + Map Model minADE ↓ minFDE ↓ MR↓
GT Map 0.8809 1.489 0.1903
MapTRv2-Centerline [23] 0.8466 (−4%)1.345 (−10%)0.1520 (−20%)
MapTRv2-Centerline [23] + Ours 0.8135 (−8%)1.311 (−12%)0.1593 (−16%)
Table 3. DenseTNT [13] is able to achieve better prediction perfor-
mance with MapTRv2-Centerline [23] compared to the GT map.
tion performance.
Training Convergence Improvements. Immediately
during training, we found that all trajectory prediction mod-
els converge significantly faster when incorporating map
uncertainty. As can be seen in Tab. 2, DenseTNT trains to
convergence much more quickly when incorporating map
uncertainty, achieving optimal validation performance 2-4
epochs earlier than when only incorporating coordinates.
Prediction Visualizations. While each model can pre-
dict trajectories for every agent, for clarity we only plot pre-
dictions for the center agent. Fig. 7 visualizes a compli-
cated intersection with many map elements (Fig. 7a). For
HiVT + MapTR, predictions without map uncertainty over-
shoot the ground truth, directly into another lane (Fig. 7b).
With map uncertainty, HiVT’s predictions stay within the
correct lane (Fig. 7c). For DenseTNT + StreamMapNet,
predictions without map uncertainty end up offroad, ignor-
ing the left road boundary (Fig. 7d). With map uncertainty,
DenseTNT’s predictions stay within the road boundary, as
StreamMapNet produced them with high certainty (Fig. 7e).
Fig. 8 visualizes the parking lot of a bus terminal, con-
taining many occlusions from stationary vehicles (Fig. 8a).
For HiVT + MapTR, predictions without map uncertainty
significantly overshoot the ground truth, directly towardsthe road boundary where many motorcycles are parked
(Fig. 8b). With map uncertainty, HiVT’s predictions much
better match the GT motion (Fig. 8c). StreamMapNet has
particular difficulty mapping this environment, predicting
road boundaries that pass through the middle of the road
and yielding errant predictions that move towards pedes-
trians (Fig. 8d). With map uncertainty, DenseTNT’s pre-
dictions stay within the road boundary and tightly cluster
around the GT future (Fig. 8e).
Fig. 9 visualizes an interesting tunnel-like entrance to
a building, with significant occlusions from trucks behind
the center agent (Fig. 9a). For HiVT + MapTR, predictions
without map uncertainty overshoot the ground truth and col-
lide with the road boundary (Fig. 9b). With map uncer-
tainty, HiVT’s predictions stay within the correct lane and
closely match the GT future (Fig. 9c). StreamMapNet again
has particular difficulty mapping this environment, predict-
ing a road boundary that directly passes over the middle of
the road, yielding errant predictions (Fig. 9d). With map un-
certainty, DenseTNT’s predictions nearly completely over-
lap with the GT future (Fig. 9e).
6. Conclusion
In this work, we propose a general vectorized map uncer-
tainty formulation and extend multiple state-of-the-art on-
line map estimation methods MapTR [22], MapTRv2 [23],
and StreamMapNet [38] to additionally output uncertainty.
We systematically analyze the resulting uncertainties and
find that our approach captures many sources of uncer-
tainty (occlusion, distance to camera, time of day, and
weather). Finally, we combine these online map estima-
tion models with state-of-the-art trajectory prediction ap-
proaches (DenseTNT [13] and HiVT [41]) and show that
incorporating online mapping uncertainty significantly im-
proves the performance and training characteristics of pre-
diction models, by up to 50% and 15%, respectively. An
exciting future research direction is leveraging these un-
certainty outputs to measure the calibration of map models
(similar to [16]). However, this is complicated by the need
for fuzzy point set matching, a challenging problem itself.
7
(a) GT
 (b) HiVT+MapTR
 (c) HiVT+MapTR
 (d) DenseTNT+Stream
 (e) DenseTNT+Stream
Figure 7. A complicated intersection with many map elements. By leveraging uncertainty information, both combinations of map estima-
tion and prediction models show enhancements in prediction, correctly predicting that the center vehicle will stay in its current lane.
(a) GT
 (b) HiVT+MapTR
 (c) HiVT+MapTR
 (d) DenseTNT+Stream
 (e) DenseTNT+Stream
Figure 8. The parking lot of a bus terminal, with many occlusions from stationary vehicles. By leveraging uncertainty information, both
combinations reduce overshoot, minimizing endpoint error, and tightly cluster the predicted trajectories around the GT future.
(a) GT
 (b) HiVT+MapTR
 (c) HIVT+MapTR
 (d) DenseTNT+Stream
 (e) DenseTNT+Stream
Figure 9. A tunnel-like entrance to a building, with significant occlusions from trucks behind the center agent. By leveraging this uncer-
tainty information, both HiVT and DenseTNT are able to produce sensible, on-road predictions, even with significant map uncertainty.
8
References
[1] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh V ora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom. nuScenes: A multi-
modal dataset for autonomous driving. In IEEE Conf. on
Computer Vision and Pattern Recognition , 2020. 4, 6, 7, 1
[2] Yigit Baran Can, Alexander Liniger, Danda Pani Paudel, and
Luc Van Gool. Structured bird’s-eye-view traffic scene un-
derstanding from onboard images. In IEEE Int. Conf. on
Computer Vision , 2021. 2
[3] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jag-
jeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter
Carr, Simon Lucey, Deva Ramanan, and James Hays. Argo-
verse: 3d tracking and forecasting with rich maps. In IEEE
Conf. on Computer Vision and Pattern Recognition , 2019. 4
[4] Daniel Dauner, Marcel Hallgarten, Andreas Geiger, and
Kashyap Chitta. Parting with misconceptions about learning-
based vehicle motion planning. In Conf. on Robot Learning ,
2023. 3, 6
[5] Nachiket Deo, Eric M. Wolff, and Oscar Beijbom. Mul-
timodal trajectory prediction conditioned on lane-graph
traversals. In Conf. on Robot Learning , 2021. 2
[6] Wenjie Ding, Limeng Qiao, Xi Qiu, and Chi Zhang. Pivot-
Net: Vectorized pivot learning for end-to-end HD map con-
struction. In IEEE Int. Conf. on Computer Vision , 2023. 2
[7] Hao Dong, Xianjing Zhang, Jintao Xu, Rui Ai, Weihao Gu,
Huimin Lu, Juho Kannala, and Xieyuanli Chen. SuperFu-
sion: Multilevel LiDAR-camera fusion for long-range HD
map generation. arXiv preprint arXiv:2211.15656 , 2022. 2
[8] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi
Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp,
Charles Qi, Yin Zhou, Zoey Yang, Aur ´elien Chouard, Pei
Sun, Jiquan Ngiam, Vijay Vasudevan, Alexander McCauley,
Jonathon Shlens, and Dragomir Anguelov. Large scale in-
teractive motion forecasting for autonomous driving: The
waymo open motion dataset. In IEEE Int. Conf. on Com-
puter Vision , 2021. 4
[9] Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir
Anguelov, Congcong Li, and Cordelia Schmid. VectorNet:
Encoding HD maps and agent dynamics from vectorized rep-
resentation. In IEEE Conf. on Computer Vision and Pattern
Recognition , 2020. 2, 4
[10] T. Gilles, S. Sabatini, D. Tsishkou, B. Stanciulescu, and F.
Moutarde. HOME: Heatmap output for future motion esti-
mation. In Proc. IEEE Int. Conf. on Intelligent Transporta-
tion Systems , 2021. 2
[11] T. Gilles, S. Sabatini, D. Tsishkou, B. Stanciulescu, and F.
Moutarde. GOHOME: Graph-oriented heatmap output for
future motion estimation. In Proc. IEEE Conf. on Robotics
and Automation , 2022. 2
[12] T. Gilles, S. Sabatini, D. Tsishkou, B. Stanciulescu, and
F. Moutarde. THOMAS: Trajectory heatmap output with
learned multi-agent sampling. In Int. Conf. on Learning Rep-
resentations , 2022. 2
[13] J. Gu, C. Sun, and H. Zhao. DenseTNT: End-to-end trajec-
tory prediction from dense goal sets. In IEEE Int. Conf. on
Computer Vision , 2021. 2, 3, 4, 6, 7, 1[14] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,
Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai
Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu
Qiao, and Hongyang Li. Planning-oriented autonomous driv-
ing. In IEEE Conf. on Computer Vision and Pattern Recog-
nition , 2023. 2
[15] Junjie Huang and Guan Huang. BEVPoolv2: A cutting-
edge implementation of BEVDet toward deployment. arXiv
preprint arXiv:2211.17111 , 2022. 3
[16] Boris Ivanovic, James Harrison, and Marco Pavone. Ex-
panding the deployment envelope of behavior prediction via
adaptive meta-learning. In Proc. IEEE Conf. on Robotics and
Automation , 2023. 2, 7
[17] Boris Ivanovic, Guanyu Song, Igor Gilitschenski, and Marco
Pavone. trajdata: A unified interface to multiple human tra-
jectory datasets. In Conf. on Neural Information Process-
ing Systems Datasets and Benchmarks Track , New Orleans,
USA, 2023. 4, 6
[18] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie
Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang,
and Xinggang Wang. V AD: Vectorized scene representation
for efficient autonomous driving. In IEEE Int. Conf. on Com-
puter Vision , 2023. 2
[19] Qi Li, Yue Wang, Yilun Wang, and Hang Zhao. HDMapNet:
An online HD map construction and evaluation framework.
InProc. IEEE Conf. on Robotics and Automation , 2022. 2
[20] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-
hao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. BEVFormer:
Learning bird’s-eye-view representation from multi-camera
images via spatiotemporal transformers. In European Conf.
on Computer Vision , 2022. 2
[21] Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song
Feng, and Raquel Urtasun. Learning lane graph representa-
tions for motion forecasting. In European Conf. on Computer
Vision , 2020. 2
[22] Bencheng Liao, Shaoyu Chen, Xinggang Wang, Tianheng
Cheng, Qian Zhang, Wenyu Liu, and Chang Huang. MapTR:
Structured modeling and learning for online vectorized HD
map construction. In Int. Conf. on Learning Representations ,
2023. 1, 2, 3, 4, 7
[23] Bencheng Liao, Shaoyu Chen, Yunchi Zhang, Bo Jiang, Qian
Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang.
MapTRv2: An end-to-end framework for online vectorized
HD map construction. arXiv preprint arXiv:2308.05736 ,
2023. 2, 3, 4, 5, 6, 7, 1
[24] Yicheng Liu, Jinghuai Zhang, Liangji Fang, Qinhong Jiang,
and Bolei Zhou. Multimodal motion prediction with stacked
transformers. In IEEE Conf. on Computer Vision and Pattern
Recognition , 2021. 2
[25] Yicheng Liu, Yuan Yuantian, Yue Wang, Yilun Wang, and
Hang Zhao. VectorMapNet: End-to-end vectorized HD map
learning. In Int. Conf. on Machine Learning . PMLR, 2023.
2
[26] Zhijian Liu, Haotian Tang, Alexander Amini, Xingyu Yang,
Huizi Mao, Daniela Rus, and Song Han. BEVFusion: Multi-
task multi-sensor fusion with unified bird’s-eye view repre-
sentation. In Proc. IEEE Conf. on Robotics and Automation ,
2023. 2
9
[27] T. Phan-Minh, E. C. Grigore, F. A. Boulton, O. Beijbom,
and E. M. Wolff. CoverNet: Multimodal behavior prediction
using trajectory sets. In IEEE Conf. on Computer Vision and
Pattern Recognition , 2020. 2
[28] Jonah Philion and Sanja Fidler. Lift, Splat, Shoot: Encoding
images from arbitrary camera rigs by implicitly unprojecting
to 3D. In European Conf. on Computer Vision , 2020. 2, 3
[29] Limeng Qiao, Wenjie Ding, Xi Qiu, and Chi Zhang. End-to-
end vectorized HD-map construction with piecewise bezier
curve. In IEEE Conf. on Computer Vision and Pattern Recog-
nition , 2023. 2
[30] Andrey Rudenko, Luigi Palmieri, Michael Herman, Kris M.
Kitani, Dariu M. Gavrila, and Kai O. Arras. Human mo-
tion trajectory prediction: A survey. Int. Journal of Robotics
Research , 39(8):895–935, 2020. 2, 3
[31] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and
Marco Pavone. Trajectron++: Dynamically-feasible trajec-
tory forecasting with heterogeneous data. In European Conf.
on Computer Vision , 2020. 2
[32] Juyeb Shin, Francois Rameau, Hyeonjun Jeong, and Dong-
suk Kum. InstaGraM: Instance-level graph modeling for vec-
torized HD map learning. arXiv preprint arXiv:2301.04470 ,
2023. 2
[33] Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, Silei
Wu, Hanming Deng, Yi Gu, Lewei Lu, Ping Luo, Dahua Lin,
and Hongyang Li. Scene as occupancy. In IEEE Int. Conf.
on Computer Vision , 2023. 2
[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Conf. on Neural
Information Processing Systems , 2017. 2
[35] Waymo. Safety report, 2021. Available at https://
waymo.com/safety/safety-report . 1
[36] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lam-
bert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Rat-
nesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes,
Deva Ramanan, Peter Carr, and James Hays. Argoverse 2:
Next generation datasets for self-driving perception and fore-
casting. In Conf. on Neural Information Processing Systems
Datasets and Benchmarks Track , 2021. 4
[37] Zhenhua Xu, Kenneth KY Wong, and Hengshuang Zhao.
InsightMapper: A closer look at inner-instance informa-
tion for vectorized high-definition mapping. arXiv preprint
arXiv:2308.08543 , 2023. 2
[38] Tianyuan Yuan, Yicheng Liu, Yue Wang, Yilun Wang, and
Hang Zhao. StreamMapNet: Streaming mapping network
for vectorized online HD map construction. In IEEE Winter
Conf. on Applications of Computer Vision , 2024. 2, 3, 4, 7, 1
[39] Ye Yuan, Xinshuo Weng, Yanglan Ou, and Kris M. Kitani.
AgentFormer: Agent-aware transformers for socio-temporal
multi-agent forecasting. In IEEE Int. Conf. on Computer Vi-
sion, pages 9813–9823, 2021. 2
[40] H. Zhao, J. Gao, T. Lan, C. Sun, B. Sapp, B. Varadarajan, Y .
Shen, Y . Shen, Y . Chai, C. Schmid, C. Li, and D. Anguelov.
TNT: Target-driveN Trajectory Prediction. In Conf. on Robot
Learning , 2020. 2
[41] Zikang Zhou, Luyao Ye, Jianping Wang, Kui Wu, and Kejie
Lu. HiVT: Hierarchical vector transformer for multi-agentmotion prediction. In IEEE Conf. on Computer Vision and
Pattern Recognition , 2022. 1, 2, 3, 6, 7
10
Producing and Leveraging Online Map Uncertainty in Trajectory Prediction
Supplementary Material
A. Training Details
To account for possible differences in the rates of con-
vergence for mapping and prediction models trained with
and without uncertainty, each model’s hyperparameters are
tuned separately to optimize performance. The best out-
comes from these individually-tuned models are compared
to show the effect of integrating uncertainty. Tab. 4 summa-
rizes the core method hyperparameters.
In the probabilistic map estimation approaches for
MapTR [22] and MapTRv2 [23], we alter the loss func-
tion from the initial ℓ1loss to the Negative Log-Likelihood
(NLL) of the Laplacian distribution. The Laplace output is
added to all layers of MapTR’s Transformers. The two core
reasons we chose a Laplace distribution are: it produced
more accurate maps across models ( ∼3-5% better AP) and
was much more numerically stable during training (Fig. 14).
We also adjust the regressor’s loss weight from 5 to
0.03, compensating for the increased gradient norm result-
ing from the new loss function and difficulty in training.
Given the use of a single GPU, we reduce the learning rate
to 1.5E-4. Additionally, to avert gradient explosion, we clip
gradients to a maximum norm of 3. All other settings are
retained as per the original configurations.
For StreamMapNet [38], we incorporate two distinct
dataset splits: the original nuScenes [1] split and a newly
proposed split. This new split is designed to address the
overlapping scene challenges in the original training and
validation splits [38]. To enhance StreamMapNet’s perfor-
mance, we train using the new split to reduce overfitting
risks and assess on the original nuScenes validation set,
aligning with the scenarios used in MapTR and MapTRv2.
In line with these adjustments, the loss weight of the regres-
sor is lowered to 2, the learning rate is set to 1.25e-4, and a
maximum gradient norm of 3 is maintained for clipping.
After modification, most of the map estimation models
maintain their original performance. As shown in Tab. 5,
MapTR and MapTRv2 produce 1% better AP when produc-
ing uncertainty. This is admittedly a small improvement,
but it comes for free along with the other benefits stated in
the main body of the paper.
For the HiVT [41] prediction model, we have increased
the dropout rate to 0.2. All other hyperparameters are un-
changed. For DenseTNT [13], the hyperparameters are
tuned separately for each combination to yield the optimal
results. The hyperparameters used for different methods are
shown in Tab. 6.
For HiVT, we double the node dimension to account for
uncertainty in both the xandydirections. For DenseTNT,
the configurations of layer sizes and structures are main-Method Regression Loss Weight LR Gradient Norm
MapTR [22] 0.03 1.50E-4 3
MapTRv2 [23] 0.03 1.50E-4 3
MapTRv2-Centerline [23] 0.03 1.50E-4 3
StreamMapNet [38] 2 1.25E-4 3
Table 4. Training hyperparameters.
Online HD Map Method mAP
MapTR [22] 0.4488
MapTR [22] + Ours 0.4525 ( −1%)
MapTRv2 [23] 0.5540
MapTRv2 [23] + Ours 0.5592 ( −1%)
MapTRv2-Centerline [23] 0.4789
MapTRv2-Centerline [23] + Ours 0.4655 ( +3% )
StreamMapNet [38] 0.7789
StreamMapNet [38] + Ours 0.7043 ( +10% )
Table 5. Map estimation performance when producing uncertainty.
Online HD Map Method LR Batch Size Dropout
MapTR [22] 0.001 64 0.5
MapTR [22] + Ours 0.0005 64 0.5
MapTRv2 [23] 0.0005 64 0.5
MapTRv2 [23] + Ours 0.0005 64 0.5
MapTRv2-Centerline [23] 0.001 64 0.5
MapTRv2-Centerline [23] + Ours 0.00018 64 0.5
StreamMapNet [38] 0.0005 16 0.1
StreamMapNet [38] + Ours 0.001 64 0.5
Table 6. Hyperparameters chosen for different mapping methods
for DenseTNT [13]
tained without significant alterations. The model utilizes
a 128-dimensional vector to represent lane information,
including details like vertices, intersection signals, traffic
lights, etc. In our adaptation, we merely integrate uncer-
tainty information into this raw feature vector.
Distant Agents. For both HiVT [41] and
DenseTNT [13], there is no special treatment for agents
that are beyond map perception range. This means that
some far-away agents do not have agent-lane interactions to
incorporate, making the model rely only on the agent’s past
history, surrounding agent motion (agent-agent interactions
remain unchanged), and any learned priors as a result of
training with the absence of far-away map information.
B. Additional Visualizations
Fig. 10 visualizes the predictions of other agents when using
multi-agent prediction models such as HiVT.
Fig. 11 shows another qualitative example of how occlu-
sion impacts model uncertainty.
Fig. 12 shows that StreamMapNet [38] produces more
1"
2005.11794,D:\Database\arxiv\papers\2005.11794.pdf,"The paper describes a system for controlling a crane and damping payload oscillations.  What are the limitations of the proposed cable length estimation algorithm, and how do these limitations affect the overall performance of the crane control system?","The cable length estimation algorithm relies on the payload oscillating, and its performance degrades significantly when the oscillations are small or nonexistent. This limitation can affect the overall performance of the crane control system, particularly during initial maneuvers where the payload may not be oscillating significantly.","4
TABLE I
GEOMETRICAL PARAMETERS OF THE CRANE
Term Value Term Value Term Value
l10.711m eb20.154m ap30.167m
l21.500m ap20.600m ep30.076m
l30.205m ep20.130m θ4−39.4deg
l40.992m ab30.750m
ab20.550m eb30.160m
the frame has the same orientation as the inertial frame, tha t isR0
5=I. The rotation matrix from frame 5to frame 6is
R5
6=Rx(φx)Ry(φy), which also leads to R0
6=Rx(φx)Ry(φy). Frame 6 is body-ﬁxed frame of the payload cable. The
matrices Rx,Ry,Rz∈SO(3) are the orthogonal rotation matrices about the x,yandzaxes, respectively [39]. The angle
αi(qi)is deﬁned as
αi(qi) =arccosq2
i−b2
i1−b2
i2
−2bi1bi2+arctanebi
abi
+arctanepi
api−ci,fori= 2,3,(2)
wherec2= 0.5π,c3=π,b2
i1=a2
bi+e2
bi,b2
i2=a2
pi+e2
piandabi,api,ebi,epiare deﬁned in Fig. 3. The rates of the orientation
angles (2) can be found by time differentiation which gives ˙αi= ˙qi∂αi/∂qi. The relative angular velocities between the
frames are given as
ω1
01= ˙q1z1
1,ωi
i−1,i= ˙αizi
i,fori= 2,3, (3)
wherezi
i= [0,0,1]Tandω4
34=0. The distance vectors between the origins of the frames give n in the coordinate of the local
frame as
p0
01=0,p1
12=[0 0l1]T,
pi
i,i+1=[
li0 0]T,fori= 2,3,4.(4)
The distance vector from the origin of frame 1to the origin of frame 5given in the coordinates of frame 0is found as
p0
15=R0
1{
p1
12+R1
2[
p2
23+R2
3(p3
34+R3
4p4
45)]}
, (5)
the distance vector from the origin of frame 2to the origin of frame 5given in the coordinates of frame 0is found as
p0
25=R0
1R1
2[
p2
23+R2
3(p3
34+R3
4p4
45)]
(6)
and the distance vector from the origin of frame 3to the origin of frame 5given in the coordinates of frame 0is found as
p0
35=R0
1R1
2R2
3(p3
34+R3
4p4
45). (7)
The linear velocity of the origin of frame 5due to the rotations of the joints is given in the coordinates of the inertial frame as
v0
05=[ˆz0
1p0
15ˆz0
2p0
25ˆz0
3p0
35]
˙q1
˙α2
˙α3
, (8)
whereˆ·denotes the skew-symmetric form of a vector, z0
1=R0
1z1
1,z0
2=R0
2z2
2andz0
3=R0
3z3
3. The expression of the velocity
(8) can be written as
v0
05=J˙qc, (9)
where the Jacobian
J=[
ˆz0
1p0
15ˆz0
2p0
25ˆz0
3p0
35]
1 0 0
0∂α2
∂q20
0 0∂α3
∂q3
 (10)
maps the rates of the generalized coordinates of the crane to the linear velocity of the crane tip.
5
Fig. 4. Three cameras are installed on a rack, which is rigidl y attached to the king of the crane. The spherical markers are projected as pixels into the image
planes of the cameras.
C. Payload modeling
The crane payload is modeled as a spherical pendulum, where t he mass of the cable is neglected and the mass mof the
payload is lumped at the end of the cable of length L. The payload has a body-ﬁxed frame 6, which has the origin coinciding
with the origin of frame 5. The equations of motion are derived using Kane’s method [40 ]. Assume that velocity of the origin
of frame 5relative to frame 0is given in the coordinates of frame 0as¯v0
05= [˙x5,˙y5,˙z5]T, where we assume that ˙z5= 0.
Then the velocity of the payload relative to frame 0and expressed in the coordinates of frame 0is
v0
p=¯v0
05+ˆω0
06R0
6p6
L, (11)
wherep6
L= [0,0,L]T,ω0
06=ω0
56is the angular velocity of frame 6relative to frame 0expressed in the coordinates of
frame0, andˆ·denotes the skew-symmetric form of a vector. The accelerati on of the payload can be derived from (11) as
a0
p=dv0
p/dt. Provided that the partial velocities with respect to the pe ndulum generalized speeds are given as v0
p,1=∂v0
p/∂˙φx
andv0
p,2=∂v0
p/∂˙φy, then the equations of motion of the spherical pendulum are f ormulated as
v0
p,i(−a0
pm+g0) = 0,fori= 1,2, (12)
whereg0= [0,0,mg]Tis the force of gravity given in the coordinates of frame 0andgis the acceleration of gravity. The
equations of motion (12) can explicitly be written as
¨φxcy+ω2
0sx= 2˙φx˙φysy+ ˙vycx/L,
¨φy+ω2
0cxsy=−˙φ2
xsycy−(˙vxcy+ ˙vysxsy)/L,(13)
whereω2
0=g/L,˙vx= ¨x5,˙vy= ¨y5,si= sinφiandci= cosφi.
III. E STIMATION OF PAYLOAD MOTION
A. Vision
In this section we present the procedure for determination o f the payload orientation angles (φx,φy)(see Fig. 4) using three-
camera measurements. The measured angles are further used i n combination with an extended Kalman ﬁlter for estimation o f
the orientation angles and angular velocities. Each camera ihas a camera-ﬁxed frame ci.
Consider two points X1= [X1,Y1,Z1]TandX2= [X2,Y2,Z2]Tgiven in Fig. 4, where each of the points is located in
the center of a spherical marker. These points can be given re lative to frame c1of camera 1, and expressed in the coordinates
of the inertial frame as
Xj=p0
c1,5+R0
6∆je3,forj= 1,2, (14)
wherep0
c1,5=p0
05−p0
0,c1is the vector from the origin of frame c1to the origin of frame 5, andR0
6∆je3is the position of
the marker relative to the crane tip. The term ∆jis the scalar distance from the origin of frame 5to the center of a spherical
6
marker and e3= [0,0,1]T. Consider a line through the points Xjwith a normalized direction vector rp= [rx,ry,rz]Tgiven
as
rp=X2−X1
∆2−∆1=R0
6e3=[
sy−sxcycxcy]T. (15)
The points Xjare seen in the image plane Iiof cameraias pixels xi
j= [ui
j,vi
j]T. The coordinates of the pixels in the image
plane can be obtained as
˜xi
j=Pi˜Xj, (16)
where the tilde notation ˜·is a homogeneous representation of a vector [39]. The method for tracking and extracting the points
xi
jcan be summed up by the following steps
•Blur image Iiby a Gaussian function;
•ConvertIifrom the RBG to the HSV color space and deﬁne a binary image ˆIibased on the range of pixel values in Ii;
•Apply the morphology operators erosion and dilation to ˆIi;
•Enclose objects by circles and remove objects that are outsi de the range of desired radii;
•Find raw image moments of the objects and calculate the centr oidsxi
1= [ui
1,vi
1]Tandxi
2= [ui
2,vi
2]Twherevi
2>vi
1.
The camera matrix Pifor cameraiin (16) is deﬁned as
Pi=Ki[
Rci
0|tci
ci,c1]
, (17)
where the rotation matrix from the inertial frame to frame ciis given as R0
ci=R0
1Rx(π/2)TRy(π/2)fori= 1,2,3andKi
is the camera calibration matrix for camera i[39]. The constant translation vectors from frame cito framec1, expressed in
the coordinates of ciaretc2
c2,c1=−δ12e1andtc3
c3,c1=−(δ12+δ23)e1, wheree1= [1,0,0]T. The terms in (16) will satisfy
the equality ˆ˜xi
jPi˜Xj=0. Deﬁne ¯Ai
j=ˆ˜xi
jPi∈R3×4, and let Ai
j∈R2×4denote the matrix which is formed by removing
the last row of ¯Ai
j. ThenAi
j˜Xj=0, where
Ai
j=[vi
jP3
i−P2
i
P1
i−ui
jP3
i]
, (18)
andPk
idenotes the k-th row of Pi. Similarly as in (18) the equality Aj˜Xj=0can be formulated taking into account
measurement from all the cameras, where the matrix Ajis given as
Aj=[(A1
j)T(A2
j)T(A3
j)T]T. (19)
The constraint equation Aj˜Xj=0requires that the matrices Aj∈R6×4have rank 3 if the points xi
jare exact without noise.
Singular value decomposition of Ajgives
Aj=4∑
k=1σj
kuj
k(νj
k)T, (20)
whereσj
4= 0 if the points xi
jare exact without noise and σj
4>0if the points are noisy. The measurements ¯Xjof the actual
pointsXjare given by
˜¯Xj=λjνj
4, (21)
whereλj̸= 0 are scaling factors and νj
4= [Xj,Yj,Zj,1]T/λj. The measurement ¯rp= [¯rx,¯ry,¯rz]Tof the direction vector
rpis given by
¯rp=¯X2−¯X1
∥¯X2−¯X1∥2. (22)
The measurement y= [y1,y2]Tof the pendulum orientation angles (φx,φy)is then
y1=arctan[
−¯ry
¯rz]
, y2=arctan[¯rx
(¯r2y+ ¯r2z)1/2]
, (23)
where (15) is used. This solution is based on the direct linea r transformation algorithm in [25].
7
B. Extended Kalman Filter
Consider the state vector given as
z=[
φxφy˙φx˙φynxny]T, (24)
whereφiand˙φiare the pendulum orientation angles and angular velocities , whileniare the bias states due to calibration
error. The input vector is given as
a=[˙vx˙vy]T, (25)
where˙vxand˙vyare the crane tip accelerations in xandydirections with respect to the inertial frame. The spherica l pendulum
dynamics are assumed to be imposed by the white process noise wsuch that it can be written as a nonlinear stochastic system
˙z=f(z,a,L)+w, (26)
wheref(z,a,L)is given by (13) and the bias error n= [nx,ny]Tis modeled as random walk. The measurement (23)
is obtained at discrete times tk,tk+1,tk+2,...with a constant time step ∆tasyk=y(tk). Using this measurement and
discretization of (26) yields to
zk+1=fk(zk,ak,L)+wk,
yk=h(zk)+vk,(27)
where
fk(zk,ak,L) =zk+f(zk,ak,L)∆t (28)
and
h(z) =[φx+nxφy+ny]T. (29)
The process and measurement noise are wk∼ N(0,Q)andvk∼ N(0,R)with covariance matrices QandR. The extended
Kalman ﬁlter [41] is summarized in Algorithm 1 with transiti on and observation matrices
Fk=∂fk
∂z⏐⏐⏐
ˆzk,ak,H=Hk=∂h
∂z⏐⏐⏐
¯zk. (30)
The pendulum oscillation angles and angular velocities [φx,φy,˙φx,˙φy]Tare then extracted from ˆzk.
Algorithm 1 Extended Kalman Filter Implementation
1:k= 1,ˆzk−1=ˆz0,ˆPk−1=ˆP0
2:loop
3:¯zk=fk−1(ˆzk−1,ak−1,L)
4:¯Pk=Fk−1ˆPk−1FT
k−1+Q
5:Kk=¯PkHT(
R+H¯PkHT)−1
6:ˆzk=¯zk+Kk(yk−H¯zk)
7:ˆPk= (I−KkH)¯Pk
8:k=k+1
9:end loop
IV. C ABLE LENGTH ESTIMATION
In this work the payload cable length Lis assumed to be the distance from the payload suspension poi nt to the center of
gravity of the payload.
Modern cranes are equipped with encoders for measuring the l ength of the released cable Lhdown to the hook, however,
in practice, the payload is normally suspended from the hook using additional slings or chains, with unknown length Ls, as
shown in Fig. 5. In some cases Lscan be signiﬁcant in relation to Lh, which means that the total length of the payload cable
Lin the pendulum model should be found as a sum of both. In this s ection we present the procedure for estimation of the
total cable length L, which is required both in the control law and in the Kalman ﬁl ter algorithm. We assume that the cable
lengthLis bounded by Lmin≤L≤Lmax. In fact, the dynamics of the pendulum in one plane is sufﬁcie nt to estimate the
length of the cable, therefore we propose to use only φxpendulum oscillations.
Provided that φxis sufﬁciently small, we can linearize the ﬁrst equation of ( 13) about the equilibrium point (φx,˙φx) = (0,0),
which leads to
¨φx=1
L∗(−gφx+ ˙vy), (31)
8
Fig. 5. The total cable length is a sum of crane cable outlet Lhand the effective length of slings Ls.
whereL∗is the true unknown cable length. In our application its not p ossible to measure ¨φxand the use of differentiation is
not desirable. One way to solve it is to ﬁlter both side of (31) with a1-order stable ﬁlter 1/Λ(s), whereΛ(s) =s+λ0is a
Hurwitz polynomial in s. Then the Laplace transformation of (31) yields to the linea r parametric model
z=η∗ψ, (32)
whereη∗= 1/L∗and
z= [˙φx(s)s]/Λ(s),
ψ= [−gφx(s)+ ˙vy(s)]/Λ(s).(33)
The variables zandψcan be obtained without using differentiation. Consider η(t)to be the estimate of η∗at timet, then the
estimated value ˆzof the output zis obtained as ˆz=ηψ. Since the model (32) is an approximation of the true model (1 3), we
choose a least-square method for estimating η. We introduce the normalized estimation error
ǫ= (z−ˆz)/m2
s= (z−ηψ)/m2
s, (34)
wherem2
s= 1+n2
s, the normalizing signal nsis chosen to be n2
s=γψ2andγis a time-varying adaptive gain to be decided.
The optimal ηshould minimize a cost function J(η), whereη∈ S andSis a convex set given by
S={η∈R|g(η)≤0}, (35)
whereg:R→Ris a smooth function. In [33] the author suggested the follow ing cost function
J(η) =1
2∫t
0e−β(t−τ)ǫ2(t,τ)m2
s(τ)dτ+1
2γ0e−βt(η−η0)2, (36)
whereǫ(t,τ) = (z(τ)−η(t)ψ(τ))/m2
s(τ), the initial value of γisγ0>0, the forgetting factor β >0and the initial value of
ηisη0. If estimates η(t)are bounded by 1/Lmax≤η≤1/Lmin, then the new variable ˘η=η−ηais bounded by
−¯η≤˘η≤¯η, (37)
whereηa= 1/Lmax+ ¯ηand
¯η=1
2Lmax−Lmin
LmaxLmin. (38)
The inequality (37) can be re-written as |˘η| ≤¯η, which leads to that the inequality ˘η2−¯η2≤0is also satisﬁed. The inequality
˘η2−¯η2≤0can be explicitly written as
η2−2ηηa+η2
a−¯η2≤0. (39)
Provided that g(η)should be not greater than zero (35), then we suggest that the left-hand side of (39) is a reasonable admissible
function for g(η), that isg(η) =η2−2ηηa+η2
a−¯η2, which can alternatively be written as
g(η) =η2−ηLmax+Lmin
LmaxLmin+1
LmaxLmin, (40)
then the gradient of (40) is deﬁned as
∇g(η) = 2η−Lmax+Lmin
LmaxLmin. (41)
9
In [33] the solution to the deﬁned optimisation problem was c alled the least-squares algorithm with projection and was g iven
by
˙η=

γǫψ ifg(η)<0
or ifg(η) = 0 and(γǫψ)∇g(η)≤0
0 otherwise(42)
and
˙γ=

βγ−γ2ψ2/m2
sifg(η)<0
or ifg(η) = 0 and(γǫψ)∇g(η)≤0
0 otherwise.(43)
The initial guess of the pendulum length L0= 1/η0should satisfy Lmin≤1/η0≤Lmax. The performance of the cable length
estimation algorithm is studied by the experiment. The esti mate of the cable length L= 1/ηis used in the control problem
and the extended Kalman ﬁlter.
V. C ONTROL
The controller was designed with a payload damping controll er in an inner loop, and a controller for the crane tip motion
in an outer loop. We propose a controller
¨x5= 2Lζω0˙φy+ux,
¨y5=−2Lζω0˙φx+uy(44)
with feedback from the angular rates ˙φxand˙φy, where the acceleration (¨x5,¨y5)of the crane tip in the horizontal plane is
the control variable, and uxanduyare the control variables of the outer control loop. Impleme ntation issues related to the
use of acceleration for the control variables are discussed at the end of this section. The closed loop dynamics of the pay load
linearized about (φx,φy,˙φx,˙φy) =0are found from (13) and (44) to be
¨φx+2ζω0˙φx+ω2
0φx=uy
L,
¨φy+2ζω0˙φy+ω2
0φy=−ux
L,(45)
It is seen that in a special case when (ux,uy) = (0,0)the linearized closed loop system is two harmonic oscillato rs with
undamped natural frequency ω0and relative damping ζ. In a general case when (ux,uy)are not necessarily zero, the Laplace
transform of the closed loop dynamics (45) gives
φx(s) =G(s)uy(s), φy(s) =−G(s)ux(s), (46)
where the transfer function G(s)is
G(s) =1
L(s2+2ζω0s+ω2
0). (47)
Insertion of (46) into the Laplace transform of (44) gives
x5(s) =H(s)
s2ux(s), y5(s) =H(s)
s2uy(s), (48)
where
H(s) = 1−2Lζω0sG(s) =s2+ω2
0
s2+2ζω0s+ω2
0. (49)
For frequencies ω≪ω0it follows that H(jω)→1and (48) simpliﬁes to
x5(s) =1
s2ux(s), y5(s) =1
s2uy(s). (50)
The position of the crane tip can be controlled with a PD contr oller
ux=kp(xd−x5)+kd(˙xd−˙x5),
uy=kp(yd−y5)+kd(˙yd−˙y5),(51)
wherep0
05= [x5,y5,z5]Tis the position of the crane tip relative to the inertial fram e and(xd,yd)is the desired position of
the crane tip. The gains can be selected as kp=w2
sandkd= 2ζsωs, whereωs≪ω0andζscan be selected in the range
[0.7,1]. The condition ωs≪ω0should be sufﬁciently well satisﬁed if ωs=ω0/ks, whereks≥5andωsis the bandwidth in
the outer loop.
10
Fig. 6. Flow chart of the vision-based controller where (1) i s the forward kinematics and Jacobian, (2) is the controller , (3) is the velocity loop, (4) is the
inverse of the Jacobian, (5) is the physical crane, (6) is the physical payload, (7) is the vision system, (8) is the extend ed Kalman Filter and (9) is the cable
length estimation.
In practice it will not be possible to command the accelerati on of the crane tip. The solution is to command the velocity
instead, as the crane used in the experiments and most indust rial cranes will have velocity control with the desired velo city
is input variable. Therefore the acceleration input was con verted to velocity inputs, as described in [42]. This was don e by
integrating the two acceleration commands ¨x5and¨y5in (44) to velocity commands wxandwy, and then using wxandwyas
inputs to the velocity loops given by
˙wx= ¨x5,˙wy= ¨y5, (52)
˙vx=1
Tv(wx−vx),˙vy=1
Tv(wy−vy). (53)
If the bandwidth 1/Tvof the velocity loop is sufﬁciently fast compared to the band width of the damping controller, the resulting
velocitiesvyandvywill be close to the velocity commands wxandwy, and it follows that the accelerations ¨x5and¨y5will
be sufﬁciently close to the commanded accelerations ˙vxand˙vy. The commanded velocities vxandvyare further transformed
to the crane joint space according to
˙qcom=J−1[vxvy0]T, (54)
where the Jacobian Jis given in (10) and ˙qcomis the commanded joint velocities of the crane.
VI. E XPERIMENTAL RESULTS
The performance of the proposed mechatronic system was eval uated in laboratory experiments. A setup with a scaled knuck le
boom crane was designed and constructed. The crane setup is s hown in Fig. 2. The crane was driven by one servo motor
and two electro-mechanical cylinders (EMCs) driven by serv o motors. All servo motors were equipped with encoders for
measuring angles and angular velocities. The vision-based sensor system consisted of three consumer grade web cameras ,
where the resolution was selected as 1280×720pixels. The distance between the cameras was δ12=δ23= 0.24m , and the
spherical markers had a diameter of 0.03m . The control hardware consisted of a personal computer (PC) and a programmable
logic controller (PLC). The PLC was used to read the data from the motor encoders and send commands to the servo drives to
control the servo motor. The PC was used for computation and c ommunication with the PLC. The measurements of qcand˙qc
were obtained from the PLC. For the software part, MATLAB/Si mulink was used on the PC for running the controller and the
cable length estimation algorithm, while Python with OpenC V was used for the vision calculations and for the extended Ka lman
ﬁlter. The control period was set to 50 ms. The communication between MATLAB/Simulink and Python was implemented
with UDP. The full overview of the signals in the system is giv en in Fig. 6.
The covariance of the process noise in the extended Kalman ﬁl ter wasQ= 10−4diag(0.3,0.3,5,5,1,1)and the covariance
of the measurement noise was
R= 10−3[
3.77597 −2.10312
−2.10312 1.25147]
.
The initial a posteriori state was ˆz0=0and the error covariance matrix was ˆP0=0.
The forgetting factor in the cable length estimation algori thm wasβ= 0.5and the initial adaptive gain was γ(0) = 100 .
11
Fig. 7. Estimation of the cable length with |φx|<15deg ,Lis the estimate and ¯Lis the estimate processed by a low-pass ﬁlter. The initial gu ess was
L0= 0.5m, and the true length was L∗= 1.05m .
Fig. 8. Estimation of the cable length with |φx|<0.5deg does not converge, Lis the estimate and ¯Lis the estimate processed by a low-pass ﬁlter.
A. Cable Length Estimation
The performance of the cable length estimation algorithm wa s studied in an experiment where the payload was oscillating
and the crane tip position was stationary. The true cable len gth, which is the distance from the suspension point to the ce nter of
gravity of the payload, was L∗= 1.05m . In the experiments the payload was excited by manually appl ying initial displacements
of different magnitude. During the experiment the amplitud e of the oscillations was naturally damped by few degrees. Th e
estimated cable length Land the low-pass ﬁltered estimate ¯Lwere logged in the experiments, where the low-pass ﬁltered
estimate ¯Lof the cable length was used as an input to the extended Kalman ﬁlter. In all the tests the estimate of the cable
lengthLwas bounded by Lmin= 0.3mandLmax= 1.5m.
In the ﬁrst run, the initial amplitude of the payload oscilla tions wasφx= 15deg and the initial cable length guess was
L0= 0.5m. The estimate of the cable length converged in less than 10s, as shown in Fig. 7. Next, an experiment with the
initial angle φx= 0.5deg was performed. In this case, the estimate of the length was mo re sensitive to noise, and the estimate
oscillated between 0.75m and1.5mafter the initial convergence, as shown in Fig. 8. The reason for the loss of performance
in this case is that the input data to the adaptive algorithm w as not persistently exciting to achieve high quality in the e stimates.
More test were run with initial angles of 5deg ,10deg ,15deg and20deg , and initial cable length guesses of 0.5m,0.7m
and1.4m, where the ﬁltered cable length estimate ¯Lperformed well with convergence after 10s and small variations after
convergence (Fig. 9).
The experimental results shown in Fig. 7, 8 and 9 demonstrate that the cable length estimate was converging to the true len gth
L∗= 1.05m after approximately 10s in all the presented cases, except for the case with close-to -zero payload oscillations.
It is seen that for small payload oscillation angles the esti mate was more noisy, as in Fig. 9(a), which could happen due to
that the Kalman ﬁlter estimates were more noisy for small ang les. For large payload oscillations, as in Fig. 9(d), the est imate
deviated more from the true value, which could happen due to t hat the linearized pendulum model was used for the cable
length estimation algorithm. As expected, the cable length failed to converge when the payload did not oscillate. It is s uggested
that the cable length should be estimated before the controller starts damping the payload motion. It is feas ible in practice,
because in most of the cases the crane operator would do a manu al maneuver before reaching the desired crane tip position
over the landing site, then the payload motion can be damped r ight before the payload landing.
B. Crane control
The performance of the crane controller was investigated in an experiment that represented a realistic hoisting operat ion, where
a payload is ﬁrst carried over a landing site, and then the pay load oscillations are damped out so that the payload can be la nded
safely. The experiment was executed in the following order. The crane tip was initially at the position p0
05= [1.27,1.27]m,
and the payload was manually excited. Then at t= 1s the desired crane tip position was set to pd= [0.70,1.80]m, and the
crane moved to the desired position and ﬁnished the maneuver att= 12s . The controller for payload damping was turned
off during this maneuver. At time t= 20s the payload damping controller was turned on. The cable leng th was estimated
throughout the whole experiment until the damping controll er was turned on at t= 20s , then the estimate was frozen, see the
discussion in Section VI-A. The initial estimate of the cabl e length was set to L0= 0.3m. The bandwidth in the outer control"
2203.03204,D:\Database\arxiv\papers\2203.03204.pdf,"How might incorporating elements of alternative educational approaches, such as Montessori or Waldorf, into robotics workshops for children enhance the learning experience and foster inclusivity?",Integrating elements from alternative educational approaches could create a more engaging and inclusive learning environment by tailoring activities to different learning styles and fostering a sense of individual exploration and discovery.,"years old from the town Xicohtzinco Tlaxcala M ´exico. The
workshops were free of cost as a way encourage participation
and inclusion of anyone. During the pilot workshop, children
were enthusiastic about learning the fundamentals for AIR by
coding, designing and playing with open-sourced robots. The
instructors embraced the different set of skills each child had
by working in small groups and supporting the students during
all the activities. However, we noted that grouping children
of four participants with one instructor was not creating an
engaging experience as each group has only one robot and
one computer and the space and number of participants was
leaving sometimes one participant outside of the reachable
robot-computer setup.
In terms of limitations, the pilot surveys only helped us to
identify the gender and age of the participants and no other
insights such as needs of the target group were considered.
Similarly, this work did no consider metrics to quantify the
impact of the workshop but to identify the needs of the work-
shop that might be addressed in future work. The workshops
were free of cost but no sustainable model was considered for
this pilot experiment.
As a future work, we are planing to run another pilot
in late 2022 or early 2023 with more lessons and perhaps
more participants considering the addition of a study design
and to run a pre-survey to identify the needs of participants
of the workshops. For the curriculum of the workshops, we
are planning to improve the activities to be more engaging,
diverse and inclusive and to provide further evidence on
how alternative education programs (e.g. Montessori, Waldorf,
Regio Emilia [13]; and ”synthesis program” [19]) with new
technologies might lead to potential new avenues of inclusivity
and diversity.
ACKNOWLEDGMENT
To Marta P ´erez and Donato Badillo for their support in
organising the pilot of the workshops. To Rocio Montenegro
for her contributions with the design of the Montessori cur-
riculum for the workshops. To Donato Badillo Per ´ez, Antonio
Badillo Per ´ez and Diego Coyotzi Molina for volunteering as
instructors of the workshops. To Leticia V ´azquez for her sup-
port with the logistics and feedback to improve the workshops.
To Adriana P ´erez Fortis for her contributions and discussion
to prepare draft pilot surveys for the parents and children.
To El ´ıas M ´endez Zapata for his support and feedback on the
hardware design of the robot. To Dago Cruz for his feedback
and discussions on the design of the workshops. To Angel
Mandujano, Elva Corona and others who have contributed with
feedback and support to keep AIR4children project alive
CONTRIBUTIONS
Antonio Badillo-Per ´ez: Contributing to design and write
up of lesson 02. Donato Badillo-Per ´ezo: Contributing to
design and write up of lesson 01. Diego Coyotzi-Molina:
Contributing to design and write up of lesson 03. Dago Cruz:
Contributing to proofreading, edition and feedback. Rocio
Montenegro: Contributing to the write up of designing andpiloting workshops. Leticia V ´azquez: Write up and reﬁnement
of the conclusions. Miguel Xochicale: Contributing to create
the open source and reproducible workﬂow, drafting, write-up,
edition, and submission of the paper.
REFERENCES
[1] Y . Kaga and D. Sretenov, “Inclusion in early childhood care and educa-
tion : Brief on inclusion in education,” https://unesdoc.unesco.org/ark:
/48223/pf0000379502, accessed: 29 January 2022.
[2] A. Monasterio Astobiza, M. Toboso, M. Aparicio, T. Aus ´ın, D. L ´opez,
R. Morte, and J. L. Pons, “Bringing inclusivity to robotics with inbots,”
Nature Machine Intelligence , vol. 1, no. 4, pp. 164–164, Apr 2019.
[Online]. Available: https://doi.org/10.1038/s42256-019-0040-5
[3] A. Peixoto, M. Castro, M. Blazquez, S. Martin, E. Sancristobal,
G. Carro, and P. Plaza, “Robotics tips and tricks for inclusion and
integration of students,” in 2018 IEEE Global Engineering Education
Conference (EDUCON) , 2018, pp. 2037–2041.
[4] A. Peixoto, C. S. G. Gonz ´alez, R. Strachan, P. Plaza, M. de los
Angeles Martinez, M. Blazquez, and M. Castro, “Diversity and inclusion
in engineering education: Looking through the gender question,” in 2018
IEEE Global Engineering Education Conference (EDUCON) , 2018, pp.
2071–2075.
[5] C. Pannier, C. Berry, M. Morris, and X. Zhao, “Diversity and
inclusion in mechatronics and robotics engineering education,” ASEE
annual conference exposition proceedings , 2020. [Online]. Available:
https://par.nsf.gov/biblio/10184534
[6] R. Montenegro, E. Corona, D. Badillo-Perez, A. Mandujano,
L. Vazquez, D. Cruz, and M. Xochicale, “Air4children: Artiﬁcial
intelligence and robotics for children,” 2021. [Online]. Available:
https://github.com/air4children/hri2021
[7] , “The national institute of statistics and geography (inegi),” https://en.
www.inegi.org.mx/, accessed: 10 January 2022.
[8] , “Sistema de informaci ´on y gesti ´on educativa (siged),” https://www.
siged.sep.gob.mx/SIGED/escuelas.html, accessed: 10 January 2022.
[9] V . Clinton-Lisell, E. M. Legerski, B. Rhodes, and S. Gilpin, Open
Educational Resources as Tools to Foster Equity . Cham: Springer
International Publishing, 2021, pp. 317–337.
[10] D. Wiley, T. J. Bliss, and M. McEwen, Open Educational Resources: A
Review of the Literature . New York, NY: Springer New York, 2014,
pp. 781–789.
[11] C. Parra-Palacio, T. Svarcova, and E. Clime. (2016) Otto diy robots.
[Online]. Available: https://www.ottodiy.com/
[12] O. Renato, B. Carlos, and A. Perrine, “Thematic notes 1: Inclusion
in education,” https://unesdoc.unesco.org/ark:/48223/pf0000378427, ac-
cessed: 29 January 2022.
[13] C. Edwards, “Three approaches from europe: Waldorf, montessori, and
reggio emilia,” Early Childhood Research and Practice , vol. 4, 03 2002.
[14] M. C. A. and C. Maria, Absorbent Mind . New York: Dell Pub, 1969.
[15] M. Elkin, A. Sullivan, and M. Bers, “Implementing a robotics curriculum
in an early childhood montessori classroom,” Journal of Information
Technology Education: Innovations in Practice , vol. 13, pp. 153–169,
01 2014.
[16] H. Aljabreen, “Montessori, waldorf, and reggio emilia: A comparative
analysis of alternative models of early childhood education,” Interna-
tional Journal of Early Childhood , vol. 52, no. 3, pp. 337–353, Dec
2020. [Online]. Available: https://doi.org/10.1007/s13158-020-00277-1
[17] A. Drigas and E. Gkeka, “Montessori method and icts,” International
Journal of Recent Contributions from Engineering, Science; IT
(iJES) , vol. 4, no. 1, p. pp. 25–30, Mar. 2016. [Online]. Available:
https://online-journals.org/index.php/i-jes/article/view/5481
[18] S. Scippo and F. Ardolino, “Computational thinking in montessori
primary school,” Ricerche di Pedagogia e Didattica. Journal of
Theories and Research in Education , vol. 16, no. 2, p. 59–76, Jan.
2021. [Online]. Available: https://rpd.unibo.it/article/view/12163
[19] “Synthesis: where kids learn how to think.” https://www.synthesis.is/,
accessed: 16 Jan 2022."
1811.02063,D:\Database\arxiv\papers\1811.02063.pdf,"While the paper highlights the effectiveness of acoustic landmarks in improving speech recognition performance, it also notes a potential drawback related to the distribution of these landmarks. What is this drawback, and how does it potentially impact the training process?","The paper points out that the distribution of acoustic landmarks is skewed, with most labels indicating transitions related to specific phone categories. This imbalance can lead to redundant information being provided during training, potentially hindering the model's ability to learn effectively from diverse phonetic features.","tra boundary information in Mixed Label 2 is useful to the
training algorithm.
Table 2 . Comparison between baseline and our proposed
models with augmented target labels in PER (%). Number in
the parentheses denotes the relative reduction over baseline.
Baseline Mixed 1 Mixed 2
random init 30.36 30.98 29.10
ﬁnetuned 30.36 28.96 (4.64%) 27.72 (8.72%)
It is not clear why a ﬁnetuning stage is needed in order for
Mixed 1 to beat the baseline. One possibility is that landmark
labels are helpful for some tokens, and harmful for others;
pretraining uses the helpful landmarks to learn better phone
alignments, then ﬁnetuning permits the network to learn to
ignore the harmful landmark tokens. We looked into the prior
distribution on TIMIT, presented in Figure 4, of both phones
(top subplot, with phones ordered in the same way as they oc-
curred in Table 1) and landmarks (bottom subplot, Mixed La-
bel 2 ordered in category permutation using continuant as the
ﬁrst variable and sonorant as the second). The table reveals
that the distribution of landmarks is not balanced. Most labels
indicate a transition related to the [ +continuant,+sonorant ]
phones. A skewed landmark support is not ideal for augment-
ing phone recognizer training as it tends to provide the same
and redundant information for many training sequences.
Phones00.51
Landmarks00.51
Fig. 4 . Prior distributions of phones and acoustic landmarks.
4.3. Datasets Smaller and Larger than TIMIT
To solidify our ﬁndings, we further investigated the sensitiv-
ity of our approaches to the size of training data on subsets of
TIMIT (smaller corpora) and WSJ (a larger corpus). In this
section, we only demonstrate the experiments using Mixed
Label 2 augmentation method since it outperforms Mixed La-
bel 1 in the previous discussion. We report PER/WER results
for ﬁnetuned models.
Figure 5 shows the PER results by stretching the amount
of training data on TIMIT. Both the proposed model and base-
line fail to converge when 75% of the training data is used. We
observe that both models start to predict a constant sequence
(usually made up of two to three most frequent phones) for all
utterances. Scheduled reducing the learning rate by New-Bob
60 70 80 90 100
Percetage of Training Data Used20406080100Phone Error Rate %Baseline
Mixed Label 2 RefineFig. 5 . PERs by stretching the amount of training data on
TIMIT.
annealing can’t help to converge to an optimal. Increasing
the amount of training data helps both models converge. The
baseline needs 90% of TIMIT to converge, while the proposed
system only needs 80% of TIMIT.
When scaling up to a even larger corpus on WSJ, the
proposed Mixed Label 2 system could achieve better perfor-
mance over the baseline consistently in terms of all metrics
as shown in Table 3. Our baseline system slightly under-
performs the results published in EESEN [5] because our net-
work is shallower and the acoustic inputs do not include any
dynamic (delta) features, but the beneﬁt of the proposed land-
mark augmentation method still applies. To our knowledge,
this is the ﬁrst work to show that manner-change acoustic
landmarks reduce both PER and WER on a mid-sized ASR
corpus.
Table 3 . Label Error Rate (%) on WSJ, where tg and tgpr
denote decoding graphs with primitive and pruned trigrams.
PER WER ( tgpr / tg )
eval92 dev93 eval92 dev93
Baseline 8.7 12.38 8.75/8.17 13.15/12.31
Mixed 2 8.12 11.49 8.35/8.19 12.86 /12.28
5. CONCLUSION
We proposed to augment CTC with acoustic landmarks. We
modiﬁed the classic landmark deﬁnition to suit the CTC
criterion and implemented a pretraining-ﬁnetuning training
procedure to improve CTC AMs. Experiments on TIMIT
and WSJ demonstrated that CTC training becomes more sta-
ble and rapid when phone label sequences are augmented
by landmarks, and achieves a signiﬁcantly lower (8.72%
relative reduction) asymptotic PER. The advantage is consis-
tent across corpora (TIMIT, WSJ) and across metrics (PER,
WER). CTC with landmarks converges when the dataset is
too small to train the baseline, and it also converges without
the need of time alignments on a mid-sized standard ASR
training corpus (WSJ).
Acknowledgements : The ﬁfth author was supported by
the DARPA LORELEI program. All results and conclusions
are those of the authors, and are not endorsed by DARPA.
4
6. REFERENCES
[1] Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide,
Mike Seltzer, Andreas Stolcke, Dong Yu, and Geoffrey Zweig,
“Toward human parity in conversational speech recognition,”
IEEE/ACM Trans. Audio Speech and Language , vol. 25, no.
12, pp. 2410–2423, 2017.
[2] George Saon, Gakuto Kurata, Tom Sercu, Kartik Audhkhasi,
Samuel Thomas, Dimitrios Dimitriadis, Xiaodong Cui, Bhu-
vana Ramabhadran, Michael Picheny, Lynn-Li Lim, et al.,
“English conversational telephone speech recognition by hu-
mans and machines,” in Interspeech . ISCA, 2017, pp. 132–
136.
[3] Alex Graves, Santiago Fern ´andez, Faustino Gomez, and J ¨urgen
Schmidhuber, “Connectionist temporal classiﬁcation: la-
belling unsegmented sequence data with recurrent neural net-
works,” in International Conference on Machine Learning .
ACM, 2006, pp. 369–376.
[4] Has ¸im Sak, Andrew Senior, Kanishka Rao, and Franc ¸oise Bea-
ufays, “Fast and accurate recurrent neural network acoustic
models for speech recognition,” in Interspeech . ISCA, 2015,
pp. 1468–1472.
[5] Yajie Miao, Mohammad Gowayyed, and Florian Metze,
“EESEN: End-to-end speech recognition using deep RNN
models and WFST-based decoding,” in ASRU . IEEE, 2015,
pp. 167–174.
[6] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anub-
hai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper,
Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al., “Deep
speech 2: End-to-end speech recognition in English and Man-
darin,” in International Conference on Machine Learning ,
2016, pp. 173–182.
[7] Yajie Miao, Mohammad Gowayyed, Xingyu Na, Tom Ko, Flo-
rian Metze, and Alexander Waibel, “An empirical exploration
of CTC acoustic models,” in ICASSP . IEEE, 2016, pp. 2623–
2627.
[8] Has ¸im Sak, Andrew Senior, Kanishka Rao, Ozan Irsoy, Alex
Graves, Franc ¸oise Beaufays, and Johan Schalkwyk, “Learning
acoustic frame labeling for speech recognition with recurrent
neural networks,” in ICASSP . IEEE, 2015, pp. 4280–4284.
[9] Chuanying Niu, Jinsong Zhang, Xuesong Yang, and Yanlu Xie,
“A study on landmark detection based on CTC and its applica-
tion to pronunciation error detection,” in APSIPA ASC . IEEE,
2017, pp. 636–640.
[10] Kenneth N Stevens, “Toward a model for lexical access based
on acoustic landmarks and distinctive features,” Journal of the
Acoustical Society of America , vol. 111, no. 4, pp. 1872–1891,
2002.
[11] Di He, Boon Pang Lim, Xuesong Yang, Mark Hasegawa-
Johnson, and Deming Chen, “Improved ASR for under-
resourced languages through multi-task learning with acoustic
landmarks,” in Interspeech . ISCA, 2018, pp. 2618–2622.
[12] Di He, Boon Pang P Lim, Xuesong Yang, Mark Hasegawa-
Johnson, and Deming Chen, “Selecting frames for automatic
speech recognition based on acoustic landmarks,” Journal of
the Acoustical Society of America , vol. 141, no. 5, pp. 3468–
3468, 2017.[13] Di He, Boon Pang Lim, Xuesong Yang, Mark Hasegawa-
Johnson, and Deming Chen, “Acoustic landmarks contain
more information about the phone string than other frames for
automatic speech recognition with deep neural network acous-
tic model,” Journal of the Acoustical Society of America , vol.
143, no. 6, pp. 3207–3219, 2018.
[14] John S Garofalo, Lori F Lamel, William M Fisher, Johnathan G
Fiscus, David S Pallett, and Nancy L Dahlgren, “The
DARPA TIMIT acoustic-phonetic continuous speech corpus
CD-ROM,” in Linguistic Data Consortium , 1993.
[15] Douglas B Paul and Janet M Baker, “The design for the Wall
Street Journal-based CSR corpus,” in the Workshop on Speech
and Natural Language . Association for Computational Lin-
guistics, 1992, pp. 357–362.
[16] Alex Graves and Navdeep Jaitly, “Towards end-to-end speech
recognition with recurrent neural networks,” in International
Conference on Machine Learning . ACM, 2014, pp. 1764–
1772.
[17] Sharlene A Liu, “Landmark detection for distinctive feature-
based speech recognition,” Journal of the Acoustical Society
of America , vol. 100, no. 5, pp. 3417–3430, 1996.
[18] Mark Hasegawa-Johnson, James Baker, Sarah Borys, Ken
Chen, Emily Coogan, Steven Greenberg, Amit Juneja, Katrin
Kirchhoff, Karen Livescu, Srividya Mohan, et al., “Landmark-
based speech recognition: Report of the 2004 Johns Hopkins
summer workshop,” in ICASSP . IEEE, 2005, pp. 213–216.
[19] Xiang Kong, Xuesong Yang, Mark Hasegawa-Johnson, Jeung-
Yoon Choi, and Stefanie Shattuck-Hufnagel, “Landmark-
based consonant voicing detection on multilingual corpora,”
Journal of the Acoustical Society of America , vol. 141, no. 5,
pp. 3468–3468, 2017.
[20] Kenneth N Stevens, Sharon Y Manuel, Stefanie Shattuck-
Hufnagel, and Sharlene Liu, “Implementation of a model
for lexical access based on features,” in Second International
Conference on Spoken Language Processing . ISCA, 1992, pp.
499–502.
[21] Mark Hasegawa-Johnson, “Time-frequency distribution of par-
tial phonetic information measured using mutual information,”
inInternational Conference on Spoken Language Processing .
ISCA, 2000, pp. 133–136.
[22] Kenneth N Stevens, “Evidence for the role of acoustic bound-
aries in the perception of speech sounds,” Journal of the Acous-
tical Society of America , vol. 69, no. S1, pp. S116–S116, 1981.
[23] John J McCarthy, “Feature geometry and dependency: A re-
view,” Phonetica , vol. 45, no. 2-4, pp. 84–108, 1988.
[24] Xavier Glorot and Yoshua Bengio, “Understanding the dif-
ﬁculty of training deep feedforward neural networks,” in In-
ternational Conference on Artiﬁcial Intelligence and Statistics ,
2010, pp. 249–256.
[25] Herve A Bourlard and Nelson Morgan, Connectionist speech
recognition: a hybrid approach , vol. 247, Springer Science &
Business Media, 2012.
5"
2009.02183,D:\Database\arxiv\papers\2009.02183.pdf,"How does the choice of the polynomial tail in the radial basis function (RBF) interpolant affect the numerical stability of the optimization algorithm, particularly when dealing with categorical variables?","The choice of the polynomial tail degree in the RBF interpolant influences the numerical stability of the optimization algorithm, especially when dealing with categorical variables.  A polynomial tail of degree 1 can lead to linearly dependent columns in the system of equations used to compute the interpolant, requiring additional steps to ensure a solution.","whereC:×nc
h=1[1,...,mh]→×nc
h=1Shis a one-to-one map of the integers [1 ,...,mh] to elements of the mh-
dimensional set Sh. Notice that to construct the function C, we must arbitrarily deﬁne an order of each set
Sh. This allows us to apply any algorithm for mixed-integer black-box problems directly to (1). However, it is
an inherently ﬂawed approach, because the sets Share originally unordered. Since virtually all derivative-free
optimization algorithms use metric information, we are imposing on the problem artiﬁcial structure that is not
reﬂected in its original formulation.
The second formulation, which we call extended space formulation, uses a unary encoding for the categorical
variables. Deﬁne the vectors xe,L,xe,U∈Rnr+nd+ ˆmncas:
xe,L
i={
xL
iifi≤nr+nd
0 otherwisexe,U
i={
xU
iifi≤nr+nd
1 otherwise .
Then the extended space formulation is deﬁned as:
minf(x,ˆC(xnr+nd+1,...,xnr+nd+ ˆmnc))
x∈[xe,L,xe,U]
x∈Rnr×Znd×{0,1}ˆmnc
∀h= 1,...,nc∑ˆmh
j= ˆmh−1+1xnr+nd+j= 1.

(6)
where ˆC:{0,1}ˆmnc→×nc
h=1Shmaps the binary vector ( xnr+nd+1,...,xnr+nd+ ˆmnc)∈{0,1}ˆmncto a choice of
elements from the sets Sh, by viewing it as the juxtaposition of the characteristic vectors of the sets Sh. This
mapping assigns one value to each categorical variable, because of the constraints∑ˆmh
j= ˆmh−1+1xnr+nd+j= 1 for
allh= 1,...,nc. In the following we denote the feasible region of (6) as Ω e. Notice that similar to the previous
formulation, (6) also suﬀers from the ﬂaw of imposing an order on the sets Sh; however, we show next that a
surrogate model of (6) with radial basis functions ignores the order, thereby avoiding ranking points based on
artiﬁcial metric information (i.e., that does not exist in the original problem).
Proposition 3.1 (Invariance to permutation) Letx1,...,xk∈Ωe⊂Rnr+nd×{0,1}ˆmncwith corresponding
function values y1,...,yk. Letπ=×nc
h=1πhbe a permutation of {0,1}ˆmnc, where for h= 1,...,nc,πhis a
permutation of{0,1}mh. Letπe:=Inr+nd×πbe the extension of πto an operator on (nr+nd+ ˆmnc)-dimensional
vectors that acts as the identity on the ﬁrst nr+ndcomponents. Let λ∈Rk,α∈Rnr+nd+ ˆmnc,α0∈Rdeﬁne an
interpolant
sk(x) :=k∑
i=1λiφ(∥x−xi∥) +⟨α,x⟩+α0(7)
to the points x1,...,xkwith values y1,...,yk. (If the polynomial tail is of degree 0according to Table 1, then α
is the all-zero vector; if the degree is −1,α0is 0 as well.) Then for any x∈Rnr+nd×{0,1}ˆmnc, the function s′
k,
deﬁned as:
s′
k(x) :=k∑
i=1λiφ(∥x−πe(xi)∥) +⟨πe(α),x⟩+α0,
is such that sk(x) =s′
k(πe(x)).
Proof. Since by deﬁnition πeis a permutation of the components of the vector ( xnr+nd+1,...,xnr+nd+ ˆmnc), and
acts as the identity on the ﬁrst nr+ndcomponents, we have:
∥πe(x)−πe(xi)∥=∥x−xi∥
and
⟨πe(α),πe(x)⟩=⟨α,x⟩.
This immediately implies sk(x) =s′
k(πe(x)). □
Prop. 3.1 implies that the surrogate model in extended space is invariant to the order adopted in the unary-
encoding representation of the categorical variables. Indeed, if the solution to (4) is unique, yielding a unique
4
surrogate model sk, after permuting the unary encoding of the categorical variables we would obtain the same
surrogate model from (4). We remark that if the solution to (4) is not unique (see Sect. 4.1), then one may obtain
a diﬀerent skafter permuting the unary encoding of the categorical variables; however, even in this case, each
solution to (4) has an equivalent solution for the system obtained after permutation. Note that similar properties
do not hold when using the original space formulation: if categorical variables are represented by integers in the
interval [1,...,mh], permuting these integers is not a component-wise permutation of the vector x, and could in
general change the norms of ∥x−xi∥.
We can therefore use the extended space formulation (6) together with surrogate models of the form (2) to
ensure that the sets Siare correctly treated as unordered. However, (6) is a constrained formulation, whereas the
algorithms of [16] and [33] (that RBFOpt is based on) assume unconstrained problems. In the next sections we
describe one way to deal with the constraints in (6). Another diﬃculty is given by the fact that the constraints
lead to linearly dependent columns in the submatrix Pof (4); this issue is also discussed in the next sections.
From now on, we deﬁne n:=nr+nd+ ˆmnc, i.e., the dimension of the extended space: the interpolation model
(2) lives in n-dimensional space. Note that we always require the representation of the categorical variables
in extended space to take on integer values, as is natural. This is in contrast with popular hyperparameter
optimization approaches, where the categorical variables are often treated as continuous for simplicity, and the
fractional values are then mapped to a valid discrete value in some heuristic way (such as setting the variable
with the largest fractional value to 1, and the rest to 0; this is the approach implemented, e.g., in Spearmint
[37]). From an optimization standpoint, it seems more rigorous to treat integer variables as such, because it is
well-known that a solution to the relaxed problem could be very far from the integer optimum, even for linear
optimization problems [27].
Finally, to better understand the structure of the surrogate model skin the extended space, we rewrite it as
follows. For x∈Rnr×Znd×{0,1}ˆmnc, deﬁne cat( x,h) := (xnr+nd+ ˆmh−1+1,...,xnr+nd+ ˆmh), i.e., the subvector
corresponding to the unary representation of the h-th categorical variable. With this deﬁnition, note that skcan
be rewritten as:
sk(x) =k∑
i=1λiφ(∥(x1,...,xnr+nd)−(xi
1,...,xi
nr+nd) +nc∑
h=11cat(x,h)̸=cat(xi,h)∥) +⟨α,x⟩+α0.
From the above equation we can see that the argument of the radial basis function centered at the interpolation
pointxiis shifted by the number of categorical variables that disagree with xi. Thus, for the radial basis function
part of the interpolant, the surrogate model is determined by the non-categorical variables, as well as the number
of disagreements with the categorical variables at the interpolation nodes: the only notion of distance between
categorical variables is reduced to the binary information agreement/disagreement, which is independent from
the order assigned to the sets Sh. Furthermore, depending on the degree of the polynomial tail, there can be an
additional shift of the entire surrogate model that depends only on the values of the categorical variables (i.e.,
the part corresponding to categorical variables in the inner product term ⟨α,x⟩).
It should be noted that if a categorical variable, say the ﬁrst categorical variable for simplicity, has only two
possible values, i.e., |S1|= 2, then the extended space formulation is redundant: the constraint xnr+nd+1+
xnr+nd+2= 1 implies that xnr+nd+2is simply the complement of xnr+nd+1. As will be discussed in Sect. 4.1,
one among xnr+nd+1,xnr+nd+2would always be eliminated when determining the coeﬃcients of the surrogate
model. Hence, we use the extended space formulation only for categorical variables that have strictly more than
two possible values: for those that have exactly two, we use the original space formulation, mapping them to a
binary variable.
4 Description of the optimization algorithm
Many RBF-based global optimization methods use a similar scheme that attempts to balance exploration (trying
to improve a surrogate model of the objective function in unknown parts of the domain) with exploitation (trying
to ﬁnd the best objective function value based on the current surrogate model); see, e.g., [16, 33, 25, 24, 14].
The algorithm that we propose is no exception, although we introduce some additional steps (Reﬁnement step
and Restoration step, see below) as compared to the more traditional framework. More speciﬁcally, we use the
following optimization scheme:
5
•Initial step: Setkequal to the size of the initial sample set. Choose kaﬃnely independent points
x1,...,xk∈Ωeusing an initialization strategy.
•Iteration step: Repeat the following steps until kexceeds the prescribed number of function evaluations.
(i) Compute the RBF interpolant skto the points x1,...,xk, solving (4). If the system is not full rank,
ﬁnd the least squares solution. If the system cannot be solved, go to Restoration step.
(ii) Choose a trade-oﬀ between exploration and exploitation .
(iii) Determine the next point xk+1based on the choice at step (ii).
(iv) Evaluate fatxk+1.
(v) Setk←k+ 1. If the last Reﬁnement step was performed suﬃciently many iterations ago, go to the
Reﬁnement step. Otherwise, repeat the Iteration step.
•Reﬁnement step:
(i) Selectn+ 1 points out of x1,...,xkto initialize a local model.
(ii) Apply a local search method for a speciﬁed number k′of iterations, obtaining points xk+1,...,xk+k′.
(iii) Setk←k+k′and go back to the Iteration step.
•Restoration step: Attempt to change the set of interpolation points so that (4) admits a solution. If
successful, return to Iteration step. Otherwise, restart the algorithm.
The algorithm described above can be considered a meta-algorithm, with many possible instantiations. The
choice of the initial sample points is discussed in [10]; in this paper we always select them by constructing a latin
hypercube design aimed at maximizing the minimum distance between the sample points. In the following, we
provide an overview of the main diﬀerent implementations of the above meta-algorithm available in RBFOpt.
We remark that [10] describes several improvements to the meta-algorithm (in the context of Gutmann’s RBF
method [16]); all of them are used by default in RBFOpt. Most notably, these are: automatic scaling of the
domain of the function; clipping and rescaling of the codomain; restriction of the search box during global search
— see [10] for details.
4.1 Solution of linear systems and non-unique interpolants
To compute the surrogate model skwe must solve system (4). However, when the polynomial p(x) is of degree
1, if some of the interpolation points are aﬃnely dependent then (4) has determinant 0. In continuous space, the
algorithm never generates aﬃnely dependent points4. With categorical variables, this is bound to happen: because
of the constraints∑ˆmh
j= ˆmh−1xnr+nd+j= 1 for all h= 1,...,nc, the binary representation of each categorical
variable in extended space adds up to the all-one vector, which is already a column of (4) whenever d= 1 in
Table 1. To solve this issue, whenever the problem has categorical variables and d= 1, we eliminate the columns
xnr+nd+ ˆmhforh= 1,...,ncand the corresponding rows from (4). These are precisely the last columns of each
constraint∑ˆmh
j= ˆmh−1xnr+nd+j= 1 for allh= 1,...,nc. This is motivated by the following simple observation.
Proposition 4.1 (Reduced linear system) Supposed= 1 andnc≥1, i.e., there is at least one categorical
variable. Suppose further that we employ the extended space formulation of the problem (6). Denote by ˆPthe
matrix obtained by eliminating the columns xnr+nd+ ˆmhforh= 1,...,ncfromP. Then if the system (4)has a
solution, so does the system:
(Φ ˆP
ˆP⊤0(n+1−nc)×(n+1−nc))(λ
α)
=(F
0n+1−nc)
, (8)
4To be precise, the algorithm only guarantees pairwise distinct points; but the probability of selecting a new point that is aﬃnely
spanned by the previous points is 0 with the MSRSM algorithm [33], and only happens in ill-conditioned cases for Gutmann’s
algorithm [16].
6
Proof. Let¯λ,¯αbe a solution to (4). Since xnr+nd+ ˆmh= 1−∑ˆmh−1
j= ˆmh−1xnr+nd+jfor allh= 1,...,nc, we can
eliminatexnr+nd+ ˆm1fromP; if we deﬁne v= (1,..., 1,−1)⊤∈Rn+1as the vector with −1 in the last component
and 1 in all other components, the substitution yields:
(ΦP
P⊤0(n+1)×(n+1))(¯λ
¯α−¯αˆm1v)
=(F
0n+1)
.
This shows that ( ¯λ,¯α−¯αˆm1v) is also a solution to (4). However, by deﬁnition the ˆ m1-component of ¯ α−¯αˆm1v
is zero, implying that we can eliminate the column corresponding to xnr+nd+ ˆm1fromP(this also eliminates one
row fromP⊤, which obviously does not restrict the set of solutions to the system). We can repeat this process
forxnr+nd+ ˆmhforh= 2,...,nc, showing that the reduced system admits a solution and completing the proof. □
By the above proposition, we can solve (8) rather than (4), ﬁnd a solution to the smaller system, and extend it
to a full solution by inserting zeroes in the positions corresponding to eliminated columns. The advantage of this
approach is that (8) may be an invertible system whereas (4) is not invertible under the stated conditions.
Aﬃnely dependent points aﬀect not only the nonsingularity of the system (4), but also the unisolvence property
of RBF interpolants, i.e., uniqueness of the interpolant [29]. In particular, when d= 1 in Table 1, the suﬃcient
condition for unisolvence — using a basis of polynomials of degree 1 — fails because we eliminate one or more
monomials from the polynomial basis. Thus, when d= 1 we can no longer guarantee the unisolvence property.
However, in practice we observe that the system (4) often has a solution even when this condition fails, and
sometimes a unique solution; this was also observed in [13].
Even when using the reduced matrix ˆP, it can sometimes happen that the algorithm generates aﬃnely de-
pendent interpolation points. Speciﬁcally, this can occur when there are integer or categorical variables, where
column entries belong to a discrete set; empirically, we observe this especially when the problem has many binary
variables. When this happens, we solve (8) as a least-squared-residuals problem. This is computationally more
expensive, but guarantees a solution. (The time spent in the solution of linear systems is negligible in practice.)
The least squares solution to the linear system is also used whenever there are not enough sample points to
build a full interpolant, i.e., k≤n+ 1. Whereas the majority of the literature assumes that at least n+ 1 points
are sampled in the initialization phase (see, e.g., [16, 33, 10]), in practice this can be a severe drawback when n
is large. Approaches to begin the optimization before sampling n+ 1 points are described in [31, 34]; we follow
the approach of [34]5. Speciﬁcally, the number of initial sample points ninitis heuristically chosen according to
the following formula:
ninit={
⌊0.5(n+ 1)⌉ifn≤20
⌊0.4(n+ 1)⌉otherwise.(9)
If RBFOpt is executed in parallel with at least 2 threads, then the number of initial sample points is chosen as:
ninit=

n+ 1 if n≤20
⌊0.75(n+ 1)⌉21≤n≤50
⌊0.5(n+ 1)⌉otherwise.
As long as k≤n, we use the least squares solution to determine the coeﬃcients of the surrogate model sk; the
rest of the optimization algorithm remains unchanged. Whenever k≥n+ 1 points are available and they are
aﬃnely independent, system (4) has a unique solution and we compute it using a direct method.
The reduced matrix ˆPis also employed in the Initial step of the algorithm. After generating an initial sample
set (see [10] for a description of the strategies to do so implemented in RBFOpt), we compute a singular value
decomposition of ˆP; as long as some singular value is close to zero, we generate a new sample set. Notice that if
there are no categorical variables then ˆPcoincides with P.
We remark that for all RBFs that do not have a polynomial tail of degree 1, i.e., all except the cubic and
thin plate splines, these additional steps are not necessary. However, the cubic and thin plate spline RBFs are
empirically among the most accurate, see e.g. [10], and the automatic model selection procedure employed by
RBFOpt (see Sect. 4.5) chooses one of these two RBFs very often in practice. Hence, the additional eﬀort is
justiﬁed.
5The numerical tests in [34] are based on a customized version of RBFOpt.
7
4.2 Determining the next point: Iteration step
We implement a variation of two algorithms for global optimization using RBFs: Gutmann’s RBF algorithm [16]
and the Metric Stochastic Response Surface Method (MSRSM) [33]. Both algorithms proceed in cycles, and use
a parameter κthat determines the length of an optimization cycle.
4.2.1 Gutmann’s RBF algorithm
A detailed description is given in [10]; here we report the main steps only. Let ℓkbe the RBF interpolant to the
points{xi:i= 1,...,k}∪{y}, with function values 0 ,0,..., 0,1 respectively. Let µk(y) be the coeﬃcient of ℓk
corresponding to the RBF centered at y. Deﬁne
gk(y) = (−1)d+1µk(y)[sk(y)−f∗
k]2, y∈Ωe\{x1,...,xk},
wheref∗
kis a given value. Furthermore, deﬁne:
hk(x) ={
1
gk(x)ifx̸∈{x1,...,xk}
0 otherwise .(10)
Gutmann’s RBF method then implements the following Iteration step:
•Iteration step (for Gutmann’s RBF algorithm):
(ii) Choose a target value f∗
k∈R∪{−∞} :f∗
k≤minx∈Ωesk(x).
(iii) Compute
xk+1= arg max
x∈Ωehk(x), (11)
whereh(x) is deﬁned as in (10).
Lety∗:= arg min x∈Ωesk(x),fmin:= mini=1,...,kf(xi), andfmax:= maxi=1,...,kf(xi). We employ a cyclic
strategy that picks target values f∗
k∈R∪{−∞} according to the following sequence of length κ+ 2:
•Step−1 (InfStep ): Choosef∗
k←−∞ . In this case the problem of ﬁnding xk+1can be rewritten as:
xk+1= arg max
x∈Ωe1
(−1)d+1µk(x).
This is a pure exploration phase, yielding a point far from x1,...,xk.
•Stepℓ∈{0,...,κ−1}(Global search ): Choose
f∗
k←sk(y∗)−(1−ℓ/κ)2(fmax−sk(y∗)). (12)
In this case, we try to strike a balance between improving model quality and ﬁnding the minimum.
•Stepκ(Local search ): Choosef∗
k←sk(y∗). Notice that in this case (10) is maximized at y∗. Hence, if
sk(y∗)<fmin−10−10|fmin|we accepty∗as the new sample point xk+1without solving (11). Otherwise we
choosef∗
k←fmin−10−2|fmin|. This is an exploitation phase.
4.2.2 MSRSM algorithm
Deﬁne dist( x) := mini=1,...,k∥x−xi∥. The MSRSM algorithm implements the following Iteration step:
•Iteration step (for the MSRSM algorithm):
(ii) Choose a target value α∈[0,1]∪{∞} .
(iii) Choose a ﬁnite set of reference points R⊂Ωe\{x1,...,xk}, and compute
xk+1= arg min
x∈Ωeαmaxy∈Rdist(y)−dist(x)
maxy∈Rdist(y)−miny∈Rdist(y)+sk(x)−miny∈Rsk(y)
maxy∈Rsk(y)−miny∈Rsk(y). (13)
8
Essentially, (13) tries to solve a bi-objective optimization problem in which the two objective functions are the
(negative of the) maximin distance from the points x1,...,xk, and the value of the surrogate model. The paper
[33] uses a variation of (13), in which the second fraction in the expression has weight (1 −α) rather than 1.
RBFOpt supports this version, but by default it uses equation (13) instead (see also [9]).
The value of αis chosen according to a cyclic strategy of length κ+ 2 in which each step has similar goals to
the corresponding step discussed in Gutmann’s RBF method. The cyclic strategy is as follows:
•Step−1 (InfStep ): Chooseα←∞ . In this case the problem of ﬁnding xk+1can be rewritten as:
xk+1= arg max
x∈Ωemin
i=1,...,k∥x−xi∥.
This is a pure exploration phase.
•Stepℓ∈{0,...,κ−1}(Global search ): Chooseα←max{1−(ℓ+1)/κ,0.05}. This aims for balance between
exploration and exploitation.
•Stepκ(Local search ): Chooseα←0. In this case, the solution to (13) is the point that minimizes the
surrogate model, i.e., y∗= arg min y∈Ωesky. Ify∗is such that sk(y∗)<fmin−10−10|fmin|, we accept y∗as
the new point xk+1. Otherwise, choose α←0.05. This is an exploitation phase.
4.2.3 Solution of the search problems
We implement three diﬀerent approaches for the solution of the optimization problems (11) and (13):
(1) Problems (11) and (13) are solved with a simple genetic algorithm, that works by generating an initial
population Xuniformly at random, then iteratively constructing a new population by taking:
•The 0.25|X|best points in X(surviving population), according to the objective function being optimized;
•0.25|X|points obtained by repeatedly performing the following procedure: we randomly pick two points
x1,x2from the surviving population, and create a new point by choosing each entry from either x1or
x2(mating);
•0.5|X|points generated uniformly at random (new individuals);
•a point obtained by taking the best individual in X, and randomly perturbing some of its entries
(mutation). The number of mutated entries increases as the number of iterations of the genetic algorithm
increases.
We appropriately round the above quantities so that the size of the population |X|remains constant. In the
presence of categorical variables we sample points in the original space, where uniform random sampling is
easily implemented, then map them to the extended space.
(2) Rather than solving (11) and (13) directly, we sample a large number of points in Ω eand choose the best
point in the sample. This is the approach advocated in [33]. In the presence of categorical variables we sample
points in the original space, where uniform random sampling is easily implemented, then map them to the
extended space.
(3) Problems (11) and (13) are solved by means of the mathematical programming solvers Ipopt and Bonmin.
This is the approach advocated in [16]. Since Bonmin supports constrained problems, we work directly in
the extended space when this approach is chosen (note that we must use Bonmin if discrete variables are
present).
We remark that the MSRSM scoring function requires a set of reference points R, see (13): the set of reference
points is taken to be the current population for the genetic algorithm, the whole sample when using the sampling
scheme, and x1,...,xkfor when using a mathematical programming solver.
9
4.3 Determining the next point: Reﬁnement step
As indicated at the beginning of Sect. 4, during the search we periodically execute a Reﬁnement step, with the
purpose of improving the best solution available by performing a local search around it. The scheme employed
in the Reﬁnement step is reminiscent of a trust region method [6, 39]. However, it is not a trust region method,
mainly because we construct a local model using points that may be outside the trust region, and we do not
require that the model is fully-linear or a similar property [8] (although the QR-like algorithm that we use to
improve the geometry of the interpolation set would in principle yield a fully-linear model, if it were allowed to
run to completion [39, 7]). Furthermore, our scheme is adapted to work on mixed-variable problems, rather than
only problems with continuous variables; proving rigorous local convergence guarantees in the discrete setting is
an involved task in itself, see e.g., [21], and here we limit ourselves to a heuristic approach to reﬁne candidate
solutions. While trust region methods enjoy strong convergence properties [8], managing the set of sample points
and converging to a stationary point can be expensive, compared to surrogate model methods, in terms of number
of objective function evaluations. Empirically, we found that embedding a full trust region method for local search
could severely slow the global search, which is the main strength of RBF-based surrogate model methods; hence,
we opted for the methodology described below, that is guided by two design priciples: (1) it is initialized using
information from known points only; (2) it is quickly stopped if it fails to yield any improvement. Note that with
our approach the RBF surrogate model is still used for global and local search, but it is complemented by a local
linear model to search around the best known solution; this is contrast to the approach recently proposed in [15],
where the global surrogate model is abandoned altogether, and is replaced by multiple local models managed
with a trust-region-like algorithm.
We deﬁne the following algorithmic parameters, utilized in the algorithm.
•βmr: minimum radius of the reﬁnement search.
•βrm: (logarithm of the) radius multiplier for initialization.
•κrs: threshold to shrink the reﬁnement search radius.
•κre: threshold to expand the reﬁnement search radius.
•κrm: threshold to accept the new iterate.
•Trf: frequency parameter of the reﬁnement search.
•Trs: maximum number of consecutive reﬁnement iterations.
•ϵgrad: minimum norm of the gradient of the linear model.
The Reﬁnement step works as follows:
•Model initialization: Letj←arg mini=1,...,kf(xi). Sort the points x1,...,xkby increasing distance
fromxj, and select the ﬁrst n+ 1 (this includes xjitself). Let Sbe the set containing these points. Set
¯x←xj.
•Let ˆxbe the point in Swith the⌈n+1
2⌉
smallest distance to ¯ x. Compute the initial radius of the reﬁnement
searchρas:
ρ= max{∥¯x−ˆx∥,βmr×2βrm}.
•Reﬁnement: repeat a given number of times, or until a stopping criterion is met.
(i) LetMbe the matrix obtained using the points xi∈Sas columns.
(ii) IfMdoes not contain n+ 1 aﬃnely independent columns, use a QRfactorization of Mto replace one
point inSwith a new point (obtained by moving from ¯ xin a direction taken from the columns of Q
after rescaling, with step length ρ) that increases the rank of M, and go back to (i).
(iii) Otherwise, build a linear model c⊤x+bof the objective function using points ( xi,f(xi)),xi∈S.
10
(iv) Move from the current iterate ¯ xin the direction of improvement −cwith step length:
t= max
0≤t≤ρ{t: ¯x−tc∈[xeL,xe,U]}.
Let ¯x′= ¯x−tcbe the new candidate point.
(v) Evaluate f(¯x′). Update the reﬁnement search radius based on the expected decrease c⊤(¯x−¯x′) and
the actual decrease f(¯x)−f(¯x′): iff(¯x)−f(¯x′)
c⊤(¯x−¯x′)≤κrs, setρ←ρ/2, iff(¯x)−f(¯x′)
c⊤(¯x−¯x′)≥κresetρ←2ρ.
(vi) Iff(¯x)−f(¯x′)
c⊤(¯x−¯x′)≥κrm, set ¯x←¯x′.
(vii) Replace the point in Sfurthest from ¯ xwith the new point ¯ x′, if it is closer, and go back to (i).
The Reﬁnement step is triggered after Trffull cycles of the global search strategy in the Iteration step (i.e.,
the strategy to select f∗
kin Gutmann’s RBF method, or αin MSRSM), but only if one of the following two
conditions apply: (i) a better solution was discovered since the last execution of the Reﬁnement step, or (ii) the
last Reﬁnement step was stopped because of its iteration limit (parameter Trs, see below), rather than for lack
of improvement.
When the Reﬁnement step ends, all points at which fhas been evaluated are added to x1,...,xk, and the
algorithm goes back to the Iteration step. The Reﬁnement step ends when one of the following conditions is
veriﬁed:
•afterTrsconsecutive iterations, unless we are close to hitting the limit on the maximum number of objective
function evaluations, or the CPU time limit (this is deﬁned by a further parameter);
•if the radius ρof the reﬁnement search drops below βmr;
•if the norm of the gradient of the linear model drops below ϵgrad.
The above scheme is designed with continuous variables in mind, but we heuristically apply the Reﬁnement
step also in the presence of integer or categorical variables. When the problem has integer or categorical variables,
the Reﬁnement step proceeds as described above, but every candidate point is rounded to an integer point before
being evaluated with f. In particular, every integer variable that takes on a fractional value in the candidate
point, say ¯xj, is rounded down with probability ⌈¯xj⌉−¯xj, and rounded up with probability ¯ xj−⌊¯xj⌋; whereas
every unary representation of a categorical variable, say (¯ z1,..., ¯zmh) such that∑mh
j=1¯zj= 1 in extended space,
is rounded to the orthonormal basis vector eiwith probability ¯ zi/∑mh
j=1¯zjfor alli= 1,...,mh. The rounding
process for integer and categorical variable is repeated a given number times, and the point with the best linear
model score is chosen as the next candidate. A similar procedure is applied in step (ii) to the column of Qthat is
about to replace one column in M: each entry is projected to the closest feasible vector in extended space, using
ℓ1-norm distance.
4.4 Repairing numerical errors: Restoration step
Whenever numerical errors are detected in the solution of the linear system (4), we switch to a Restoration step
that works as follows. Given the list of interpolation points x1,...,xk, fori=k,k−1,..., 1 we heuristically solve
the problem:
max
x∈Ωemin
j=1,...,k,j̸=i∥x−xj∥,
then temporarily replace xiwith the solution to the above problem, say ¯ x. If the system (8) for the points
x1,...,xi−1,¯x,xi+1,...,xkis invertible, we permanently replace xiwith ¯x, and the Restoration step is successful.
Otherwise, we reinstate xiand continue the Restoration step by decreasing i. We remark that several interpolation
points may be added in between successive solutions of (4), because the Reﬁnement step may perform multiple
iterations and it does not recompute the interpolant sk. For this reason, we cannot hope that removing the last
interpolation node is always suﬃcient to ﬁx numerical errors.
The rationale for solving a maxmin distance problem when trying to improve the numerics is that proximity
to other interpolation points necessarily leads to an ill-conditioned linear system: if two points are very close
to each other, the corresponding rows in (4) are almost identical. This suggests maximizing the distance from
11"
2109.12424,D:\Database\arxiv\papers\2109.12424.pdf,"Given the increasing availability of biomedical language representation models, what are the key challenges and opportunities for improving coreference resolution in the biomedical domain, particularly when dealing with full-text articles?","While biomedical language representation models have shown promise in improving coreference resolution, challenges remain in handling the complexity of full-text articles, including their length and the diversity of coreference types. Opportunities lie in exploring more effective integration of external biomedical knowledge bases and developing robust evaluation metrics that capture the nuances of biomedical coreference.","pronominal anaphora, this corpus also annotated
abbreviation/acronyms and numerical anaphora.
CRAFT-CR (Cohen et al., 2017) consists of 97
full-text biomedical journal articles. Similar to the
general domain, this corpus was annotated with
coreferent chains in full-text articles, while most
other biomedical coreference datasets focuse on an-
notating the pairwise coreference relation between
an anaphor and its antecedent. In addition, all coref-
erence expressions were annotated regardless of
semantic type.
These datasets are summarized in Table 1.
4 Biomedical Language Representation
Models
The news domain and the biomedical domain are
different in a number of respects, such as markable
types. Some authors have argued that biomedical
domain knowledge is the key to bridging the gap
(Choi et al., 2014), and that therefore, incorporat-
ing biomedical speciﬁc representation is beneﬁcial
for resolving coreferring expressions in the biomed-
ical domain. In this section, we will give a brief
introduction to biomedical language representation
models.
4.1 Pre-training on biomedical corpora
Following the success of large-scale pre-training
language models (PLMs) in the general domain,
several biomedical-domain PLMs have been devel-
oped in recent years by pre-training on large-scale
biomedical corpora.
Most biomedical PLMs conduct continual pre-
training of the general domain PLMs and still
use vocabulary trained on the general domain text.
BioBERT (Lee et al., 2020) is the ﬁrst transformer-
based biomedical PLM, pre-trained on PubMed ab-
stracts and PubMed Central full-text articles. Clini-
calBERT and Bio_ClinicalBERT (Alsentzer et al.,
2019) are pre-trained on MIMIC-III Clinical Notes,
whereas BlueBERT (Peng et al., 2019) uses both
PubMed and MIMIC-III for pre-training. All these
models are pre-trained based on general BERT, ex-
cept Bio_ClinicalBERT which is initialized from
BioBERT.
In addition to initializing from general BERT,
some biomedical PLMs are directly pre-trained
on biomedical text from scratch and use domain-
speciﬁc custom vocabulary. SciBERT (Beltagy
et al., 2019) is pre-trained on biomedical and com-
puter science papers from scratch and achievedgood performance on many scientiﬁc NLP tasks.
PubMedBERT (Gu et al., 2020) and BioELECTRA
(raj Kanakarajan et al., 2021) are both pre-trained
on PubMed abstract and PubMed Central full text
articles, but the latter adopts ELECTRA architec-
ture (Clark et al., 2019). BioMegatron (Shin et al.,
2020) is a large-scale model based on Megatron
(Shoeybi et al., 2019) architecture. It also investi-
gated the effect of vocabulary and corpora domain
on the performance of biomedical tasks.
4.2 Integrating biomedical knowledge bases
Although the biomedical PLMs, such as BioBERT,
have achieved good performance on many biomed-
ical tasks, however, these models can be further en-
hanced by integrating biomedical knowledge bases,
such as UMLS (Bodenreider, 2004).
Several models enhance biomedical PLMs by in-
tegrating synonym knowledge from UMLS. Each
mention in the biomedical text can be linked to a
Concept Unique Identiﬁer (CUI) in UMLS, and
each CUI has a synonym set. SAPBERT (Liu et al.,
2021), UMLSBERT (Michalopoulos et al., 2021)
and BIOSYN (Sung et al., 2020) further pre-trained
PubMedBERT, Bio_Clinical BERT and BioBERT
on UMLS synonyms, using multi-similarity loss,
multi-label loss and synonym marginalization algo-
rithm respectively.
In addition to synonym knowledge, Clinical KB-
BERT (Hao et al., 2020) injects UMLS relation
knowledge into BioBERT. Whereas CODER (Yuan
et al., 2020) learns both synonym and relation
knowledge based on PubMedBERT or mBERT
(Devlin et al., 2019) via contrastive learning. Also,
some research focus on fusing the UMLS entity em-
beddings with contextual embeddings to improve
biomedical PLMs (He et al., 2020; Fei et al., 2021;
Yuan et al., 2021).
This paper selected some of the models above
to evaluate the ability of biomedical-speciﬁc repre-
sentation for biomedical coreference task, detailed
in Section 6.
5 Coreference Models for the Biomedical
Domain
5.1 Rule-based models
Early approaches to biomedical coreference resolu-
tion are primarily rule-based. These models rely on
syntactic parsers to extract hand-crafted features
and rules.
Nguyen et al. (2012) implemented a protein
coreference system that makes use of syntactic
information from the parser output, and protein-
indicated information. The results showed that
domain-speciﬁc semantic information is important
for coreference resolution. Miwa et al. (2012)
developed a rule-based coreference system, as a
part of the EventMine event extraction system.
A set of rules was developed based on syntactic
trees and predicate-argument structures. The sys-
tem achieved 55.9% F1 score on BioNLP 2011
protein coreference task. Kilicoglu and Demner-
Fushman (2016) developed a new corpus of struc-
tured drug labels and proposed a general frame-
work based on a smorgasbord architecture for ﬁne-
grained biomedical coreference resolution. The
framework adopted different strategies for each
coreference type and mention type, and combined
them to reach desired performance, like selecting
dishes from a smorgasbord. Li et al. (2018) pre-
sented two methods for bio-entity coreference reso-
lution: a rule-based method and a recurrent neural
network (RNN) model. The rule-based model cre-
ated a set of syntactic rules or semantic constraints
for coreference and achieved a state-of-the-art per-
formance with 62.0% F1 score on BioNLP 2011
protein coreference task.
These rule-based models mostly designed rules
for speciﬁc type of coreference relation and even
speciﬁc corpus, which limits the scope of the reso-
lution.
5.2 Machine learning-based models
In the early years, due to the lack of publicly avail-
able annotated corpora, researchers have to an-
notate their own corpora for developing machine
learning approaches (Yang et al., 2004, Torii and
Vijay-Shanker, 2005, Su et al., 2008, Gasperin,
2009).
After the BioNLP 2011 protein coreference
dataset was made publicly available, several ma-
chine learning-based models were developed for
this task. Kim et al. (2011) adapted a general coref-
erence system Reconcile (Stoyanov et al., 2010) for
the biomedical domain by modifying several com-
ponents to biomedical texts. It trained two separate
classiﬁers for detecting anaphora and antecedent
mentions.
In addition to using machine learning-based
methods only, several models adopted hydrid ap-
proach, i.e., combining both machine learning-BioNLP CRAFT
Training set (docs) 800 60
Development set (docs) 150 7
Test set (docs) 260 30
Avg. sent. per doc 9.2 312.4
Avg. words per doc 258.0 8181.0
Table 2: Statistics of BioNLP and CRAFT.
based and rule-based methods. D’Souza and Ng
(2012) proposed a hybrid approach that used a clas-
siﬁer with syntactic path-based features. It investi-
gated ﬁve different learning-based methods, and a
rule-based approach for anaphora resolution. This
model achieved a superior performance than pre-
vious either rule-based or learning-based models
on BioNLP 2011 protein coreference task. Li et al.
(2014) later also used a hybrid approach, adopt-
ing the rule-based method or the machine learning
method for three types of anaphora. As the method
of D’Souza and Ng (2012), they also used different
rules for different types of anaphora. The system
achieved better performance with 68.6% F1 score
than previous methods on BioNLP 2011 protein
coreference development data.
5.3 Deep learning-based models
In recent years, much effort has been made on using
deep learning methods for biomedical coreference.
Trieu et al. (2018) applied general domain end-
to-end neural coreference resolution system (Lee
et al., 2017) to biomedical text, integrating the
domain speciﬁc features to enhance the system.
The model was evaluated on BioNLP 2011 pro-
tein coreference dataset and CRAFT-CR dataset.
The results indicated that in-domain embeddings
and domain-speciﬁc features helped improve the
performance. Then, Trieu et al. (2019) proposed
a system to address the challenge of coreference
resolution in the full-text articles in the CRAFT-CR
dataset. The model also applied end-to-end system
(Lee et al., 2017), but enhanced the system by uti-
lizing a syntax-based mention ﬁltering method and
replacing LSTM with BERT. This model achieved
better performance on the CRAFT-CR dataset.
Different from the models above, Li et al. (2021)
integrated external knowledge to enhance the neu-
ral coreference system for biomedical texts. A
knowledge attention module was developed to se-
lect the most related and helpful knowledge triplets.
This model achieved the state-of-the-art perfor-
Classiﬁcation Model dev test
rule- machine deep R P F1 R P F1
based learning learning
✓ Reconcile (Kim et al., 2011) 26.7 74.0 39.3 22.2 73.3 34.1
✓ (Nguyen et al., 2012) 57.8 67.8 62.4 52.5 50.2 51.3
✓ EventMine (Miwa et al., 2012) 53.5 69.8 60.5 50.4 62.7 55.9
✓ ✓ (D’Souza and Ng, 2012) 59.9 77.1 67.4 55.6 67.2 60.9
✓ ✓ (Li et al., 2014) 69.8 67.5 68.6 - - -
✓ Simple system (Choi et al., 2016) 64.4 63.4 63.9 50.0 46.3 48.1
✓ (Kilicoglu and Demner-Fushman, 2016) 63.2 72.4 67.5 - - -
✓ (Li et al., 2018)-rule 68.8 76.0 72.2 60.2 63.8 62.0
✓ (Li et al., 2018)-neural 60.4 61.9 61.2 54.9 58.0 56.4
✓ E2E_MetaMap (Trieu et al., 2018) 56.7 71.7 63.1 47.5 55.6 51.2
✓ KB-attention (Li et al., 2021) 63.4 68.1 65.6 69.4 69.6 69.5
Table 3: Performance of biomedical coreference models on BioNLP 2011 protein correference development and
test sets.
mance on the BioNLP 2011 protein coreference
dataset and CRAFT-CR dataset.
6 Comparing the Biomedical Language
Representation Models for Coreference
In Section 4, we introduced a series of biomedi-
cal language representation models. To investigate
the ability of these models for biomedical coref-
erence task, we conduct experiments to evaluate
these models on CRAFT-CR dataset.
6.1 Baseline model
We employ the higher-order coreference model
(Lee et al., 2018) as the baseline model, but use
different pre-trained language models with BERT
architecture to replace LSTM encoder.
The goal is to learn a distribution P(yi) over
possible antecedents Y(i)for each span i:
P(yi) =es(i,yi)
∑
y′∈Y(i)es(i,y′)(1)
wheres(i,j)is a pairwise score for a coreference
link between span iand spanj. The pairwise score
is computed by the mention score of i, the mention
score ofj, and two kinds of joint compatibility
scores ofiandj:
s(i,j) =sm(i) +sm(j) +sc(i,j) +sa(i,j)(2)
The mention score and joint compatibility scores
are computed using span representation giandgj
from bidirectional LSTMs:
sm(i) =FFNN m(gi) (3)
sc(i,j) =gT
iWcgj (4)sa(i,j) =FFNN a([gi,gj,gi◦gj,φ(i,j)]) (5)
where FFNN( ·)represents a feed-forward neural
network,Wcis a learned weight matrix, ◦de-
notes element-wise product, and φ(i,j)represents
speaker and metadata features.
6.2 Applying pre-trained language models
We apply two types of PLM to replace LSTM en-
coder respectively:
Biomedical PLMs : to enhance the baseline
model with biomedical domain knowledge, several
biomedical PLMs are selected, including models
pre-training on biomedical corpora or integrating
biomedical knowledge bases.
SpanBERT : since SpanBERT (Joshi et al., 2020)
is a state-of-the-art coreference resolution model
in the general domain, we also evaluate SpanBERT
and general BERT (Joshi et al., 2019) on biomedi-
cal coreference.
Since CRAFT-CR is a more challenging biomed-
ical coreference dataset consisting of full-text arti-
cles, we choose CRAFT-CR to ﬁne-tune and eval-
uate these models. The details are introduced in
Section 7.2.
7 Results
In this section, we ﬁrst present the performance
achieved by previous biomedical coreference mod-
els decribed in Section 5. Then we describe our
experiment and report the results.
7.1 Results by datasets
Recent biomedical coreference models are mostly
evaluated on BioNLP 2011 protein coreference
Model B3BLANC CEAFE CEAFM LEA MUC Avg.
E2E_MetaMap (Trieu et al., 2018) 36.4 46.5 33.1 41.0 32.4 51.8 40.2
BERT_ﬁlter (Trieu et al., 2019) 44.0 48.9 39.8 49.0 40.0 57.0 46.4
KB-attention (Li et al., 2021) 54.9 63.1 48.6 59.4 51.3 64.5 57.0
Table 4: F1 scores of biomedical coreference models on CRAFT-CR test set.
Model B3BLANC CEAFE CEAFM LEA MUC Avg.
BioBERT 41.67 42.39 32.44 45.15 39.00 53.66 42.38
SciBERT 25.66 28.30 16.76 30.34 22.67 40.70 27.41
Bio_ClinicalBERT 38.19 36.91 30.11 41.56 35.57 48.22 38.43
PubMedBERT 34.96 33.14 25.49 38.49 32.32 47.02 35.24
UMLSBERT 27.53 26.95 19.95 31.40 24.95 39.80 28.43
Clinical KB-BERT 44.56 44.99 37.25 48.29 41.67 55.17 45.32
BERT_base 32.96 31.36 22.58 36.25 30.73 44.34 33.04
SpanBERT_base 47.05 46.30 39.90 51.27 44.36 57.69 47.76
Table 5: F1 scores of different PLMs combined with c2f-coref model on CRAFT-CR test set.
dataset1and CRAFT-CR dataset2, of which the
statistics are shown in Table 2. The performance
on the two datasets are summarized and analysed
respectively as follows.
Table 3 shows the performance of different
biomedical coreference models on BioNLP 2011
protein coreference development and test sets.
These models are evaluated using the scorer pro-
vided by the BioNLP shared task organisers. As
shown in Table 3, KB-attention (Li et al., 2021)
achieved the best performance of 69.5% F1 score
on the test set of BioNLP. This indicates that inte-
grating external biomedical knowledge base can
further enhance the coreference models for the
biomedical domain. In addition, compared with
deep learning-based models, some rule-based (Kil-
icoglu and Demner-Fushman 2016, Li et al. 2018)
or hybrid models (D’Souza and Ng 2012, Li et al.
2014) still achieved favorable performance.
Table 4 shows the F1 scores of different biomedi-
cal coreference models on CRAFT-CR test set. We
can see that the best performance is also achieved
by KB-attention (Li et al., 2021), showing the ad-
vantage of ﬁne-grained knowledge base integration.
However, the results on CRAFT-CR are overall
lower than those on BioNLP. The possible reason is
that CRAFT-CR consists of full-text articles, hence
the length of documents in CRAFT-CR is much
1http://2011.bionlp-st.org/home/
protein-gene-coreference-task
2https://github.com/UCDenver-ccp/
craft-shared-tasksgreater. This makes CRAFT-CR more challenging
than BioNLP dataset which comprises abstracts
only.
7.2 Experiments
7.2.1 Experimental setup
We conduct experiment using following models:
-biomedical PLMs+c2f-coref : we refer to the
higher-order coreference model (Lee et al.,
2018) as c2f-coref . We build the c2f-coref
system on top of different biomedical PLMs
respectively, including BioBERT, SciBERT,
Bio_ClinicalBERT, PubMedBERT, UMLS-
BERT, and Clinical KB-BERT. Among these
models, UMLSBERT and Clinical KB-BERT
integrate external biomedical knowledge base,
i.e., UMLS, while other models are pre-
trained on large-scale biomedical datasets.
-BERT_base+c2f-coref (Joshi et al., 2019):
the c2f-coref system on top of BERT repre-
sentation.
-SpanBERT_base+c2f-coref (Joshi et al.,
2020): the c2f-coref system on top of Span-
BERT_base, which pre-trained span represen-
tations to better represent and predict spans of
text.
We run these models on the CRAFT-CR dataset
of latest released version 4.0.13. CRAFT-CR con-
3https://github.com/UCDenver-ccp/
CRAFT/releases/tag/v4.0.1
Model Evaluation script Programming Time cost MUC B3CEAFE Avg.
language
SpanBERT_base
+c2f-corefCoNLL scorer 9.0 Perl about 1.5h 48.67 8.61 18.90 25.39
CoVal script Python about 30s 55.99 43.97 40.01 46.66
CRAFT evaluation script Clojure about 20m 57.69 47.05 39.90 48.21
Table 6: F1 scores of SpanBERT_base+c2f-coref on CRAFT-CR test set using different evaluation scripts.
sists of 97 full-text journal articles from PMC. As
shown in Table 2, 60 documents are used for ﬁne-
tuning these models.
These models are ﬁne-tuned using learning rate
of1×10−5for PLMs parameters and 2×10−4for
task parameters with Adam optimizer, a dropout of
0.3, and max_training_len of 384 for Span-
BERT_base and 128 for other PLMs respectively.
For SciBERT and PubMedBERT, we use the spe-
ciﬁc domain vocabulary, while general BERT vo-
cabulary is used for other models.
For evaluation, we calculate F1 scores on six
common metrics including B3, BLANC, CEAFE,
CEAFM, LEA and MUC using the ofﬁcial evalu-
ation script4provided by the CRAFT shared task
organizers, which is also used by previous models
(Trieu et al. 2018, Trieu et al. 2019, Li et al. 2021).
7.2.2 Results
The F1 scores of different PLMs combined with
c2f-coref model on the CRAFT-CR test set are
shown in Table 5. We can see that SpanBERT_base
achieved the best performance of 47.76% F1 score,
even without biomedical domain pre-training. This
proves the powerful ability of SpanBERT on coref-
erence resolution task.
In addition, biomedical PLMs outperform
BERT_base on CRAFT-CR, except SciBERT (Belt-
agy et al., 2019) and UMLSBERT (Michalopoulos
et al., 2021), which shows that biomedical domain
knowledge can generally beneﬁt coreference mod-
els for the biomedical domain. Moreover, Clinical
KB-BERT (Hao et al., 2020), which is initialized
from BioBERT (Lee et al., 2020), achieved better
performance than other biomedical PLMs, indi-
cating that biomedical PLMs can be further en-
hanced by integrating external biomedical knowl-
edge bases. However, SciBERT performs worse
than BERT_base on the CRAFT-CR dataset, al-
though pre-trained on scientiﬁc texts and achieved
better performance than BERT_base on some other
scientiﬁc NLP tasks such as NER, as reported in
4https://github.com/UCDenver-ccp/
craft-shared-tasksBeltagy et al. (2019). One possible reason is that
the pre-training corpora of SciBERT contain a num-
ber of computer science articles, which is unlikely
to be beneﬁcial for biomedical tasks.
Among these biomedical PLMs, SciBERT (Belt-
agy et al., 2019) and PubMedBERT (Gu et al.,
2020) are pre-trained on domain-speciﬁc text from
scratch, while others conduct continual pre-training
based on the general domain. Although Gu et al.
(2020) shows that domain-speciﬁc pre-training
from scratch outperforms continual pre-training
from general-domain language models, the results
of our experiment are the opposite. Presumably the
reason is that CRAFT-CR annotated all semantic
type markables and covered a wider range of coref-
erences, so pre-training on the general domain is
also beneﬁcial.
7.2.3 Results using different evaluation
scripts
Apart from the ofﬁcial evaluation script provided
by CRAFT shared task organizers, we also used
two other evaluation scripts, i.e., CoNLL scorer
9.0 and the CoVal script, to evaluate these models
for comparing the differences between these eval-
uation scripts on the CRAFT-CR dataset. CoNLL
scorer 9.05is a modiﬁed version of the original
reference coreference scorer (Pradhan et al., 2014)
used for CoNLL-2011/2012 shared tasks. It added
an optional partial mention matching scheme and
handling for discontinuous mentions, i.e., men-
tions composed of non-contiguous tokens. The
CoVal script6is a python coreference scorer for
both CoNLL and ARRAU datasets (Uryupina et al.,
2020).
The results of three different evaluation scripts
for SpanBERT_base+c2f-coref model on CRAFT-
CR test set are shown in Table 6. The F1 scores
of MUC,B3and CEAFE metrics as well as the
averaged value are provided. As shown in Table 6,
a strange phenomenon is that the results of CoNLL
5https://github.com/bill-baumgartner/
reference-coreference-scorers
6https://github.com/ns-moosavi/coval
scorer 9.0 are much lower than those of the other
two evaluation scripts, especially on the B3and
CEAFE metrics. The reason of that is not clear
and needs further analysis. Whereas, the results
of the CoVal script and CRAFT ofﬁcial evaluation
script are close, although the scores of the latter are
a little higher.
In addition to the F1 scores, the time cost of the
three evaluation scripts are quiet different. The
CoNLL scorer 9.0 took about one and a half hours,
while the CoVal script only needed about 30 sec-
onds for evaluation.
8 Conclusion
In this paper, we review and analyse the progress
of biomedical coreference datasets, biomedical lan-
guage representation models and coreference mod-
els for the biomedical domain. Biomedical coref-
erence is an essential but challenging task. Some
efforts have been made in this ﬁeld, but there is still
a much room for improvement. The experiments
which we conducted indicate biomedical domain
knowledge from either pre-training on biomedical
texts or integrating biomedical knowledge bases
can enhance coreference models for the biomedical
domain.
Acknowledgements
This research was supported in part by the China
Scholarship Council, and the DALI project, ERC
Grant 695662.
References
Emily Alsentzer, John Murphy, William Boag, Wei-
Hung Weng, Di Jindi, Tristan Naumann, and
Matthew McDermott. 2019. Publicly available clini-
cal bert embeddings. In Proceedings of the 2nd Clin-
ical Natural Language Processing Workshop , pages
72–78.
Riza Theresa Batista-Navarro and Sophia Ananiadou.
2011. Building a coreference-annotated corpus
from the domain of biochemistry. In Proceedings
of BioNLP 2011 Workshop , pages 83–91.
Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:
A pretrained language model for scientiﬁc text. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 3615–
3620.Olivier Bodenreider. 2004. The uniﬁed medical lan-
guage system (umls): integrating biomedical termi-
nology. Nucleic acids research , 32(suppl_1):D267–
D270.
José Castano, Jason Zhang, and James Pustejovsky.
2002. Anaphora resolution in biomedical literature.
Miji Choi, Karin Verspoor, and Justin Zobel. 2014.
Evaluation of coreference resolution for biomedical
text. In MedIR@ SIGIR .
Miji Choi, Justin Zobel, and Karin Verspoor. 2016. A
categorical analysis of coreference resolution errors
in biomedical texts. Journal of biomedical informat-
ics, 60:309–318.
Kevin Clark, Minh-Thang Luong, Quoc V Le, and
Christopher D Manning. 2019. Electra: Pre-training
text encoders as discriminators rather than genera-
tors. In International Conference on Learning Rep-
resentations .
K Bretonnel Cohen, Arrick Lanfranchi, Miji Joo-young
Choi, Michael Bada, William A Baumgartner, Na-
talya Panteleyeva, Karin Verspoor, Martha Palmer,
and Lawrence E Hunter. 2017. Coreference annota-
tion and resolution in the colorado richly annotated
full text (craft) corpus of biomedical journal articles.
BMC bioinformatics , 18(1):1–14.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186.
Jennifer D’Souza and Vincent Ng. 2012. Anaphora res-
olution in biomedical literature: a hybrid approach.
InProceedings of the ACM Conference on Bioin-
formatics, Computational Biology and Biomedicine ,
pages 113–122.
Hao Fei, Yafeng Ren, Yue Zhang, Donghong Ji, and
Xiaohui Liang. 2021. Enriching contextualized lan-
guage model from knowledge graph for biomedical
information extraction. Brieﬁngs in Bioinformatics ,
22(3):bbaa110.
Caroline Gasperin, Nikiforos Karamanis, and Ruth
Seal. 2007. Annotation of anaphoric relations in
biomedical full-text articles using a domain-relevant
scheme. In Proceedings of DAARC , volume 2007.
Citeseer.
Caroline V Gasperin. 2009. Statistical anaphora reso-
lution in biomedical texts. Technical report, Univer-
sity of Cambridge, Computer Laboratory.
Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas,
Naoto Usuyama, Xiaodong Liu, Tristan Naumann,
Jianfeng Gao, and Hoifung Poon. 2020. Domain-
speciﬁc language model pretraining for biomedi-
cal natural language processing. arXiv preprint
arXiv:2007.15779 .
Boran Hao, Henghui Zhu, and Ioannis Paschalidis.
2020. Enhancing clinical bert embedding using a
biomedical knowledge base. In Proceedings of the
28th international conference on computational lin-
guistics , pages 657–661.
Bin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu,
Nicholas Jing Yuan, and Tong Xu. 2020. Integrat-
ing graph contextualized knowledge into pre-trained
language models. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing: Findings , pages 2281–2290.
Lynette Hirschman. 1997. Muc-7 coreference task def-
inition, version 3.0. Proceedings of MUC-7, 1997 .
Eduard Hovy, Mitch Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of the human lan-
guage technology conference of the NAACL, Com-
panion Volume: Short Papers , pages 57–60.
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,
Luke Zettlemoyer, and Omer Levy. 2020. Spanbert:
Improving pre-training by representing and predict-
ing spans. Transactions of the Association for Com-
putational Linguistics , 8:64–77.
Mandar Joshi, Omer Levy, Luke Zettlemoyer, and
Daniel S Weld. 2019. Bert for coreference reso-
lution: Baselines and analysis. In Proceedings of
the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 5803–5808.
Halil Kilicoglu and Dina Demner-Fushman. 2016. Bio-
scores: A smorgasbord architecture for corefer-
ence resolution in biomedical text. PloS one ,
11(3):e0148538.
J-D Kim, Tomoko Ohta, Yuka Tateisi, and Jun’ichi
Tsujii. 2003. Genia corpus—a semantically anno-
tated corpus for bio-textmining. Bioinformatics ,
19(suppl_1):i180–i182.
Jin-Dong Kim, Tomoko Ohta, and Jun’ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from literature. BMC bioinformatics , 9(1):1–
25.
Jung-Jae Kim and Jong C Park. 2004. Bioar: Anaphora
resolution for relating protein names to proteome
database entries. In Proceedings of the Conference
on Reference Resolution and Its Applications , pages
79–86.
Youngjun Kim, Ellen Riloff, and Nathan Gilbert. 2011.
The taming of reconcile as a biomedical coreference
resolver. In Proceedings of BioNLP Shared Task
2011 Workshop , pages 89–93.Jinhyuk Lee, Wonjin Yoon, Sungdong Kim,
Donghyeon Kim, Sunkyu Kim, Chan Ho So, and
Jaewoo Kang. 2020. Biobert: a pre-trained biomed-
ical language representation model for biomedical
text mining. Bioinformatics , 36(4):1234–1240.
Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle-
moyer. 2017. End-to-end neural coreference reso-
lution. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing ,
pages 188–197.
Kenton Lee, Luheng He, and Luke Zettlemoyer. 2018.
Higher-order coreference resolution with coarse-to-
ﬁne inference. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 2 (Short Papers) , pages
687–692.
Chen Li, Zhiqiang Rao, Qinghua Zheng, and Xian-
grong Zhang. 2018. A set of domain rules and
a deep network for protein coreference resolution.
Database , 2018.
Lishuang Li, Liuke Jin, Zhenchao Jiang, Jing Zhang,
and Degen Huang. 2014. Coreference resolution in
biomedical texts. In 2014 IEEE International Con-
ference on Bioinformatics and Biomedicine (BIBM) ,
pages 12–14. IEEE.
Y Li, X Ma, X Zhou, P Cheng, K He, and C Li. 2021.
Knowledge enhanced lstm for coreference resolu-
tion on biomedical texts. Bioinformatics (Oxford,
England) .
Yu-Hsiang Lin and Tyne Liang. 2004. Pronominal and
sortal anaphora resolution for biomedical literature.
InProceedings of the 16th Conference on Compu-
tational Linguistics and Speech Processing , pages
101–109.
Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco
Basaldella, and Nigel Collier. 2021. Self-alignment
pretraining for biomedical entity representations. In
Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 4228–4238.
George Michalopoulos, Yuanxin Wang, Hussam Kaka,
Helen Chen, and Alexander Wong. 2021. Umlsbert:
Clinical domain knowledge augmentation of contex-
tual embeddings using the uniﬁed medical language
system metathesaurus. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 1744–1753.
Makoto Miwa, Paul Thompson, and Sophia Ananiadou.
2012. Boosting automatic event extraction from the
literature using domain adaptation and coreference
resolution. Bioinformatics , 28(13):1759–1765.
Ngan Nguyen, Jin-Dong Kim, Makoto Miwa, Takuya
Matsuzaki, and Junichi Tsujii. 2012. Improving pro-
tein coreference resolution by simple semantic clas-
siﬁcation. BMC bioinformatics , 13(1):1–12.
Ngan Nguyen, Jin-Dong Kim, and Jun’ichi Tsujii.
2011. Overview of the protein coreference task
in bionlp shared task 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop , pages 74–82.
Citeseer.
Yifan Peng, Shankai Yan, and Zhiyong Lu. 2019.
Transfer learning in biomedical natural language
processing: An evaluation of bert and elmo on ten
benchmarking datasets. In Proceedings of the 18th
BioNLP Workshop and Shared Task , pages 58–65.
Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-
uard Hovy, Vincent Ng, and Michael Strube. 2014.
Scoring coreference partitions of predicted men-
tions: A reference implementation. In Proceedings
of the conference. Association for Computational
Linguistics. Meeting , volume 2014, page 30. NIH
Public Access.
James Pustejovsky, José Castano, Roser Sauri, Jason
Zhang, and Wei Luo. 2002. Medstract: creat-
ing large-scale information servers from biomedical
texts. In Proceedings of the ACL-02 workshop on
Natural language processing in the biomedical do-
main , pages 85–92.
Kamal raj Kanakarajan, Bhuvana Kundumani, and
Malaikannan Sankarasubbu. 2021. Bioelectra: Pre-
trained biomedical text encoder using discriminators.
InProceedings of the 20th Workshop on Biomedical
Language Processing , pages 143–154.
Isabel Segura-Bedmar, Mario Crespo, César de Pablo-
Sánchez, and Paloma Martínez. 2010. Resolving
anaphoras for the extraction of drug-drug interac-
tions in pharmacological documents. In BMC bioin-
formatics , volume 11, pages 1–9. BioMed Central.
Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina,
Raul Puri, Mostofa Patwary, Mohammad Shoeybi,
and Raghav Mani. 2020. Bio-megatron: Larger
biomedical domain language model. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP) , pages
4700–4706.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
Patrick LeGresley, Jared Casper, and Bryan Catan-
zaro. 2019. Megatron-lm: Training multi-billion pa-
rameter language models using model parallelism.
arXiv preprint arXiv:1909.08053 .
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010.
Coreference resolution with reconcile. In Proceed-
ings of the ACL 2010 Conference Short Papers ,
pages 156–161.Jian Su, Xiaofeng Yang, Huaqing Hong, Yuka Tateisi,
and Jun’ichi Tsujii. 2008. Coreference resolution
in biomedical texts: a machine learning approach.
InDagstuhl Seminar Proceedings . Schloss Dagstuhl-
Leibniz-Zentrum für Informatik.
Mujeen Sung, Hwisang Jeon, Jinhyuk Lee, and Jae-
woo Kang. 2020. Biomedical entity representations
with synonym marginalization. In Proceedings of
the 58th Annual Meeting of the Association for Com-
putational Linguistics , pages 3641–3650.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun’ichi Tsujii. 2005. Syntax annotation for the ge-
nia corpus. In Companion Volume to the Proceed-
ings of Conference including Posters/Demos and tu-
torial abstracts .
Manabu Torii and K Vijay-Shanker. 2005. Anaphora
resolution of demonstrative noun phrases in medline
abstracts. In Proceedings of , pages 332–339.
Hai-Long Trieu, Anh-Khoa Duong Nguyen, Nhung
Nguyen, Makoto Miwa, Hiroya Takamura, and
Sophia Ananiadou. 2019. Coreference resolution in
full text articles with bert and syntax-based mention
ﬁltering. In Proceedings of The 5th Workshop on
BioNLP Open Shared Tasks , pages 196–205.
Hai Long Trieu, Nhung TH Nguyen, Makoto Miwa,
and Sophia Ananiadou. 2018. Investigating domain-
speciﬁc information for neural coreference resolu-
tion on biomedical texts. In Proceedings of the
BioNLP 2018 workshop , pages 183–188.
Olga Uryupina, Ron Artstein, Antonella Bristot, Feder-
ica Cavicchio, Francesca Delogu, Kepa J Rodriguez,
and Massimo Poesio. 2020. Annotating a broad
range of anaphoric phenomena, in a variety of gen-
res: the arrau corpus. Natural Language Engineer-
ing, 26(1):95–128.
Wei Wu, Fei Wang, Arianna Yuan, Fei Wu, and Ji-
wei Li. 2020. Corefqa: Coreference resolution as
query-based span prediction. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics , pages 6953–6963.
Xiaofeng Yang, Jian Su, Guodong Zhou, and
Chew Lim Tan. 2004. An np-cluster based approach
to coreference resolution. In COLING 2004: Pro-
ceedings of the 20th International Conference on
Computational Linguistics , pages 226–232.
Deming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu, Peng
Li, Maosong Sun, and Zhiyuan Liu. 2020. Coref-
erential reasoning learning for language representa-
tion. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP) , pages 7170–7186.
Zheng Yuan, Yijia Liu, Chuanqi Tan, Songfang Huang,
and Fei Huang. 2021. Improving biomedical pre-
trained language models with knowledge. In Pro-
ceedings of the 20th Workshop on Biomedical Lan-
guage Processing , pages 180–190."
2008.04555,D:\Database\arxiv\papers\2008.04555.pdf,"The paper describes a method for optimizing functions on Riemannian manifolds.  What are the key advantages of this method compared to other existing methods, and how do these advantages translate to practical applications?","The method offers a significant improvement in computational complexity compared to other adaptive methods, achieving a near-optimal rate of convergence with a single-sample batch. This translates to faster convergence and reduced computational cost in practical applications, particularly for large-scale problems involving high-dimensional data.","−Txt
xt−1(gradfSt(xt−1)−dt−1))
(4)
=gradfSt(xt) + (1−ρt)Txt
xt−1(
dt−1−gradfSt(xt−1))
,
(5)
which hybrids stochastic gradient grad fSt(xt)with the re-
cursive gradient estimator in (2) for ρt∈[0,1]. This can be
also viewed as combining momentum estimator in (3) with
a scaled difference of grad fSt(xt)−Txtxt−1gradfSt(xt−1).
Note that we recover vanilla RSGD when ρt= 0 and the
recursive estimator in (2) when ρt= 1. As we will demon-
strate in Section 4, ρtshould be decreasing rather than ﬁxed,
thereby enabling a smooth transition from RSGD to RSRG.
As a result, we do not require restarting the algorithm to
achieve the optimal convergence.
Compared with algorithm designs in Euclidean versions
of SRM (Cutkosky and Orabona 2019; Tran-Dinh et al.
2019), our formulation and parameter settings are largely
different. Speciﬁcally, Cutkosky and Orabona (2019) further
adapts the recursive momentum parameter ρtto the learn-
ing rateηtwhere the latter itself is adapted to the norm
of stochastic gradient. This is claimed to relieve the pa-
rameter tuning process. However, they reintroduce three pa-
rameters, which are even less intuitive to be tuned (even
though some are ﬁxed to a default value). As shown in
Section 4, we only require tuning the initial step size η0
and initial momentum parameter ρ0(where the latter can
be ﬁxed to a good default value). Furthermore, the adaptive
step size requires a uniform gradient Lipschitz condition, the
same as in (Kasai, Jawanpuria, and Mishra 2019) and also
a uniform smoothness assumption, which is stronger than
mean-sqaured smoothness in our setting. On the other hand,
Tran-Dinh et al. (2019) replaces grad fSt(xt)in (4) with
gradfBt(xt)whereBtis independent with St. This increases
sampling complexity per iteration and also complicates its
convergence analysis. In addition, they still require a large
initial batch size|S0|=O(ϵ−1)while our|S0|=O(1).
4 Convergence results
In this section, we prove convergence of RSRM. Deﬁne an
increasing sigma-algebra Ft:={S1,...,St−1}. Hence by
update rules in Algorithm 1, xtanddt−1are measurable in
Ft. We ﬁrst present a Lemma that bounds the estimation
error of the recursive momentum estimator.
Lemma 1 (Estimation error bound) .Suppose Assumptions
1 to 4 hold and consider Algorithm 1. Then we can bound
the expected estimation error of the estimator as
E∥dt+1−gradF(xt+1)∥2
≤(1−ρt+1)2(
1 +4η2
t˜L2
|St+1|)
E∥dt−gradF(xt)∥2
+4(1−ρt+1)2η2
t˜L2
|St+1|E∥gradF(xt)∥2+2ρ2
t+1σ2
|St+1|.(6)
The proof of this Lemma can be found in Supplementary
material where it follows an idea similar to the bound in
RSRG/RSPIDER (Han and Gao 2020). The key difference
is that we further use E∥dt∥2≤2E∥dt−gradF(xt)∥2+2E∥gradF(xt)∥2to show dependence on the full gradi-
ent. Based on the claims in (Cutkosky and Orabona 2019),
we consider ρt=O(t−2/3)andηt=O(t−1/3), so that
E∥dt+1−gradF(xt+1)∥2=O(t−2/3+∥gradF(xt)∥2).
To see this, denote st+1=dt+1−gradF(xt+1). Then
by noting that (1−ρt)2≤1−ρt≤1andη2
t≤
ηt≤1, Lemma 1 suggests that E∥st+1∥2≤ O (1−
t−2/3)E∥st∥2+O(t−2/3)E∥gradF(xt)∥2+O(t−4/3). Sim-
ply setting E∥st+1∥2=E∥st∥2yields the result. This im-
plies that E∥gradF(xt)∥2=O(T−2/3), which matches the
optimal rate of convergence. This claim is stated formally in
the following Theorem. For simplicity, we consider |St|=b
for allt.
Theorem 1 (Convergence and complexity of RSRM) .Sup-
pose Assumptions 1 to 4 hold and consider Algorithm 1 with
ηt=cη(t+ 1)−1/3,ρt=cρt−2/3wherecη≤1
Land
cρ= (10˜L2
b+1
3)c2
η. Then we have
E∥gradF(˜x)∥2=1
TT∑
t=1E∥gradF(xt)∥2
≤O(M
T2/3) =˜O(1
T2/3),
whereM:= (6∆ +σ2
2˜L2+σ2ln(T+1)
˜L2)/cη. To achieve
ϵ-approximate solution, we require an SFO complexity of
˜O(ϵ−3).
Proof of Theorem 1 is included in Supplementary ma-
terial. The proof idea is similar to (Cutkosky and Orabona
2019) where we construct a Lyapunov function Rt:=
E[F(xt)] +C
ηt−1E∥dt−gradF(xt)∥2withC=b
12˜L2. The-
orem 1 claims that RSRM achieves a near-optimal complex-
ity of ˜O(ϵ−3)with one-sample batch, i.e. b=O(1). And
speciﬁcally under noiseless case where σ2= 0 , we can
further improve this result to the lower bound complexity
O(ϵ−3). One ﬁnal remark can be made that our step size
decays at a rate of O(t−1/3), which is slower compared to
the SGD-based rate of O(t−1/2). This step size sequence is
crucial for achieving the faster convergence, coupled with
gradually reduced variance.
5 Experiments
In this section, we compare our proposed RSRM with other
one-sample online methods, which are described as follows.
The benchmark is the standard stochastic gradient method
(RSGD) (Bonnabel 2013). We also consider cSGD-M and
cRMSProp (Roy, Mhammedi, and Harandi 2018) where past
gradients are transported by vector transport operator. For
cRMSProp, we do not project and vector-transport its adap-
tation term, which is an element-wise square of stochastic
gradient. Instead, we treat it as an element in the ambient
Euclidean space and therefore we only project the resulting
scaled gradient after applying this term. This modiﬁcation
turns out to yield signiﬁcantly better convergence compared
to its original design. Also we compare with RAMSGRAD
(B´ecigneul and Ganea 2018), which is proposed for a prod-
uct of manifolds. We thus modify the gradient momentum
020406080100
SFO/n10-2100102Optimality gapRSGD (5e-3)
cSGD-M (5e-3)
cRMSProp (1e-2)
RAMSGRAD (5e-1)
RASA-L (5e-3)
RASA-R (5e-3)
RASA-LR (1e-2)
RSRM (5e-3)(a) Optimality gap vs. SFO ( SYN1 )
020406080100
SFO/n10-1100101102103Optimality gapRSGD (1e-3)
cSGD-M (1e-3)
cRMSProp (5e-2)
RAMSGRAD (5e-1)
RASA-L (5e-3)
RASA-R (5e-3)
RASA-LR (1e-2)
RSRM (1e-3) (b) Optimality gap vs. SFO ( SYN2 )
020406080100
SFO/n10-1100101102103Optimality gapRSGD (5e-3)
cSGD-M (5e-3)
cRMSProp (1e-2)
RAMSGRAD (5e-1)
RASA-L (5e-3)
RASA-R (5e-3)
RASA-LR (5e-3)
RSRM (1e-3) (c) Optimality gap vs. SFO ( SYN3 )
0 10 20 30 40
Time (s)10-2100102Optimality gapRSGD
cSGD-M
cRMSProp
RAMSGRAD
RASA-L
RASA-R
RASA-LR
RSRM
(d) Optimality gap vs. Runtime ( SYN1 )
01020304050
SFO/n10-210-1100101102Optimality gapRSGD (5e-3)
cSGD-M (5e-3)
cRMSProp (1e-2)
RAMSGRAD (5e-1)
RASA-L (5e-3)
RASA-R (5e-3)
RASA-LR (1e-2)
RSRM (5e-3) (e) Optimality gap vs. SFO ( MNIST )
0 2 4 6
SFO/n10-310-210-1100101Optimality gapRSGD (1e-1)
cSGD-M (5e-2)
cRMSProp (5e-2)
RAMSGRAD (1e0)
RASA-L (5e-2)
RASA-R (5e-2)
RASA-LR (5e-2)
RSRM (5e-2) (f) Optimality gap vs. SFO ( COVTYPE )
020406080100
SFO/n10-2100102Optimality gapRSGD (5e-3)
RSRM (1e-3)
RSRM (5e-3)
RSRM (1e-2)
RSRM (5e-2)
RSRM (1e-1)
(g) Performance under different η0(SYN1 )
020406080100
SFO/n10-2100102Optimality gapRSGD
RSRM (20)
RSRM (50)
RSRM (100)
RSRM (200)
RSRM (500)(h) Performance under different |S0|(SYN1 )
Figure 1: PCA problems on Grassmann manifold
similar as in (Roy, Mhammedi, and Harandi 2018) while ac-
cumulating square norm of gradient instead of element-wise
square. Hence, it only adapts the step size rather than the gra-
dient. Finally, we consider RASA (Kasai, Jawanpuria, and
Mishra 2019) that adapts column and row subspaces of ma-
trix manifolds. We similarly label its variants as RASA-L,
RASA-R and RASA-LR to respectively represent adapting
row (left) subspace, column (right) subspace and both.
All methods start with the same initialization and are ter-
minated when the maximum iteration number is reached.
For competing methods, we consider a square-root decay-
ing step size ηt=η0/√
t, which is suggested in (Kasai,
Jawanpuria, and Mishra 2019). We set the parameters ofRSRM according to the theory, which is ηt=η0/t1/3
andρt=ρ0/t2/3. A default value of ρ0= 0.1provides
good empirical performance. For all methods, η0are se-
lected from{1,0.5,0.1,...,0.005,0.001}. The gradient mo-
mentum parameter in cSGD-M and RAMSGRAD is set to
be0.999and the adaptation momentum parameter in cRM-
SProp, RAMSGRAD and RASA is set to be 0.9. We choose
a mini-batch size of 5for RSRM and 10for algorithms
excluding RSRM. Hence, the per-iteration cost of gradient
evaluation is identical across methods. The initial batch size
for RSRM is ﬁxed to be 100(except for the problem of ICA
where it is set to be 200). All algorithms are coded in Mat-
lab compatible to ManOpt toolbox (Boumal et al. 2014) and
0 50 100 150
SFO/n100Optimality gapRSGD (1e-1)
cSGD-M (1e-1)
cRMSProp (5e-3)
RAMSGRAD (1e0)
RASA-L (5e-1)
RASA-R (1e0)
RASA-LR (1e0)
RSRM (5e-2)(a) Optimality gap vs. SFO ( YALEB )
01020304050
SFO/n100Optimality gapRSGD (1e-1)
cSGD-M (5e-2)
cRMSProp (1e-2)
RAMSGRAD (1e0)
RASA-L (5e-1)
RASA-R (5e-1)
RASA-LR (1e0)
RSRM (1e-1) (b) Optimality gap vs. SFO ( CIFAR100 )
0 20 40 60
SFO/n10-410-2100102104Optimality gapRSGD (1e-1)
cSGD-M (1e-1)
cRMSProp (1e-3)
RAMSGRAD (1e0)
RASA-L (1e0)
RASA-R (1e0)
RASA-LR (5e-1)
RSRM (1e-1) (c) Optimality gap vs. SFO ( COIL100 )
Figure 2: ICA problems on Stiefel manifold
01020304050
SFO/n10-410-2100102Optimality gapRSGD (5e-2)
cSGD-M (5e-2)
cRMSProp (1e-2)
RAMSGRAD (5e-1)
RASA-L (1e-2)
RASA-R (1e-2)
RASA-LR (5e-3)
RSRM (1e-2)
(a) Optimality gap vs. SFO ( SYN1 )
0 1 2 3 4
SFO/n10-5100Optimality gapRSGD (1e-1)
cSGD-M (1e-1)
cRMSProp (1e-2)
RAMSGRAD (5e-1)
RASA-L (1e-2)
RASA-R (1e-2)
RASA-LR (5e-3)
RSRM (1e-2) (b) Optimality gap vs. SFO ( YABLEB )
0246810
SFO/n10-410-2100102Optimality gapRSGD (5e-2)
cSGD-M (5e-2)
cRMSProp (1e-2)
RAMSGRAD (5e-1)
RASA-L (1e-2)
RASA-R (1e-2)
RASA-LR (5e-3)
RSRM (1e-2) (c) Optimality gap vs. SFO ( KYLBERG )
Figure 3: RC problems on SPD manifold
results are reported on a i5-8600 3.1GHz CPU processor.
We consider principal component analysis (PCA) on
Grassmann manifold, joint diagonalization of independent
component analysis (ICA) on Stiefel manifold and com-
puting Riemannian centroid (RC) on SPD manifold. Stiefel
manifold St (r,d) ={X∈Rd×r:XTX=Ir}is deﬁned as
the set ofd×rcolumn orthonormal matrices, which is a nat-
ural embedded submanifold of Rd×r. Therefore orthogonal
projection is a valid vector transport. Grassmann manifold
G(r,d)is the set ofr-dimensional subspaces in Rd. One rep-
resentation of Grassmann manifold is by a matrix X∈Rd×r
with orthonormal columns that span the subspace. This rep-
resentation is not unique. Indeed, any XR forR∈O(r)
is equivalent to X, whereO(r)is the orthogonal group of
dimensionr. Hence, Grassmann manifold can be viewed as
a quotient of Stiefel manifold, written as St (r,d)/O(r). Fi-
nally, SPD manifold Sd
++is the set ofd×dsymmetric pos-
itive deﬁnite matrices, which forms the interior of a convex
cone embedded in Rn(n+1)/2.
PCA on Grassmann manifold
Consider a set of nsamples, represented by xi∈Rd,i=
1,...,n . PCA aims to ﬁnd a subspace where projection onto
this subspace minimizes reconstruction error. This natu-
rally deﬁnes a problem on Grassmann manifold, writtenasminU∈G(r,d)1
n∑n
i=1∥xi−UUTxi∥2. This can be fur-
ther simpliﬁed into minU∈G(r,d)1
n∑n
i=1xT
iUUTxi. We
ﬁrst test RSRM on a baseline synthetic dataset ( SYN1 ) with
(n,d,r ) = (104,102,10). Then we increase dimension to
d= 500 (SYN2 ) and consider a higher rank case with
r= 20 (SYN3 ). In addition, we also consider two empir-
ical datasets, MNIST (LeCun et al. 1998) with (n,d,r ) =
(60000,784,10)and COVTYPE from LibSVM (Chang and
Lin 2011) with (n,d,r ) = (581012 ,54,10). We measure
the performance in optimality gap, which calculates the dif-
ference between current function value to the minimum, pre-
calculated using Matlab function PCA. Convergence results
and the best-tuned η0are shown in Figure 1. We ﬁnd that
RSRM consistently outperforms others on both synthetic
datasets (Figure 1(a) to (c)) as well as on real datasets (Fig-
ure 1(e) and (f)). It is also observed that on ‘easy’ datasets,
such as SYN1 andSYN2 , adaptive methods perform sim-
ilarly compared with well-tuned SGD. Figure 1(d) further
illustrates the convergence behaviour against runtime where
RSRM still maintains its superiority due to high per-iteration
efﬁciency. Indeed, compared to SGD, RSRM only needs one
extra vector transport operation, which can be as efﬁcient as
orthogonal projection for both Grassmann and Stiefel mani-
fold. Other methods, such as cRMSProp, may further require
large matrix computations for adaptation. In addition, Fig-
ure 1(g) and (h) illustrates how convergence performance of
RSRM varies under different choices of initial step size and
batch size. It is noticed that RSRM with η0= 0.005,0.001
outperforms best-tuned SGD. Also, RSRM seems to be in-
sensitive to the initial batch size and surprisingly, larger
batch size provides no beneﬁt for its convergence under this
problem.
ICA on Stiefel manifold
Independent component analysis (or blind source sepa-
ration) aims to recover underlying components of ob-
served multivariate data by assuming mutual independence
of source signals. Joint diagonalization is a useful pre-
processing step that searches for a pseudo-orthogonal ma-
trix (i.e. Stiefel matrix) (Theis, Cason, and Absil 2009)
by solving minU∈St(r,d)−1
n∑n
i=1∥diag(UTXiU)∥2
Fwith
diag(A)returning diagonal elements of matrix A. The sym-
metric matrices Xi∈Rd×dcan be time-lagged covari-
ance matrices (Belouchrani et al. 1997) or cumulant ma-
trices (Cardoso 1999) constructed from the observed sig-
nals. We consider three image datasets described as follows.
YALEB (Wright et al. 2008) collects n= 2414 face im-
ages taken from various lighting environments. CIFAR100
(Krizhevsky, Hinton, and others 2009) contains n= 60000
images of 100objects and COIL100 (Nene et al. ) is made
up ofn= 7200 images from 100classes. To construct co-
variance representations from these datasets, we ﬁrst down-
size each image to 32×32before applying Gabor-based ker-
nel to extract Gabor features. Then the feature information
is fused in a region covariance descriptors of size 43×43.
We chooser=d= 43 for all problems and the results
are presented in Figure 2. The optimal solution is obtained
by running RSRM for sufﬁciently long. Similarly, we ﬁnd
that RSRM, although showing slow progress at the initial
epochs, quickly converges to a lower value compared with
others. This is mainly attributed to its variance reduction na-
ture.
RC on SPD manifold
Computing Riemannian centroid on SPD manifold Sd
++are
fundamental in many computer vision tasks, including ob-
ject detection (Jayasumana et al. 2015), texture classiﬁca-
tion (Faraki, Harandi, and Porikli 2015) and particularly
medical imaging (Cheng, Salehian, and Vemuri 2012). The
problem concerns ﬁnding a mean representation of a set
of SPD matrices, Xi∈Rd×d. Among the many Rieman-
nian metrics, Afﬁne Invariant Riemannian Metric (AIRM)
is most widely used to measure the closeness between SPD
matrices. This induces a geodesic distance on SPD man-
ifold, given by d2(X1,X2) =∥log(X−1/2
1X2X−1/2
1)∥2
F
where log(·)is the principal matrix logarithm. Riemannian
centroid with respect to this distance is obtained by solv-
ingminC∈Sd
++1
n∑n
i=1d2(C,Xi). We ﬁrst consider a sim-
ulated dataset consisting of n= 5000 SPD matrices in
R10×10, each with a condition number of 20. Then we test
our methods on YALEB face dataset (Wright et al. 2008) and
KYLBERG (Kylberg 2014) dataset that consists of n= 4480texture images of 28classes. For each pixel, we generate a
5-dimensional feature vector ( d= 5), including pixel in-
tensity, ﬁrst- and second-order gradients. Subsequently, the
covariance representation is similarly constructed for each
image. Convergence results are shown in Figure 3 where
the optimal solutions are calculated by Riemannian Barzilai-
Borwein algorithm (Iannazzo and Porcelli 2018). By exam-
ining the ﬁgures, we also verify superiority of RSRM where
it enjoys a more stable convergence due to variance reduc-
tion and sometimes converges to a lower objective value,
as shown in Figure 3(a) and (b). For the two real datasets,
cRMSProp and RASA fails to perform comparably.
6 Conclusion
In this paper, we develop a one-sample online method that
achieves the state-of-the-art lower bound complexity up to
a logarithmic factor. This improves on SGD-based adaptive
methods by a factor of ˜O(ϵ−1). In particular, we use the
stochastic recursive momentum estimator that only requires
O(1)per-iteration gradient computation and achieves vari-
ance reduction without restarting the algorithm with a large
batch gradient. Our experiment ﬁndings conﬁrm the superi-
ority of our proposed algorithm.
References
[Arjevani et al. 2019] Arjevani, Y .; Carmon, Y .; Duchi, J. C.;
Foster, D. J.; Srebro, N.; and Woodworth, B. 2019. Lower
bounds for non-convex stochastic optimization. arXiv
preprint arXiv:1912.02365 .
[B´ecigneul and Ganea 2018] B ´ecigneul, G., and Ganea, O.-
E. 2018. Riemannian adaptive optimization methods. arXiv
preprint arXiv:1810.00760 .
[Belouchrani et al. 1997] Belouchrani, A.; Abed-Meraim,
K.; Cardoso, J.-F.; and Moulines, E. 1997. A blind source
separation technique using second-order statistics. IEEE
Transactions on Signal Processing 45(2):434–444.
[Bonnabel 2013] Bonnabel, S. 2013. Stochastic gradient de-
scent on riemannian manifolds. IEEE Transactions on Au-
tomatic Control 58(9):2217–2229.
[Boumal and Absil 2011] Boumal, N., and Absil, P.-a. 2011.
RTRMC: A Riemannian trust-region method for low-rank
matrix completion. In Advances in neural information pro-
cessing systems , 406–414.
[Boumal et al. 2014] Boumal, N.; Mishra, B.; Absil, P.-A.;
and Sepulchre, R. 2014. Manopt, a Matlab toolbox for op-
timization on manifolds. The Journal of Machine Learning
Research 15(1):1455–1459.
[Cardoso 1999] Cardoso, J.-F. 1999. High-order contrasts
for independent component analysis. Neural Computation
11(1):157–192.
[Chang and Lin 2011] Chang, C.-C., and Lin, C.-J. 2011.
LIBSVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology 2(3):1–
27.
[Cheng, Salehian, and Vemuri 2012] Cheng, G.; Salehian,
H.; and Vemuri, B. C. 2012. Efﬁcient recursive algorithms
for computing the mean diffusion tensor and applications to
dti segmentation. In European Conference on Computer Vi-
sion, 390–401. Springer.
[Cutkosky and Orabona 2019] Cutkosky, A., and Orabona,
F. 2019. Momentum-based variance reduction in non-
convex sgd. In Advances in Neural Information Processing
Systems , 15236–15245.
[Duchi, Hazan, and Singer 2011] Duchi, J.; Hazan, E.; and
Singer, Y . 2011. Adaptive subgradient methods for online
learning and stochastic optimization. Journal of machine
learning research 12(7).
[Faraki, Harandi, and Porikli 2015] Faraki, M.; Harandi,
M. T.; and Porikli, F. 2015. Material classiﬁcation on
symmetric positive deﬁnite manifolds. In IEEE Winter
Conference on Applications of Computer Vision , 749–756.
IEEE.
[Farid and Adelson 1999] Farid, H., and Adelson, E. H.
1999. Separating reﬂections from images by use of inde-
pendent component analysis. JOSA A 16(9):2136–2145.
[Ghadimi and Lan 2013] Ghadimi, S., and Lan, G. 2013.
Stochastic ﬁrst-and zeroth-order methods for nonconvex
stochastic programming. SIAM Journal on Optimization
23(4):2341–2368.
[Han and Gao 2020] Han, A., and Gao, J. 2020. Variance re-
duction for riemannian non-convex optimization with batch
size adaptation. arXiv preprint arXiv:2007.01494 .
[Hosseini and Sra 2017] Hosseini, R., and Sra, S. 2017.
An alternative to em for Gaussian mixture models: Batch
and stochastic Riemannian optimization. arXiv preprint
arXiv:1706.03267 .
[Iannazzo and Porcelli 2018] Iannazzo, B., and Porcelli, M.
2018. The Riemannian Barzilai–Borwein method with non-
monotone line search and the matrix geometric mean com-
putation. IMA Journal of Numerical Analysis 38(1):495–
517.
[Jayasumana et al. 2015] Jayasumana, S.; Hartley, R.; Salz-
mann, M.; Li, H.; and Harandi, M. 2015. Kernel methods
on Riemannian manifolds with Gaussian RBF kernels. IEEE
Transactions on Pattern Analysis and Machine Intelligence
37(12):2464–2477.
[Kasai, Jawanpuria, and Mishra 2019] Kasai, H.; Jawan-
puria, P.; and Mishra, B. 2019. Riemannian adaptive
stochastic gradient algorithms on matrix manifolds. arXiv
preprint arXiv:1902.01144 .
[Kasai, Sato, and Mishra 2018] Kasai, H.; Sato, H.; and
Mishra, B. 2018. Riemannian stochastic recursive gradient
algorithm. In International Conference on Machine Learn-
ing, 2516–2524.
[Kingma and Ba 2014] Kingma, D. P., and Ba, J. 2014.
Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
[Krizhevsky, Hinton, and others 2009] Krizhevsky, A.; Hin-
ton, G.; et al. 2009. Learning multiple layers of features
from tiny images.
[Kylberg 2014] Kylberg, G. 2014. The kylberg texture
dataset v. 1.0. external report (blue series) 35, centre forimage analysis, swedish university of agricultural sciences
and uppsala university, uppsala, sweden (2011). URL
http://www. cb. uu. se/gustaf/texture .
[LeCun et al. 1998] LeCun, Y .; Bottou, L.; Bengio, Y .; and
Haffner, P. 1998. Gradient-based learning applied to doc-
ument recognition. Proceedings of the IEEE 86(11):2278–
2324.
[Li, Fuxin, and Todorovic 2020] Li, J.; Fuxin, L.; and Todor-
ovic, S. 2020. Efﬁcient Riemannian optimization on the
Stiefel manifold via the Cayley transform. arXiv preprint
arXiv:2002.01113 .
[Nene et al. ] Nene, S. A.; Nayar, S. K.; Murase, H.; et al.
Columbia object image library (coil-100).
[Qian 1999] Qian, N. 1999. On the momentum term in
gradient descent learning algorithms. Neural networks
12(1):145–151.
[Robbins and Monro 1951] Robbins, H., and Monro, S.
1951. A stochastic approximation method. The Annals of
Mathematical Statistics 400–407.
[Roy, Mhammedi, and Harandi 2018] Roy, S. K.;
Mhammedi, Z.; and Harandi, M. 2018. Geometry
aware constrained optimization techniques for deep learn-
ing. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , 4460–4469.
[Sato, Kasai, and Mishra 2019] Sato, H.; Kasai, H.; and
Mishra, B. 2019. Riemannian stochastic variance re-
duced gradient algorithm with retraction and vector trans-
port. SIAM Journal on Optimization 29(2):1444–1472.
[Staib et al. 2019] Staib, M.; Reddi, S. J.; Kale, S.; Kumar,
S.; and Sra, S. 2019. Escaping saddle points with adaptive
gradient methods. arXiv preprint arXiv:1901.09149 .
[Theis, Cason, and Absil 2009] Theis, F. J.; Cason, T. P.; and
Absil, P.-A. 2009. Soft dimension reduction for ica by joint
diagonalization on the stiefel manifold. In International
Conference on Independent Component Analysis and Signal
Separation , 354–361. Springer.
[Tieleman and Hinton 2012] Tieleman, T., and Hinton, G.
2012. Lecture 6.5-rmsprop: Divide the gradient by a run-
ning average of its recent magnitude. COURSERA: Neural
networks for machine learning 4(2):26–31.
[Tran-Dinh et al. 2019] Tran-Dinh, Q.; Pham, N. H.; Phan,
D. T.; and Nguyen, L. M. 2019. Hybrid stochastic gradient
descent algorithms for stochastic nonconvex optimization.
arXiv preprint arXiv:1905.05920 .
[Wright et al. 2008] Wright, J.; Yang, A. Y .; Ganesh, A.; Sas-
try, S. S.; and Ma, Y . 2008. Robust face recognition via
sparse representation. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence 31(2):210–227.
[Zhang, Reddi, and Sra 2016] Zhang, H.; Reddi, S. J.; and
Sra, S. 2016. Riemannian SVRG: Fast stochastic optimiza-
tion on Riemannian manifolds. In Advances in Neural In-
formation Processing Systems , 4592–4600.
[Zhang, Zhang, and Sra 2018] Zhang, J.; Zhang, H.; and Sra,
S. 2018. R-spider: A fast riemannian stochastic optimization
algorithm with curvature independent rate. arXiv preprint
arXiv:1811.04194 .
[Zhou et al. 2019] Zhou, P.; Yuan, X.; Yan, S.; and Feng, J.
2019. Faster ﬁrst-order methods for stochastic non-convex
optimization on Riemannian manifolds. IEEE Transactions
on Pattern Analysis and Machine Intelligence .
Supplementary Material
A Proof of Lemma 1
Proof. By deﬁnition ofFt, we have E∥dt−gradF(xt)∥2=E[E[∥dt−gradF(xt)∥2|Ft]]. Then
E[∥dt−gradF(xt)∥2|Ft]
=E[∥(1−ρt)Txt
xt−1(dt−1−gradfSt(xt−1)) + gradfSt(xt)−gradF(xt)∥2|Ft]
=E[∥(1−ρt)(Txt
xt−1dt−1+gradfSt(xt)−Txt
xt−1gradfSt(xt−1)) +ρtgradfSt(xt)−gradF(xt)∥2|Ft]
=E[∥(1−ρt)Txt
xt−1(dt−1−gradF(xt−1)) +ρt(gradfSt(xt)−gradF(xt))
+ (1−ρt)(gradfSt(xt)−Txt
xt−1gradfSt(xt−1) +Txt
xt−1gradF(xt−1)−gradF(xt))∥2|Ft]
= (1−ρt)2∥dt−1−gradF(xt−1)∥2+E[∥ρt(gradfSt(xt)−gradF(xt))
+ (1−ρt)(gradfSt(xt)−Txt
xt−1gradfSt(xt−1) +Txt
xt−1gradF(xt−1)−gradF(xt))∥2|Ft]
≤(1−ρt)2∥dt−1−gradF(xt−1)∥2+ 2ρ2
tE[∥gradfSt(xt)−gradF(xt)∥2|Ft]
+ 2(1−ρt)2E[∥gradfSt(xt)−Txt
xt−1gradfSt(xt−1) +Txt
xt−1gradF(xt−1)−gradF(xt)∥2|Ft]
≤(1−ρt)2∥dt−1−gradF(xt−1)∥2+ 2ρ2
tE[∥gradfSt(xt)−gradF(xt)∥2|Ft]
+ 2(1−ρt)2E[∥gradfSt(xt)−Txt
xt−1gradfSt(xt−1)∥2|Ft]
= (1−ρt)2∥dt−1−gradF(xt−1)∥2+2ρ2
t
|St|Eω∥gradf(xt,ω)−gradF(xt)∥2+2(1−ρt)2
|St|Eω∥gradf(xt,ω)
−Txt
xt−1gradf(xt−1,ω)∥2
≤(1−ρt)2∥dt−1−gradF(xt−1)∥2+2ρ2
tσ2
|St|+2(1−ρt)2η2
t−1˜L2
|St|∥dt−1∥2
≤(1−ρt)2(
1 +4η2
t−1˜L2
|St|)
∥dt−1−gradF(xt−1)∥2+4(1−ρt)2η2
t−1˜L2
|St|∥gradF(xt−1)∥2+2ρ2
tσ2
|St|. (7)
where the last equality uses isometry property of vector transport Txtxt−1and the fact that it is measurable in Ft. Also, we use
the unbiasedness of stochastic gradient grad fSt(xt). The ﬁrst and last inequalities follow from ∥a+b∥2≤2∥a∥2+ 2∥b∥2. The
second inequality applies E∥x−E[x]∥2≤E∥x∥2for random variable x. The second last inequality follows from Assumptions
2 and 4. By taking full expectation, we obtain the desired result.
B Proof of Theorem 1
Proof. By retraction Lsmoothness of F, we have
F(xt+1)≤F(xt)−⟨gradF(xt),ηtdt⟩+η2
tL
2∥dt∥2
=F(xt)−ηt
2∥gradF(xt)∥2−ηt
2∥dt∥2+ηt
2∥dt−gradF(xt)∥2+Lη2
t
2∥dt∥2
=F(xt)−ηt
2∥gradF(xt)∥2+ηt
2∥dt−gradF(xt)∥2−(ηt
2−Lη2
t
2)∥dt∥2
≤F(xt)−ηt
2∥gradF(xt)∥2+ηt
2∥dt−gradF(xt)∥2, (8)
where for the last inequality we choose ηt≤1
L.Givenηt=cη(t+ 1)−1/3, we can ensure this condition by requiring cη≤1
L.
Given the choice that ηt=cη(t+ 1)−1/3,ρt=cρt−2/3,|St|=bfor allt, we construct a Lyapunov function
Rt:=E[F(xt)] +C
ηt−1E∥dt−gradF(xt)∥2, (9)
for some constant C > 0. Denote estimation error as st=dt−gradF(xt). Then we can bound the scaled difference between
two consecutive estimation error as
E∥st+1∥2
ηt−E∥st∥2
ηt−1≤(1−ρt+1)2(
1 +4η2
t˜L2
b)
E∥st∥2+4(1−ρt+1)2η2
t˜L2
bE∥gradF(xt)∥2+2ρ2
t+1σ2
b
ηt−E∥st∥2
ηt−1
=(1
ηt(1−ρt+1)2(1 +4η2
t˜L2
b)−1
ηt−1)
E∥st∥2+4(1−ρt+1)2ηt˜L2
bE∥gradF(xt)∥2+2ρ2
t+1σ2
bηt
≤(1
ηt−1
ηt−1+4ηt˜L2
b−ρt+1
ηt)
E∥st∥2+4ηt˜L2
bE∥gradF(xt)∥2+2σ2
b(t+ 1), (10)
where the second inequality uses the fact that (1−ρt+1)2≤1−ρt+1≤1. Next we bound the ﬁrst and third terms as follows.
The third term∑T
t=12σ2
b(t+1)≤2σ2ln(T+1)
bfrom the bound on Harmonic series. Now we bound the ﬁrst term. Consider the
convex function h(x) :=x1/3. By ﬁrst order characterization, h(x+ 1)≤h(x) +h′(x) =x1/3+1
3x−2/3. Therefore, we have
1/ηt−1/ηt−1≤cη
3t−2/3≤cη
3(t+ 1)−1/3=ηt
3, where we can easily verify that t−2/3≤(t+ 1)−1/3fort≥2. Also given
thatρt+1
ηt= (10˜L2
b+1
3)cη(t+ 1)−1/3= (10˜L2
b+1
3)ηtby the choice that cρ/cη= (10˜L2
b+1
3)cη. Therefore,
1
ηt−1
ηt−1+4ηt˜L2
b−ρt+1
ηt≤ηt
3+4ηt˜L2
b−(10˜L2
b+1
3)ηt=−6˜L2ηt
b. (11)
Then substituting this results in (10) gives
T∑
t=1(E∥st+1∥2
ηt−E∥st∥2
ηt−1)
≤−6˜L2
bT∑
t=1ηtE∥st∥2+4˜L2
bT∑
t=1ηtE∥gradF(xt)∥2+2σ2ln(T+ 1)
b. (12)
Now choose C=b
12˜L2. Then we have
Rt+1−Rt=E[F(xt+1)−F(xt)] +E[b
12˜L2ηtE∥st+1∥2−b
12˜L2ηt−1E∥st∥2]
≤−ηt
2E∥gradF(xt)∥2+ηt
2E∥st∥2+E[b
12˜L2ηtE∥st+1∥2−b
12˜L2ηt−1E∥st∥2]. (13)
Telescoping this result from t= 1,...,T gives
RT+1−R1≤−T∑
t=1ηt
2E∥gradF(xt)∥2+T∑
t=1ηt
2E∥st∥2−T∑
t=1ηt
2E∥st∥2+T∑
t=1ηt
3E∥gradF(xt)∥2+σ2ln(T+ 1)
6˜L2
=−T∑
t=1ηt
6E∥gradF(xt)∥2+σ2ln(T+ 1)
6˜L2. (14)
Givenηtis a decreasing sequence, we have∑T
t=1ηt
6E∥gradF(xt)∥2≥ηT
6∑T
t=1E∥gradF(xt)∥2. Therefore, we obtain
1
TT∑
t=1E∥gradF(xt)∥2≤6(R1−RT+1) +σ2ln(T+1)
˜L2
ηTT≤6F(x1)−6E[F(xT+1)] +b
2˜L2E∥d1−gradF(xt)∥2+σ2ln(T+1)
˜L2
cη(T+ 1)−1/3T
≤6∆ +σ2
2˜L2+σ2ln(T+1)
˜L2
cηT(T+ 1)1/3
≤6∆ +σ2
2˜L2+σ2ln(T+1)
˜L2
cηT(T1/3+ 1) =M
T2/3+M
T=˜O(1
T2/3),
(15)
where we use the fact that (a+b)1/3≤a1/3+b1/3andM:= (6∆ +σ2
2˜L2+σ2ln(T+1)
˜L2)/cη. Hence to achieve ϵ-approximate
solution, we require E∥gradF(˜x)∥2=1
T∑T
t=1E∥gradF(xt)∥2≤ϵ2. Hence requiringM
T2/3≤ϵ2is sufﬁcient for this purpose,
which gives ˜O(ϵ−3)
C Additional experiment results
Optimality gap against runtime"
2401.09198,D:\Database\arxiv\papers\2401.09198.pdf,"How does the proposed method address the challenge of predicting trajectories from sparsely measured physical variables, and what theoretical results support its effectiveness?","The method tackles this challenge by introducing a latent state-space where the dynamics are Markovian, allowing for more accurate predictions. Theoretical results demonstrate that using a latent space for forecasting sparse observations leads to smaller upper bounds on the prediction error compared to traditional auto-regressive approaches.","Published as a conference paper at ICLR 2024
…
Figure 1: Model overview – We achieve space and time continuous simulations of physics systems
by formulating the task as a double observation problem. System 1 is a discrete dynamical model
used to compute a sequence of latent anchor states zdauto-regressively, and System 2 is used to
design a state estimator ψqretrieving the dense physical state at arbitrary locations (x, t).
Note that this task involves generalization to new ICs, as well as estimation to unseen spatial loca-
tions within Ωand unseen time instants within J0, TK. We do not explicitly require extrapolation to
instants t > T , although it comes as a side benefit of our approach up to some extent.
3.1 T HE DOUBLE OBSERVATION PROBLEM
The task implies extracting regularities from weakly informative physical variables that are sparsely
measured in space and time, since XandTcontain very few elements. Consequently, the possi-
bility to forecast their trajectories from off-the-shelf auto-regressive methods is very unlikely (as
confirmed experimentally). To tackle this challenge, we propose an approach accounting for the fact
that the phenomenon is not directly observable from the sparse trajectories, but can be deduced from
a richer latent state-space in which the dynamics is markovian. We introduce two linked dynami-
cal models lifting sparse observations to dense trajectories guided by observability considerations,
namely
System 1 :
zd[n+1] = f1 
zd[n]
sd[n] = h1 
zd[n],System 2 :˙s(x, t) = f2 
s,x, t
z(x, t) = h2 
s,x, t∀(x, t)∈Ω×J0, TK
(2)
where for all n∈N, we note sd[n] =s(X, n∆)the sparse observation at some instant n∆(the
sampling rate ∆is not necessarily equal to the sampling rate ∆∗used for data acquisition, which we
will exploit during training to improve generalization. This will be detailed later).
System 1 – is a discrete-time dynamical system where the available measurements sd[n]are con-
sidered as partial observations of a latent state variable zd[n]. We aim to derive an output predictor
from System 1 to forecast trajectories of sparse observations auto-regressively from the sparse IC.
As mentioned earlier, sparse observations are unlikely to be sufficient to perform predictions, hence
we introduce a richer latent state variable zdin which the dynamics is truly markovian, and obser-
vations sd[n]are seen as measurements of the state zdusing the function h1.
System 2 – is a continuous-time dynamical system describing the evolution of the to-be-predicted
dense trajectory S(s0,x, t). It introduces continuous observations z(x, t)such that z(X, n∆) =
zd[n]. The insight is that the state representation zd[n]obtained from System 1 is designed to contain
sufficient information to predict sd[n], but not necessarily to predict the dense state. Formally, zd
represents solely the observable part of the state, in the sense of control theory.
At inference time, we forecast at query location (x, t)with a 2-step algorithm: (Step-1) System
1 is used as an output predictor from the sparse IC sd[0], and computes a sequence z[0],z[1],...,
which we refer to as “ anchor states ”. This sequence allows the dynamics to be Markovian, provides
sufficient information for the second state estimation step and holds information to predict the sparse
observations, allowing supervision during training. (Step-2) We derive a state observer from System
2 leveraging the anchor states over the whole time domain to estimate the dense solution at an
arbitrary location in space and time (see figure 1). Importantly, for a given IC, the anchor states are
computed only once and reused within System 2 to estimate the solution at different points.
4
Published as a conference paper at ICLR 2024
3.2 T HEORETICAL ANALYSIS
In this section, we introduce theoretical results supporting the use of Systems 1 and 2. In particular,
we show that using System 1 to forecast the sparse observations in latent space zdrather than directly
operating in the physical space leads to smaller upper bounds on the prediction error. Then, we show
the existence of a state estimator from System 2 and compute an upper bound on the estimation error
depending on the length of the sequence of anchor states.
Step 1 – consists of computing the sequence of anchor states guided by an output prediction task of
the sparse observations. As classically done, we introduce an encoder (formally, a state observer)
e 
sd[0]
=zd[0]coupled to System 1 to project the sparse IC into a latent space zd. Following
System 1, we compute the anchor states zdauto-regressively (with f1) in the latent space. The
sparse observations are extracted from zdusing h1. In comparison, existing baselines (Pfaff et al.,
2020; Sanchez-Gonzalez et al., 2020; Stachenfeld et al., 2021) maintain the state in the physical
space and discard the intermediate latent representation between iterations. Formally, let us consider
approximations ˆf1,ˆh1,ˆe(in practice realized as deep networks trained from data D) off1, h1ande
and compare the prediction algorithm for the classic auto-regressive (AR) approach and ours
Classic AR: ˆsar
d[n] := ( ˆh1◦ˆf1◦ˆe)n 
sd[0]
Ours: ˆsd[n] :=ˆh1◦ˆf1n◦ˆe 
sd[0]
(3)
Classical AR approaches re-project the latent state into the physical space at each step and repeat
“encode-process-decode”. Our method encodes the sparse IC, advances the system in the latent
space, and decodes toward the physical space at the end. A similar approach has also been explored
in Wu et al. (2022); Kochkov et al. (2020), albeit in different contexts, without theoretical analysis.
Proposition 1 Consider a dynamical system of the form of System 1 and assume the existence of a
state observer ealong with approximations ˆf1,ˆh1,ˆewith Lipschitz constants Lf, LhandLerespec-
tively such that LhLfLe̸= 1. If there exist δf, δh, δe∈R+such that ∀(z,s)∈Rnz×Rns
|f1(z)−ˆf1(z)|⩽δf,|h1(z)−ˆh1(z)|⩽δh,|e(s)−ˆe(s)|⩽δe (4)
for the Euclidean norm | · |, then for all integer n >0, with ˆsd[n]andˆsar
d[n]as in equation 3,
|sd[n]−ˆsd[n]|⩽δh+Lh
δfLn
f−1
Lf−1+Ln
fδe
(5)
|sd[n]−ˆsar
d[n]|⩽δLn−1
L−1(6)
withδ=δh+Lhδf+LhLfδeandL=LhLfLe.
Proof : See appendix B.
This result shows that falling back to the physical space at each time step degrades the upper bound of
the prediction error. Indeed, if L <1, the upper bound converges trivially to zero when nincreases,
and hence can be ignored. Otherwise, the upper bound for the classic AR scheme appears to be more
sensitive to approximation errors δh, δfandδecompared to our approach (for a formal comparison,
see appendix C). Intuitively it means that information is lost in the observation space, which thus
needs to be re-estimated at each iteration when using the classic AR scheme. By maintaining a
state variable in the latent space, we allow this information to flow readily between each step of the
simulator (see blue frame in figure 1).
Step 2 – The state estimator builds upon System 2 and relies on the set of anchor states from the
previous step to estimate the dense physical state at arbitrary locations in space and time. Formally,
we look for a function ψqleveraging the sequence of anchor states zd[0],···zd[q](simulated from
the sparse IC sd[0]) to retrieve the dense solution3. In what follows, we show that (1) such a function
ψqexists and (2) we compute an upper bound on the estimation error depending on the length of the
sequence. To do so, consider the functional which outputs the anchor states from any IC s0∈ S
Op(s0)=h
h2 
s0(X)
h2 
S(s0,X,∆)
···h2 
S(s0,X, p∆)i
=h
zd[0]···zd[p]i
(7)
In practice, the ground truths zd[n]are not perfectly known, as they are obtained from a data-driven
output predictor (step 1) using the sparse IC. Inspired from Janny et al. (2022b), we state:
3Since the simulation is conducted up to T, and considering the time step ∆, in practice q⩽⌊T
∆⌋
5
Published as a conference paper at ICLR 2024
Proposition 2 Consider a dynamical system defined by System 2 and equation 7. Assume that
A1.f2is Lipschitz with constant Ls,
A2.there exists p >0and a strictly increasing function αsuch that ∀sa,sb∈ S2and∀q⩾pOq(sa)− Oq(sb)⩾α(q)|sa−sb|S (8)
where·
Sis an appropriate norm for S.
Then,∀q⩾p, there exists ψqsuch that, for (x, t)∈Ω×J0, TKandδnsuch that ˆzd[n] =zd[n]+δn,
for all n⩽q,
ψq 
zd[0],···,zd[q],x, t
=S(s0,x, t) (9)S(s0,x, t)−ψq ˆzd[0],···,zd[q],x, t
S⩽2α(q)−1δ0|qeLst. (10)
where δ0|q=
δ0···δq
.
Proof: See appendix D. Assumption A2. states that the longer we observe two trajectories from
different ICs, the easier it will be to distinguish them, ruling out systems collapsing to the same
state. Such systems are uncommon since forecasting their trajectory becomes trivial after some time.
This assumption is related to finite-horizon observability in control theory, a property of dynamical
systems guaranteeing that the (markovian) state can be retrieved given a finite number pof past
observations. Equation 8 is associated with injectivity of Oq, hence the existence of a left inverse
mapping the sequence of anchor states to the IC s0.
Proposition 2 highlights a trade-off on the performance of ψq. On one hand, longer sequences of
anchor states are harder to predict, leading to a larger |δ0|q|, which impacts the state estimator ψq
negatively. On the other hand, longer sequences hold more information that can still be leveraged
byψqto improve its estimation, represented by α(q)−1in equation 10. In contrast to competing
baselines or conventional interpolation algorithms, our approach takes this trade-off into account,
by explicitly leveraging the sequence to estimate the dense solution, as will be discussed below.
Discussion and related work – the competing baselines can be analyzed using our setup, yet in a
weaker configuration. For instance, one can see Step 2 as an interpolation process, and replace it
with a conventional interpolation algorithm, which typically relies on spatial neighbors only. Our
method not only exploits spatial neighborhoods but also leverages temporal data, improving the
performance, as shown in proposition 2 and empirically corroborated in Section 4.
MAgNet (Boussif et al., 2022) uses a reversed interpolate-forecast scheme compared to ours. The IC
sd[0]is interpolated right from the start to estimate s0(corresponding to our Step 2, with q=1), and
then simulated with an auto-regressive model in the physical space (with the classic AR scheme).
Propositions 1 and 2 show that the upper bounds on the estimation and prediction error are higher
than ours. Moreover, if the number of query points exceeds the number of known points ( |Ω|≫|X| ),
the input of the auto-regressive solver is filled with noisy interpolations, which impacts performance.
DINo (Yin et al., 2022) is a very different approach leveraging a spatial implicit neural representation
modulated by a context vector, whose dynamics is modeled via a learned ODE. This approach is
radically different than ours and arguably involves stronger hypotheses, such as the existence of
a learnable ODE modeling the dynamics of a suitable weight modulation vector. In contrast, our
method relies on arguably more sound assumptions, i.e. the existence of an observable discrete
dynamics explaining the sparse observation, and the finite-time observability of System 2.
3.3 I MPLEMENTATION
The implementation follows the algorithm described in the previous section: (Step-1) rolls out pre-
dictions of anchor states from the IC, (Step-2) estimates the state at the query position from these
anchor states. The encoder ˆefrom Step 1 is a multi-layer perceptron (MLP) which takes as in-
put the sparse IC sd[0]and the positions Xand outputs a latent state variable zd[0]structured
as a graph, with edges computed with a Delaunay triangulation. Hence, each anchor is a graph
zd[n] ={zd[n]i}, but we will omit index iover graph nodes in what follows if not required for
understanding.
We model ˆf1as a multi-layer Graph Neural Network (GNN) (Battaglia et al., 2016). The anchor
states zd[n]are defined at fixed time steps n∆, which might not match ∆∗used in the data T. We
6
Published as a conference paper at ICLR 2024
found it beneficial to choose ∆=k×∆∗withk>1∈Nsuch that the model can be queried during
training on time points t∈ T that do not match exactly with every time-steps in zd[0],zd[1], ...,
but rather on a subset of them, hence encouraging generalization to unseen time. The observation
function ˆh1is an MLP applied on the vector at node level in the graph zd.
The state estimator ψqis decomposed into a Transformer model (Vaswani et al., 2017) coupled to a
recurrent neural network to provide an estimate at query spatio-temporal query position (x, t). First,
through cross-attention we translate the set of anchor states zd[n](one embedding per graph node i
and per instant n) into a set of estimates of the continuous variable z(x, t)conditioned at the instant
n∆, which we denote zn∆(x, t)(one embedding per instant n). Following advances in geometric
mappings in computer vision (Saha et al., 2022), we use multi-head cross-attention to query from
coordinates (x, t)toKeys corresponding to the nodes iin each graph anchor state zd[n],∀n:
zn∆(x, t) =fmha 
Q=ζω(x, t),K=V={zd[n]i}+ζω(X, n∆)
,// attention over nodes i (11)
where Q, K, V are, respectively, Query ,Key andValue inputs to the cross-attention layer fmha
(Vaswani et al., 2017) and ζωa Fourier positional encoding with a learned frequency parameter
ω. Finally, we leverage a state observer to estimate the dense solution at the query point from the
sequence of conditioned anchor variables, over time. This is achieved with a Gated Recurrent Unit
(GRU) Cho et al. (2014) maintaining a hidden state u[n],
u[n] =rgru 
u[n−1],zn∆(x, t)
,ˆS(s0,x, t) =D(u[q]), (12)
which shares similarities with conventional state-observer designs in control theory (Bernard et al.,
2022). Finally, an MLP Dmaps the final GRU hidden state to the desired output, that is, the value
of the solution at the desired spatio-temporal coordinate (x, t). See appendix E for details.
3.4 T RAINING
Generalization to new input locations during training is promoted by creating artificial generalization
situations using sub-sampling techniques of the sparse sets XandT.
Artificial generalization – The anchor states zd[n]are computed at time rate ∆larger than the
available rate ∆∗. This creates situations during training where the state estimator ψqdoes not
have access to a latent state perfectly matching with the queried time. We propose a similar trick
to promote spatial generalization. At each iteration, we sub-sample the (already sparse) IC sd[0]
randomly to obtain ˜sd[0]defined on a subset of X. We then compute the anchor states ˜zdusing
System 1. On the other hand, the query points are selected in the larger set X. Consequently,
System 2 is exposed to positions that do not always match with the ones in zd[n]. Note that the
complete domain of definition Ω×J0, TKremains unseen during training.
Training objective – To reduce training time, we randomly sample Mquery points (xm, τm)in
X × T at each iteration, with a probability proportional to the previous error of the model at this
point since its last selection (see appendix E) and we minimize the loss
L=KX
k=1Lcontinuousz }| {
MX
m=1S(sk
0,xm, τm)−ψq ˜zd[0|q],x, τm2
+Ldynamicsz }| {
⌊T/∆⌋X
n=0˜sd[n]−ˆh1 ˜zd[n]2
, (13)
with ˜zd[n] =ˆf1n◦ˆe ˜sd[0]
.Lcontinuous supervises the model end-to-end, and Ldynamics trains the
latent anchor states zdto predict the sparse observations from the IC.
4 E XPERIMENTAL RESULTS
Experimental setup –X×T results from sub-sampling Ω×J0, TKwith different rates to control
the difficulty of the task. We evaluate on three highly challenging datasets (details in appendix F):
Navier (Yin et al., 2022; Stokes, 2009) simulates the vorticity of a viscous, incompressible flow
driven by a sinusoidal force acting on a square domain with periodic boundary conditions. Shallow
Water (Yin et al., 2022; Galewsky et al., 2004) studies the velocity of shallow waters evolving on
the tangent surface of a 3D sphere. Eagle (Janny et al., 2023) is a challenging dataset of turbulent
airflow generated by a moving drone in a 2D environment with many different scene geometries.
We evaluate our model against three baselines representing the state-of-the-art in continuous simu-
lations. Interpolated MeshGraphNet (MGN) (Pfaff et al., 2020) is a standard multi-layered GNN
7
Published as a conference paper at ICLR 2024
Navier Shallow Water Eagle
High Mid Low High Mid Low High Low
In-X 1.557 1.130 1.878 0.1750 0.1814 0.2733 287.3 302.7 DINo
(Yin et al., 2022) Ext-X 1.600 1.253 5.493 4.638 13.40 21.55 381.7 489.6
In-X 1.913 0.9969 0.6012 0.3663 0.2835 0.7309 64.44 83.58 Interp. MGN
(Pfaff et al., 2020) Ext-X 2.694 4.784 14.80 1.744 4.221 8.187 173.4 241.5
In-X n/a n/a n/aTime Oracle (n.c)Ext-X 0.851 4.204 15.63 1.617 4.327 8.522 147.0 221.2
In-X 18.17 6.047 8.679 0.3196 0.3358 0.4292 99.79 124.5 MAgNet
(Boussif et al., 2022) Ext-X 35.73 26.24 57.21 10.21 23.20 30.55 194.3 260.7
In-X 0.1989 0.2136 0.2446 0.2940 0.3139 0.2700 70.02 78.83OursExt-X 0.2029 0.2463 0.5601 0.4493 1.051 2.800 90.88 117.2
Table 1: Space Continuity – we evaluate the spatial interpolation power of our method vs. the
baselines and standard interpolation techniques. We vary the number of available measurement
points in the data for training from High (25% of simulation grid), Middle (10%), and Low (5%)
amount of points and show that our model outperforms the baselines. Evaluation is conducted over
20 frames in the future (10 for Eagle ) and we report the MSE to the ground truth solution ( ×10−3).
used auto-regressively and extended to spatiotemporal continuity using physics-agnostic interpola-
tion. MAgNet (Boussif et al., 2022) interpolates the IC at the query position in latent space before
using MGN. The original implementation assumes knowledge of the target graph during training,
including new queries. When used for superresolution, the authors kept the ratio between the amount
of new query points and available points constant. Hence, while MAgNet is queried at unseen loca-
tions, it also benefits from more information. In our setup, the model is exposed to a fixed number of
points but does not receive more samples during evaluation. This makes our problem more challeng-
ing than the one addressed in Boussif et al. (2022). DINo (Yin et al., 2022) models the solution as
an Implicit Neural Representation (INR) s(x, αt)where the spatial coordinates xare fed to a MFN
(Fathony et al., 2021) and αtis a context vector modulating the weights of the INR. The dynamics
ofαis modeled with a Neural-ODE, where the dynamics is an MLP. We share common objectives
with DINo and take inspiration from their evaluation tasks yet in a more challenging setup. Details
of the baselines are in appendix F. We highlight a caveat on MAgNet: the model can handle a lim-
ited amount of new queries, roughly equal to the number of observed points. Our task requires the
solution at up to 20 times more queries than available points. In this situation, the graph in MaGNet
is dominated by noisy states from interpolation, and the auto-regressive forecaster performs poorly.
During evaluation, we found it beneficial to split the queries into chunks of 10nodes and to apply
the model several times. This strongly improves the performance at the cost of an increased runtime.
Space Continuity – Table 1 compares the spatial interpolation power of our method versus several
baselines. The MSE values computed on the training domain (In- X=X) and outside (Ext- X=Ω\X)
show that our method offers the best performance, especially for the Ext-domain task, which is our
aim. To ablate dynamics and evaluate the impact of trained interpolations, we also report the pre-
dictions of a Time Oracle which uses sparse ground truth values at all time steps and interpolates
(bicubic) spatially. This allows us to assess whether the method is doing better than a simple ax-
iomatic interpolation. While MGN offers competitive in-domain predictions, the cubic interpolation
fails to extrapolate reliably on unseen points. This can be seen in the In/Ext gap for Interpolated
MGN which is very close to the Time Oracle error. MaGNet, which builds on a similar framework,
is hindered by the larger amount of unobserved data in the input mesh. At test time, the same num-
ber of initial condition points are provided but the method interpolates substantially more points.
DINo achieves a very low In/Ext gap, yet fails on highly (5%) down-sampled tasks. One of the
key difference with DINo is that the dynamics relies on an internal ODE for the temporal evolution
of a modulation vector. In contrast, our model uses an explicit auto-regressive backbone, and time
forecasting is handled in an arguably more meaningful space, which we conjecture to be the reason
why we achieve better results (see fig. 5 in the appendix).
Time Continuity – is a step forward in difficulty, as the model needs to interpolate not only to
unseen spatial locations (datasets are undersampled at 25%) but also on intermediate timesteps (Ext-
T, Table 2). All models perform well on Shallow Water , which is relatively easy. Both DINo and
MAgNet leverage a discrete integration scheme (Euler for MAgNet and RK4 for DINo) allowing
querying the model between timesteps seen at training. These schemes struggle to capture the data
dependencies effectively and therefore the methods fail on Navier (see also Figure 6 for qualitative
8
Published as a conference paper at ICLR 2024
Ground Truth Ours DINo MeshGraphNet MAgNetˆs−s2
2t0+2 t0+4 t0+6 t0+8 t0+10 t0
Figure 2: Results on Eagle – Per point error of the flow prediction on an Eagle example in the Low
spatial down-sampling scenario. Our model exhibits lower errors as also shown in Tables 1 and 2.
Navier Shallow Water Eagle
1/1 1/2 1/4 1/1 1/2 1/4 1/1 1/2 1/4
In-T 1.590 36.31 46.02 3.551 6.005 6.249 444.5 447.1 448.6 DINo
(Yin et al., 2022) Ext-T n/a 39.42 54.72 n/a 6.015 6.265 n/a 479.4 470.7
In-T 2.506 4.834 12.77 1.408 1.289 1.333 203.4 210.4 263.3 Interp. MGN
(Pfaff et al., 2020) Ext-T n/a 5.922 36.43 n/a 1.287 1.355 n/a 209.8 263.8
In-T n/a n/a n/aSpatial Oracle (n.c)Ext-T n/a 1.296 28.58 n/a 0.003 0.119 n/a 29.46 54.53
In-T 31.51 135.0 243.9 7.804 6.433 1.884 227.9 220.3 225.8 MAgNet
(Boussif et al., 2022) Ext-T n/a 142.8 255.5 n/a 6.291 1.947 n/a 229.8 230.6
In-T 0.2019 0.1964 0.4062 0.4115 0.4278 0.4549 108.0 106.1 278.6OursExt-T n/a 0.2138 11.36 n/a 0.4326 0.4802 n/a 119.9 306.9
Table 2: Time Continuity – we evaluate the time interpolation power of our method vs. the base-
lines. Models are trained and evaluated with 25% of Ω, and with different temporal resolutions (full,
half, and quarter of the original). The Spatial Oracle ( not comparable! ) uses the exact solution at
every point in space, and performs temporal interpolation. Evaluation is conducted over 20 frames
in the future (10 for Eagle ) and we report MSE compared to the ground truth solution ( ×10−3).
results). Eagle is particularly challenging, the main source of error being the spatial interpolation,
as can be seen in Figure 2 – our method yields lower errors in flow estimation.
Many more experiments – are available in appendix G. We study the impact of key design
choices , artificial generalization, and dynamical loss. We show qualitative results on time
interpolation, time extrapolation on the Navier dataset. We explore generalization to dif-
ferent grids . We provide more empirical evidence of the soundness of Step 2 in an ablation
study (including comparison with attentive neural process Kim et al. (2018), an attention-
based structure somehow close to ours), and observe attention maps on several examples.
We show that our state estimator goes beyond local interpolation , as conventional inter-
polation algorithms would do. Finally, we also measure the computational burden of the
discussed methods and show that our approach is more efficient.
5 C ONCLUSION
We exploit a double dynamical system formulation for simulating physical phenomena at arbitrary
locations in time and space. Our approach comes with theoretical guarantees on existence and ac-
curacy without knowledge of the underlying PDE. Furthermore, our method generalizes to unseen
initial conditions and reaches excellent performances outperforming existing methods. Potential
applications of our model goes beyond fluid dynamics and can be applied to various PDE-based
problem. Yet, our approach relies on several hypotheses such as regular time sampling and observ-
ability. Finally, for known and well-studied phenomena, it would be interesting to add physics priors
in the system, a nontrivial extension that we leave for future work.
9
Published as a conference paper at ICLR 2024
Reproducibility – the detailed model architecture is described in the appendix. For the sake of
reproducibility, in the case of acceptance, we will provide the source code for training and evaluating
our model, as well as trained model weights. For training, we will provide instructions for setting
up the codebase, including installing external dependencies, pre-trained models, and pre-selected
hyperparameter configuration. For the evaluation, the code will include evaluation metrics directly
comparable to the paper’s results.
Ethics statement – While our simulation tool is unlikely to yield unethical results, we are mindful
of potential negative applications of improving fluid dynamics simulations, particularly in military
contexts. Additionally, we strive to minimizing the carbon footprint associated with our training
processes.
6 A CKNOWLEDGEMENTS
We recognize support through French grants “ Delicio ” (ANR-19-CE23-0006) of call CE23 “ Intelli-
gence Artificielle ” and “ Remember ” (ANR-20-CHIA0018), of call “ Chaires IA hors centres ”. This
work was performed using HPC resources from GENCI- IDRIS (Grant 2023-AD010614014).
REFERENCES
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks
for learning about objects, relations and physics. Neural Information Processing Systems , 2016.
Pauline Bernard, Vincent Andrieu, and Daniele Astolfi. Observer design for continuous-time dy-
namical systems. Annual Reviews in Control , 2022.
Emmanuel De B ´ezenac, Arthur Pajot, and Patrick Gallinari. Deep learning for physical processes:
Incorporating prior scientific knowledge. Journal of Statistical Mechanics: Theory and Experi-
ment , 2019.
Oussama Boussif, Yoshua Bengio, Loubna Benabbou, and Dan Assouline. Magnet: Mesh agnostic
neural pde solver. In Neural Information Processing Systems , 2022.
Johannes Brandstetter, Daniel E. Worrall, and Max Welling. Message passing neural PDE solvers.
InInternational Conference on Learning Representations , 2022.
Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis. Physics-
informed neural networks (pinns) for fluid mechanics: A review. Acta Mechanica Sinica , 2021.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. Neural Information Processing Systems , 2018.
Kyunghyun Cho, Bart Van Merri ¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint , 2014.
Filipe de Avila Belbute-Peres and J Zico Kolter. Simple initialization and parametrization of sinu-
soidal networks via their kernel bandwidth. In International Conference on Learning Represen-
tations , 2023.
MWMG Dissanayake and Nhan Phan-Thien. Neural-network-based approximations for solving
partial differential equations. Communications in Numerical Methods in Engineering , 1994.
Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. Neural Information
Processing Systems , 2019.
Rizal Fathony, Anit Kumar Sahu, Devin Willmott, and J Zico Kolter. Multiplicative filter networks.
InInternational Conference on Learning Representations , 2021.
Marc Anton Finzi, Andres Potapczynski, Matthew Choptuik, and Andrew Gordon Wilson. A sta-
ble and scalable method for solving initial value pdes with neural networks. In International
Conference on Learning Representations , 2023.
10
Published as a conference paper at ICLR 2024
Joseph Galewsky, Richard K. Scott, and Lorenzo M. Polvani. An initial-value problem for testing
numerical models of the global shallow-water equations. Tellus A: Dynamic Meteorology and
Oceanography , 2004.
Vincent Le Guen and Nicolas Thome. Disentangling physical dynamics from unknown factors for
unsupervised video prediction. In Conference on Computer Vision and Pattern Recognition , 2020.
Xu Han, Han Gao, Tobias Pfaff, Jian-Xun Wang, and Liping Liu. Predicting physics in mesh-
reduced space with temporal attention. In International Conference on Learning Representations ,
2021.
Chuanbo Hua, Federico Berto, Michael Poli, Stefano Massaroli, and Jinkyoo Park. Efficient con-
tinuous spatio-temporal simulation with graph spline networks. In Internation Conference on
Machine Learning (AI for Science Workshop) , 2022.
Steeven Janny, Fabien Baradel, Natalia Neverova, Madiha Nadri, Greg Mori, and Christian Wolf.
Filtered-cophy: Unsupervised learning of counterfactual physics in pixel space. In International
Conference on Learning Representation , 2022a.
Steeven Janny, Quentin Possama ¨ı, Laurent Bako, Christian Wolf, and Madiha Nadri. Learning
reduced nonlinear state-space models: an output-error based canonical approach. In Conference
on Decision and Control , 2022b.
Steeven Janny, Aur ´elien Beneteau, Nicolas Thome, Madiha Nadri, Julie Digne, and Christian Wolf.
Eagle: Large-scale learning of turbulent fluid dynamics with mesh transformers. In International
Conference on Learning Representation , 2023.
Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol
Vinyals, and Yee Whye Teh. Attentive neural processes. In International Conference on Learning
Representations , 2018.
Georgios Kissas, Yibo Yang, Eileen Hwuang, Walter R. Witschey, John A. Detre, and Paris
Perdikaris. Machine learning in cardiovascular flows modeling: Predicting arterial blood pressure
from non-invasive 4d flow mri data using physics-informed neural networks. Computer Methods
in Applied Mechanics and Engineering , 2020.
Dmitrii Kochkov, Alvaro Sanchez-Gonzalez, and Peter Battaglia. Learning latent field dynamics of
pdes. In Third Workshop on Machine Learning and the Physical Sciences (NeurIPS 2020) , 2020.
Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Char-
acterizing possible failure modes in physics-informed neural networks. Neural Information Pro-
cessing Systems , 2021.
Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving
ordinary and partial differential equations. Transactions on Neural Networks , 1998.
Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B Tenenbaum, and Antonio Torralba. Learning par-
ticle dynamics for manipulating rigid bodies, deformable objects, and fluids. In International
Conference on Learning Representations , 2018.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhat-
tacharya, and Anima Anandkumar. Multipole graph neural operator for parametric partial differ-
ential equations. In Neural Information Processing Systems , 2020a.
Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew
Stuart, Anima Anandkumar, et al. Fourier neural operator for parametric partial differential equa-
tions. In International Conference on Learning Representations , 2020b.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, An-
drew Stuart, and Anima Anandkumar. Markov neural operators for learning chaotic systems.
arXiv preprint , 2021.
11"
2210.08692,D:\Database\arxiv\papers\2210.08692.pdf,"In the context of building dialogue systems, what are the advantages and disadvantages of using a generative user simulator (GUS) compared to a rule-based simulator like ABUS, and how do these advantages and disadvantages impact the performance of the dialogue system?","GUS offers advantages in generating more realistic and coherent user utterances, leading to better performance in dialogue systems trained with it. However, GUS requires more computational resources and may be more complex to implement than rule-based simulators.","as will be introduced later. To be clear, GPT-2
(Radford et al., 2019) in this paper refers to the
particular class of causal LMs, which computes
conditional probabilities for next-token generation
via self-attention based Transformer neural net-
work (Vaswani et al., 2017). Given a particu-
lar form of conditional model, p(output|input ),
whereinput andoutput are token sequences, the
GPT-2 model can be ﬁnetuned over training sam-
ples(input,output )(often referred to as training
sequences (Hosseini-Asl et al., 2020)), and after
ﬁnetuning, the model can be used for generation,
i.e., generating output after receiving input .
Generative Dialog System The main task for a
dialog system (DS) is, for each dialog turn t, to
generate (or say, predict)4bs
t,as
tandrt, givenut
and dialog history u1,r1,···,ut−1,rt−1. A recent
progress in building DS is that all variables are rep-
resented by token sequences, and the workﬂow of
a dialog system (DST, DP and NLG) is uniﬁed into
a single sequence generation problem, which can
be accomplished by a causal LM such as GPT-2
(Hosseini-Asl et al., 2020; Liu et al., 2022). In
this paper, we employ the Markov generative ar-
chitecture (MGA) for DS, which is introduced in
Liu et al. (2022) and shows efﬁciency advantages
in memory, computation and learning over non-
Markov DS models like SimpleTOD (Hosseini-Asl
et al., 2020). Speciﬁcally, for DS to predict bs
t,as
t
andrtat each turn t, we use only the belief state
bt−1and response rt−1from previous turn along
with current user utterance ut, as shown in Figure
2(a). The DS can thus be trained via ﬁnetuning
GPT-2 by maximizing the following conditional
likelihood over labeled training sequences for su-
pervised learning (SL):
JDS-SL = logpθ(bs
t,as
t,rt|bs
t−1,rt−1,ut)
=|bs
t⊕as
t⊕rt|∑
i=1logpθ(ci|bs
t−1,rt−1,ut,c<i)
(1)
where⊕denotes the concatenation of sequences,
|bs
t⊕as
t⊕rt|denotes the length in tokens, and
cidenotes the i-th token. The DS parameters are
actually a set of GPT-2 parameters, collectively
denoted byθ.
4Note that database result dbtis deterministically obtained
by querying database using the predicted bs
t. We omit dbtin
the discussion for simplicity.4 Method: Generative User Simulator
An end-to-end trainable US needs several modules
- natural language understanding (NLU) of system
responses, goal state tracking (GST), user policy
(UP), and natural language generation (NLG). In-
spired by the recent approach of ﬁnetuning PLMs
such as GPT-2 to build end-to-end trainable gen-
erative DSs, we propose an end-to-end trainable
generative user simulator (GUS), which generally
refer to the approach of recasting all the modules
in the US (NLU, UP, and NLG) as conditional gen-
eration of tokens based on ﬁnetuning PLMs such
as GPT-2. In the following, we ﬁrst introduce the
GUS model including goal state tracking and GPT-
2 based architecture. Then, we describe how GUS
is trained and used for reinforcement training of
the DS.
4.1 GUS Model
Goal State Deﬁnition Crucially, the interaction
between the user and the system is motivated by the
user goal, which is composed of user constraints
and requests such as booking a cheap hotel. The
goal state, in this paper, is deﬁned as the uncom-
pleted part of the user goal at each turn. Similar
to Kreyssig et al. (2018), we accumulate the an-
notated user acts backwards turn by turn to obtain
the goal state annotation at each turn. The accumu-
lation process is illustrated in Appendix A.1.The
goal state obtained at the ﬁrst turn corresponds to
the initial user goal for the whole dialog session.
Goal State Tracking Given the goal state an-
notations at each turn, the US can be trained via
teacher-forcing to mimic the user behaviors. When
the US is applied to interact with the DS for evalu-
ation or for reinforcement training of the DS, the
US needs to track the completion process of the
goal to update the goal state turn by turn, which we
call goal state tracking. There are three types of
user intents in the goal state gt-inform, book and
request . The slots and values for the ﬁrst two types
of intents in gtare denoted by gconstraint
t and those
of the request intent asgrequest
t . The update rule of
gtat turntis designed to be as follows:
gconstraint
t =gconstraint
t−1⊖au,inform
t−1
grequest
t =grequest
t−1⊖bu,inform
t(2)
whereau,inform
t−1,bu,inform
t are the informable slots
and values in user act au
t−1and user belief state
bu
trespectively and⊖denotes removing the cor-
responding slots and values. Moreover, the slot
values in the initial user goal may be changed dur-
ing the interaction (i.e., goal change). When the
DS expresses no-offer intent, which means no en-
tities in the database satisfy the constraints of the
goal, we randomly select one slot in the no-offer
intent and replace its value with another value in
the ontology.
GPT-2-based Architecture The main task for a
US is, conditional on the user goal, to iteratively
understand the system response, track goal state,
decide user act, and generate user utterance. In this
work, we ﬁnd that the recent approach of ﬁnetun-
ing GPT-2 for conditional generation can be simi-
larly applied to build US. Speciﬁcally, we employ
Markov generative architecture (Liu et al., 2022).
The US is designed to ﬁrstly infer the system intent,
i.e., user belief state bu
tof turntfrom the previous
system response rt−1, which could be modeled as
pφ(bu
t|rt−1). After obtaining bu
t, the goal state will
be updated according to the rule in Eq. (2). Then,
the US will generate user act and user utterance
sequentially conditioned on the previous system
response, user belief state, and the updated goal
state. The resulting US is called GUS and could
be modeled as pφ(au
t,ut|rt−1,bu
t,gt). The GUS
parameters are actually another set of GPT-2 pa-
rameters, collectively denoted by φ.
4.2 GUS Training
The GUS model can thus be trained via ﬁnetuning
GPT-2 by maximizing the following conditional
likelihood over labeled training sequences for su-
pervised learning (SL):
JUS-SL = logpφ(bu
t|rt−1)
+ logpφ(au
t,ut|rt−1,bu
t,gt)(3)
Note that during supervised learning, the user belief
statebu
tis labeled by directly copying the system
actas
t−1from the previous turn.
4.3 Reinforcement Optimization of DS
through Interaction with US
RL Setup The DS and US described above
will ﬁrst be trained using supervised learning
with the objectives in Eq. (1)and Eq. (3)respec-
tively. After supervised learning, we can per-
form RL optimization on the DS through inter-
actions with the US. The DS agent view the US
as the environment and use its conditional modelpθ(bs
t,as
t,rt|bs
t−1,rt−1,ut)as its policy. Here the
policy of the DS involves generating not only sys-
tem actas
t, but also belief state bs
tand system re-
sponsert. This is different from some previous
studies of learning reinforced DS, e.g., (Liu and
Lane, 2017; Papangelis et al., 2019; Tseng et al.,
2021), which only use RL to optimize the selec-
tion of system acts (but all use traditional LSTM
seq2seq architectures). However, thanks to the rep-
resentation power of GPT-2, recursively predict (or
say, decide about) bs
t,as
tandrtin one policy yields
the best performance in our experiment. In Sec-
tion 7.3, we compare different schemes for policy
deﬁnition for the DS agent with more discussions.
RL Optimization We apply the policy gradient
method (Sutton et al., 2000) to optimize the DS for
RL. We ﬁrst let the two agents interact with each
other based on the user goals from the goal gen-
erator provided by ConvLab-2 (Zhu et al., 2020).
Then we calculate the reward Rtfor each turn, as
detailed below. The return Ui,tfor the action of
turntat thei-th step isγ|At|−iRt, whereγis the
discounting factor and |At|is the policy sequence
length of turn t. We update the DS with the follow-
ing policy gradients:
∇θJDS-RL =|bs
t⊕as
t⊕rt|∑
i=1Ui,t∇θlogpθ(ci) (4)
wherepθ(ci)denotespθ(ci|bs
t−1,rt−1,ut,c<i).
Reward Settings A number of different settings
for reward have been studied, as described in
the following. The three settings are separately
tested, and the experimental results are given in
Section 7.2.
1) Success. If a dialog is successful, we set the
reward of each turn to 1, otherwise it is set to be 0;
2) A turn-level synthetic reward similar to Tseng
et al. (2021); Takanobu et al. (2020), which consists
of requesting reward (+0.1 for each), repeating pun-
ishment (-0.5 for each) and task completion reward
(the proportion of tasks completed) of the DS;
3) A Sigmoid synthetic reward obtained by map-
ping the synthetic reward to [0,1] interval using
the Sigmoid function. This setting is designed to
exclude the inﬂuence of the value range of reward
because the value range is different between the
Success reward and the synthetic reward.
5 Experiments
5.1 Dataset
Experiments are conducted on MultiWOZ2.1 (Eric
et al., 2020), which is an English multi-domain
task-oriented dialog dataset of human-human con-
versations. It contains 10.4k dialogs, collected in
a Wizard-of-Oz setup over seven domains. The
dataset contains the annotations of system belief
state, system act, and user act for every turn.
5.2 Evaluation Metrics
Evaluating the quality of a US is not trivial. The
performance of the reinforced DS trained with a
speciﬁc US gives an indirect assessment of the
quality of the US. Considering that a main purpose
of developing USs is to help train RL based DSs,
this indirect assessment makes sense and is widely
employed (Kreyssig et al., 2018; Shi et al., 2019;
Lin et al., 2021). We conduct both automatic evalu-
ation and human evaluation of the DSs trained with
different USs. Additionally, we also ask human
graders to directly assess the performance of dif-
ferent USs, by reading and scoring the generated
utterances from the USs.
Automatic Evaluation It could be interaction-
based or corpus-based. For both manners, we can
calculate Inform andSuccess for measuring the
performance of the DSs. Inform Rate measures
how often the entities provided by the system are
correct. Success Rate refers to how often the sys-
tem is able to answer all the requested attributes by
user. BLEU Score is used to measure the ﬂuency of
the generated system responses when conducting
corpus-based evaluation.
Human Evaluation We conduct human evalua-
tion, where human graders are recruited to assess
the quality of dialogs generated by the US and the
DS trained with it. Similar to Su et al. (2021), for
each dialog, the grader will score the conversation
on a 3-point scale (0, 1, or 2)5by the following 3
metrics for the DS and 2 metrics for the US:
•Success. This metric measures if the DS suc-
cessfully completes the user goal by interact-
ing with the US;
•DS Coherency (DS-coh). This metric mea-
sures whether the system’s response is logi-
cally coherent with the dialogue context;
5Three scales (0, 1 and 2) denote three degrees - not at all,
partially and completely, respectively.•DS Fluency (DS-Flu). This metric measures
the ﬂuency of the system’s response.
•US Coherency (US-Coh). This metric mea-
sures whether the simulator’s utterance is log-
ically coherent with the dialogue context;
•US Fluency (US-Flu). This metric measures
the ﬂuency of the simulator’s utterance.
5.3 Baseline
The DS model is as described in Section 3. We
compare GUS with the classic rule-based simulator
ABUS (Schatzmann et al., 2007). We use the sim-
ulator in the ConvLab-2 (Zhu et al., 2020) toolkit,
which provides an instantiation of ABUS on Multi-
WOZ (Budzianowski et al., 2018), including BERT-
based NLU and template-based NLG. The ABUS
in ConvLab-2 has a goal generator module, which
we use for driving the interaction between the DSs
and the proposed GUS. Remarkably, the TUS pa-
per (Lin et al., 2021) has revealed the shortcoming
of VHUS (Gür et al., 2018), which performs much
worse than ABUS. Also, it is concluded that TUS
has a comparable performance to the rule-based
ABUS in cross-model evaluation. Thus, in this pa-
per, we mainly compare GUS with ABUS, which
is a very strong baseline.
6 Main Results
6.1 Cross-Model Evaluation
Cross-model evaluation is a type of automatic eval-
uation (Schatztnann et al., 2005) to compare differ-
ent USs. The main idea is that if the DS trained
with a speciﬁc US performs well on all USs (not
just on the one that the DS was trained with), it
indicates the speciﬁc US with which the DS was
trained is of good quality (realistic), and thus the
DS is likely to perform better on real users.
Speciﬁcally, we ﬁrst train a DS and a US sep-
arately on training data based on the supervised
learning objectives described in Eq. (1)and Eq. (3).
The resulting models are referred to as DS-SL and
GUS respectively. Then we further optimize DS-
SL by policy gradient in Eq. (4)on interaction with
either ABUS or GUS, and obtain DS-ABUS and
DS-GUS respectively. For either of ABUS and
GUS, RL trainings (all starting from DS-SL) are in-
dependently taken for three times with different ran-
dom seeds. Each speciﬁc DS model is then tested
on both ABUS and GUS. We use the same 1000
randomly generated goals for each test. Further im-
DS\USABUS GUS
Inform Success Inform Success
DS-SL 0.864 0.791 0.781 0.736
DS-ABUS best 0.885 0.816 0.783 0.741
DS-GUSbest 0.881 0.810 0.864 0.808
DS-ABUS avg 0.889 0.793 0.793 0.735
DS-GUSavg 0.872 0.801 0.859 0.802
Table 2: Cross-model evaluation results. The sub-
scriptsbest andavg denote the best and the average
from 3 independent RL experiments with different ran-
dom seeds.
plementation details can be found in Appendix A.2.
Table 2 shows the cross-model evaluation results6.
It can be seen from Table 2 that the DS trained
with GUS (DS-GUS) performs well on both ABUS
and GUS, while the DS trained with ABUS (DS-
ABUS) only performs well on ABUS and achieves
much lower Inform and Success when tested with
GUS. This indicates the superiority of GUS over
ABUS, being more helpful in training reinforced
DSs that perform well on both USs. Moreover, DS-
GUS also outperforms the supervised baseline (DS-
SL) on both USs. This shows the practical beneﬁt
brought by training DSs via RL on interaction with
the proposed GUS. Such comparison of RL and SL
is overlooked in some prior work, as reviewed in
Table 1.
6.2 Corpus-based Evaluation
Corpus-based evaluation has become a widely-used
type of automatic evaluation to compare different
end-to-end DSs. In the context of studying USs,
it is relevant to conduct corpus-based evaluation
for the following two aspects. First, running test-
ing of the DS trained with a speciﬁc US over a
ﬁxed testing set of dialogs could be an indirect as-
sessment of the quality of the US. Second, it is
possible for the trained DS via RL to achieve high
task success and yet not generate human language
(Zhao et al., 2019), particularly when the reward is
mainly deﬁned to encourage task success. With the
ﬁxed testing set, we could calculate BLEU which
measures the NLG performance of the trained DS.
We use the standard evaluation scripts from
Nekvinda and Dušek (2021) for corpus-based eval-
uation. The results are shown in Table 3 with some
6Similar tables to Table 2 have been used in previous work
such as NUS (Kreyssig et al., 2018) and TUS (Lin et al.,
2021). A common practice of reading such tables is row-
by-row comparison. This is exactly what the cross-model
evaluation means.DS Inform Success BLEU Combined
AuGPT (Kulhánek et al., 2021) 76.6 60.5 16.8 85.4
SOLOIST (Li et al., 2020) 82.3 72.4 13.6 90.9
UBAR (Yang et al., 2021) 83.4 70.3 17.6 94.4
DS-SL 84.10 72.10 19.24 97.34
DS-ABUS best 84.20 71.00 19.44 97.04
DS-ABUS avg 85.37 69.70 19.10 96.64
DS-GUSbest 85.70 74.60 19.80 99.95
DS-GUSavg 85.17 73.33 19.83 99.01
Table 3: Corpus-based evaluation. Above the dashed
line are GPT-2-based results from the ofﬁcial website
of MultiWOZ. Below are the results from DS-SL and
the DSs trained with ABUS and GUS respectively.
interesting ﬁndings. First, the DS trained with GUS
(DS-GUS) achieves higher combined score than
the DS trained with ABUS (DS-ABUS). This is
consistent with the results in Table 2 and again
demonstrate the advantage of GUS over ABUS.
Second, note that DS-GUS is initialized from DS-
SL and further trained via RL on interaction with
GUS, and Table 2 shows that DS-GUS improves
over DS-SL not only in Inform and Success but
also in BLEU. This result indicates that RL train-
ing of the DS with GUS does not suffer from the
tradeoff problem between policy learning and NLG
in ofﬂine RL (Zhao et al., 2019)7, achieving higher
success and being faithful to human language. See
more discussions in Section 7.3.
6.3 Human Evaluation
We further perform human evaluation of the perfor-
mances of USs and DSs. For each pair of US and
DS, 100 dialogs were gathered, which were scored
by 5 human graders. The details of evaluation met-
rics have been described in Sec. 5.2 and the results
are shown in Table 4. For convenience, we refer to
the results of each row by the name of the DS in
the table. It can seen that the overall performance
of DS-GUS is superior over both DS-ABUS and
DS-SL. Further, we conduct signiﬁcance tests by
comparing either DS-ABUS or DS-SL with DS-
GUS respectively, using the matched-pairs method
(Gillick and Cox, 1989) and add a superscript∗
to the score in the ﬁrst two rows in Table 4 if the
p-value is less than 0.05. All the speciﬁc p-values
can be seen in Appendix A.4. The results show
that DS-GUS signiﬁcantly improves over DS-SL
for Success and US-Coh, while the differences in
terms of DS-Coh, DS-Flu and US-Flu are not sig-
niﬁcant. Moreover, all the human evaluation met-
rics by DS-GUS are stronger than or equal to those
7This problem for ofﬂine RL is further studied and allevi-
ated in Lubis et al. (2020).
DS US Success DS-Coh DS-Flu US-Coh US-Flu
DS-ABUS ABUS 1.71 1.51 1.65∗1.27∗1.30∗
DS-SL GUS 1.73∗1.60 1.85 1.61∗1.88
DS-GUS GUS 1.84 1.52 1.79 1.75 1.90
Table 4: Human evaluation of the dialogs generated by
different DSs and USs. The score with∗in the ﬁrst two
rows denotes the difference between this score and the
score in the last row (DS-GUS with GUS) is signiﬁcant
(p-value<0.05); otherwise, the difference is not signiﬁ-
cant (p-value>=0.05).
US Inform Success
ABUS 0.863 0.790
GUS 0.825 0.777
GUS w/o GST 0.743 0.502
Table 5: The ablation results about goal state tracking
(GST). The DS trained with GUS w/o GST is tested on
ABUS, GUS and GUS w/o GST respectively.
by DS-ABUS. Particularly, DS-GUS signiﬁcantly
outperforms DS-ABUS for DS-Flu, US-Coh and
US-Flu. This indicates that GUS is able to generate
more coherent and ﬂuent utterances than ABUS.
To illustrate this point, we provide some generated
dialogues in Appendix A.3.
7 Ablation Study
7.1 The Importance of Goal State Tracking
In our GUS model, we use Eq. (2)to update
the goal state at every turn. In the section, we
consider a variant of GUS, which sets the goal
state at all turns to be the initial goal, that is,
gt=g0,t= 1,...,T , like in Asri et al. (2016);
Gür et al. (2018); Papangelis et al. (2019). Such
model is referred to as GUS w/o GST, and could
be similarly trained according to Eq. (3). Then
we train a DS with this US (called “DS-GUS w/o
GST”) and test it with ABUS, GUS and GUS w/o
GST respectively. The results are shown in Table 5.
We can see that the Inform and Success rates ob-
tained by “DS-GUS w/o GST” are lower than those
by DS-GUS as shown in Table 2, when testing on
ABUS and GUS. This indicates the importance of
using GST in GUS. Besides, we can see that the re-
sults are pretty low when testing on GUS w/o GST.
Presumably, this is because GUS w/o GST cannot
accurately distinguish the uncompleted part in the
complex goal, which will easily cause omission
and repetition when generating user acts.Reward Inform Success
None 0.781 0.736
Success 0.842 0.787
Synthetic 0.864 0.808
Sigmoid synthetic 0.850 0.780
Table 6: Interaction-based results of testing DS-GUS
on GUS under different reward settings, as introduced
in Section 7.2. “None” denotes the testing results of
DS-SL with GUS, as also reported in the ﬁrst row in
Table 2.
Policy Inform Success
bs
t⊕as
t⊕rt0.864 0.808
as
t⊕rt 0.845 0.770
as
t 0.848 0.796
Table 7: The ablation experiments of using different
policy schemes.
7.2 Different Reward Settings
The results of optimizing DS on GUS using differ-
ent reward settings are reported in Table 6. It is
found that all reward settings achieve better results
than supervised baseline (Reward=None) and the
synthetic reward setting achieves the best result,
which is reasonable since the ﬁne-grained rewards
reﬂect more than simple success rate in terms of
the nature of the tasks (Tseng et al., 2021). All
RL results in this paper are based on this setting of
reward, unless here for ablation study.
7.3 Different Policy Schemes for DS
The policy in RL refers to the probabilistic map-
ping from states to actions. Previous studies of
learning reinforced DS, e.g., (Liu and Lane, 2017;
Papangelis et al., 2019; Tseng et al., 2021), mainly
employ RL to optimize the policy module, i.e., use
system acts for actions. In contrast, the policy of
DS-GUS and DS-ABUS involves generating not
only system act as
t, but also belief state bs
tand
system response rt, which can be represented as
bs
t⊕as
t⊕rt, as illustrated in Eq. (4). To compare
policy schemes for reinforced DS, we try two other
policy schemes when optimizing DS-GUS. The
ﬁrst policy scheme only involves the generation of
system actas
tand the second one involves the gen-
eration of both system act as
tand system response
rt. We denote the two policy schemes as as
tand
as
t⊕rtrespectively. Table 7 shows the interaction
results when the DS-GUS trained under different
policy schemes is tested with GUS.
It can be seen from Table 7 that using bs
t⊕as
t⊕rt
for policy achieves the highest Inform and Success
rate. We provide two points, which may explain the
advantage of our model in using bs
t⊕as
t⊕rtfor RL.
First, since the DST, DP and NLG modules in GPT-
2 based DS share the model parameters, parameter
adjust in one module will affect other modules.
Only optimizing DP during RL without considering
other modules may mislead other modules. Using
bs
t⊕as
t⊕rtleads to better overall optimization
and decision-making. Second, the balance between
policy learning and NLG, which was a concern
in previous studies when using modular or small-
capacity architectures (Zhao et al., 2019), could be
relieved, thanks to the high-capacity of GPT-2.
8 Conclusion
In this paper, towards developing an end-to-end
trainable US for multi-domains, a generative user
simulator (GUS) with GPT-2 based architecture
and goal state tracking is proposed and systemat-
ically evaluated. We train GPT-2 based DSs and
USs and conduct cross-model evaluation, corpus-
based evaluation and human evaluation. The results
show that the DS trained with GUS outperforms
both the supervised trained DS and the DS trained
with ABUS. The human evaluation further con-
ﬁrms the superiority of GUS and shows that GUS
can generate much more coherent and ﬂuent utter-
ances than ABUS. Moreover, GUS is simple and
easy to use, in addition to its strong performance.
Hope this work will stimulate further work on de-
veloping and using user simulators in the study of
building dialog systems.
9 Limitations
There are some limitations of this work. First, due
to computational constraints, both the DSs and the
USs are experimented based on a distilled version
of GPT-2. Studies using larger GPT-2 and other
classes of larger PLMs such as T5 (Raffel et al.,
2020) would enhance our results in this paper. Sec-
ond, we only utilize the policy gradient method for
RL in this paper. Other advanced RL methods such
as proximal policy optimization (PPO) and actor-
critic are also worth trying in future work. Those
being said, while we agree that experimenting with
larger PLMs and more complex RL methods are
meaningful, we believe the extensive experiments
presented in this paper (cross-model evaluation,
corpus-based evaluation, human evaluation, and
ablation studies) can well support the evaluationsof GUS and should not affect the main ﬁnding and
contribution of this paper.
References
Layla El Asri, Jing He, and Kaheer Suleman. 2016. A
sequence-to-sequence model for user simulation in
spoken dialogue systems. In INTERSPEECH .
Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang
Tseng, Iñigo Casanueva, Ultes Stefan, Ramadan Os-
man, and Milica Gaši ´c. 2018. Multiwoz - a large-
scale multi-domain wizard-of-oz dataset for task-
oriented dialogue modelling. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing (EMNLP) .
Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi,
Sanchit Agarwal, Shuyang Gao, Adarsh Kumar,
Anuj Kumar Goyal, Peter Ku, and Dilek Hakkani-
Tür. 2020. Multiwoz 2.1: A consolidated multi-
domain dialogue dataset with state corrections and
state tracking baselines. In LREC .
Laurence Gillick and Stephen J Cox. 1989. Some sta-
tistical issues in the comparison of speech recog-
nition algorithms. In International Conference on
Acoustics, Speech, and Signal Processing , pages
532–535. IEEE.
Izzeddin Gür, Dilek Hakkani-Tür, Gokhan Tür, and
Pararth Shah. 2018. User modeling for task oriented
dialogues. In 2018 IEEE Spoken Language Technol-
ogy Workshop (SLT) , pages 900–906.
Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,
Semih Yavuz, and Richard Socher. 2020. A simple
language model for task-oriented dialogue. arXiv
preprint arXiv:2005.00796 .
Florian Kreyssig, Iñigo Casanueva, Paweł
Budzianowski, and Milica Gaši ´c. 2018. Neural user
simulation for corpus-based policy optimisation
of spoken dialogue systems. In Proceedings of
the 19th Annual SIGdial Meeting on Discourse
and Dialogue , pages 60–69, Melbourne, Australia.
Association for Computational Linguistics.
Jonáš Kulhánek, V ojt ˇech Hude ˇcek, Tomáš Nekvinda,
and Ond ˇrej Dušek. 2021. Augpt: Dialogue with
pre-trained language models and data augmentation.
arXiv preprint arXiv:2102.05126 .
Yohan Lee. 2021. Improving end-to-end task-oriented
dialog system with a simple auxiliary task. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2021 , pages 1296–1303, Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Baolin Peng Chunyuan Li, Jinchao Li, Shahin Shayan-
deh, Lars Liden, and Jianfeng Gao. 2020. Soloist:
Building task bots at scale with transfer learning and
machine teaching. Transactions of the Association
for Computational Linguistics (TACL), 2021 .
Hsien-chin Lin, Nurul Lubis, Songbo Hu, Carel van
Niekerk, Christian Geishauser, Michael Heck, Shu-
tong Feng, and Milica Gasic. 2021. Domain-
independent user simulation with transformers for
task-oriented dialogue systems. In Proceedings of
the 22nd Annual Meeting of the Special Interest
Group on Discourse and Dialogue , pages 445–456,
Singapore and Online. Association for Computa-
tional Linguistics.
Bing Liu and Ian R. Lane. 2017. Iterative policy learn-
ing in end-to-end trainable task-oriented neural dia-
log models. In 2017 IEEE Automatic Speech Recog-
nition and Understanding Workshop, ASRU 2017,
Okinawa, Japan, December 16-20, 2017 , pages 482–
489. IEEE.
Hong Liu, Yucheng Cai, Zhijian Ou, Yi Huang, and
Junlan Feng. 2022. Building Markovian generative
architectures over pretrained LM backbones for ef-
ﬁcient task-oriented dialog systems. ArXiv preprint
arXiv:2204.06452 .
Nurul Lubis, Christian Geishauser, Michael Heck,
Hsien-Chin Lin, Marco Moresi, Carel van Niek-
erk, and Milica Gasic. 2020. LA V A: Latent action
spaces via variational auto-encoding for dialogue
policy optimization. In Proceedings of the 28th In-
ternational Conference on Computational Linguis-
tics, pages 465–479.
Tomáš Nekvinda and Ond ˇrej Dušek. 2021. Shades of
BLEU, ﬂavours of success: The case of MultiWOZ.
InProceedings of the 1st Workshop on Natural Lan-
guage Generation, Evaluation, and Metrics (GEM
2021) , pages 34–46, Online. Association for Com-
putational Linguistics.
Alexandros Papangelis, Yi-Chia Wang, Piero Molino,
and Gokhan Tur. 2019. Collaborative multi-agent di-
alogue model training via reinforcement learning. In
Proceedings of the 20th Annual SIGdial Meeting on
Discourse and Dialogue , pages 92–102, Stockholm,
Sweden. Association for Computational Linguistics.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAI
Blog , 1(8):9.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
the limits of transfer learning with a uniﬁed text-to-
text transformer. Journal of Machine Learning Re-
search , 21(140):1–67.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108 .
Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based
user simulation for bootstrapping a POMDP dia-
logue system. In Human Language Technologies2007: The Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics; Companion Volume, Short Papers , pages 149–
152, Rochester, New York. Association for Compu-
tational Linguistics.
Jost Schatztnann, Matthew N Stuttle, Karl Weilham-
mer, and Steve Young. 2005. Effects of the user
model on simulation-based learning of dialogue
strategies. In IEEE Workshop on Automatic Speech
Recognition and Understanding, 2005. , pages 220–
225. IEEE.
Weiyan Shi, Kun Qian, Xuewei Wang, and Zhou
Yu. 2019. How to build user simulators to train
RL-based dialog systems. In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 1990–2000, Hong Kong,
China. Association for Computational Linguistics.
Yixuan Su, Lei Shu, Elman Mansimov, Arshit Gupta,
Deng Cai, Yi-An Lai, and Yi Zhang. 2021. Multi-
task pre-training for plug-and-play task-oriented di-
alogue system. CoRR , abs/2109.14739.
Richard S Sutton, David A McAllester, Satinder P
Singh, and Yishay Mansour. 2000. Policy gradient
methods for reinforcement learning with function ap-
proximation. In Advances in neural information pro-
cessing systems , pages 1057–1063.
Ryuichi Takanobu, Runze Liang, and Minlie Huang.
2020. Multi-agent task-oriented dialog policy learn-
ing with role-aware reward decomposition. In Pro-
ceedings of the 58th Annual Meeting of the Associa-
tion for Computational Linguistics , pages 625–638,
Online. Association for Computational Linguistics.
Bo-Hsiang Tseng, Yinpei Dai, Florian Kreyssig, and
Bill Byrne. 2021. Transferable dialogue systems
and user simulators. In Proceedings of the 59th An-
nual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers) , pages 152–166, Online. Association
for Computational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems , pages 5998–6008.
Yunyi Yang, Yunhao Li, and Xiaojun Quan. 2021.
Ubar: Towards fully end-to-end task-oriented dialog
system with gpt-2. In Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence (AAAI) .
Steve Young, Milica Gaši ´c, Blaise Thomson, and Ja-
son D. Williams. 2013. Pomdp-based statistical spo-
ken dialog systems: A review. Proceedings of the
IEEE , 101(5):1160–1179.
Yichi Zhang, Zhijian Ou, Min Hu, and Junlan Feng.
2020. A probabilistic end-to-end task-oriented di-
alog model with latent belief states towards semi-
supervised learning. In Proc. of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP) .
Yichi Zhang, Zhijian Ou, and Zhou Yu. 2020. Task-
oriented dialog systems that consider multiple ap-
propriate responses under the same context. In The
Thirty-Fourth AAAI Conference on Artiﬁcial Intelli-
gence (AAAI) .
Tiancheng Zhao, Kaige Xie, and Maxine Eskenazi.
2019. Rethinking action spaces for reinforcement
learning in end-to-end dialog agents with latent vari-
able models. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 1208–1218, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Qi Zhu, Zheng Zhang, Yan Fang, Xiang Li, Ryuichi
Takanobu, Jinchao Li, Baolin Peng, Jianfeng Gao,
Xiaoyan Zhu, and Minlie Huang. 2020. Convlab-
2: An open-source toolkit for building, evaluating,
and diagnosing dialogue systems. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics ."
2106.13967,D:\Database\arxiv\papers\2106.13967.pdf,"The paper explores the use of human pose information, extracted using OpenPose, as a feature for action detection and anticipation.  How does the effectiveness of OpenPose as a feature vary depending on the complexity of the underlying model used for action recognition, and what insights can be drawn from this variation?","The paper finds that OpenPose features provide a significant boost in action detection accuracy when used with simpler models like C3D, but its effectiveness diminishes when used with more powerful models like I3D, suggesting that the benefit of OpenPose features is more pronounced when the model relies heavily on motion information for action recognition.","Fig. 5. Visualization of our best method - I3D
TABLE I
CHUNK SIZE6 EXPERIMENTS , USING RESNET-200, BN-I NCEPTION AND OPENPOSE
Method Features Encoder Decoder - Time predicted into the future (seconds)
Chunk size = 6 frames 0.25s 0.50s 0.75s 1.00s 1.25s 1.50s 1.75s 2.00s Avg
Baseline RGB – Flow 25.93 26.15 25.89 25.79 25.73 25.66 25.68 25.66 25.57 25.77
Ours {RGB + OpenPose }– Flow 24.25 23.11 25.63 26.72 26.18 25.57 24.94 24.40 23.94 25.06
Ours RGB – OpenPose 37.57 25.54 25.93 26.44 26.60 26.28 25.57 24.75 24.00 25.64
Ours OpenPose – Flow 36.30 21.77 22.59 23.57 23.19 22.28 21.30 20.49 19.83 21.88
TABLE II
CHUNK SIZE16 E XPERIMENTS , USING C3D AND OPENPOSE
Method Features Encoder Decoder - Time predicted into the future (seconds)
Chunk size = 16 frames 0.25s 0.50s 0.75s 1.00s 1.25s 1.50s 1.75s 2.00s Avg
Ours C3D (One-Stream) 35.43 34.34 31.05 28.22 26.46 25.37 24.75 24.39 24.22 27.35
Ours {C3D (RBG) }– OpenPose 36.44 32.98 30.56 28.37 26.61 25.38 24.54 23.78 23.22 26.93
TABLE III
CHUNK SIZE16 E XPERIMENTS , USING I3D AND OPENPOSE
Method Features Encoder Decoder - Time predicted into the future (seconds)
Chunk size = 16 frames 0.25s 0.50s 0.75s 1.00s 1.25s 1.50s 1.75s 2.00s Avg
Ours I3D 55.25 52.57 46.69 41.94 38.39 35.90 34.22 33.00 32.08 39.35
Ours {I3D (RGB) + OpenPose }–{I3D (Flow) } 49.21 46.65 40.78 36.42 33.19 30.90 29.42 28.43 27.71 34.19
Ours {I3D (RGB) }– OpenPose 47.43 44.59 40.08 36.77 34.24 32.37 31.29 30.56 30.06 35.00
Ours {I3D (RGB) }–{I3D (Flow) + OpenPose } 44.47 29.55 31.92 29.62 27.21 25.63 24.78 24.20 23.68 27.07
By inspecting Table I, where chunk size has been set
to 6 frames, we observe that the baseline method, using
ResNet-200 for RGB information and BN-Inception for ﬂow
information, displays the highest accuracy of 25.77% for the
precision task, the prediction accuracy for the classiﬁcation
task however is only 25.93%. By replacing ﬂow information
with OpenPose features we notice that, while the average
decoder accuracy decreases slightly to 25.67%, the encoder
accuracy is signiﬁcantly improved and reaches 37.57%. How-
ever, it is true that the use of OpenPose features to enhance
or replace RGB information does not provide any further
improvement, either in the anticipation phase or in the process
of detection. It is worth mentioning though that, in both cases
that we used OpenPose, we observe better performance for the
period 0.5s - 1.25s, but also much smaller than the baseline
in longer-term predictions, something that leads to a decline
of the average accuracy in these cases.
Table II shows the results for C3D, giving one-stream
features, where the chunk size has been set to 16. We highlight
that the use of human pose as motion features, by introducing
a second stream in our model, gives a boost compared to thesimple C3D in the phase of action detection, from 35.43% for
the former to 36.44% for the latter, as opposed to action antic-
ipation, for which the accuracy drops to 26.93%. Additionally,
we should note the large discrepancy between the performance
of short-term and long-term anticipation, reaching as much
as 10%. By comparing this table to the previous one, we
observe that OpenPose shows, as motion information, better
anticipation performance, compared to the corresponding sim-
ple model, in the interval 0.75s - 1.25s. Moreover, although
in C3D models we observe larger anticipation accuracy, the
action detection accuracy does not exceed that of the models
of Table I.
The results of employing I3D as well as its variations are
shown in Table III, where the chunk size is set to 16 frames.
We notice that both the simple I3D model and its modiﬁcations
show much better performance, with the simplest I3D model
giving the biggest boost and reaching 39.35% in the antic-
ipation phase and 55.25% in the detection phase. However,
the use of OpenPose in this set of experiments, both as an
additional cue to RGB and ﬂow information and as a unique
motion information did not offer any improvement. On the
contrary, it limited its effectiveness. This divergence is likely
due to the substantially strong capacity, offered by I3D ﬂow
information.
VI. C ONCLUSION
In this paper, we propose several ways to improve online
action detection, building upon Temporal Recurrent Networks.
Our results highlight the value of temporal context and human
pose as useful cues for localizing action in time. We demon-
strate that most of our models outperform the original TRN
method [1] by a signiﬁcant margin, even though our baseline
results are lower than the original paper’s due to the smaller
batch size we used, with the best of them (I3D) achieving
state-of-the-art results. Speciﬁcally, observing the variations
of models’ behavior in the analysis and detection phase, we
believe that the use of different models for anticipation and
recognition could beneﬁt the task of online action detection.
We plan to pursue this goal in our future work.
REFERENCES
[1] M. Xu and M. Gao and Y . Chen and L. S. Davis and D. J. Crandall:
Temporal Recurrent Networks for Online Action Detection. In Proc.
ICCV , 2019.
[2] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image
recognition. In Proc. CVPR, 2016.
[3] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network
training by reducing internal covariate shift.arXiv:1502.03167, 2015.
[4] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, Learning
Spatiotemporal Features with 3D Convolutional Networks. In Proc.
ICCV , 2015.
[5] J. Carreira, & A. Zisserman (2017). Quo vadis, action recognition? a
new model and the kinetics dataset. In Proc. CVPR, 2017
[6] Z. Cao, G. Hidalgo, T. Simon, S. Wei, Y . Sheikh: OpenPose: Realtime
Multi-Person 2D Pose Estimation using Part Afﬁnity Fields. In Proc.
CVPR, 2017.
[7] A. Shahroudy, J. Liu, T. Ng, G. Wang: NTU RGB+D: A Large Scale
Dataset for 3D Human Activity Analysis. In Proc. CVPR, 2016.
[8] Y .-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev,M. Shah,
and R. Sukthankar. THUMOS challenge: Action recognition with a large
number of classes. http://crcv.ucf.edu/THUMOS14/, 2014.
[9] I. Laptev, M. Marszałek, C. Schmid, and B. Rozenfeld. Learning realistic
human actions from movies. In Proc. CVPR, 2008.
[10] A. Kl ¨aser, M. Marszałek, and C. Schmid. A spatio-temporal descriptor
based on 3D-gradients. In Proc. BMVC, 2008.
[11] G. Willems, T. Tuytelaars, and L. Van Gool. An efﬁcient dense and
scale-invariant spatio-temporal interest point detector. In ECCV , 2008
[12] H. Wang, A. Kl ¨aser, C. Schmid et al. Dense Trajectories and Motion
Boundary Descriptors for Action Recognition. Int J Comput Vis 103,
60–79 (2013).
[13] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Vebu-
gopalan, K. Saenko, and T. Darrell. Long-term recurrent convolutional
networks for visual recognition and description. In Proc. CVPR, 2015.
[14] K. Simonyan, & A. Zisserman. Two-Stream Convolutional Networks for
Action Recognition in Videos. In NIPS, 2014.
[15] S. Ji, W. Xu, M. Yang, and K. Yu. 3D convolutional neural networks for
human action recognition. IEEE Trans on pattern analysis and machine
intelligence, 35(1):221–231, 2013.
[16] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.Learning
spatiotemporal features with 3d convolutional networks. In ICCV , 2015
[17] J. Carreira and A. Zisserman. Quo vadis, action recognition? a new
model and the kinetics dataset. In Proc. CVPR, 2017.
[18] Y . Lecun , L. Bottou , Y . Bengio , P. Haffner , Gradient-based learning
applied to document recognition. In IEEE, vol. 86, pp.2278, 1998
[19] C. Feichtenhofer, A. Pinz, and A. Zisserman. Convolutional two-stream
network fusion for video action recognition. In Proc. CVPR, 2016.
[20] F. M. Noori, B. Wallace, Md. Z. Uddin,and J. Torresen: A Robust Human
Activity Recognition Approach Using OpenPose, Motion Features, and
Deep Recurrent Neural Network. In Proc. SCIA, 2019.[21] Z. Cao, T. Simon, S. Wei, and Y . Sheikh, “OpenPose: realtime multi-
person 2D pose estimation using part afﬁnity ﬁelds”. In CVPR, 2017.
[22] Q. Ke, M. Bennamoun, S. An, F. Sohel, F. Boussaid: A New Repre-
sentation of Skeleton Sequences for 3D Action Recognition. In Proc.
CVPR, 2017
[23] A. Shahroudy, J. Liu, T. Ng, G. Wang: NTU RGB+D: A Large Scale
Dataset for 3D Human Activity Analysis. In Proc. CVPR, 2016
[24] S. Karaman, L. Seidenari, and A. Del Bimbo. 2014. Fast saliency
based pooling of ﬁsher encoded dense trajectories. In ECCV THUMOS
Workshop.
[25] D. Oneata, J. Verbeek, and C. Schmid. 2014. The LEAR submission at
Thumos 2014. In ECCV THUMOS Workshop, 2014.
[26] L. Wang, Y . Qiao, and X. Tang. Action recognition and detectionby
combining motion and appearance features. In THUMOS14 Action
Recognition Challenge, 2014.
[27] V . Escorcia, F. C. Heilbron, J. C. Niebles, and B. Ghanem. DAPs: Deep
Action Proposals for Action Understanding. In ECCV , 2016.
[28] J. Gao, Z. Yang, C. Sun, K. Chen, and R. Nevatia. TURN TAP: Temporal
Unit Regression Network for Temporal Action Proposals. In ICCV , 2017.
[29] Z. Shou, J. Chan, A. Zareian, K. Miyazawa, and S.-F. Chang. CDC:
Convolutional-de-convolutional networks for precise temporal action
localization in untrimmed videos. In Proc. CVPR, 2017.
[30] F. Long, T. Yao, Z. Qiu, X. Tian, J. Luo and T. Mei. Gaussian Temporal
Awareness Networks for Action Localization. In Proc. CVPR, 2019.
[31] R. De Geest, E. Gavves, A. Ghodrati, Z. Li, C. Snoek, and T. Tuytelaars.
Online action detection. In Proc. ECCV , 2016.
[32] R. De Geest and T. Tuytelaars. Modeling temporal structure with lstm
for online action detection. In Proc. WACV , 2018.
[33] J. Gao, Z. Yang, and R. Nevatia. RED: Reinforced encoder-decoder
networks for action anticipation. In Proc. BMVC, 2017.
[34] Z. Shou, J. Pan, J. Chan, K. Miyazawa, H. Mansour, A. Vetro, X. Giro-i
Nieto, and S.-F. Chang. Online action detection in untrimmed, streaming
videos-modeling and evaluation. In Proc. ECCV , 2018.
[35] A. Zlatintsi, A.C. Dometios, N. Kardaris, I. Rodomagoulakis, P. Koutras,
X. Papageorgiou, P. Maragos, C.S. Tzafestas, P. Vartholomeos, K. Hauer,
C. Werner, R. Annicchiarico, M.G. Lombardi, F. Adriano, T. Asfour,
A.M. Sabatini, C. Laschi, M. Cianchetti, A. G ¨uler, I. Kokkinos, B. Klein,
and R. L ´opez. 2020. I-Support: A robotic platform of an assistive bathing
robot for the elderly population. In Robot. Auton. Syst., 2020.
[36] G. Chalvatzaki, Petros Koutras, A. Tsiami, C. Tzafestas and P. Maragos.
i-Walk Intelligent Assessment System: Activity, Mobility, Intention,
Communication. In Proc. ECCV Workshops, 2020.
[37] J. Hadﬁeld, G. Chalvatzaki, P. Koutras, M. Khamassi, C. S. Tzafestas
and P. Maragos. A Deep Learning Approach for Multi-View Engagement
Estimation of Children in a Child-Robot Joint Attention Task. In Proc.
IROS, 2019.
[38] Y . Gao, X. Xiang, N. Xiong, B. Huang, H. J. Lee, R. Alrifai, X. Jiang,
Z. Fang. Human Action Monitoring for Healthcare Based on Deep
Learning. In IEEE Access vol. 6, pp. 52277, 2018.
[39] D. Burns, N. Leung, M. Hardisty, C. Whyne, P. Henry, S.tewart McLach-
lin. Shoulder Physiotherapy Exercise Recognition: Machine Learning the
Inertial Signals from a Smartwatch. In Physiol, vol. 39, 2018.
[40] Y . Yao, M. Xu, Y . Wang, D. J. Crandall, E. M. Atkins. Unsupervised
Trafﬁc Accident Detection in First-Person Videos. In Proc. IROS, 2019.
[41] G. Serpen and R. H. Khan. Real-time Detection of Human Falls in
Progress: Machine Learning Approach. In Proc. CASE, 2018.
[42] M. Ramezani and F. Yaghmaee. A review on human action analysis in
videos for retrieval applications. Artif. Intell. Rev. 46, 4, 2016, 485–514.
[43] X. Zhai, Y . Peng, and J. Xiao. 2013. Cross-media retrieval by intra-
media and inter-media correlation mining. Multimedia Syst. 19, 5, 2013,
395–406.
[44] H. Wang and C. Schmid. Action Recognition with Improved Trajecto-
ries. In Proc. ICCV , 2013
[45] Y . Du, Y . Fu and L. Wang. Representation Learning of Temporal
Dynamics for Skeleton-Based Action Recognition. In IEEE Trans. on
Im. Proc., vol. 25, no. 7, pp. 3010, July 2016.
[46] Y .-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev, M. Shah,
and R. Sukthankar. THUMOS challenge: Action recognition with a large
number of classes. In Proc. ICCV 2013.
[47] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.
arXiv:1412.6980, 2014.
[48] J. S ´anchez & E. Meinhardt-Llopis, & G. Facciolo. TV-L1 optical ﬂow
estimation. In Proc. IPOL, 2013."
1812.07608,D:\Database\arxiv\papers\1812.07608.pdf,"The paper discusses the performance of NbDE in comparison to other algorithms for function optimization.  Considering the context of the paper and the characteristics of different optimization algorithms, what are the potential advantages and disadvantages of NbDE compared to other methods, particularly in terms of convergence speed and the ability to escape local optima?","NbDE demonstrates faster convergence speed and a smaller population size compared to other algorithms, likely due to its improved mutation and crossover strategy. However, NbDE might struggle to escape local optima in certain cases, requiring more iterations for convergence, especially when dealing with higher dimensional problems.","those of DE/best/2 when D=10. D=30 and D=50 and DE when 
D=30, and the success rate on F7 is a little bit lower than tha t 
of DE/best/2 when D=30. Besides, NbDE performs much better than other algorithms.  
TABLE .V SR AND RANKS OF ALL ALGORITHMS WHEN D=50 
Fun NbDE DE/rand/1 DE/best/2 JADE WSA GA PSO 
F1 1/1 0/2 0/2 0/2 0/2 0/2 0/2 
F2 1/1 0/5 1/1 0.38/3 0.14/4 0/5 0/5 
F3 0/1 0/1 0/1 0/1 0/1 0/1 0/1 
F4 0.3/1  0/2 0/2 0/2 0/2 0/2 0/2 
F5 1/1 0/6 0.02/5 1/1 0/6 0.14/4 0.92/3 
F6 0.68/1 0.28/2 0.02/3 0/4 0/4 0/4 0/4 
F7 0/1 0/1 0/1 0/1 0/1 0/1 0/1 
F8 1/1 0/3 1/1 0/3 0/3 0/3 0/3 
F9 0.96/2 0/5 1/1 0.14/4 0.32/3 0/5 0/5 
Rank Score  10 27 17 21 26 27 26 
F r o m  T a b l e . 3  a n d  T a b l e . 5 ,  w e  c a n  c o n c l u d e  t h a t  N b D E  
performs best on success rate of benchmark function test when D=10 and D=50, because the rank score of NbDE is much better than those of other methods. We have also noticed that NbDE 
and DE/best/2 got the same rank score when D=30. But we can see in Table.4 the rank of DE/best/2 on F1, F3, F4 is 2 while the SR of DE/best/2 is 0 which means that DE/best/2 cannot get exactly results on these benchmark functions, but NbDE can get exactly results in most cases, so we can infer that NbDE outperform DE/best/2 when D=30. Therefore, we can conclude that NbDE outperforms other methods on index of success rate 
in benchmark functions test. 
B.  Quality of Optima Found 
In this part, NbDE is compared with other methods in the 
index of the optima found accuracy. As we can see from Table. 6-8, we have found that NbDE gets the optima found with best accuracy on F1, F3, F4, F6, F7, F8, F9 when D=10, on F1, F2, 
F3, F5, F6, F8 when D=30, on F1, F2, F3, F4, F6, F8 when D=50.  
What more, We have also noticed that the performance index 
of F9 reached by NbDE is relatively small(less than 5E-3), considering that the success (SR) of F9 when D=30 and D=50
TABLE .VI QUALITY OF OPTIMA FOUND OF ALL ALGORITHMS WHEN D=10  (MEASUREMENT : MEAN/STD) 
Fun NbDE DE/rand/1 DE/best/2 JADE WSA GA PSO 
F1 3.3E-40/1.6E-39 6.8E+3/2.8E+3 2.7E+2/1.4E+2 1.5E-1/8.2E-1 5.3E-10/3.0E-9 2.1E+4 /7.7E+3 4.8E-25/2.7E-24 
F2 5.3E-39/7.5E-39 5.2E-40/4.9E-40 7.8E-47/6.5E-47 3.2E-12/2.2E-11 2.26E+0/1.1E-2 1.3E-3/7.3E-4 1.4E-2/1.5E-2 
F3 6.6E-23/1.1E-22 3.9E-06/3.0E-6 1.2E-12/6.4E-13 2.6E-2/4.5E-3 4.0E+0/2.2E+0 4.1E -1/2.8E-1 1.8E-2/1.5E-2 
F4 8.0E-2/5.6E-1 6.7E+0/8.3E+0 1.0E+0/1.9E+0 5.9E+0/2.3E+0 2.0E+1/3.2E+1 1.4E+1/ 8.8E+0 1.0E+0/1.8E+0 
F5 1.2E-3/5.2E-4 3.2E-3/1.2E-3 1.4E-3/5.2E-4 3.8E-4/2.5E-4 4.1E-3/3.0E-3 5.6E-3/2.6E-3 1.9E-3/1.6E-3 
F6 -418.98E+1/1.8E-12 -409.85E+1/1.2E+2 -409.93E+1/9.1E+1 -402.88E+1/1.3E+2 -336.3E+1 /2.7E+2 -134.86E+1/2.5E+2 -221.21E+1/5.2E+2 
F7 0/0 1.4E+0/1.2E+0 2.0E-2/1.4E-1 6.0E-2/2.3E-1 9.4E+0/5.8E+0 7.3E-5/ 1.4E-4 1.0E+1/3.6E+0 
F8 4.2E-15/8.5E-16 4.5E-15/5.0E-16 4.4E-15/0 2.3E-2/1.6E-1 3.7E-1/6.8E-1 5.9E-3/3. 3E-3 7.9E-1/9.3E-1 
F9  2.1E-4/1.1E-3 3.3E-3/5.3E-3 7.9E-4/2.4E-3 9.5E-3/1.1E-2 8.6E-2/6.7E-2 6.6E-2/ 2.6E-2 4.5E-1/2.7E-1 
TABLE .VII  QUALITY OF OPTIMA FOUND OF ALL ALGORITHMS WHEN D=30  (MEASUREMENT : MEAN/STD) 
Fun NbDE DE/rand/1  DE/best/2  JADE WSA GA PSO 
F1 5.4E-33/8.6E-33 6.3E+4/7.5E+3 1.6E+4/2.8E+3 4.0E-2/6.2E-2 2.18E+2/1.5E+3 8.1E+4 /1.6E+4 4.6E-3/3.0E-3 
F2 1.1E-50/1.6E-50 3.1E-15/7.23E-16 2.3E-27/5.7E-28 4.7E-11/3.3E-10 2.39E+0/4.7E+0  2.2E-2/8.6E-3 6.3E-1/5.6E-1 
F3 3.9E-17/8.8E-17 5.5E+0/4.8E-1 5.6E-4/9.0E-5 8.0E-1/4.1E-1 2.5E+1/6.1E+0 6.1E+0/ 1.5E+1 2.5E-1/2.1E-1 
F4 2.3E-2/9.5E-1 6.4E+1/2.0E+1 2.7E+1/1.3E+1 3.6E+1/1.8E+1 1.9E+3/1.3E+4 2.5E+2/ 2.3E+2 2.8E+1/1.0E+1 
F5 2.0E-3/6.4E-4 2.7E-2/5.5E-3 6.1E-3/1.4E-3 8.3E-4/2.7E-4 1.1E-1/3.7E-1 9.2E-3/ 3E-3 4.0E-3/2.4E-3 
F6 -125.69E+2/7.3E-12 124.89E+2/1.2E+2 125.08E+2/1.0E+2 -107.87E+2/7.0E+2 -729.15E+1/ 7.0E+2 -261.42E+1/4.0E+2 -628.17E+1/1.2E+3 
F7 1.0E-1/3.6E-1 7.8E-1/8.5E-1 0/0 4.5E+1/5.0E+0 1.1E+2/3.5E+1 1.3E-1/3.2E-1 4.4E+1/1.4E+1 
F8 6.2E-15/1.8E-15 5.0E-10/4.5E-10 8.9E-15/1.6E-15 1.4E-5/5.7E-5 5.6E+0/2.2E+0 3.4 E-2/1.1E-2 2.6E+0/8.0E-1 
F9 1.0E-3/3.3E-3 0/0 0/0 7.9E-4/3.2E-3 1.5E-2/1.9E-2 1.1E-1/7.7E-2 1.1E-2/1.3E-2 
TABLE .VIII  QUALITY OF OPTIMA FOUND OF ALL ALGORITHMS WHEN D=50  (MEASUREMENT : MEAN/STD) 
Fun NbDE DE/rand/1  DE/best/2  JADE WSA GA PSO 
F1 1.9E-21/7.3E-21 1.3E+5/1.2E+4 6.1E+4/5.3E+3 8.1E+3/1.4E+3 4.1E+3/4.1E+3 1.4E+5/ 2.4E+4 4.65E-2/1.5E-2 
F2 1.4E-61/1.6E-61 2.7E-6/2.8E-7 2.7E-17/3.6E-18 3.2E-6/1.2E-5 1.5E+1/2.6E+1 2.0E- 1/5.1E-2 1.6E+0/8.5E-1 
F3 8.9E-2/1.7E-1 4.2E+1/1.6E+0 1.4E-1/1.4E-2 1.6E+0/4.7E-1 3.6E+1/7.2E+0 2.4E+1/ 5.3E+0 6.5E-1/3.0E-1 
F4 2.4E-1/9.6E-1 2.4E+2/1.8E+1 4.7E+1/1.1E+1 7.0E+1/2.8E+1 3.7E+3/1.8E+4 7.8E+2/ 3.6E+2 6.1E+1/2.9E+1 
F5 3.5E-3/1.0E-3 1.3E-1/1.6E-1 1.3E-2/2.0E-3 1.3E-3/3.1E-4 1.2E+0/6.8E+0 1.4E-2/3.8E-3 5.1E-3/3.7E-3 
F6 -209.04E+2/7.1E+1 -207.70E+2/2.1E+2 -196.81E+2/4.6E+2 -111.50E+2/4.4E+2 -111.25E+ 2/1.0E+3 -383.82E+1/6.0E+2 -102.79E+2/1.5E+3 
F7 3.7E+0/1.7E+0 6.5E+2/6.5+0 4.5E-2/4.1E-2 1.7E+2/8.7E+0 2.6E+2/4.6E+1 1.3E+1/3.8E+0 6.5E+1/1.5E+1 
F8 9.3E-15/2.7E-15 9.3E-2/2.1E-2 2.1E-14/1.4E-15 2.2E-4/1.8E-4 1.2E+1/3.4E+0 8.0E- 1/4.9E-1 3.3E+0/4.7E-1 
F9  7.9E-4/4.4E-3 7.1E-6/1.6E-6 0/0 9.2E-4/3.1E-3 4.1E-2/1.0E-1 9.8E-1/8.1E-2 7.1E-3/7.5E-3 
are even higher than those when D=10, so we can conclude that 
N b D E  h a s  j u m p e d  o u t  o f  l o c a l  o p t i m a l  a n d  n e e d s  m o r e  iterations for convergence in these cases. Based on this analys is, we can also make the similar conclusion for F4 when D=50. 
Table.7 and Table.8 have shown that DE/best/2 get the best result for F7 and F9 when D=30 and D=50, but it can hardly 
get a feasible solution for F1, F3, F4 when D=30, F1, F3, F4, 
F5, F6, F7 when D=50. So we can also conclude that NbDE outperforms DE/best/2. We can also find that JADE underperforms NbDE in most cases while it can get satisfactory solutions when the number of iterations is enough[16], so we can conclude that JADE converge slower than NbDE. GA and PSO is classical meta-heuristic algorithms for function optimization, in this experiment we can observe that NbDE 
outperforms GA and PSO significantly. WSA is the new meta-
heuristic method we proposed previously, it’s featured with strong local exploration ability but its convergence speed is relatively slow especially for high dimensions problems which can also be demonstrated in this simulation. From inferences mentioned above, we can find that NbDE outperforms other algorithms and is featured with fast convergence speed and small population size because of the improved mutation and crossover strategy. 
IV.
  CONCLUSION  
In this paper, inspired by whale swarm algorithm and 
differential evolution, a meta-heuristic algorithm for function  
optimization  called differential evolution with better and nearest option (NbDE), is proposed. NbDE is compared with several other meta-heuristic methods on success rate and optima found accuracy of benchmark functions. Simulation results have shown that NbDE outperforms other algorithms in overall index of benchmark functions and have demonstrated the effectiveness of NbDE. In the future we will pay more attention on the following aspects: 
1) NbDE application in humanoid robot, such as the 
design of mechanical structure and the optimization of 
fuzzy logic controller; 
2) Improvement of NbDE such as parallel computing 
optimization, parameter self-adjustment. 
A
CKNOWLEDGMENT  
This research was supported by the National Natural 
Science Foundation for Distinguished Young Scholars of 
China under Grant No.51825502, National Natural Science Foundation of China (NSFC) (51775216, and 51721092), Natural Science Foundation of Hubei Province (2018CFA078) and the Program for HUST Academic Frontier Youth Team.  
R
EFERENCES  
[1] Mahi, M., Baykan, Ö.K., Kodaz, H.: A new hybrid 
method based on particle swarm optimization, ant colony 
optimization and 3-opt algorithms for traveling salesman 
problem. Appl. Soft Comput. 30, 484–490 (2015) 
[2] Zeng, B., Dong, Y.: An improved harmony search based 
energy-efﬁcient routing algorithm for wireless sensor networks. Appl. Soft Comput. 41, 135–147 (2016) 
[3] Strogatz, S. (2015). Nonlinear Dynamics and Chaos. 
Boca Raton: CRC Press.Storn, R., Price, K.: Differential 
Evolution - A Simple and Efﬁcient Adaptive Scheme for 
Global Optimization Over Continuous Spaces. ICSI, Berkeley (1995)  
[5] Qing, A.: Dynamic differential evolution strategy and 
applications in electromagnetic inverse scattering 
problems. IEEE Trans. Geosci. Remote Sens. 44(1), 116–125 (2006)  
[6] Gao, Z., Pan, Z., Gao, J.: A new highly efﬁcient 
differential evolution scheme and its application to waveform inversion. IEEE Geosci. Remote Sens. Lett. 
11(10), 1702–1706 (2014)  
[7] Das, S., Suganthan, P.N.: Differential evolution: a survey 
of the state-of-the-art. IEEE Trans. Evol. Comput. 15(1), 
4–31 (2011) 
[8] Das, Swagatam, Sankha Subhra Mullick, and Ponnuthurai 
N. Suganthan. ""Recent advances in differential evolution–
an updated survey."" Swarm and Evolutionary Computation 27 (2016): 1-30. 
[9] Zeng, Bing, L. Gao, and X. Li. Whale Swarm Algorithm 
for Function Optimization. Intelligent Computing Theories and Application. 2017. 
[10] Zeng, Bing, et al. ""Whale swarm algorithm with the 
mechanism of identifying and escaping from extreme points for multimodal function optimization."" Neural 
Computing and Applications: 1-21. 
[11] R. Storn and K. Price, “Differential evolution a simple 
and efficient heuristic for global optimization over 
continuous spaces,” J. Global Optimization, vol. 11, no. 4, pp. 341–359, 1997. 
[12] K. V . Price, R. M. Storn, and J. A. Lampinen, 
Differential Evolution: A Practical Approach to Global 
Optimization, 1st ed. New York: SpringerVerlag, Dec. 
2005. 
[13] “Differential evolution for multiobjective optimization,” 
in Proc. IEEE Congr. Evol. Comput., Dec. 2003, pp. 2696–2703. 
[14] R. Gamperle, S. D. Muller, and P. Koumoutsakos, “A 
parameter study for differential evolution,” in Proc. Advances Intell. Syst., Fu zzy Syst., Evol. Comput., 
Crete, Greece, 2002, pp. 293–298. 
[15] U. Pahner and K. Hameyer, “Adaptive coupling of 
differential evolution and multiquadrics approximation 
for the tuning of the optimization process,” IEEE Trans. 
Magnetics, vol. 36, no. 4, pp. 1047–1051, Jul. 2000. 
[16] J., Z. and C.S. A., JADE: Adaptive Differential Evolution 
With Optional External Archive. IEEE Transactions on Evolutionary Computation, 2009. 13(5): p. 945-958. 
[17] Yuan, X., et al., A Genetic  Algorithm-Based, Dynamic 
Clustering Method Towards Improved WSN Longevity. Journal of Network and Systems Management, 2017. 
25(1): p. 21-46. 
[18] Bansal J.C. (2019) Particle Swarm Optimization. In: 
Bansal J., Singh P., Pal N. (eds) Evolutionary and Swarm Intelligence Algorithms. Studies in 
Computational Intelligence, vol 779. Springer, Cham
 "
2207.14584,D:\Database\arxiv\papers\2207.14584.pdf,"What are the key challenges in using decentralized technologies for medical data analysis, and how do these challenges impact the development of a decentralized electronic health record (EHR) system?","The challenges include ensuring data anonymity and privacy, correlating and analyzing noncontextualized data, and addressing the computational demands of decentralized machine learning (ML) models. These challenges necessitate the development of robust solutions for data anonymization, secure data sharing, and efficient model training on resource-constrained devices.","58 COMPUTER    WWW.COMPUTER.ORG/COMPUTER
RESEARCH FEATUREmedical information across DLTs also 
faces serious challenges. One essen -
tial issue is to transparently classify 
one anonymous patient’s medical 
data among various others through a 
decentralized collaboration of a set of 
local ML models (with similar charac -
teristics/algorithms), logically located 
at different medical institutions. In 
addition, it is difficult to correlate 
and analyze millions of anonymous, 
noncontextualized medical records 
produced by various devices, distrib -
uted into different locations with dif -
ferent attributes. In this scenario, it 
is difficult to determine whether the 
data comes from different patients (or 
even different sensors belonging to 
the same patient), affecting predictive 
analysis. Furthermore, the feasibil -
ity of training decentralized ML mod -
els for medical information analysis, 
research, and its integration with IoT 
devices has never been explored.
Therefore, it is important to explore 
decentralized approaches for the fed -
eration of ML training with guided ana -
lytics. The approach should address 
the problem of noncontextualized 
training data aggregation, knowledge 
extraction, and cognitive learning about 
users’ medical and personal data 
in an  anonymous manner. This could 
occur through a seamless coupling of ML predictive analysis algorithms on 
noncontextualized and anonymous 
medical information.
Gap 3: Insufficient computing 
resources and computationally 
inefficient DLT and ML 
solutions for edge training
DLT and ML approaches are known 
to be computationally demanding.14 
However, in large-scale heterogeneous 
and fragmented environments where 
patient data span geographical bound -
aries, the important limiting factors are 
insufficient computational resources 
and technical expertise. Concretely, 
hospitals do not own a vast infra -
structure, and the utilization of high-  
performance computing services is 
not always feasible. Furthermore, the 
employment of local hospital infra -
structure for decentralized ML train -
ing can lead to reduced accuracy of 
the ML model and errors during pre -
dictive analysis, especially if the med -
ical data for training are generated by 
IoT devices.
Therefore, we should address scalable 
approaches for efficient model updates 
in an ML overlay with an increasing 
number of learners/algorithms dis -
tributed across various physical loca -
tions. In practice, we should approach 
scalability, concerning the available resources across the computing con -
tinuum, from various angles such as 
latency for a consensus and transac -
tion validation time (for example, a 
model update). It is therefore essential 
to explore whether we can sacrifice ML 
model accuracy to allow for execution 
on computing continuum resources 
connected directly to personal medical 
IoT devices (such as heart rate or blood 
saturation monitors) or other med -
ical equipment, which might not be 
directly accessible over the network.
DECENTRALIZED EHR 
SYSTEM ARCHITECTURE
We propose a conceptual EHR system 
architecture, named STIGMA  (see 
 Figure 1). With the STIGMA system, 
medical data always stay at medical 
institutions and form local ML models, 
but only after performing anonymiza -
tion. Medical professionals interact 
with the system through multimodal 
diagnosis equipment, enriched with 
sensor data from personal IoT devices. 
Medical institutions can register in 
the STIGMA EHR system by utilizing 
strictly defined protocols for interop -
erability, as defined by Roehrs et al.6 
The STIGMA EHR system performs in 
the following manner:
 ›A data analysis instance receives 
a direct multimodal data stream 
(magnetic resonance imaging, 
computed tomography scans, 
IoT heart rate sensors, electroen -
cephalogram sensors, and so on) 
of medical procedures.
 ›Afterward, the data stream is 
analyzed on the available com -
puting continuum resources.
 ›Data analysis filters anonymize 
the data stream, which is then 
sent to the model training 
instance.WITH THE STIGMA SYSTEM, MEDICAL 
DATA ALWAYS STAY AT MEDICAL 
INSTITUTIONS AND FORM LOCAL ML 
MODELS, BUT ONLY AFTER PERFORMING 
ANONYMIZATION.
 OCTOBER  2022  59Hospital 1 Hospital 2 Hospital N
Distrib uted LedgerComputing 
Continuum 
Resources
Consensus f or 
Register ing 
TransactionsPointers to the ML Models
Transactions on the P erformance of the Edge Resources
Local Cop y 
of the DL T+2'# *94,-""'$-*""0&,)*)$.12*34.& # +2'# *94,-""'$-*""0&,)*)$.12*34.& #
2
346
1MLML ML 8
5
7 Vote Vote VoteData
AnalysisModel
TrainingData
AnalysisModel
Training
Diagnosis Equipment and
IoT De vicesDiagnosis Equipment and
IoT De vicesData
AnalysisModel
Training
Diagnosis Equipment and
IoT De vices
ML ML ML
FIGURE 1. A conceptual architecture of the STIGMA EHR system.
60 COMPUTER    WWW.COMPUTER.ORG/COMPUTER
RESEARCH FEATURE ›The model training instance 
applies ML algorithms to train a 
model on the available comput -
ing continuum resources.
 ›After the model is trained, the 
model training instance utilizes 
the distributed ledger to register 
the model (only as a pointer, 
without exposing the data) and 
checks for other suitable regis -
tered models.
 ›Thereafter, if suitable models 
are found in the distributed led -
ger registry, model training con -
tacts the model owners directly, 
namely, other medical institu -
tions, to receive rolling updates 
or exchange (share) relevant 
data for model improvement.
 ›The STIGMA EHR system can 
perform only the rolling updates 
and the data sharing after a 
consensus (by voting) is reached 
among all the medical institu -
tions, federated by the distrib -
uted ledger.
 ›The information is then used for 
improving the model, which is 
used to provide real-time sup -
port for diagnosis and therapy 
assessment and is again regis -
tered in the distributed ledger.
All of the aforementioned steps are 
continuously managed and synchro -
nized in a decentralized manner by 
the STIGMA EHR network. It logically 
forms a peer-to-peer group that main -
tains records on all the transactions 
(model updates, inference performance 
data, and accuracy). The STIGMA EHR 
network also contains information 
for available computing continuum 
resources (in terms of computing power 
and available ML models) at each medi -
cal institution. The EHR network, there -
fore, allows all parties of the system to confirm or reject any piece of data 
added to it, while no data can be deleted 
from it. This provides a full history of all 
transactions appearing on the DLT, giv -
ing EHRs a method to ensure the cor -
rectness of retrieved information.
ML overlays with decentralized 
medical data control
To support the creation of a decentral -
ized STIGMA EHR, as depicted in Fig-
ure 1 , we research a DLT-based overlay 
for the federation of multiple medical 
institutions through the following 
actions, directly related to identified 
technological gaps.
DLT for a decentralized federation of 
ML models in an overlay. The STIGMA 
EHR system uses a permissioned15 pro -
tocol to create an appropriate configu -
rable and modular federating archi -
tecture addressing EHR systems’ 
requirements for anonymous ML model 
updates with full control of the private 
data that do not leave hospital infra -
structure. The EHR system relies on 
scalable DLT management approaches 
capable of reaching a consensus with 
a minimal number of communication 
steps with a limited number of  ledgers 
in a permissioned environment. 
Model provenance for  decentralized 
ML. Another important aspect of the 
STIGMA EHR is data provenance, a 
key concept for supporting ML-based 
analysis over decentralized networks, 
especially when data from IoT devices 
are used. Data provenance enables effi -
cient access approaches that allow all 
the participating ML models in the ML 
overlay to maintain a copy of the DLT 
and ensure the availability of the same 
version of truth. The DLT contains only 
the transaction logs that the ML model 
updates’ fingerprints, exclusively stored in hospital computing infrastructures 
to reduce replication and network 
throughput. 
Data immutability and secure prop -
agation of decentralized ML model 
updates with multiparty computa -
tion. This aspect addresses the immu -
tability and propagation of ML models 
without violating data privacy during 
updates. It is used to publish, update, 
and activate anonymous information 
exchange among the ML algorithms 
during model training across the over -
lay. Unfortunately, current technologies 
are computationally inefficient, thus 
not allowing straightforward utiliza -
tion of DLTs for complex data sets. We 
therefore utilize the concept of multi -
party computation16 to enable compu -
tation on data from different providers. 
The other participating actors gain no 
additional information about each oth -
er’s inputs, except what they learn from 
the ML model’s collaborative output, 
that is, decoupling the model from the 
training data.
Predictive data analysis 
with IoT integration
Another important enabler for deploy -
ment of the STIGMA EHR system is 
cross-medical data analysis for improved 
diagnosis and therapy assessment 
through distributed ML with IoT med -
ical device integration. Therefore, we 
rely on the following solutions.
Predictive analysis with decentral -
ized ML. The STIGMA EHR system 
utilizes approaches for automated ML 
reasoning with distributed non- and 
cross-referenced data received from 
professional medical equipment and 
personal medical IoT devices. This pro -
cess reduces uncertainty and mistrust 
by double-checking the validity of the 
 OCTOBER  2022  61information and its sources in poten -
tially unpredictable environments. 
The approach enables shared knowl -
edge and improves data acquisition 
from IoT devices.
Scalable ML, and a consensus 
on the computing continuum
Multiple research works, such as Paxos 
and RAFT,17 agree on a single majority 
value (that is, a state transaction), with 
reduced overhead and power require -
ments. Unfortunately, they still require 
the large computational resources that 
a resource-constrained hospital infra -
structure cannot provide, thus making 
deployment of the STIGMA EHR sys -
tem challenging. Therefore, we modify 
the current approaches to make them 
suitable for execution on computing 
continuum devices. To achieve this, 
the STIGMA EHR system assesses the 
complexity of ML algorithms and the 
training data structure to select suit -
able resources in the computing contin -
uum with higher computational capa -
bilities, close to where the data reside in 
terms of network distance. Then, based 
on the available hospital computational 
infrastructure, a decision is made about where to conduct the training, and the 
accuracy level is identified. 
REAL TESTBED EVALUATION
To validate the proposed conceptual 
EHR system, we deployed DLT-based 
ML systems on a real-world experimen -
tal testbed. We emulated the computing 
infrastructure of medical institutions 
by using adequate cloud, fog, and edge 
resources, as described in the “Physical 
Testbed” section. For the evaluation, we 
implemented the Paxos three-phase 
commit protocol, where each institu -
tion in the DLT network kept track of 
current changes. To allow for execution 
on multiple heterogeneous systems, we 
developed a Paxos protocol in Java 11.0.
Physical testbed
We utilized Carinthian Computing Con -
tinuum (C3),18 a real computing contin -
uum testbed, located at the University 
of Klagenfurt, to emulate a network of 
multiple medical institutions with lim -
ited computing capacities. C3 encom -
passes heterogeneous resources, pro -
vided as containers or virtual machines, 
in multiple performance categories. We 
have therefore identified a subset of resources usually available in hospitals 
(such as fog and private cloud infra -
structures), and user-specific devices 
(such as ECs composed of low-powered, 
portable devices), to conduct the concep -
tual evaluation (see Table 1 ).
Centralized computing infrastruc -
tures (CCIs) consist of virtualized 
instances provisioned on demand from 
Amazon Web Services. For evaluation 
purposes, we selected m5a.xlarge  and 
c5.large  as general-purpose instances 
powered by an AMD EPYC 7000 proces -
sor at 2.5 GHz and an Intel Xeon Plati -
num 8000 series processor at 3.6 GHz, 
respectively. 
A fog cluster (FC) comprises reso -
urces from the local Exoscale (ES) cloud 
provider, which enables communication 
latency of ≤12 ms and a maximal band -
width of ≤10 Gb/s. For evaluation pur -
poses, we identified medium and large 
ES instances, as described in Table 1 . 
An edge cluster (EC) includes five 
NVIDIA Jetson Nano (NJN) and 32 Rasp -
berry Pi-4 (RPi4) single-board comput -
ers. We installed a Raspberry Pi operat -
ing system  (version 2020-05-27) on the 
RPi devices. We used Linux for Tegra  
for the NJN resources. We utilized a 
TABLE 1. The C3 testbed configuration.
CCI (Amazon Web Services) FC EC
Instance/device m5a.xlarge c5.large Exoscale large Exoscale medium EGS NJN RPi4
CPU type AMD EPYC 7000 Intel Xeon  
Platinum 8180Intel Xeon 
Platinum 8180Intel Xeon 
Platinum 8180AMD Ryzen 
2920Tegra X1 and 
ARM Cortex A57ARM 
Cortex 72
CPU clock (GHz) 2.5 3.6 3.6 3.6 3.5 1.43 1.5 
Memory (GB) 32 8 8 4 32 4 4 
Storage (GB) 120 120 120 120 1,000 64 64 
BW (Mb/s) 27 26 65 65 813 450 800 
FC: fog cluster; EC: edge cluster; EGS: Edge Gateway System; NJN: NVIDIA Jetson Nano; RPi4: Raspberry Pi-4.
62 COMPUTER    WWW.COMPUTER.ORG/COMPUTER
RESEARCH FEATUREmanaged, 48-port, three-layer HP Aruba 
switch to interconnect all resources in 
the EC. The switch supports 1 Gb/s per 
port, with a latency of 3.8 µs and an 
aggregate data transfer rate of 104 Gb/s. 
The EC is managed by the Edge Gateway 
System (EGS), based on a 12-core AMD 
Ryzen Threadripper 2920X processor 
at 3.5 GHz with 32 GB of random-ac -
cess memory, which is easily available 
in many medical and business envi -
ronments. For cases when there are not 
sufficient resources available at the EC, 
the EGS is responsible for partially off -
loading the execution of the compute  
processes to other computing contin -
uum resources, including FC or CCI.
Experimental design
We designed the following four sets of 
experiments according to character -
istics of the conceptual decentralized 
EHR system and averaged the results 
over 10 runs for statistical significance:
1. The DLT network initialization 
time evaluates the initializa -
tion time of the EHR network, 
encompassing multiple medical 
institutions in the range of {3, 
5, 7, 10}. The medical institu -
tion that initializes the EHR network is considered the 
first leader, where the leader 
interval is 30 ms and the delay 
between voting rounds is 
100 ms. The medical institu -
tions join the EHR network in 
regular intervals of 10 s.
2. The consensus time evaluates 
the time needed for the net -
work encompassing all medical 
institutions in the range of {3, 
5, 7, 10} to reach a consensus 
on a single value. Similar to 
the previous experiment, the 
leader interval is 30 ms and the 
delay between voting rounds 
is 100 ms. The consensus time 
is measured only after the net -
work is fully initialized with all 
participating institutions.
3. The ML training time evalu -
ates the training process of a 
convolutional neural network 
for object detection with med -
ical multimodal data from 
laparoscopic procedures19 
limited to 500 samples. The 
convolutional network has 
three layers, with a kernel size 
in the range of {32, 64, 128} and 
an accuracy of 97%. The ML 
training time also included the overhead required for transfer -
ring the trained model to the 
device where inference will be 
performed.
4. Edge accuracy evaluates the 
tradeoff between accuracy and 
training time for the afore -
mentioned convolutional 
neural network on the com -
puting continuum devices. 
This experiment compares the 
execution time for training the 
neural network with an aver -
age accuracy of 85 and 70%, 
respectively. 
5. The data transfer time mea -
sures the time needed for 
transfer of 1 MB of raw data 
between an IoT device, con -
nected to the C3 infrastructure, 
and the corresponding des -
tination resource. The trans -
fer time was measured using 
the Prometheus monitoring 
system.
Results
Figure 2 (a) shows that current consen -
sus algorithms have limited scalability, 
considering network initialization. We 
observe that initialization of the EHR 
network with 10 medical institutions 
FIGURE 2. A consensus evaluation of the STIGMA EHR system. (a) The DL T network initialization and (b) DL T consensus time.3 5 7 100100200300400
13.3
26.6
82.7
365.2
Number of Medical InstitutionsInitialization Time (s)
02040
1.9
3.2
8.3
37.3Consensus Time (s)
3 5 7 10
Number of Medical Institutions (a) (b)
 OCTOBER  2022  63can take up to 28 times more time com -
pared to the small network of three 
institutions, which limits the number 
of participating institutions in a single 
decentralized EHR system. However, the 
standard deviation ranges from 29% for 
10 participating institutions to 58% for 
three. The reason for the scalability lim -
itation is that all consensus messages 
must be relayed through a single coordi -
nator, which, although not a single point 
of failure, is a potential performance 
bottleneck. This is evident during net -
work initialization for a large number of 
institutions. However, this experiment 
proves that up to 10 medical institutions 
can be federated in a single overlay with 
minimal initialization overhead.
Furthermore, in Figure 2 (b), we ob  -
ser ve a similar trend related to the time needed to reach a consensus. The 
EHR network composed of 10 institu -
tions required nearly 19 times more 
time to reach a consensus compared to 
the small network of three institutions. 
However, we observe a much lower 
standard deviation, which ranges from 
18% for seven participating institutions 
to 31% for three. Furthermore, com -
pared to the proof-of-work approach 
implemented in the blockchain proto -
col, our approach is more efficient in 
terms or computing resources.
Figure 3 (a) evaluates the suitabil -
ity of the most commonly available 
resources for performing ML train -
ing over multimodal medical data. We 
observe that specialized devices for ML, 
such as the NJN device, are very suit -
able for performing these tasks and can be easily afforded by medical institu -
tions. In addition, available EC devices, 
extended with other resources from the 
computing continuum, can achieve very 
low model-training times, making them 
suitable for supporting decentralized 
EHR systems, especially in cases when 
the system utilization is low. The reason 
for this is that resources across the com -
puting continuum can meet the conflict -
ing requirements of EHR systems (such 
as close proximity to the data source and 
high-performance analysis) due to their 
high heterogeneity.
Figure 3 (b) evaluates the relation -
ship between accuracy of the ML 
model and execution time on the com -
puting continuum. We observe that by 
reducing the accuracy from 97 to 85%, 
we can reduce the execution time 
FIGURE 3. ML training in the STIGMA EHR system. AWS: Amazon Web Services. (a) The average training time needed to achieve 97%  
accuracy and (b) average training time needed to lower model accuracy.  RPi4
Jetson
EGS
ES Medium
ES Large
AWS c5.large
AWS m5a.xlarge02004006008001,0001,200
1,050
432
65.2
192.1
134.26
178.83
180.3
Resource Designation Resource Designation (b) (a)
Training Time (s)
02004006008001,0001,200Training Time (s)
RPi4
Jetson
EGS1,050
432
65.2467.1
232.2
33.1102.3
152.2
17.1
97% Accuracy 85% Accuracy 70% Accuracy
64 COMPUTER    WWW.COMPUTER.ORG/COMPUTER
RESEARCH FEATUREby more than 60%. Furthermore, by 
reducing the accuracy to 70%, we can 
reduce the execution time on the con -
strained devices by 90%. However, the 
tradeoff between accuracy and execu -
tion time depends on requirements of 
the EHR system and the specific med -
ical procedure. In general, this allows 
for various proximity computing 
techniques to be applied to improve 
the performance of ML training with -
out any significant accuracy penalty.
Finally, Figure 4  analyzes the net -
work performance of raw medical 
data exchanges among the different 
resources available across the comput -
ing continuum. We observe that the 
RPi4 and EGS devices can each achieve 
very low data transfer times compared 
to the CCI and FC instances, which 
could significantly reduce any com -
puting performance advantage the CCI 
alone can provide.
To unleash the potential of decen -
tralized EHR systems and their 
transparent support by IoT devices, in this article, we explored the 
need for the creation of an ecosystem 
that supports the complete lifecycle 
of medical data sharing and process -
ing. The presented approach enables 
knowledge extraction for improved 
medical diagnosis, therapy, and stigma 
reduction on top of decentralized het -
erogeneous infrastructures as a part 
of the computing continuum and IoT 
environments. We therefore identified 
critical research gaps. Based on the 
identified considerations, we defined 
concrete research and technical actions 
required for their implementation. 
Finally, we discussed the implementa -
tion of STIGMA, a conceptual, decen -
tralized EHR system as a proof of con -
cept. The system yielded promising 
results in terms of scalability, which 
indicate that up to seven different 
medical institutions can be integrated 
in a decentralized overlay, with a con -
sensus latency of 8 s or lower. In terms 
of ML learning time, we observed that 
edge devices can perform similar to 
cloud resources, and some of them, 
such as the EGS, can even reduce training time by 60% compared to 
the cloud.
Finally, based on the evaluation 
results of the conceptual STIGMA EHR 
system, we conclude that decentral -
ized ML over the computing contin -
uum for medical data analysis can be 
achieved through the utilization of 
scalable consensus algorithms over 
a permissioned DLT network with 
transparent integration of personal 
IoT devices. In the future, we plan to 
explore further how we can identify 
the optimal tradeoff between train -
ing accuracy and execution time on 
low-performance devices across the 
computing continuum. 
REFERENCES
1. S. E. Stutterheim et al. , “Patient and 
provider perspectives on HIV and 
HIV-related stigma in Dutch health 
care settings,” AIDS Patient Care 
STDs , vol. 28, no. 12, pp. 652–665, 
2014, doi: 10.1089/apc.2014.0226.
2. P. Beckman et al. , “Harnessing the 
computing continuum for program -
ming our world,” Fog Comput., Theory 
Pract. , pp. 215–230, Apr. 2020, doi: 
10.1002/9781119551713.ch7. 
3. T.-T. Kuo and L. Ohno-Machado, “Mod -
elchain: Decentralized privacy-pre -
serving healthcare predictive model -
ing framework on private blockchain 
networks,” 2018, arXiv:1802.01746 .
4. M Mettler. “Blockchain technology 
in healthcare: The revolution starts 
here,” in Proc. 2016 IEEE 18th Int. 
Conf. e-Health Netw., Appl. Services 
(Healthcom) , pp. 1–3, doi: 10.1109/
HealthCom.2016.7749510.
5. M. J. Sheller, G. Anthony Reina, B. 
Edwards, J. Martin, and S. Bakas, 
“Multi-institutional deep learning 
modeling without sharing patient 
data: A feasibility study on brain 
tumor segmentation,” in Proc. Int. FIGURE 4. The effective time for transferring 1 MB of data. AWS: Amazon Web Services.RPi4
Jetson
EGS
ES Medium
ES Large
AWS c5.large
AWS m5a.xlarge00.10.20.30.4
1× 10−2
2× 10−2
1 × 10−2
0.12
0.13
0.35
0.31Seconds per MB
 OCTOBER  2022  65MICCAI Brainlesion Workshop,  2018, 
pp. 92–104. 
6. A. Roehrs, C. A. Da Costa, and R. da 
Rosa Righi, “OmniPHR: A distrib -
uted architecture model to integrate 
personal health records,” J. Biomed. 
Inf., vol. 71, pp. 70–81, Jul. 2017, doi: 
10.1016/j.jbi.2017.05.012.
7. A. Roehrs, C. A. da Costa, R. da Rosa 
Righi, V. F. da Silva, J. R. Goldim, 
and D. C. Schmidt, “Analyzing the 
performance of a blockchain-based 
personal health record implemen -
tation,” J. Biomed. Inf. , vol. 92, pp. 
103,140, Apr. 2019, doi: 10.1016/j.
jbi.2019.103140.
8. “The GemOS System,” Gem, 2021. 
https:/ /enterprise.gem.co/gemos/ 
(Accessed: May 5, 2021).
9. S. Wang et al. , “When edge meets 
learning: Adaptive control for 
resource-constrained distributed 
machine learning,” in Proc. IEEE 
INFOCOM 2018-IEEE Conf. Comput. 
Commun. , pp. 63–71, doi: 10.1109/
INFOCOM.2018.8486403.
10. A. Kumar, S. Goyal, and M. Varma, 
“Resource-efficient machine learn -
ing in 2 kb ram for the internet 
of things,” in Proc. 34th Int. Conf. 
Mach. Learn.,  2017, vol. 70, pp. 
1935–1944.
11. S. A. Osia, A. S. Shamsabadi, A. 
Taheri, H. R. Rabiee, and H. Haddadi, 
“Private and scalable personal data 
analytics using hybrid edge-to-cloud 
deep learning,” Computer , vol. 51, 
no. 5, pp. 42–49, 2018, doi: 10.1109/
MC.2018.2381113.
12. G. Wang et al. , “Interactive medical 
image segmentation using deep 
learning with image-specific fine 
tuning,” IEEE Trans. Med. Imag. , vol. 
37, no. 7, pp. 1562–1573, 2018, doi: 
10.1109/TMI.2018.2791721.
13. T. S. Brisimi, R. Chen, T. Mela, 
A. Olshevsky, I. C. Paschalidis, and W. Shi, “Federated learning of 
 predictive models from federated 
electronic health records,” Int. J. Med. 
Inf., vol. 112, pp. 59–67, Apr. 2018, doi: 
10.1016/j.ijmedinf.2018.01.007.
14. T.-T. Kuo, H.-E. Kim, and L. Ohno-  
Machado, “Blockchain distributed 
ledger technologies for biomedical 
and health care applications,” J. 
Amer. Med. Inf. Assoc. , vol. 24, no. 
6, pp. 1211–1220, 2017, doi: 10.1093/
jamia/ocx068.
15. E. Gaetani, L. Aniello, R. Baldoni, F. 
Lombardi, A. Margheri, and V. Sas -
sone, “Blockchain-based database to 
ensure data integrity in cloud com -
puting environments,” in  Proc. Ital -
ian Conf. Cybersecurity,  2017, pp. 1–10.
16. O. Goldreich, “Secure multi-party 
computation,” Weizmann Inst. 
Sci., Rehovot, Israel, 1998. [Online]. Available: https:/ /citeseerx.ist.psu.
edu/viewdoc/download?doi=10.1.1.  
11.2201&rep=rep1&type=pdf 
17. D. Ongaro and J. K. Ousterhout, “In 
search of an understandable con -
sensus algorithm,” in Proc. USENIX 
Annu. Tech. Conf ., 2014, pp. 305–319. 
18. D. Kimovski, R. Mathá, J. Hammer, 
N. Mehran, H. Hellwagner, and R. 
Prodan, “Cloud, fog or edge: Where 
to compute?”  IEEE Internet Comput. , 
vol. 25, no. 4, pp. 30–36, 2021, doi: 
10.1109/MIC.2021.3050613.
19. A. Leibetseder, S. Kletz, K. Schoeff -
mann, S. Keckstein, and J. Keckstein, 
“GLENDA: Gynecologic laparoscopy 
endometriosis dataset,” in Proc. 
26th Int. Conf., MultiMedia Model -
ing (MMM) , Daejeon, South Korea, 
Jan. 5–8, 2020, pp. 439–450, doi: 
10.1007/978-3-030-37734-2_36.ABOUT THE AUTHORS
DRAGI KIMOVSKI is a tenure-track researcher at the Institute of Information Tech -
nology, Klagenfurt University, Klagenfurt, 9020, Austria. His research interests 
include fog and edge computing, multiobjective optimization, and distributed stor -
age. Kimovski received a Ph.D. in computer science from the Technical University 
of Sofia. Contact him at: dragi.kimovski@aau.at. 
SASKO RISTOV is a postdoctoral university assistant at the University of Inns -
bruck, Innsbruck, 6020, Austria. His research interests include performance 
modeling and optimization of parallel and distributed systems, particularly 
workflow applications and serverless computing. Ristov received a Ph.D. in 
computer science from The Saints Cyril and Methodius University of Skopje. 
Contact him at sashko@dps.uibk.ac.at. 
RADU PRODAN is a professor of distributed systems at the Institute of Informa -
tion Technology, Klagenfurt University, Klagenfurt, 9020, Austria. His research 
interests include performance and resource management tools for parallel and 
distributed systems. Prodan received a Ph.D. in computer science from the 
Vienna University of Technology. Contact him at radu.prodan@aau.at. "
2305.19894,D:\Database\arxiv\papers\2305.19894.pdf,"The paper describes a method for mitigating community bias in cross-lingual medical vision-language processing. What specific linguistic phenomenon, often observed in cross-lingual text representations, contributes to this bias?","The paper identifies that cross-lingual text representations tend to form distinct clusters based on their respective languages, leading to community bias in medical VLP.","lossLSSV, to enhance the robustness of visual representation [ 23]. More importantly, we introduce a
cross-lingual text alignment regularization, encompassing sample-level and feature-level approaches,
to mitigate community bias stemming from different languages. This regularization is supervised by
the loss LCTR. The learning objective of Med-UniC integrates the three strategies mentioned above
and can be formulated as follows:
L=LCVL+LSSV+LCTR (1)
This training scheme compels Med-UniC to assimilate information from diverse perspectives, fos-
tering a robust cross-modal, cross-lingual framework that concurrently learns visual invariants,
visual-textual invariants, and text invariants.
3.2 Cross-lingual Vision-language Alignment
Cross-lingual Medical LM To initialize Med-UniC with the ability to process different languages
and learn fundamental cross-lingual syntactic and semantic knowledge. We select CXR-BERT [ 24],
a uni-lingual LM pre-trained on a large-scale biomedical corpus, as our text encoder and further
adapted it for cross-lingual operation. The concrete steps proceed as follows: (1)Constructing a
cross-lingual vocabulary set T: we collected the radiology reports of the second language (e.g.,
Spanish PadChest dataset [ 25]), which is seldom used for medical pre-training compared to English
dataset [ 26,27]. Then we leverage a general Spanish Spacy3to build a tokenizer and make use
of TF-IDF tool4to generate Mranked new tokens Tsp=
t1
sp, t1
sp, . . . , tM
sp	
according to their
importance from multiple reports. (2)Building new wording embeddings W: we augment the
original vocabulary Tenwith the sorted Spanish tokens to generate T={Ten,Tsp}then expand
a length of Mrandom initialized vectors Wspon CXR-BERT’s word embeddings Wen, where
W= [Wen;Wsp].(3)Masked Cross-lingual Modeling: we following BERT [ 28], randomly mixed
English [ 26] and Spanish [ 25] medical reports as pre-train corpus. Then we randomly choose tokens
in the mixed sequences, replace them with the [MASK] token and set 15 %masking probability as in
BERT [ 29]. We selectively update several high layers to alleviate catastrophic forgetting [ 30] during
vision-language pre-training. More details will be show in Sec 4.5.
Vision-Language Alignment Following CLIP framework [ 31], we incorporate a contrastive learning
object to predict the matched pair (ve,i, le,i)from N×Npossible image-text pairs while mapping
N2−Nnegative pairs far apart. Specifically, two non-linear visual and linguistic projectors PlandPv
are used to transform ve,iandle,iinto the same dimension d, where ˆ ve,i=Pv(ve,i),ˆle,i=Pl(le,i),
andˆ ve,i,ˆle,i∈Rd. Obtaining image feature vectors [ˆ ve,i]N
i=1and text feature vectors [ˆle,i]N
i=1from
a training batch, we compute cosine similarities sv2l
i,i=ˆ v⊤
e,iˆle,iandsl2v
i,i=ˆl⊤
e,iˆ ve,i, representing
image-text and text-image similarities, respectively. LCVL is then formulated as follows:
Lv2l
v=−logexp(sv2l
i,i/σ1)
PK
j=1exp(sv2l
i,j/σ1),Ll2v
i=−logexp(sl2v
i,i/σ1)
PK
j=1exp(sl2v
i,j/σ1)(2)
LCVL=1
2KNX
i=1 
Lv2l
v+Ll2v
l
, (3)
where Lv2l
vandLl2v
lare image-text and text-image InforNCE [ 32] contrastive loss, respectively. σ1
is the temperature hyper-parameter set to 0.07 in our experiments, Kis the batch size for each step
andK∈N. Through overall loss LCVL, the model learns maximal mutual information between the
matched image-text pairs containing cross-lingual attributes within a batch.
3.3 Self-supervised Vision Alignment
To obtain more exhaustive visual representation, we include visual invariant learning as a parallel
objective during VLP. Drawing inspiration from [ 23], we initially apply random augmentations (such
as random cropping and flipping) to the original images to create augmented views as positive pairs
[(ve,i, v′
e,i)]N
i=1, while treating the rest of the images in the mini-batch as negative samples. We
follow the data augmentation strategy as per [ 23]. Subsequently, we extract the representations of
3https://spacy.io/models/es-dep-news-trf
4TfidfVectorizer: https://scikit-learn.org/
4
the augmented views [ˆ v′]N
i=1using the vision projector pvand vision encoder Fv, similar to [ˆ v]N
i=1.
Therefore, the visual invariant learning objective becomes:
LSSV=−1
KNX
j=1logexp(sv2v′
i,i/σ2)
PN
j=1exp(sv2v′
i,j/σ2), sv2v′
i,i=ˆ v⊤
e,iˆ v′
e,i (4)
where σ2is the temperature hyper-parameter also set to 0.07 for overall loss objective LSSV.
3.4 Cross-lingual Text Alignment Regularization
As illustrated in Fig 1, and corroborated by research in natural language processing [ 33,34], cross-
lingual text representations tend to form distinct clusters based on their respective languages. This
trend introduces a community bias within data from different language communities, even when
no explicit language attribute is provided. This suggests that VLP processes medical data based
on their language community, risking unfairness in clinical applications and potentially decreasing
downstream task performance in both uni-modal and vision-language tasks [ 35–37]. To mitigate this
bias and potential risks, we introduce Cross-lingual Text Alignment Regularization (CTR) to learn
language-independent text representations and neutralize the adverse effects of community bias on
other modalities. CTR comprises three components:
Text augmentation We first adopt the dropout strategy [ 38] to generate augmented the text repre-
sentation l′
e,ifrom the text encoder and obtain the matched pairs [(le,i,l′
e,i)]N
i=1, and then a separate
linguistic projector Pddesigned for de-correlation is leveraged to generate two different view pairs
[(ZAe,i,ZBe,i)]N
i=1, where ZAe,i=Pd(le,i),ZBe,i=Pd(l′
e,i)), and the new feature dimension
D′> D .
Text-feature alignment To further alleviate information redundancy [ 39,40] and obtain the shared
cross-lingual text representation, we first normalize the augmented embedding pairs
ZA
e,ZB
e	
∈
RN×D′along the batch Kdimension so that each feature dimension has a zero-mean and 1/√
K
standard deviation distribution to generate ˜Ze, and then compute their cross-correlation ˜Zcorr
e=
˜ZAT
e˜ZB
e. The formulas of feature-dimension decorrelation can be defined as:
LTF=1
D′

D′X
j 
1−KX
i˜ZA,jT
e,i˜ZB,j
e,i!2
| {z }
cross-lingual invariance+λD′X
jKX
i̸=j˜ZA,jT
e,i˜ZB,j
e,i
| {z }
cross-lingual gap reduction

,˜Ze=Ze−µK(Ze)√
Kσ(Ze)
(5)
The first term’s objective is to learn a language-invariant representation by optimizing the diagonal
elements of the correlation matrix to equal one. Simultaneously, the second term aims to shrink
the cross-lingual gap and optimize information utilization in each latent dimension by driving the
off-diagonal elements towards zero. Finally, We normalize the loss along with the feature dimension
D′.
Text-to-text alignment : Similarly, the text-to-text alignment decorrelates the cross-correlation matrix
along with feature dimension D′, andˆZeis the normalized embeddings, ˆZcorr
e=ˆZA
eˆZBT
e:
LTT=1
K

KX
j
1−D′X
iˆZA,j
e,iˆZB,j,T
e,i
2
+
| {z }
text instance alignmentλKX
jD′X
i̸=jˆZA,j
e,iˆZB,j,T
e,i
| {z }
text consistency regularizer

,ˆZe=Ze−µD′(Ze)√
D′σ(Ze)
(6)
where the text instance alignment term attempts to maximize the mutual information of a batch
of cross-lingual text samples, and the text consistency regularizer can also be deemed as the text
in-modal consistency [ 41] by reducing the mismatched text pairs into 0 in a batch K. Where λin Eq
5, 6, is a non-negative hyperparameter trading off two terms. We also normalize the loss with the
batch dimension K. Therefore, the loss of Cross-lingual Text Alignment Regularization LCTR is:
LCTR =LTF+LTT (7)
5
4 Experiments
4.1 Pre-training Configuration
Dataset We pre-train Med-UniC framework using MIMIC-CXR [ 42], which contains CXR images
and their corresponding radiology reports in English. Also, we involve PadChest [ 43], which includes
CXR images and their corresponding radiology reports collected in Valencia region, Spain. Both
datasets are pre-processed following the approach described in [ 5–7], including image resizing, pixel
value normalization, and text tokenization. Additionally, the dataset is filtered by excluding lateral
views and reports with less than three tokens. This results in a pre-training dataset of approximately
220kimage-text pairs for MIMIC-CXR [42] and 160kpairs for PadChest [43].
Implementation In the VLP stage, we employ ResNet-50 [ 44] and ViT [ 45] as the vision backbones.
We report the linear classification results for these two vision encoders to illustrate the model-agnostic
capabilities of Med-UniC. Med-UniC is trained over 50 epochs using an early stop strategy on 16
V100 GPUs with a batch size of 128 per GPU. We utilize AdamW [ 46] as the optimizer, setting the
learning rate to 4e−5and the weight decay to 5e−2. A linear warm-up and cosine annealing scheduler
are also deployed in this process. Additionally, The coefficients λis set to 5.1e−3following [40].
4.2 Downstream Tasks
Medical Image Linear Classification We perform this task on CheXpert [ 27], RSNA [ 47], and
COVIDx [ 48] datasets. Following the previous work [ 5–7], we only update the parameter of a random
initialized linear layer for classification and freeze the pre-trained vision backbone. We report the
AUC scores (AUC) on CheXpert and RSNA and accuracy (ACC) on COVIDx as the evaluation
metric following [6, 7].
Medical Image Zero-shot Classification We conduct this experiment on the CXP500 [ 49] and
PDC [ 43] datasets, which comprise CXR images annotated by clinicians from English-speaking and
Spanish-speaking countries, respectively. To circumvent prompt bias, we designate English positive
prompt as ‘{ disease }’ and negative prompt as ‘No { disease }’. Prompts in Spanish are prepared by
native Spanish speakers, with the disease indicated as ‘{ disease }’ and the negative prompt represented
as ‘No hay { disease }’. Med-UniC is evaluated using both English and Spanish prompts across the two
datasets, with additional experimental details provided in the Appendix. The results are represented
as the macro average of AUC and F1 scores across all categories.
Medical Image Semantic Segmentation This task is performed on the RSNA [ 47] and the
SIIM [ 50] datasets, following the data preprocessing in [ 6,7]. Identical to [ 6,7], the U-Net [ 51]
fine-tuning settings are adopted for segmentation. All pre-trained vision backbones are considered as
frozen encoders, and only the decoders of U-Net are updated during the fine-tuning. The segmentation
performance is evaluated using Dice scores (Dice).
Medical Image Object Detection This task is performed on the RSNA [ 47] and Object-CXR [ 52]
datasets, following the same preprocessing of [ 7]. Same as [ 7], we utilize YOLOv3 [ 53] as the
detection architecture, using our pre-trained vision encoder as the backbone and only updating the
detection head during fine-tuning. Mean Average Precision (mAP) with IOU thresholds 0.4 ∼0.75, is
adopted to evaluate the detection task.
For all downstream tasks, except zero-shot classification, we fine-tune with 1%,10%,100% of the
training data. More downstream tasks’ settings, including split information and train/valid/test set
details, can be found in the Appendix.
4.3 Comparison to the state-of-the-art
Zero-shot Classification To assess the cross-lingual visual-textual representation learned from Med-
UniC, we implement the zero-shot image classification task on two CXR datasets, which originate
from distinct linguistic communities and utilize different language prompts. Tab 1 illustrates that
Med-UniC surpasses all SOTA methods on both datasets, regardless of the language setting or the
linguistic community data source. Across both datasets, Med-UniC delivers an average increase
of over 20% in the F1 score when using English prompts and more than 15% when using Spanish
6
Table 1: Zero-shot Image Classification results. F1 and AUC scores are reported. Best results of
each setting are in boldface. ‘En’ and ‘Sp’ respectively stand for prompts in English and Spanish
languages. Methods with ⋆leverage disease-level annotations for pre-training.
CXP500(En) CXP500(Sp) PDC(En) PDC(Sp)
Method AUC F1 AUC F1 AUC F1 AUC F1
ConVIRT[5] 59.5 19.2 60.5 15.8 45.1 26.5 49.1 12.6
GLoRIA[6] 43.2 2.4 40.2 16.1 52.3 10.1 50.3 8.2
GLoRIA-MIMIC [6] 46.2 5.5 51.5 20.3 53.1 12.1 52.2 11.3
MGCA⋆[7] 72.1 6.5 50.4 22.3 46.4 32.5 49.8 26.1
MedKILP⋆[8] 70.5 14.7 55.6 21.9 50.5 28.7 51.7 25.8
MRM [9] 65.2 10.4 48.3 16.1 50.1 24.6 51.4 25.1
Ours 75.4 30.3 71.3 32.2 72.9 42.6 71.4 37.1
prompts. This showcases the effectiveness and adaptability of Med-UniC in managing cross-lingual
vision-language tasks.
Interestingly, the AUC score of other SOTA methods experiences a substantial drop when the prompts
transition from English to Spanish on CXP500 [ 49], a dataset collected from English-speaking
communities. Similarly, all compared methods show comparably poor performance on PDC [ 43],
a dataset derived from Spanish-speaking communities. MedKLIP [ 8], despite its commendable
performance on the CXP500 [ 49] in the English setting due to its supervised pre-training with disease
annotation, persistently shows a drop in performance on both the CXP500 [49] and PDC [43] when
Spanish prompts are used, and also on the PDC [ 43] when using English prompts. These results
highlight the significant community bias inherent in uni-lingual medical VLP models, even those
utilizing annotations for pre-training. This bias adversely impacts the diagnostic quality when dealing
with patients from non-English-speaking communities or those who do not speak English.
The unsatisfied performance of the compared methods also suggests that these models might in-
corporate linguistic community attributes during VLP, which negatively influences the learning of
semantic meanings. Consequently, As a result, these models have difficulties in effectively interpret-
ing CXR scans or prompts from non-English communities, which substantially limits the models’
transferability. Further analysis can be found in Sec 4.4.
Table 2: Linear classification results for CheXpert, RSNA, and COVIDx datasets with 1%, 10%,
and 100% training data. The best results are highlighted in bold. A standard ResNet-50 backbone is
denoted as CNN-based . Methods with ⋆leverage disease-level annotations for pre-training.
CheXpert (AUC) RSNA (AUC) COVIDx (ACC)
Method 1% 10% 100% 1% 10% 100% 1% 10% 100%
Random Init 56.1 62.6 65.7 58.9 69.4 74.1 50.5 60.3 70.0
ImageNet Init 74.4 79.7 81.4 74.9 74.5 76.3 64.8 78.8 86.3
CNN-based
GLoRIA [6] 86.6 87.8 88.1 86.1 88.0 88.6 67.3 77.8 89.0
ConVIRT [5] 85.9 86.8 87.3 77.4 80.1 81.3 72.5 82.5 92.0
GLoRIA-MIMIC [6] 87.1 88.7 88.0 87.0 89.4 90.2 66.5 80.5 88.8
MedKLIP⋆[8] 86.2 86.5 87.7 87.3 88.0 89.3 74.5 85.2 90.3
MGCA⋆[7] 87.6 88.0 88.2 88.6 89.1 89.9 72.0 83.5 90.5
Med-UniC (ResNet-50) 88.2 89.2 89.5 89.1 90.4 90.8 76.5 89.0 92.8
ViT-based
MRM [9] 88.5 88.5 88.7 91.3 92.7 93.3 66.9 79.3 90.8
MGCA⋆(ViT-B/16) [7] 88.8 89.1 89.7 89.1 89.9 90.8 74.8 84.8 92.3
Med-UniC (ViT-B/16) 89.4 89.7 90.8 91.9 93.1 93.7 80.3 89.5 94.5
Med-UniC (ViT-L/32) 89.9 90.5 91.2 92.2 93.8 94.5 81.5 91.8 95.2
Medical Image Linear Classification To assess the quality of the visual representations learned by
Med-UniC, we employ linear classification [ 54] on CheXpert [ 27], RSNA [ 47], and COVIDx [ 48]. As
illustrated in Tab 2, Med-UniC framework consistently surpasses all uni-lingual pre-trained baselines
across various settings and vision backbones. Significantly, even when MedKLIP [ 8] employs
supervised VLP with disease-level annotation, Med-UniC consistently surpasses MedKLIP [ 8] across
all tasks and settings. This exemplifies the adaptability and efficacy of the visual representations
7
Table 3: Results of semantic segmentation on SIIM and RSNA datasets and object detection on
RSNA and Object-CXR datasets. The best results for each setting are highlighted in bold, and the ’-’
denotes mAP values smaller than 1%. Methods with ⋆leverage disease-level annotations.
Semantic Segmentation (Dice) Object Detection (mAP)
SIIM RSNA RSNA Object CXR
Method 1% 10% 100% 1% 10% 100% 1% 10% 100% 1% 10% 100%
Random 9.0 28.6 54.3 6.9 10.6 18.5 1.0 4.0 8.9 - 0.5 4.4
ImageNet 10.2 35.5 63.5 34.8 39.9 64.0 3.6 8.0 15.7 - 2.9 8.3
ConVIRT[5] 25.0 43.2 59.9 55.0 67.4 67.5 8.2 15.6 17.9 - 8.6 15.9
GLoRA[6] 35.8 46.9 63.4 59.3 67.5 67.8 9.8 14.8 18.8 - 10.6 15.6
GLoRIA-MIMIC [6] 37.4 57.1 64.0 60.3 68.7 68.3 11.6 16.1 24.8 - 8.90 16.6
MGCA⋆[7] 49.7 59.3 64.2 63.0 68.3 69.8 12.9 16.8 24.9 - 12.1 19.2
MedKLIP⋆[8] 50.2 60.8 63.9 66.2 69.4 71.9 8.9 16.3 24.5 - 7.1 11.6
Ours 56.7 62.2 64.4 72.6 74.4 76.7 16.6 22.3 31.1 6.6 13.3 21.6
Table 4: Ablation study of Med-UniC on linear classification, semantic segmentation and zero-shot
classification. The best results of each setting are in boldface.
Learning Objective COVIDx (ACC) RSNA (AUC) SIIM (Dice) CXP500 (F1) PDC (F1)
SSV CVL CTR MLM 1% 10% 100% 1% 10% 100% 1% 10% 100% En Sp En Sp
✓ ✓ 72.8 85.5 91.8 87.7 88.5 89.4 51.9 56.5 58.7 63.5 59.8 62.2 58.5
✓ ✓ ✓ 74.5 85.8 92.2 88.1 89.3 89.9 53.4 58.7 60.1 68.5 62.1 64.6 61.7
✓ ✓ ✓ 75.0 84.3 92.5 88.2 89.6 89.7 53.8 59.6 61.5 70.3 65.9 65.4 63.7
✓ ✓ ✓ ✓ 76.5 89.0 92.8 89.1 90.4 90.8 56.7 62.2 64.4 75.4 71.3 72.9 71.4
cultivated by Med-UniC. Furthermore, this implies that unifying cross-lingual text representations
via CTR can also improve the performance of uni-modal visual tasks.
Medical Image Semantic Segmentation and Object Detection In Tab 3, we further assessed the
representation acquired by Med-UniC on segmentation and detection tasks. Impressively, Med-UniC
outperforms all SOTA methods across every data fraction in all four tasks. Notably, Med-UniC
achieves a Dice score of 72.6% on RSNA segmentation with only 1% data fine-tuning, exceeding the
performance of all other SOTA methods fine-tuned on 100% data. Furthermore, Med-UniC reaches
a 6.6% mAP on the Object-CXR dataset using just 1% data fine-tuning, surpassing other methods
that barely achieve a 1% mAP. These findings further highlight the advantageous effects of unifying
cross-lingual representations on vision-language tasks and uni-modal visual tasks.
4.4 Ablation Study and Bias Analysis
In this section, we ablate components of Med-UniC and present their performance on
linear classification, zero-shot image classification, and semantic segmentation in Table 4.
(a)
 (b)
Figure 4: Visualization of image embed-
dings with or without CTR.In all tasks, the combinations of all learning objectives
achieve the highest performance, highlighting each com-
ponent’s crucial role in Med-UniC. Med-UniC, when in-
tegrated with CTR, significantly outperforms the version
with MLM in zero-shot tasks and nearly all uni-modal
visual tasks.
To further investigate the influence of CTR and MLM, we
randomly select 1000 English and 1000 Spanish image-
text pairs. We then illustrate the text and image embed-
dings in Fig 1 and Fig 4, respectively. Surprisingly, when
the text encoder is pre-trained with MLM, we notice that
the medical report embeddings tend to cluster by their re-
spective languages, as shown in Fig 1a,1b. However, when
employing CTR, the embeddings from diverse language
reports draw nearer, leading to a reduction in the distance between the two clusters compared to the
embeddings produced by MLM, as shown in Fig 1c,1d. This clear pattern illustrates CTR’s ability to
unify cross-lingual text representation within the latent space. Intriguingly, when pre-trained with
CTR, the image embeddings become less distinguishable by their language community. In contrast,
they form separate clusters according to their language community when pre-training does not involve
CTR. This observation implies that community bias affects not only the text modality but also the
8
visual modality, as shown in Fig 4. Consequently, the reduction in community bias contributes to
superior performance across all tasks when Med-UniC is pre-trained with all learning objectives.
More details can be found in the Appendix.
4.5 Further Analysis
Visualization of Language-agnostic Visual-textual Attention We utilize Grad-CAM [ 55] to
illustrate the areas in CXR images corresponding to various disease terminologies in both English and
Spanish, as shown in Fig 5. Evidently, Med-UniC can accurately capture relevant regions that align
closely with the indicated disease, exhibiting robustness across various languages. Consequently,
the consistent cross-lingual visual-textual representations cultivated by Med-UniC underscore its
outstanding generalizability across multiple downstream tasks and language communities.
Ground Truth   Cardiomegaly        Cardiomegalia
Ground Truth Pleural Effusion      Derrame pleural       Edema                       Edem
 Ground Truth
Ground Truth  Atelectasis               Atelectasi
Figure 5: Attention heat-maps of the visual-textual association learned by Med-UniC, compared with
ground truth annotations provided by clinicians. The blue and orange words denote Spanish and
English types, respectively.
Table 5: Comparison with large vision model.
CheXpert RSNA COVIDx
Method 10% 100% 10% 100% 10% 100%
DINOv2 [56] 81.6 83.2 84.5 86.3 85.0 92.2
Med-UniC(ViT-B/16) 89.7 90.8 93.1 93.7 89.5 94.5
Med-UniC(ViT-L/32) 90.5 91.2 93.8 94.5 91.8 95.2Table 6: Results of Med-UniC with dif-
ferent numbers of frozen layers.
COVIDx (AUC) RSNA (Dice)
Frozen layers 10% 100% 10% 100%
0 87.7 92.5 75.0 75.8
6 88.0 92.3 74.3 76.3
9 89.0 92.8 74.4 76.7
12 87.1 90.5 72.3 74.4
Comparison with Large Vision Model In a comparison with DINOv2 [ 56], a large vision model
trained on 1.2 billion images and comprising 1 billion parameters, Med-UniC outperforms it in linear
classification tasks across all datasets, using different data ratios and two ViT backbones, as shown in
Tab 5. This demonstrates the remarkable effectiveness of Med-UniC, even in scenarios with limited
domain-specific data.
Impact of Frozen Layers for LM To investigate the impact of freezing layers in LM, we experi-
mented with freezing various numbers (0, 6, 9, 12) in a 12-layer LM. Tab 6 shows that updating the
last three layers offers better results comparable to updating more layers, hinting that updating more
might cause catastrophic forgetting [ 30,57] of cross-lingual MLM-acquired semantics. Performance
declined when all layers were frozen, indicating the necessity of properly updating layers.
Error Bars We conducted three separate runs of Med-UniC, using different random seeds and
ResNet-50 as the vision backbone for three tasks. We then reported the average metric and its standard
deviation. As indicated in Tab 7, the variability between different task runs is relatively minor,
demonstrating that Med-UniC consistently performs well in a variety of experiment configurations.
Table 7: Error bars analysis of linear classification, semantic segmentation, and object detection .
Ratio COVIDx(ACC) RSNA(Dice) Object CXR(mAP)
1% 76.54 ±0.32 72.60 ±0.33 6.62 ±0.67
10% 89.01 ±0.16 74.43 ±0.19 13.34 ±0.27
100% 92.82 ±0.35 76.67 ±0.45 21.63 ±0.24
9
5 Conclusion
This work is the first to identify community bias in medical VLP stemming from diverse language
communities and illustrates its negative impact on various downstream tasks. We present Med-UniC,
a novel cross-lingual medical VLP framework, along with CTR, intended to unify cross-lingual
text representations and mitigate language-driven community bias. This bias impacts both text
and visual modalities, thereby affecting performance across vision-language and uni-modal visual
tasks. The superior performance of Med-UniC across various tasks and data ratios underscores its
efficiency and effectiveness. Through comprehensive ablation studies, we show that CTR significantly
enhances performance in both vision-language and uni-modal visual tasks by effectively reducing
community bias. This study provides a robust cross-lingual medical VLP framework and emphasizes
the importance of inclusivity beyond English-speaking communities.
References
[1]M. Ni, H. Huang, L. Su, E. Cui, T. Bharti, L. Wang, D. Zhang, and N. Duan, “M3p: Learning universal
representations via multitask multilingual multimodal pre-training,” in Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition , 2021, pp. 3977–3986.
[2]M. Zhou, L. Zhou, S. Wang, Y . Cheng, L. Li, Z. Yu, and J. Liu, “Uc2: Universal cross-lingual cross-modal
vision-and-language pre-training,” in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2021, pp. 4155–4165.
[3]A. Jain, M. Guo, K. Srinivasan, T. Chen, S. Kudugunta, C. Jia, Y . Yang, and J. Baldridge, “MURAL:
Multimodal, multitask representations across languages,” in Findings of the Association for Computational
Linguistics: EMNLP 2021 , Nov. 2021.
[4]Y . Zeng, W. Zhou, A. Luo, and X. Zhang, “Cross-view language modeling: Towards unified cross-lingual
cross-modal pre-training,” arXiv preprint arXiv:2206.00621 , 2022.
[5]Y . Zhang, H. Jiang, Y . Miura, C. D. Manning, and C. P. Langlotz, “Contrastive learning of medical visual
representations from paired images and text,” arXiv preprint arXiv:2010.00747 , 2020.
[6]S.-C. Huang, L. Shen, M. P. Lungren, and S. Yeung, “Gloria: A multimodal global-local representation
learning framework for label-efficient medical image recognition,” in Proceedings of the IEEE/CVF
International Conference on Computer Vision , 2021, pp. 3942–3951.
[7]F. Wang, Y . Zhou, S. Wang, V . Vardhanabhuti, and L. Yu, “Multi-granularity cross-modal alignment for
generalized medical visual representation learning,” arXiv preprint arXiv:2210.06044 , 2022.
[8]C. Wu, X. Zhang, Y . Zhang, Y . Wang, and W. Xie, “Medklip: Medical knowledge enhanced language-image
pre-training,” medRxiv , pp. 2023–01, 2023.
[9]H.-Y . Zhou, C. Lian, L. Wang, and Y . Yu, “Advancing radiograph representation learning with masked
record modeling,” in The Eleventh International Conference on Learning Representations .
[10] S. Long, F. Cao, S. C. Han, and H. Yang, “Vision-and-language pretrained models: A survey.”
[11] Z. Wan, X. Wang et al. , “Efficient large language models: A survey,” arXiv preprint arXiv:2312.03863 ,
2023.
[12] Z. Wan, “Text classification: A perspective of deep learning methods,” arXiv preprint arXiv:2309.13761 ,
2023.
[13] X. Wang, Z. Wan, A. Hekmati, M. Zong, S. Alam, M. Zhang, and B. Krishnamachari, “Iot in the era of
generative ai: Vision and challenges,” arXiv preprint arXiv:2401.01923 , 2024.
[14] Z. Wan, X. Wang, C. Liu, S. Alam, Y . Zheng, J. Liu, Z. Qu, S. Yan, Y . Zhu, Q. Zhang, M. Chowdhury, and
M. Zhang, “Efficient large language models: A survey,” 2024.
[15] OpenAI, “Gpt-4 technical report,” ArXiv , vol. abs/2303.08774, 2023.
[16] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg,
W.-Y . Lo et al. , “Segment anything,” arXiv preprint arXiv:2304.02643 , 2023.
[17] M. Oquab, T. Darcet, T. Moutakanni, H. V o, M. Szafraniec, V . Khalidov, P. Fernandez, D. Haziza,
F. Massa, A. El-Nouby et al. , “Dinov2: Learning robust visual features without supervision,” arXiv preprint
arXiv:2304.07193 , 2023.
10
[18] A. Gilson, C. Safranek, T. Huang, V . Socrates, L. Chi, R. A. Taylor, and D. Chartash, “How well does
chatgpt do when taking the medical licensing exams? the implications of large language models for medical
education and knowledge assessment,” medRxiv , pp. 2022–12, 2022.
[19] T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon, C. Elepaño, M. Madriaga, R. Aggabao,
G. Diaz-Candido, J. Maningo et al. , “Performance of chatgpt on usmle: Potential for ai-assisted medical
education using large language models,” PLoS digital health , vol. 2, no. 2, p. e0000198, 2023.
[20] Y . Huang, X. Yang, L. Liu, H. Zhou, A. Chang, X. Zhou, R. Chen, J. Yu, J. Chen, C. Chen et al. , “Segment
anything model for medical images?” arXiv preprint arXiv:2304.14660 , 2023.
[21] W. Ji, J. Li, Q. Bi, W. Li, and L. Cheng, “Segment anything is not always perfect: An investigation of sam
on different real-world applications,” arXiv preprint arXiv:2304.05750 , 2023.
[22] S. He, R. Bao, J. Li, P. E. Grant, and Y . Ou, “Accuracy of segment-anything model (sam) in medical image
segmentation tasks,” arXiv preprint arXiv:2304.09324 , 2023.
[23] N. Mu, A. Kirillov, D. Wagner, and S. Xie, “Slip: Self-supervision meets language-image pre-training,” in
European Conference on Computer Vision . Springer, 2022, pp. 529–544.
[24] B. Boecking, N. Usuyama, S. Bannur, D. C. de Castro, A. Schwaighofer, S. L. Hyland, M. T. A. Wetscherek,
T. Naumann, A. Nori, J. Alvarez-Valle, H. Poon, and O. Oktay, “Making the most of text semantics to
improve biomedical vision-language processing,” ArXiv , ECCV.
[25] A. Bustos, A. Pertusa, J. M. Salinas, and M. de la Iglesia-Vayá, “Padchest: A large chest x-ray image
dataset with multi-label annotated reports,” Medical image analysis , vol. 66, p. 101797, 2019.
[26] A. E. W. Johnson, T. J. Pollard, S. J. Berkowitz, N. R. Greenbaum, M. P. Lungren, C. ying Deng, R. G.
Mark, and S. Horng, “Mimic-cxr, a de-identified publicly available database of chest radiographs with
free-text reports,” Scientific Data , vol. 6, 2019.
[27] J. Irvin, P. Rajpurkar, M. Ko, Y . Yu, S. Ciurea-Ilcus, C. Chute, H. Marklund, B. Haghgoo, R. Ball,
K. Shpanskaya et al. , “Chexpert: A large chest radiograph dataset with uncertainty labels and expert
comparison,” in Proceedings of the AAAI conference on artificial intelligence , vol. 33, 2019, pp. 590–597.
[28] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers
for language understanding,” ArXiv , vol. abs/1810.04805, 2019.
[29] ——, “Bert: Pre-training of deep bidirectional transformers for language understanding,” arXiv preprint
arXiv:1810.04805 , 2018.
[30] I. J. Goodfellow, M. Mirza, X. Da, A. C. Courville, and Y . Bengio, “An empirical investigation of
catastrophic forgeting in gradient-based neural networks,” CoRR , vol. abs/1312.6211, 2013.
[31] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,
J. Clark et al. , “Learning transferable visual models from natural language supervision,” in International
Conference on Machine Learning . PMLR, 2021, pp. 8748–8763.
[32] A. van den Oord, Y . Li, and O. Vinyals, “Representation learning with contrastive predictive coding,”
ArXiv , vol. abs/1807.03748, 2018.
[33] J. Singh, B. McCann, R. Socher, and C. Xiong, “Bert is not an interlingua and the bias of tokenization,” in
Conference on Empirical Methods in Natural Language Processing , 2019.
[34] R. Xian, H. Ji, and H. Zhao, “Cross-lingual transfer with class-weighted language-invariant representations,”
inInternational Conference on Learning Representations , 2022.
[35] A. Lauscher, V . Ravishankar, I. Vulic, and G. Glavas, “From zero to hero: On the limitations of zero-
shot language transfer with multilingual transformers,” in Conference on Empirical Methods in Natural
Language Processing , 2020.
[36] K. K, Z. Wang, S. Mayhew, and D. Roth, “Cross-lingual ability of multilingual bert: An empirical study,”
International Conference on Learning Representations , vol. abs/1912.07840, 2020.
[37] P. Dufter and H. Schütze, “Identifying elements essential for bert’s multilinguality,” in Conference on
Empirical Methods in Natural Language Processing , 2020.
[38] J. Zhang and Z. Lan, “S-simcse: Sampled sub-networks for contrastive learning of sentence embedding,”
Conference on Empirical Methods in Natural Language , 2021.
11"
2402.12847,D:\Database\arxiv\papers\2402.12847.pdf,"How does the process of acquiring factual knowledge from documents differ from learning general skills in language models, and what implications does this difference have for training strategies?","Acquiring factual knowledge from documents requires exhaustive loss minimization to minimize perplexity, unlike learning general skills where over-optimization can lead to overfitting. This suggests that training strategies for factual knowledge acquisition should prioritize minimizing perplexity on the documents.","test doctrain QAtrain doctest docQA instruction-tuningcontinued pre-trainingtrain doctest doctrain QAtrain doctrain QAtest doctrain doctrain QAtest doctrain QAQA pre-instruction-tuningtest doctrain QAᬅtest doctrain QAtrain doctrain doctest doctrain QAtest doccontinued pre-trainingstandard instruction-tuningstandard instruction-tuning (w/o forgetting)mix trainingpre-instruction-tuning (QA only)pre-instruction-tuning (QA and docs sequentially)pre-instruction-tuningpre-instruction-tuning++ᬆᬇᬈᬉᬊᬋᬌFigure 4: Different experimental settings examined in this paper. Each row represents a different experimental
setting with a unique name and number, and each vertical section highlighted by a right-pointing light-blue triangle
indicates a training phase. Models are assessed on test QA across all settings. Whenever multiple datasets are
enclosed within a dashed square, they are mixed together during the training process.
(Kwiatkowski et al., 2019). To assess longer re-
sponses and accommodate minor lexical differ-
ences, we also report answer recall and ROUGE-L.
Details can be found in Appendix C.
4 How Much Knowledge Can LLMs
Absorb via Continued Pre-training
Followed by Instruction-tuning?
Factual knowledge stored in the parameters of
LLMs can be accessed and applied to answering
questions through prompting without additional
training (Brown et al., 2020; Petroni et al., 2019;
Jiang et al., 2020; Roberts et al., 2020). With addi-
tional instruction-tuning (also known as supervised
fine-tuning) on high-quality data (Sanh et al., 2022;
Wei et al., 2022), knowledge seems to be more
effectively elicited from LLMs. However, when
LLMs correctly answer a question, the source of
the knowledge is unclear due to the diversity of the
pre-training data. For instance, when answering the
question “where is the world’s largest ice sheet lo-
cated”, do LLMs derive their response by recalling
and generalizing information from a seen document
about the Antarctic ice sheet, or do they merely re-
peat answers from similar questions encountered
in the training data? This distinction is crucial, as
the former scenario implies an ability to compre-
hend documents and effectively store knowledge
within parameters in a way that can be elicited later,
whereas the latter is mere rote memorization.
Several works have studied this problem and the
predominant finding is that LMs struggle to answer
questions about documents they have been trained
on (Wang et al., 2021; Zhu and Li, 2023a). It is
important to note, however, that these experiments
were mainly conducted using relatively small LMssuch as BART, T5, or GPT-2 (Wang et al., 2021;
Jang et al., 2022; Hu et al., 2023), using randomly
initialized transformers (Zhu and Li, 2023a), or
without instruction-tuning (Ovadia et al., 2023).
This makes us wonder what are the actual limits
of modern LLMs to absorb knowledge from new
documents and answer questions about them using
the standard continued pre-training followed by
instruction-tuning recipe . In this section, we run
extensive experiments using Llama-2 7B and 70B
onWiki2023-film to test their limits.
4.1 Vanilla Continued Pre-training and
Instruction-tuning
Experimental settings We experiment with two
standard settings and assess their performance by
answering associated questions.
•Continued pre-training: train on test documents
without instruction-tuning (Fig. 4 ➀).6
•Standard instruction-tuning: train on both train
and test documents before instruction-tuning on
train QA pairs (Fig. 4 ➁).
We perform instruction-tuning for a single epoch
since more epochs usually result in diminished per-
formance. For training on documents, we opt for
multiple epochs (10/5 for a 7B/70B model), which
allows for effective knowledge acquisition and re-
mains affordable for corpora of moderate sizes.
Experimental results As shown in Tab. 1, the
relatively low performance of the original Llama-
2 model (9.5%/17.2% for 7B/70B) indicates that
6We found that LLMs struggle to adhere to the QA format
after training on raw documents for multiple epochs. There-
fore, we include a small set of QA pairs (64) during continued
pre-training to prevent LLMs from forgetting the QA format.
1.02.03.04.0
 0 51015202530
0 10 20 30 40 50
#epochs10203040
QA acc. after cont. pre-training (%)
QA acc. after cont. pre-training & instruction-tuning (%)
Perplexity on the documents
Knowledge retention accuracy (%)
9.520.327.630.631.533.0 33.3
23.130.332.834.536.137.6
minimized perplexity(a) Training dynamics w/ (Fig. 4 ➁) and w/o instruction-tuning
(Fig. 4 ➀). Reduction in perplexity consistently leads to im-
provement in QA accuracy, indicating that factual knowledge
acquisition necessitates exhaustive loss minimization.
1.02.03.04.0
 0 51015202530
0 10 20 30 40 50
#epochs10203040
QA accuracy (%) lr=1e-5 
Perplexity on the docs lr=1e-5 
Knowledge retention (%) lr=1e-5 3e-5 
3e-5 
3e-5 5e-5
5e-5
5e-5
21.325.226.331.533.0 33.3
27.830.9 31.3
minimized
perplexity(b) Training dynamics with different learning rates (Fig. 4 ➀).
After perplexity is minimized, larger learning rates usually lead
to less overfitting to deceptive patterns in documents and better
generalization when responding to questions.
Figure 5: We vary the number of epochs (Fig. 5(a)) and learning rate (Fig. 5(b)) during continued pre-training to
study the training dynamics of Llama-2 7B. The left axis is QA accuracies for test questions, measured by exact
match. On the right axis, we display 2 metrics indicated by distinct colors: the perplexity of all tokens in the
documents, and the knowledge retention accuracy, measured by QA accuracy on the Natural Questions dataset. We
highlight situations where perplexity of all document tokens is minimized to 1 .
most knowledge in the test documents is not in-
cluded in the original pre-training corpus. Af-
ter continued pre-training on documents, perfor-
mances increase to 27.2%/41.7%, indicating that
LLMs can absorb some amount of knowledge.
Instruction-tuning further increases the perfor-
mance to 30.3%/46.4%, confirming the effective-
ness of this standard recipe. This observation is
different from Zhu and Li (2023a), which demon-
strates that instruction-tuning after pre-training is
ineffective on a randomly initialized GPT-2-like
transformer. The difference probably arises be-
cause Llama-2, through its pre-training on diverse
corpora comprising raw documents and QA data,
has developed a certain degree of proficiency in
extracting knowledge from its parameters via ques-
tions. We also report the performance where the
corresponding document is directly provided to
Llama-2 as context (“open-book w/ doc” in Tab. 1).
The significant gap between closed-book and open-
book settings suggests that retrieving knowledge
from the parameters of LLMs is still challenging.
4.2 Analyzing the Training Dynamics:
Perplexity and Generalization
How does lower perplexity of documents lead to
generalization to answering related questions? We
vary the number of epochs (Fig. 5(a)) and learn-
ing rate (Fig. 5(b)) for continued pre-training on
documents and monitor three metrics to study thetraining dynamics.7
•Knowledge acquisition QA accuracies on test
questions measured by exact match.
•Perplexity of documents We compute perplexity
(PPL) on all tokens within the documents.
•Knowledge retention We approximate the re-
tention of accumulated knowledge during pre-
training by assessing the QA accuracy on the Nat-
ural Questions (NQ) dataset. NQ was released in
2019, and primarily includes questions based on
Wikipedia articles from that time.
Experiment results
•As shown in Fig. 5(a), QA accuracy consistently
improves as perplexity approaches one, indi-
cating that factual knowledge learning necessi-
tates exhaustive loss minimization over all tokens .
This contrasts with learning general skills, where
overly optimizing leads to overfitting.
•As shown in Fig. 5(a) and Fig. 5(b), among all
cases where LLMs have minimized perplexity on
documents, for reasonably small learning rates
(5e-5 is too large and leads to overfitting), cases
trained with more epochs or larger learning rates
typically exhibit superior QA performance. We
7Since we always decay the learning rate to 10% of its
initial value, training for more epochs is not the same as con-
tinuing training from a checkpoint obtained after fewer epochs.
Llama-2 7B Llama-2 70B
Settings EM Rec. R-L EM Rec. R-L
closed- and open-book performance before training
closed-book 9.5 10.0 21.2 17.2 18.1 31.4
open-book w/ doc 72.2 75.4 91.5 78.2 80.6 94.9
closed-book performance w/ standard methods
cont. pre-training ➀ 27.6 31.6 43.8 41.7 45.8 60.2
+instruction-tuning ➁30.3 34.7 47.4 46.4 50.9 64.1
mix all data ➃ 39.4 44.6 56.7 57.1 63.4 72.4
closed-book performance w/ pre-instruction-tuning (PIT)
PIT (QA only) ➄ 28.6 32.7 45.2 49.7 53.7 67.9
PIT (QA docs)➅ 32.5 37.2 49.0 54.6 60.0 73.8
PIT➆ 45.4 51.2 63.2 62.7 68.6 78.8
Table 1: Comparison of QA performance (%) between
standard instruction-tuning and pre-instruction-tuning.
The best results are in bold. Rec. is short for answer
recall, and R-L refers to ROUGE-L.
hypothesize that more aggressive training leads
to less overfitting to deceptive patterns in docu-
ments and better generalization when responding
to questions .
In summary, lower perplexity does lead to stronger
generalization when responding to questions, but
it comes at the expense of forgetting previously
acquired knowledge.
5 Improving LLMs in Absorbing
Knowledge from Documents
The amount of knowledge elicited through the stan-
dard instruction-tuning is still limited, even though
the perplexity of documents is minimized, a phe-
nomenon we refer to as the “perplexity curse”. Our
next question is how can we improve the ability
of LLMs to absorb knowledge from documents to
mitigate the perplexity curse. The main challenge
is the gap between the way knowledge is presented
in raw documents and how it is accessed through
question-answering. We found that QA pairs are
generally straightforward, while documents tend
to be more complex and cluttered, weaving many
factual statements together in a more intricate man-
ner. Using Fig. 3 as an example, the answer to the
question “who handled the editing of Oppenheimer”
is included in a sentence in the middle of the ar-
ticle “Editing was handled by Jennifer Lame ...”,
which does not explicitly mention “Oppenheimer”.
During training, LLMs must understand the con-
text and deduce that “editing” refers to “the editing
of Oppenheimer” to effectively encode this knowl-
edge in the parameters.
Zhu and Li (2023a) studied this problem by train-
ing a randomly initialized GPT-2-like transformer
from scratch on synthetic biographies and evalu-ated its ability to answer questions about the in-
dividuals. They found that training on a mix of
biographies and questions related to half of those
biographies led to strong generalization when an-
swering questions about the remaining half of bi-
ographies, which resembles setting ➃in Fig. 4.
In contrast, training on biographies and QA pairs
sequentially failed. However, the key contributor
to the success remains uncertain because the data
were blended together, and it is unclear how to
apply this practically to absorb knowledge from
new documents. Inspired by our observation of
the different difficulty levels between QA pairs
and documents, and the finding from Zhu and Li
(2023a), we hypothesize that it is beneficial to de-
liberately expose LLMs to instruction-tuning data
before continued pre-training so that the process
of encoding knowledge from complex documents
takes into account how this knowledge is accessed.
We refer to this as pre-instruction-tuning (PIT)
and study various implementations of PIT prior to
continued learning (§ 5.1), followed by detailed
ablations identifying the keys contributor to perfor-
mance (§ 5.2 and § 5.3), and finally assess how well
PIT performs across domains (§ 5.4). We adhere to
the hyperparameters outlined in § 3.2 and perform
PIT for 3 epochs unless specified otherwise.
5.1 Variants of Pre-instruction-tuning
Pre-instruction-tuning w/ QA only We start
with exposing instruction-tuning data before con-
tinued pre-training on documents—training on top-
ically related QA pairs before training on test doc-
uments (Fig. 4 ➄). This can be directly compared
with the continued pre-training setting (Fig. 4 ➀).
The intuition is that questions help LLMs recognize
key types of information, enabling LLMs to focus
on important information during pre-training on
subsequent documents, even though the questions
are not directly tied to the documents. For example,
training on a question like “who handled the editing
of Oppenheimer” could help LLMs pay attention
to screenwriters when training on new documents
like “Barbie”. As shown in Tab. 1, this method
outperforms continued pre-training, especially on
larger LLMs (27.6%/41.7% 28.6%/49.7% for
7B/70B). The ablation that trains on QA data af-
ter training on documents (“instruction-tuning w/o
train doc” in Tab. 2) is ineffective, confirming the
importance of training on questions as a warm-up
before encoding documents.
Setting names Setting configurations EM Rec. R-L
baselines
continued pre-training ➀ test doc 27.6 31.6 43.8
+instruction-tuning ➁ train doc + test doc train QA 30.3 34.7 47.4
+instruction-tuning (w/o forget) ➂train doc + test doc train QA + test doc 30.2 34.1 46.4
+instruction-tuning (w/o train doc) test doc train QA 27.1 30.7 42.3
weighted continued pre-training test doc (weighted) 27.7 32.7 43.3
adapted continued pre-training train doc test doc 26.9 32.7 44.2
mix all data ➃ train QA + train doc + test doc 39.4 44.6 56.7
various pre-instruction-tuning (PIT) methods and ablation studies
train QA + train doc (3 epochs) test doc 45.4 51.2 63.2
ablation studies of the number of epochs
1 epoch 33.3 39.1 50.3
5 epochs 45.8 52.1 63.6
10 epochs 46.5 52.3 61.9
ablation studies of different learning mechanisms
QA before doc (grouped) 38.2 43.2 56.3
QA after doc (grouped) 27.2 31.1 42.1
QA before doc (interleaved) 45.9 51.3 64.5PIT➆
QA after doc (interleaved) 43.2 49.1 61.6
PIT-- train QA + train doc train QA test doc 44.4 51.3 63.4
PIT++➇ train QA train QA + train doc test doc 48.1 54.4 66.4
Table 2: Comparison (%) of various pre-instruction-tuning methods and ablation studies to identify the key
contributors to improved performance using Llama-2 7B. Different background colors indicate different pre-
instruction-tuning methods. The best results are in bold.
Pre-instruction-tuning on QA and documents se-
quentially Our second implementation trains on
QA and associated documents sequentially (Fig. 4
➅), with the intuition that the ability to absorb
knowledge from documents can be strengthened
if an LLM is trained on the complex documents
after it has grasped the associated simpler QA pairs.
For instance, if an LLM has already learned that
“Jennifer Lame” is the answer to “who handled
the editing of Oppenheimer”, training on the docu-
ment “Editing was handled by Jennifer Lame” can
more efficiently refine its storage of knowledge
in its parameters. As shown in Tab. 1, PIT on QA
pairs and documents sequentially surpasses the QA-
only variant (Fig. 4 ➄) and standard instruction-
tuning (Fig. 4 ➁) (30.3%/46.4% 32.5%/54.6%
for 7B/70B), demonstrating its effectiveness.
Pre-instruction-tuning The effectiveness of PIT
depends on ensuring that the associated QA pairs
are already learned before encoding the respective
documents. However, we observed that after train-
ing on documents (train doc in Fig. 4 ➅), the accu-
racy for corresponding questions (train QA in Fig. 4
➅) dropped from almost perfect to 30%, indicating
severe forgetting. To fix this, we train on the asso-
ciated QA pairs and documents together (Fig. 4 ➆).
As shown in Tab. 1, this significantly improves the
performance, outperforming all other approaches,
including mixing all data together (Fig. 4 ➃), bya large margin (39.4%/57.1% 45.5%/62.7% for
7B/70B). Training on both QA pairs and documents
prevents forgetting, but it also obscures how the
learning process works. It is unclear whether LLMs
grasp QA pairs before encoding knowledge from
documents, or if it works the other way around.
In the following section, we deliberately arrange
the order of QA pairs and documents during train-
ing to examine this, which leads us to propose an
improved version of PIT.
5.2 Pre-instruction-tuning++
We first study how the performance varies with
different numbers of epochs. As shown in Tab. 2,
training for 1 epoch is insufficient, and the perfor-
mance of 3, 5, or 10 epochs is similar. We fix the
number of epochs to 3 and arrange the order of QA
pairs and corresponding documents as shown in
Fig. 6 in Appendix D. The interleaved arrangement
cycles through all the data 3 times, ensuring that
in each epoch, questions either precede or follow
their associated documents. On the other hand, the
grouped arrangement clusters each example’s 3 ap-
pearances together, guaranteeing that the repeated
questions are positioned either before or after their
respective repeated documents. As shown in Tab. 2,
positioning QA pairs before corresponding docu-
ments achieves better performance in both grouped
and interleaved arrangements, indicating that dur-
ing PIT, the learning mechanism prioritizes under-
Llama-2 7B Llama-2 70B
Settings EM Rec. R-L EM Rec. R-L
standard instruction-tuning ➁
in-domain 30.3 34.7 47.4 46.4 50.9 64.1
cross-domain 23.6 28.2 38.4 42.8 49.7 58.5
pre-instruction-tuning ➆
in-domain 45.4 51.2 63.2 62.7 68.6 78.8
cross-domain 36.9 43.2 54.9 55.2 66.7 74.0
Table 3: In-domain and cross-domain PIT.
standing how to access knowledge before learning
to absorb information from the more complex and
information-dense documents.
Based on this, we propose an improved variant
called pre-instruction-tuning++, which trains ex-
clusively on QA pairs to understand patterns of
knowledge access, then progresses to training on
a combination of QA and document data to align
knowledge access through questions and knowl-
edge encoding from documents (Fig. 4 ➇). As
shown in Tab. 2, PIT++ significantly outperforms
PIT (Fig. 4 ➆) from 45.4% to 48.1%, while train-
ing on QA data after on the mix (PIT-- in Tab. 2)
does not yield additional benefits. This reinforces
our hypothesis that understanding how knowledge
is accessed aids in absorbing knowledge from doc-
uments, and therefore, should be prioritized.
5.3 Ablation Studies
Standard instruction-tuning is inferior not due
to forgetting A drawback of standard instruction-
tuning is that knowledge in test documents might be
forgotten after training on QA pairs (a phenomenon
also known as the “alignment tax” (Ouyang et al.,
2022)). To show that the lower performance of
standard instruction-tuning is not due to forgetting,
we add a setting where we mix train QA with test
documents during instruction-tuning to prevent for-
getting (Fig. 4 ➂). As shown in Tab. 2, this does
not help, confirming our hypothesis.
Pre-instruction-tuning is not simply upweight-
ing salient tokens from documents We include
an ablation inspired by Hu et al. (2023) which up-
weights tokens when pre-training on documents to
focus on salient information. We assign a weight
of 1.0 to tokens in documents that are included
in the answers (e.g., “Jennifer Lame” in the sen-
tence “Editing was handled by Jennifer Lame”),
and assign a lower weight of 0.5 to other tokens.
As shown in Tab. 2, this weighted continued pre-
training is ineffective, confirming our hypothesis.Settings EM Rec. R-L
generalization to the biography dataset bioS
closed-book 2.9 2.9 11.0
open-book w/ doc 95.2 95.4 95.6
continued pre-training ➀ 29.6 29.8 38.7
pre-instruction-tuning ➆ 58.1 58.4 61.9
generalization to questions by real users from Google
standard instruction-tuning ➁ 21.5 30.1 36.8
pre-instruction-tuning ➆ 29.0 35.5 48.2
Table 4: Generalization of the Llama-2 7B model
trained with pre-instruction-tuning.
5.4 Cross-domain Generalization
We validated the effectiveness of PIT by train-
ing and evaluation on data from the same domain
(Wiki2023-film ).Can PIT make LLMs better at
absorbing knowledge from documents of a different
domain? To this end, we follow the cross-domain
setting outlined in Fig. 2—training on other do-
mains ( Wiki2023-other-train ) and testing on
the film domain ( Wiki2023-film-test ). The re-
sults of standard instruction-tuning and PIT, in both
in-domain and cross-domain settings, are detailed
in Tab. 3. Even though it is not as effective as the in-
domain counterparts, cross-domain PIT still signif-
icantly outperforms instruction-tuning, demonstrat-
ing that it can generalize across different domains.
This finding sheds light on the potential to scale
this method up to a broader range of documents
and instructions for more robust generalization.
We also evaluate the effectiveness of PIT in two
other scenarios: (1) when applied to non-Wikipedia
documents, and (2) when addressing questions
asked by real users. For the first scenario, we
take the Llama-2 7B model trained with PIT on
2023Wiki-other and further train it on biogra-
phies synthesized in Zhu and Li (2023a) ( bioS ).
Then, we evaluate based on questions about the
individuals. For the second scenario, we manu-
ally search Google using questions generated by
LLMs from Wiki2023-film-test , collect a total
of 93 similar questions from real users by lever-
aging Google’s “People Also Ask” feature, and
then evaluate Llama-2 7B on these questions. As
shown in Tab. 4, PIT outperforms baselines in both
scenarios, demonstrating its generalization ability.
6 Related Work
6.1 Continual Knowledge Acquisition
Several works have studied whether LMs can an-
swer questions about information in documents
they have been trained on. Wang et al. (2021);
Jang et al. (2022); Hu et al. (2023) use relatively
small LMs such as BART (Lewis et al., 2020a),
T5 (Raffel et al., 2020), or GPT-2 (Radford et al.,
2019). Ovadia et al. (2023) focus on the compari-
son between RAG and continued pre-training ap-
proaches without using instruction-tuning. Zhu
and Li (2023a,b) examine this problem from a sim-
ilar angle as ours using a GPT-2-like transformer
trained from scratch on synthetic biographies and
fine-tuned on QA pairs related to the individuals.
They examined a mixed training setting on both bi-
ographies and QA pairs, which is our major motiva-
tion to study different strategies to incorporate QA
data before continued pre-training. Other works
study adapting LLMs to new domains via various
strategies (Zhang et al., 2023; Cheng et al., 2023;
Han et al., 2023; Wu et al., 2023; Nguyen et al.,
2023; Zhao et al., 2023).
6.2 Instruction-tuning or Alignment
Instruction-tuning (also known as supervised fine-
tuning) on high-quality annotated data (Sanh et al.,
2022; Wei et al., 2022; Mishra et al., 2022; Iyer
et al., 2022; Kopf et al., 2023; Zhou et al., 2023;
Sun et al., 2023b,a) and/or data generated by pro-
prietary models (Taori et al., 2023; Chiang et al.,
2023; Wang et al., 2023b; Ivison et al., 2023), or
alignment with reinforcement learning from human
feedback (RLHF) or direct preference optimization
(DPO) (Ouyang et al., 2022; Touvron et al., 2023b;
Rafailov et al., 2023; Tian et al., 2023) has been
a central topic recently because it elicits knowl-
edge from LLMs and enhances various abilities to
handle questions from users. We focus on factual-
ity and study the best way to perform instruction-
tuning to elicit factual knowledge from LLMs.
6.3 Analyzing the Training Dynamics of LMs
Many works study the training dynamics of LMs
from different perspectives. Carlini et al. (2022)
quantifies memorization across model sizes and
the frequency of data duplication. Tirumala et al.
(2022) finds that larger LMs memorize training
data faster with less overfitting. Xia et al. (2023)
shows that perplexity is more predictive of model
behaviors than other factors. Dery et al. (2022)
studies end-task aware pre-training using classifi-
cation tasks and RoBERTa models. Jia et al. (2022)
adds a pre-training objective to encourage the vec-
tor for each phrase to have high similarity with the
vectors for all questions it answers. Our work dif-
fers in that we specifically focus on the capacity of
recalling and generalizing information from a seen
document to answer questions.6.4 Retrieval-augmented Generation
Retrieval-augmented generation (RAG) is a widely
used approach to incorporate new knowledge into
LLMs by augmenting fixed LLMs with retrieved in-
formation from external sources (Chen et al., 2017;
Guu et al., 2020; Lewis et al., 2020b; Borgeaud
et al., 2022; Wang et al., 2023a; Alon et al., 2022;
He et al., 2021; Sachan et al., 2021; Izacard et al.,
2023; Lee et al., 2022; Jiang et al., 2022; Shi
et al., 2023; Jiang et al., 2023; Asai et al., 2023;
Nakano et al., 2021; Qin et al., 2023; Lin et al.,
2023). While RAG is effective in reducing hal-
lucinations commonly experienced when relying
solely on knowledge stored in parameters, its re-
trieval and generation process adds extra latency
and complexity. In contrast, continued pre-training
to store knowledge in parameters and utilizing the
stored knowledge to answer questions in a closed-
book manner are simpler and faster at inference
time. Enhancing this capability is also scientif-
ically significant, as it represents a fundamental
step in employing LLMs as dependable assistants
for accessing information. Therefore, this paper
focuses on exploring parametric approaches.
7 Conclusion
We study the best way of continued training on new
documents with the goal of later eliciting factual
knowledge and propose pre-instruction-tuning that
learns how knowledge is accessed via QA pairs
prior to encoding knowledge from documents. Ex-
tensive experiments and ablation studies demon-
strate the superiority of pre-instruction-tuning ver-
sus standard instruction-tuning. Future directions
include scaling this method up to a broader range
of documents and instructions for more robust gen-
eralization.
8 Limitations
TheWiki2023 dataset provides a relatively clean
testbed for studying continual knowledge acquisi-
tion. However, its scope is limited to Wikipedia,
which restricts the trained models’ adaptability to
other sources like web pages from Common Crawl
or scientific documents from arXiv. We focus on
eliciting factual knowledge with instruction-tuning
on QA data in this paper. The effectiveness of pre-
instruction-tuning with different types of data for
enhancing other skills like reasoning or compre-
hension is something that needs to be explored in
future studies.
Acknowledgements
We would like to thank Zeyuan Allen-Zhu, Zexuan
Zhong, Shuyan Zhou, Frank F. Xu, Qian Liu, and
Ruohong Zhang for their help with the experiments
and constructive feedback.
References
Uri Alon, Frank F. Xu, Junxian He, Sudipta Sen-
gupta, Dan Roth, and Graham Neubig. 2022.
Neuro-symbolic language modeling with automaton-
augmented retrieval. In International Conference on
Machine Learning .
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and
Hannaneh Hajishirzi. 2023. Self-rag: Learning to
retrieve, generate, and critique through self-reflection.
CoRR , abs/2310.11511.
Lukas Berglund, Meg Tong, Max Kaufmann, Mikita
Balesni, Asa Cooper Stickland, Tomasz Korbak, and
Owain Evans. 2023. The reversal curse: Llms
trained on ""a is b"" fail to learn ""b is a"". CoRR ,
abs/2309.12288.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,
Trevor Cai, Eliza Rutherford, Katie Millican, George
van den Driessche, Jean-Baptiste Lespiau, Bogdan
Damoc, Aidan Clark, Diego de Las Casas, Aurelia
Guy, Jacob Menick, Roman Ring, Tom Hennigan,
Saffron Huang, Loren Maggiore, Chris Jones, Albin
Cassirer, Andy Brock, Michela Paganini, Geoffrey
Irving, Oriol Vinyals, Simon Osindero, Karen Si-
monyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.
2022. Improving language models by retrieving from
trillions of tokens. In International Conference on
Machine Learning, ICML 2022, 17-23 July 2022, Bal-
timore, Maryland, USA , volume 162 of Proceedings
of Machine Learning Research , pages 2206–2240.
PMLR.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual .
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
Katherine Lee, Florian Tramèr, and Chiyuan Zhang.
2022. Quantifying memorization across neural lan-
guage models. CoRR , abs/2202.07646.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading wikipedia to answer open-
domain questions. In Proceedings of the 55th AnnualMeeting of the Association for Computational Lin-
guistics, ACL 2017, Vancouver, Canada, July 30 -
August 4, Volume 1: Long Papers , pages 1870–1879.
Association for Computational Linguistics.
Daixuan Cheng, Shaohan Huang, and Furu Wei. 2023.
Adapting large language models via reading compre-
hension. CoRR , abs/2309.09530.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways. CoRR , abs/2204.02311.
Lucio M. Dery, Paul Michel, Ameet Talwalkar, and
Graham Neubig. 2022. Should we be pre-training?
an argument for end-task aware training as an alter-
native. In The Tenth International Conference on
Learning Representations, ICLR 2022, Virtual Event,
April 25-29, 2022 . OpenReview.net.
Gemini Team. 2023. Gemini: A family of highly capa-
ble multimodal models.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Ming-Wei Chang. 2020. REALM: retrieval-
augmented language model pre-training. CoRR ,
abs/2002.08909.
Tianyu Han, Lisa C. Adams, Jens-Michalis Papaioan-
nou, Paul Grundmann, Tom Oberhauser, Alexander
Löser, Daniel Truhn, and Keno K. Bressem. 2023.
Medalpaca - an open-source collection of medical
conversational AI models and training data. CoRR ,
abs/2304.08247.
Junxian He, Graham Neubig, and Taylor Berg-
Kirkpatrick. 2021. Efficient nearest neighbor lan-
guage models. In Conference on Empirical Methods
in Natural Language Processing .
Nathan Hu, Eric Mitchell, Christopher D. Manning,
and Chelsea Finn. 2023. Meta-learning online adap-
tation of language models. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2023, Singapore, De-
cember 6-10, 2023 , pages 4418–4432. Association
for Computational Linguistics.
Hamish Ivison, Yizhong Wang, Valentina Pyatkin,
Nathan Lambert, Matthew Peters, Pradeep Dasigi,
Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy,
and Hannaneh Hajishirzi. 2023. Camels in a chang-
ing climate: Enhancing LM adaptation with tulu 2.
CoRR , abs/2311.10702.
Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,
Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster,
Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li,
Brian O’Horo, Gabriel Pereyra, Jeff Wang, Christo-
pher Dewan, Asli Celikyilmaz, Luke Zettlemoyer,
and Ves Stoyanov. 2022. OPT-IML: scaling language
model instruction meta learning through the lens of
generalization. CoRR , abs/2212.12017.
Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli,
Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and
Edouard Grave. 2023. Atlas: Few-shot learning
with retrieval augmented language models. J. Mach.
Learn. Res. , 24:251:1–251:43.
Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin,
Janghoon Han, Gyeonghun Kim, Stanley Jungkyu
Choi, and Minjoon Seo. 2022. Towards continual
knowledge learning of language models. In The
Tenth International Conference on Learning Repre-
sentations, ICLR 2022, Virtual Event, April 25-29,
2022 . OpenReview.net.
Robin Jia, Mike Lewis, and Luke Zettlemoyer. 2022.
Question answering infused pre-training of general-
purpose contextualized representations. In ACL
(Findings) , pages 711–728. Association for Compu-
tational Linguistics.
Zhengbao Jiang, Luyu Gao, Zhiruo Wang, Jun Araki,
Haibo Ding, Jamie Callan, and Graham Neubig. 2022.
Retrieval as attention: End-to-end learning of re-
trieval and reading within a single transformer. In
Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2022, Abu Dhabi, United Arab Emirates, December
7-11, 2022 , pages 2336–2349. Association for Com-
putational Linguistics.
Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham
Neubig. 2020. How can we know what language
models know. Trans. Assoc. Comput. Linguistics ,
8:423–438.
Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,
Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie
Callan, and Graham Neubig. 2023. Active retrieval
augmented generation. In Proceedings of the 2023Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2023, Singapore, Decem-
ber 6-10, 2023 , pages 7969–7992. Association for
Computational Linguistics.
Andreas Kopf, Yannic Kilcher, Dimitri von Rutte,
Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens,
Abdullah Barhoum, Nguyen Minh Duc, Oliver Stan-
ley, Rich’ard Nagyfi, ES Shahul, Sameer Suri,
David Glushkov, Arnav Dantuluri, Andrew Maguire,
Christoph Schuhmann, Huu Nguyen, and Alexander
Mattick. 2023. Openassistant conversations - de-
mocratizing large language model alignment. ArXiv ,
abs/2304.07327.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur P. Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: a benchmark for question answering
research. Trans. Assoc. Comput. Linguistics , 7:452–
466.
Haejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paran-
jape, Christopher D. Manning, and Kyoung-Gu Woo.
2022. You only need one model for open-domain
question answering. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2022, Abu Dhabi, United Arab
Emirates, December 7-11, 2022 , pages 3047–3060.
Association for Computational Linguistics.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020a.
BART: denoising sequence-to-sequence pre-training
for natural language generation, translation, and com-
prehension. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
ACL 2020, Online, July 5-10, 2020 , pages 7871–7880.
Association for Computational Linguistics.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
tus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,
Tim Rocktäschel, Sebastian Riedel, and Douwe
Kiela. 2020b. Retrieval-augmented generation for
knowledge-intensive NLP tasks. In Advances in Neu-
ral Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual .
Xi Victoria Lin, Xilun Chen, Mingda Chen, Wei-
jia Shi, Maria Lomeli, Rich James, Pedro Ro-
driguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis,
Luke Zettlemoyer, and Scott Yih. 2023. RA-DIT:
retrieval-augmented dual instruction tuning. CoRR ,
abs/2310.01352.
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In 7th International
Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019 . OpenRe-
view.net."
2402.03271,D:\Database\arxiv\papers\2402.03271.pdf,"How does the paper address the challenge of evaluating the effectiveness of a question in a dynamic setting, where the immediate impact may not reflect the long-term effects?","The paper utilizes a reward propagation scheme across simulation trees, defining accumulated rewards that gather rewards over multiple simulation steps to reflect the effectiveness of past decisions. This allows for the computation of expected rewards, which guide the selection of candidate questions by indicating their likely benefits.","Multistep Simulation As shown in Figure 2 (a), the Question Generation stage generates candidate
questions such as q1
i=“Did you vomit?"" Next, during Simulation stage, for each such generated
candidate question, we simulate possible futures for a few steps, forming a tree of possibilities. This
process enables us to compute rewards for each question, helping us to decide which question to ask.
Each node of the tree can be one of two types: Answerer Nodes where it is the Answerer’s turn to
answer a question, and Questioner Nodes where it is the Questioner’s turn to ask a question. At the
root, a question has just been asked (e.g., q1
i), so the root is an Answerer Node. Next, we explain how
to construct tree by recursively expanding (or ‘branching’) each node to construct its children, i.e.,
starting from the root, then proceeding to its children, and so on.
•At each Answerer Node , a question has just been asked. Next, we need to further ‘branch’ the
tree based on the possible answers to the current question. Rather than allowing completely
open-ended answers, we instead focus on affirmative and negative responses5, as this allows
us to compute meaningful uncertainty metrics, as we discuss later. Hence, we branch the node
into two children, corresponding to affirmative and negative answers.
•At each Questioner Node , we prompt an LLM to generate mquestions using the current
history and possibility set, in the same way as in the Question Generation step. Note that
while the generation procedure is similar, the purpose is different: the Question Generation
step generates candidate questions to select from, while here we are generating simulated
questions to form a tree for the purpose of evaluating the current question. The resulting m
generated questions are added to the tree as children of the current node.
In this way, we recursively generate tree nodes, stopping at a fixed number of levels (i.e., depth).
While generating this tree, we also recursively compute the current possibility set Ωvat each node
v. Specifically, let hvbe the current conversation history up to node v, combining both the actual
conversation history hiand the simulated conversation up to node v. Then the current possibility set
at this node, denoted Ωv, is the subset of the possibility space consistent with hv. At the root, the
current possibility set is only limited by the actual conversation history, i.e., Ωi. Then, as we proceed
over the simulated tree, note that the current possibility set only changes at Answerer nodes, when
an answer is added to the current history. Hence, at each Answerer node v, we prompt a new LLM
(an ‘Answerer Simulator’ LLM ans), to determine the further subset ΩA
v⊆Ωvfor which the answer
to the current question is affirmative, and the corresponding ΩN
v= Ωv\ΩA
vfor which the answer
is negative.6This allows us to recursively compute the possibility sets of the children of v(which
themselves correspond to the affirmative and negative answers).
ΩA
v,ΩN
v=LLM ans(Prompt ans(hv,Ωv)) (2)
In this way, we can recursively compute the possibility set on each node of the tree.
2.4 Uncertainty-Based Reward Calculation
To develop suitable information-seeking approaches, a critical question is how to evaluate the
effectiveness of a question, i.e., its contribution to reducing uncertainty . To address this, we turn
to information theory, specifically the concept of information gain , which measures the amount by
which uncertainty decreases after a particular observation. To reward information-seeking behavior,
we assign rewards to questions based on how much they reduce the model’s uncertainty about the
unknown random variable. These reward signals are used by our UoT framework to determine which
question to select, to maximize the reduction of uncertainty.
Entropy. Entropy and information gain are well-known concepts in information theory Shannon
(1948). In our work, we use these concepts to measure how much information is gained (or equiva-
lently, how much uncertainty is reduced) by asking a question, to formulate our rewards. Entropy
measures the level of uncertainty in a random variable: higher entropy indicates greater uncertainty.
The entropy of a discrete random variable Xtaking values x1, ..., x nis:
H(X) =−Xn
i=1p(xi) logp(xi) (3)
5As shown Figure 2 (a), for question ‘Did you vomit?’, possible affirmative responses include ‘yes’ or ‘I
already vomited twice’, while negative responses could be ‘no’ or ‘I don’t have’.
6In practice, allowing overlap between ΩA
vandΩN
vmay be more realistic. However, in this work, we
consider only the simplified scenario where they are disjoint.
4
Since our goal is to reduce the uncertainty in the unknown ω∈Ω, we use entropy to measure
this uncertainty. Formally, let Ω ={ω1,···, ωn}, and we define an additional set of arbitrary real
numbers X={x1,···, xn} ⊆Rwhich we will associate with each of these possibilities. Define a
random variable X: Ω→ X such that X(ωi) =xi. Intuitively, Xis a discrete random variable that
takes the value xiif the ith possibility is true, i.e., if ω=ωi.Xserves to capture our uncertainty
about ω, since observing Xis equivalent to observing the true option ω. As a simple example,
suppose our possibility space is Ω ={ω1, ω2, ω3}; we accompany these with real numbers x1, x2, x3,
and have a distribution for our random variable Xreflecting prior beliefs over these possibilities:
e.g.,p(x1) = 0 .2, p(x2) = 0 .3, p(x3) = 0 .5. Conceptually, our framework allows for any prior
probability distribution over the possibilities (i.e., p(xi)), but in our experiments, we assume a
uniform distribution over them due to the lack of an informative prior.
Before asking any questions, our uncertainty about the unknown ωis given by H(X), as in Eq. (3).
At any node vof the trees described in the previous section, recall that we have a conversation history
hvwhich contains some answers given by the Answerer. This history limits the current possibility
set to those in Ωv⊆Ω, thereby reducing our uncertainty. We model this using the standard notion of
conditional probability on an event : since Ωv⊆Ω, thus Ωvis an event which we can condition on:
p(xi|Ωv) =p(xi)/p(Ωv)∀isuch that ωi∈Ωv (4)
where p(Ωv)is the sum of probabilities of the elements in Ωv. To illustrate, we continue from the
earlier example, where p(x1) = 0 .2, p(x2) = 0 .3, p(x3) = 0 .5. If the conversation history hvat
nodevis only consistent with x1andx2, i.e., Ωv={ω1, ω2}, we can adjust probability distribution
by conditioning: e.g., the adjusted probability of x1isp(x1)/p(Ωv) = 0 .2/(0.2 + 0 .3) = 0 .4.
Next, to quantify the uncertainty at node v, note that since Xis conditionally distributed based on
p(·|Ωv), the entropy of this distribution is:
Hv(X) :=X
i:ωi∈Ωvp(xi|Ωv) logp(xi|Ωv) (5)
Intuitively, Hv(X)is the remaining uncertainty in Xat node v(i.e., after observing the history hv).
Information Gain at a Node We now quantify the uncertainty reduction when receiving answers
at an Answerer node v. Recall that the answer given at vpartitions Ωvinto two disjoint subsets:
Ωv= ΩA
v∪ΩN
v, where ΩA
vandΩN
vare the subsets of possibilities resulting in affirmative and
negative answers to last asked question. Given an affirmative answer, the remaining entropy becomes:
HA
v(X) :=X
i:ωi∈ΩAvp(xi|ΩA
v) logp(xi|ΩA
v) (6)
We define HN
v(X)analogously for negative answers. Let pA
v=p(ΩA
v)/p(Ωv)andpN
v=
p(ΩN
v)/p(Ωv)be the conditional probabilities of affirmative and negative answers at node v. To
compute the expected entropy after receiving the answer at node v, since we have a pA
vprobability of
receiving an affirmative answer and pN
vof a negative answer, the expected entropy is:
pA
v·HA
v(X) +pN
v·HN
v(X) (7)
As such, the expected information gain at node vis the difference in entropies before and after
receiving the answer:
IGv(X) :=Hv(X)−pA
v·HA
v(X)−pN
v·HN
v(X) (8)
We can simplify this: as proven in Appendix A, the above equation reduces to:
IGv(X) =−pA
vlogpA
v−pN
vlogpN
v (9)
This represents the expected reduction of uncertainty in Xwhen receiving an answer at node v. Note
that it has an entropy-like expression, and is therefore nonnegative.
Reward Formulation A natural approach would be to define the reward function Ru(v)at node
vas the information gain IGv(X): that is, the reward from the question at node vis the expected
information gain IGv(X)from receiving its answer. In practice, we find that a slightly modified
function fIGv(X)is preferable. In particular, we find that IGv(X)does not result in sufficiently
sharp differences in reward over the typical ranges we encounter. Hence, we introduce an additional
hyperparameter λ≥0which helps to sharpen the rewards using a scaling approach. We compare other
5
scaling methods and determine the current design is optimal in performance and their corresponding
benefits. Details are in the Appendix B.
Ru(v) =fIGv(X) := (−pA
vlogpA
v−pN
vlogpN
v)/(1 +λ−1|pA
v−pN
v|) (10)
This definition ensures that Ru(v)falls within the range [0,1], providing a normalized and consistent
reward to measure uncertainty reduction. The reward function reaches its maximum when the subsets
ΩA
vandΩN
vhave equal probability, reflecting the maximum reduction in uncertainty. It reaches
its minimum when one of the subsets has zero probability, indicating no reduction in uncertainty.
Appendix G plots the reward function curve across values of pA
vandpN
v.
2.5 Question Selection Via Reward Propagation
Single-step rewards often fall short in dynamic settings as they only consider immediate impact,
overlooking long-term effects. To overcome this, our method uses a reward propagation scheme across
simulation trees by defining ‘accumulated rewards’ that gather rewards over multiple simulation steps
to reflect the effectiveness of past decisions. These accumulated rewards help compute ‘expected
rewards’, indicating the likely benefits of the questions and guide the selection of candidate questions.
Accumulated Reward We first define the accumulated reward at each node v, which accumulates
the rewards at vand all its ancestors on the tree, defined recursively as:
Ra(v) :=Ru(v) +
0 vis root
Ra(Parent (v))otherwise
HereRu(v)is the uncertainty-based reward at node vdefined in Eq. (10), andRa(Parent (v))is the
accumulated reward of the parent of v. We compute these accumulated rewards by starting at the root
and propagating down to the leaves. Intuitively, the accumulated reward at each leaf node represents
the total reward we end up with at the end of the conversation at that node.
Expected Reward Next, we compute the expected reward for each node Re(v), which represents
the expected total value of rewards received on expectation on a node and all its descendants on tree.
Re(v) :=

Ra(v) ifvis a leaf; otherwise:
pA
vRe(vA) +pN
vRe(vN)ifvis an Answerer Node
1
mPm
w∈Children (v)Re(w)ifvis a Questioner Node
For the case where vis an Answerer Node, recall that pA
vandpN
vare the conditional probabilities
of affirmative and negative answers at node v, defined in section 2.4. vAandvNare its children,
corresponding to the affirmative and negative answers. For the case where vis a Questioner Node,
we assign equal probability to the mquestions asked from this node. In this way, we propagate the
expected rewards from the leaves up to the root, allowing us to compute the expected gain at the root.
We compare different reward propagation schemes and find that using cumulative rewards from all
paths enhances long-term decision-making benefits. See Appendix C for details.
Determining the Optimal Question Finally, to decide the question to ask, we select the question
with highest expected reward (and therefore, the highest expected information gain, considering both
immediate and future information gains):
qi= arg max
n=1Re(qn
i) (11)
2.6 UoT Summary
UoT generates candidate questions q1
i, q2
i, . . . , qm
ibased on history and the current possibility set
Ωi. It simulates a tree for each question, calculates uncertainty-based rewards Ru(v), and computes
expected rewards Re(v). The question qn
iwith the highest expected reward is chosen for interaction.
2.7 Extensions and Discussion
Open Set UoT. Recall that in the closed set scenario, the Questioner starts with knowledge of the
possibility space Ω. In practice, the possibility space is often unknown, resulting in the open set
setting. To adapt UoT to this case, we prompt Questioner to initialize the possibility space Ωand then
reinitialize the possibility set Ωiaccording to current history hi. Then, the rest of UoT is unchanged.
The generalization in open-end answers. The UoT framework enables LLMs to update possibilities
after each interaction, including affirmative/negative or open-ended responses. Thus, it can be applied
to open-ended answers scenarios. Pruned UoT. To enhance simulation efficiency, pruning similar to
Beam Search can be used to limit the number of explored paths in the simulation trees to a fixed size.
6
3 Experiments
3.1 Experimental Setup
Models We test various LLMs to evaluate the generality of UoT, including Llama-3-70B-Instruct
AI@Meta (2024), Mistral-Large Mistral.AI (2024), Gemini-1.5-Pro Reid et al. (2024), Claude-3-
Opus Anthropic (2024) and GPT-4 OpenAI (2023b). We also validate the performance of earlier
released LLMs ( Appendix D) including Llama 2-70B-Chat Touvron et al. (2023), Cohere Cohere
(2023), PaLM 2 Anil et al. (2023), Claude 2 Anthropic (2023) and GPT-3.5-turbo OpenAI (2023a).
Baselines Direct Prompting (DP) prompts an LLM directly to generate the next response. Plan-
ning Prompting (PP) is motivated by Wang et al. (2023). We leverage another LLM to plan the
future and, consequently, determine the question to ask. Chain-of-Thought (CoT) Wei et al. (2022)
improves reasoning in LLMs by detailing reasoning steps. CoT-SC (Self-Consistency) Wang et al.
(2022) samples various paths to enhance fairness in computational comparisons. Reflexion Shinn
et al. (2023) lets agents propose actions and self-assess to foster new ideas. Tree-of-Thoughts (ToT)
Yao et al. (2023) enables LLMs to make decisions by exploring and evaluating multiple reasoning
paths over a tree structure. We examine ToT under two setups: Original-ToT , which uses the standard
approach of generating and evaluating questions, and Adapted-ToT (Ad.-ToT) , where we integrate
heuristic experience into prompt for question generation and evaluation, focusing on questions that
halve the search space. We matched the tree depth to the simulation steps in our UoT method for a
fair comparison. We evaluate methods and LLMs in open set (OS) and closed set (CS) settings. In
OS, models lack prior outcome knowledge; in CS, they have full outcome information. For details,
see Appendix I.1 for experimental settings and Appendix L for prompts.
Table 1: Results from three different scenarios, assessing Success Rate (SR), Mean Conversation
Length in Successful Cases (MSC), and Mean Conversation Length (MCL).
Model Method20 Questions Medical Diagnosis Troubleshooting
Common Thing DX MedDG FloDial
SR↑MSC ↓MCL ↓SR↑MSC ↓MCL ↓SR↑MSC ↓MCL ↓SR↑MSC ↓MCL ↓SR↑MSC ↓MCL ↓
Llama3-70BDP (OS) 34.2 13.9 17.9 15.5 14.9 19.2 26.0 3.6 4.6 25.7 3.6 4.6 11.1 15.4 19.5
UoT(OS) 36.9 12.4 17.3 21.0 13.6 18.7 35.6 2.6 4.1 50.6 2.3 3.6 26.1 9.1 17.2
DP (CS) 51.4 14.6 17.2 15.0 13.8 19.1 83.7 3.5 3.7 60.2 3.5 4.1 28.8 15.7 18.8
UoT (CS) 55.9 12.6 15.9 25.0 13.0 18.3 90.4 1.0 1.4 64.3 1.4 2.7 47.1 7.6 14.2
Mistral-LargeDP(OS) 20.7 13.1 18.6 12.5 13.6 19.2 18.3 3.4 4.7 28.3 3.2 4.5 11.1 15.8 19.5
UoT(OS) 27.0 15.1 18.7 15.0 13.1 19.0 24.0 2.5 4.4 50.0 2.9 4.0 19.6 11.3 18.3
DP (CS) 26.1 13.4 18.3 13.0 12.6 19.0 38.5 3.3 4.3 46.7 3.3 4.2 14.2 16.0 19.4
UoT (CS) 31.5 9.8 16.8 18.5 13.2 18.7 48.1 2.2 3.6 60.0 1.9 3.2 30.1 10.9 17.3
Gemini-1.5-ProDP (OS) 36.0 16.8 18.8 17.5 14.4 19.0 26.9 3.5 4.6 23.7 4.0 4.8 9.15 15.6 19.6
UoT(OS) 39.7 14.6 17.9 22.0 13.4 18.5 39.4 2.4 4.0 38.6 2.9 4.2 19.0 12.1 18.5
DP (CS) 47.7 17.0 18.6 28.5 15.0 18.6 69.2 3.2 3.8 51.4 3.2 4.1 30.1 14.0 18.2
UoT (CS) 60.4 13.9 16.3 32.0 14.0 18.1 81.7 2.1 2.6 81.4 2.1 2.6 53.6 11.5 15.4
Claude-3-OpusDP(OS) 45.0 14.2 17.4 16.5 13.8 19.0 33.7 3.4 4.5 54.3 3.2 4.0 31.4 15.7 18.6
UoT(OS) 63.1 14.4 16.5 23.5 13.3 18.4 45.9 2.6 3.9 61.5 2.3 3.3 35.9 11.0 16.8
DP (CS) 52.3 13.8 16.8 33.5 14.1 18.0 75.0 3.3 3.7 73.3 3.3 3.8 48.4 16.0 18.1
UoT (CS) 66.7 6.9 11.3 41.5 13.9 17.5 81.7 2.2 2.7 79.3 2.4 2.9 56.2 6.2 12.2
GPT-4DP(OS) 48.6 14.0 17.1 16.5 12.6 18.8 44.2 3.5 4.9 45.7 4.2 4.6 38.4 13.0 17.3
CoT(OS) 13.5 18.6 19.8 6.00 16.4 19.8 18.3 3.8 4.8 9.71 4.0 4.9 30.7 10.3 17.0
Ad.-ToT(OS) 45.0 17.8 19.0 21.0 15.2 19.0 45.2 2.4 3.8 51.4 2.7 3.8 35.3 13.3 17.7
UoT(OS) 55.3 15.1 17.4 28.0 14.9 18.6 49.1 2.4 3.7 67.4 2.5 3.5 43.5 12.0 16.8
DP (CS) 50.5 13.1 16.5 30.5 13.1 17.9 91.3 3.0 3.3 72.3 4.2 4.4 43.7 13.4 17.1
PP (CS) 38.7 14.9 18.0 18.0 14.5 19.0 58.6 2.5 3.5 62.3 3.8 4.3 39.2 14.2 17.7
CoT (CS) 20.7 16.0 19.2 10.0 16.2 19.6 33.7 3.7 4.4 20.0 3.8 4.3 32.8 10.1 16.8
CoT-SC (CS) 55.1 14.0 16.7 18.5 14.8 19.0 48.5 3.6 4.3 26.7 4.2 4.8 42.5 11.0 16.2
Reflexion (CS) 67.6 12.0 14.6 31.5 13.6 18.0 52.5 3.7 4.3 30.3 4.0 4.7 28.6 11.5 17.8
Original-ToT (CS) 28.8 15.5 18.7 18.5 15.1 19.1 70.3 3.3 3.8 60.3 3.2 3.9 40.4 11.6 16.6
Ad.-ToT (CS) 42.6 12.2 16.1 25.0 13.0 18.3 92.1 1.9 2.2 78.0 3.0 3.4 60.3 8.2 12.9
Pruned UoT (CS) 62.2 10.8 14.3 34.0 14.9 18.3 92.1 1.9 2.1 83.3 2.7 3.1 63.2 8.2 12.5
UoT (CS) 71.2 10.8 13.5 37.5 14.4 17.9 97.0 2.0 2.1 88.0 2.6 2.9 67.3 7.8 11.8
Scenarios and Datasets 20 Questions is a game where the answerer thinks of an item and the
questioner asks up to 20 yes-or-no questions to guess it. We use two datasets, Common (collected by
us, refer to Appendix I.2 for more details) and Things Hebart et al. (2019), including 111 and 1854
items separately. In this scenario, the maximal turns is set to 20. In Medical Diagnosis , the doctor
needs to ask questions to patients about their symptoms, to determine an accurate diagnosis. We use
two datasets: DX Xu et al. (2019), with 104 doctor-patient dialogues and 5 diseases in test set, and
MedDG Liu et al. (2022) with over 17K conversations across 15 disease types. We manually selected
500 high-quality samples for evaluation (see Appendix I.3). Importantly, Open-ended responses
from patient are allowed in MedDG to validate UoT’s generalization in open-ended scenarios. Both
datasets are limited to 5 turns. Troubleshooting involves a technician working with customers to
identify and fix problems in computer systems, electronic devices, or machinery. Raghu et al. (2021)
introduce FloDial with 894 dialogues, containing 153 faults. We evaluate using a maximum of 20
turns. The answerer simulated by GPT-4 is given the patient’s disease and conversation details for
each case. For more details, refer to Appendix I.2 and see examples of these scenarios in Appendix K.
7
UoT (Open Set) Setup We iteratively update LLMs’ perceived possibilities based on conversational
history, rather than defining them all upfront. In medical diagnosis and troubleshooting, initial
descriptions from symptoms or issues help set up initial possibilities. In the 20-question game, we
start with broad inquiries using the Direct Prompting method for the first three rounds to gather more
information. The ToT tree structure method employs a similar strategy. Setup details in Appendix I.4.
Evaluation Metrics To measure efficacy and efficiency, we use: Success Rate (%) :SR=S/T,
where Sis the number of successful cases, and Tis the total number of cases; Mean Conversation
Length in Successful Cases :MSC =Rs/S, where Rsis the total rounds in successful cases; Mean
Conversation Length :MCL =R/T , where Ris the total rounds in all cases.
3.2 Performance
20 Questions As illustrated in Table 5, for all types of LLMs, those equipped with UoT outperform
the baselines in both open set and close settings. Among the methods used on GPT-4 to enhance
planning and reasoning, CoT (CS) and PP (CS) show inferior performance even compared to GPT-4
alone. UoT (OS) demonstrates superior performance, with with an average 8.7% improvement than
Adapted-ToT (OS) in success rate. Moreover, UoT (CS) achieves the highest success rate, surpassing
the second-best Reflexion by an average of 4.3%.
Medical Diagnosis UoT (CS) outperforms baselines in simplified medical diagnostics, achieving a
97.0% success rate on the DX dataset with GPT-4. On the MedDG dataset, UoT (CS) on Gemini-
1.5-Pro and GPT-4 achieve success rates of 81.4% and 88.0%. It also reduces conversation lengths
to an average MSC of 2.0 on GPT-4 for DX, lower than 3.5 and 3.0 for DP methods. These results
demonstrate the versatility of our UoT in handling both binary and open-ended interactions effectively.
Troubleshooting UoT (CS) with GPT-4 similarly achieves the highest SR of 67.3%, and the lowest
MSC of 7.8. It also shows a remarkable improvement from 43.7% to 67.3% in Success Rate.
Overall Performance On average, UoT enhances the success rate by 38.1% compared to DP
across 5 datasets and 5 different LLMs, including open source and commercial models. Notably,
Success Rate increases 46.6% for Llama3-70B. Furthermore, UoT outperforms CoT-SC by 33.8% and
Reflexion by 29.9%. Even compared to tree structure methods like Original-ToT and Adapted-ToT,
UoT still shows superior performance with gains of 28.3% and 12.4% respectively. Additionally,
Pruned UoT, our pruning method to improve efficiency, outperforms Adapted-ToT by 7.36%. Addi-
tionally, our study shows that UoT’s one-step planning is effective due to effective reward design and
question selection. We limit simulations to three steps for budgetary reasons, balancing efficiency
and effectiveness. For more details, see Appendix E.
Case Studies and Reliability of GPT-4 as answerer Figure 3 shows UoT, compared to direct
prompting, more effectively reduce uncertainty and narrow down candidates, avoiding overly specific
queries. After gaining initial information (e.g., stomach pain), it generates targeted questions about
related issues rather than general inquiries. Additionally, GPT-4’s accuracy as answerer is evaluated
by analyzing 10% of interactions from each dataset, consistently showing reliable responses. For
quantitative details, see Appendix F.
3.3 Analysis
3.3.1 Comparing Model Performance at Equal Computational Efficiency
We compare the performance of approaches with similar computational costs in a closed set setting, in
terms of token consumption. To do so, we first prune our UoT as described in section 2.7. Secondly,
we expand exploration depth of Adapted-ToT method to bring its token cost in line with that of UoT.
Q1:IsXalivingthing?A1:Yes,Xisalivingthing.…Q8:IsXlargerthanahuman?A8:No,Xisnotlargerthanahuman.Q9:IsXfoundinNorthAmerica?A9:No,XisnotfoundinNorthAmerica.Q10 :IsXfoundinAfrica?A10:No,XisnotfoundinAfrica.Q11:IsXfoundinAsia?A11:Yes,XisfoundinAsia.…Q14:IsXaleopard?A14:Yes,Xisatypeofleopard.Q15:IsXasnowleopard?A15:Youguessedit.Xis'Snowleopard'.Q1:IsXaman-madeobject?A1:No,Xisnotaman-madeobject.Q2:IsXananimal?A2:Yes,Xisananimal.Q3:IsXacarnivore?A3:Yes,Xisacarnivore.Q4:IsXamammal?A4:Yes,Xisamammal.Q5:IsXamammalthatistypicallyassociatedwithwater?A5:No,Xisnotassociatedwithwater....Q9:IsXaleopard?A9:Yes,Xisatypeofleopard.Q10:IsXasnowleopard?A10:Youguessedit.Xis'Snowleopard'.Direct PromptingUoT
Q1:Haveyouexperiencedanynauseaorvomiting?A1:No,Ihaven'texperiencedanynauseaorvomiting.Thepainstartedaroundsixo'clock ….Q2:Doesthepainradiatetoanyotherpartofyourbody,suchasyourbackorshoulder?A2:No,thepaindoesn'tradiatetoanyotherpartofmybody.It'sjustinthemiddleofmyabdomen,rightabovemybellybutton.…Q5:Doyouexperiencepainaftereating,particularlyfattyorgreasyfoods?A5:No,Idon'tfeelanypainaftereating.ButIstartedfeelingpainaround6o'clock….Q1:Doyouexperienceaburningorgnawingsensationinyourstomach,especiallyonanemptystomach?A1:Yes,Idohavethissensation,especiallyonanemptystomach.Q2:Haveyounoticedanydark,tarrystoolsorvomitthatlookslikecoffeegrounds?A2:Ihaven'tnoticedthecolorofmystool,butIhaven'tvomited.A3:You areprobably experiencingGastriculcer?Self-Report: Stomachache. Pain in the upper abdominal area. No diarrhea. (Female, 27 years old)Direct PromptingUoT
Figure 3: Case studies from the 20 Questions game (left) and simplified medical diagnosis (right).
8
As shown in the top half of Table 2, the Pruned UoT method, despite its reduced efficacy compared
to UoT, still outperforms ToT and other methods. Also, the bottom part of Table 2 shows that even
when increasing the depth of Adapted ToT (Adapted-ToT ( D= 4)) to match the token cost of UoT
(D= 3), it still underperforms compared to UoT.
Table 2: Average success rates for 20Q, MD, and
TB at comparable efficiency, measured by GPT-4
token use. kis sampling count, Dis tree depth.
Method Tokens 20Q MD TB
CoT-SC( k= 33 ) 4.6k 32.6 37.6 42.5
Orig-ToT( D= 3) 4.5k 23.7 65.3 40.4
Adapt-ToT( D= 3) 4.5k 33.8 85.1 60.3
Pruned UoT( D= 3) 4.7k 48.1 88.4 63.2
Adapt-ToT( D= 4) 9.3k 40.9 86.7 63.7
UoT(D= 3) 9.2k 54.4 92.5 66.0
33.885.160.345.385.666.0
0102030405060708090
20QMDTB
ToT
ToT (+UR)
48.589.066.054.491.767.3
0102030405060708090100
20QMDTB
UoT (-UR)
UoTFigure 4: Success rate comparison between
Adapted-ToT and Adapted-ToT using uncer-
tainty reward, and between UoT and UoT with-
out uncertainty reward.
3.3.2 Effectiveness of Uncertainty Rewards
To further demonstrate the effectiveness of our uncertainty-based reward, we compare it with the
self-evaluation reward used in the original ToT based on GPT-4 model. We implement the uncertainty-
based reward in place of the self-evaluation reward in ToT, creating a variant we call ToT (+UR). The
results, as shown in left side of Figure 4, indicate that our reward significantly enhances planning
efficacy by an average of 5.9%. Additionally, we use the heuristic self-evaluation reward in Adapted-
ToT to replace our current uncertainty-based reward in UoT, a variant we refer to as UoT (-UR).
This change results in a performance decrease shown in the right part of Figure 4, further validating
the effectiveness of our uncertainty-based reward. Moreover, the performance of UoT (-UR) still
surpasses that of Adapted-ToT illustrated in Table 5,
4 Related Work
Planning and Reasoning of LLMs LLMs show prowess in planning and reasoning. Wei Wei
et al. (2022) introduced CoT prompting for intermediate reasoning; Yao et al. (2023) proposed ToT
prompting using DFS/BFS. Besta et al. (2023) present GoT to solve elaborate problems. Feng et al.
(2023) illustrated TS-LLM’s tree-search guided decoding. ReAct Yao et al. (2022) offers acting-based
prompting, while Reflexion Shinn et al. (2023) enhances this with feedback reflection. Zhou et al.
(2023) unify reasoning and planning.
Decision-making and Information-seeking by LLMs LLMs have evolved as decision-making tools,
with models like LLM+P Liu et al. (2023) and LLM-DP Dagan et al. (2023) combining external
planners and LLMs for natural language-based programming. RAP Hao et al. (2023) goes beyond
structured language, using LLMs with Monte Carlo Tree Search (MCTS) Chaslot et al. (2008) for
dynamic decision-making. This approach is also seen in the work of Zhao et al. (2023), applying
MCTS and LLM knowledge for complex tasks like robot control. However, MCTS struggles in
uncertain scenarios due to its reliance on terminal states and specific modules for rewards and action
selection. Additionally, to enhance LLMs’ questioning abilities, Deng et al. (2023) introduce the
Rephrase and Respond method. A VIS Hu et al. (2023) represents an autonomous visual question
answering system that uses external tools. Pan et al. (2023) introduce KwaiAgents for processing
queries, following guidelines, and accessing external documents.
5 Conclusion and Discussion
This paper presents the Uncertainty of Thoughts (UoT) algorithm, significantly improving LLMs in
tasks requiring active information seeking through tree-based simulation, uncertainty-based rewards
and a reward propagation scheme. On five datasets UoT increases success rate by 38.1% on average,
establishing a new benchmark for evaluating LLMs in active information-seeking tasks. We evaluate
UoT on simplified scenarios; more realistic scenarios raise challenges like allowing incomplete
elimination of possibilities by answers, and others which we leave for future work. We further discuss
these limitations and future work in Appendix H.
9
References
AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob
/main/MODEL_CARD.md .
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,
Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark
Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,
Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury,
Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A.
Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa
Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad
Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari,
Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz,
Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,
Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang
Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni,
Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John
Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov,
Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy,
Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So,
Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,
Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting
Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny
Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report. arXiv preprint arXiv:2305.10403 ,
2023.
Anthropic. Claude 2, 2023. URL https://www.anthropic.com/index/claude-2 .
Anthropic. Introducing the next generation of claude. 2024. URL https://www.anthropic.com/
news/claude-3-family .
Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna
Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al.
Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint
arXiv:2308.09687 , 2023.
Guillaume Chaslot, Sander Bakkes, Istvan Szita, and Pieter Spronck. Monte-carlo tree search: A
new framework for game ai. In Proceedings of the AAAI Conference on Artificial Intelligence and
Interactive Digital Entertainment , volume 4, pp. 216–217, 2008.
Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human
evaluations? arXiv preprint arXiv:2305.01937 , 2023.
Cohere. Cohere for ai, 2023. URL https://cohere.com/ .
Gautier Dagan, Frank Keller, and Alex Lascarides. Dynamic planning with a llm. arXiv preprint
arXiv:2308.06391 , 2023.
Yihe Deng, Weitong Zhang, Zixiang Chen, and Quanquan Gu. Rephrase and respond: Let large
language models ask better questions for themselves. arXiv preprint arXiv:2311.04205 , 2023.
Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-
search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179 ,
2023.
Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu.
Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992 ,
2023.
Martin N Hebart, Adam H Dickter, Alexis Kidder, Wan Y Kwok, Anna Corriveau, Caitlin Van Wicklin,
and Chris I Baker. Things: A database of 1,854 object concepts and more than 26,000 naturalistic
object images. PloS one , 14(10):e0223792, 2019.
10
Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou Sun, David A Ross, Cordelia Schmid,
and Alireza Fathi. Avis: Autonomous visual information seeking with large language model agent.
InThirty-seventh Conference on Neural Information Processing Systems , 2023.
Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone.
Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint
arXiv:2304.11477 , 2023.
Wenge Liu, Jianheng Tang, Yi Cheng, Wenjie Li, Yefeng Zheng, and Xiaodan Liang. Meddg: an
entity-centric medical consultation dataset for entity-aware medical dialogue generation. In CCF
International Conference on Natural Language Processing and Chinese Computing , pp. 447–459.
Springer, 2022.
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg
evaluation using gpt-4 with better human alignment, may 2023. arXiv preprint arXiv:2303.16634 .
Mistral.AI. Mistral large, our new flagship model. 2024. URL https://mistral.ai/news/mist
ral-large/ .
David Noever and Forrest McKee. Chatbots as problem solvers: Playing twenty questions with role
reversals. arXiv preprint arXiv:2301.01743 , 2023.
OpenAI. Gpt-3.5 turbo: A high-performance language model, 2023a. URL https://www.openai
.com/research/gpt-3-5-turbo . Whitepaper.
OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023b.
Haojie Pan, Zepeng Zhai, Hao Yuan, Yaojia Lv, Ruiji Fu, Ming Liu, Zhongyuan Wang, and Bing Qin.
Kwaiagents: Generalized information-seeking agent system with large language models. arXiv
preprint arXiv:2312.04889 , 2023.
Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S
Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th
Annual ACM Symposium on User Interface Software and Technology , pp. 1–22, 2023.
Dinesh Raghu, Shantanu Agarwal, Sachindra Joshi, and Mausam. End-to-end learning of flowchart
grounded task-oriented dialogs. In Proceedings of the 2021 Conference on Empirical Methods
in Natural Language Processing , pp. 4348–4366, Online and Punta Cana, Dominican Republic,
November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.
357. URL https://aclanthology.org/2021.emnlp-main.357 .
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste
Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini
1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint
arXiv:2403.05530 , 2024.
Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical
journal , 27(3):379–423, 1948.
Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic
memory and self-reflection. arXiv preprint arXiv:2303.11366 , 2023.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam
Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the
imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint
arXiv:2206.04615 , 2022.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.
Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language
models. arXiv preprint arXiv:2305.04091 , 2023.
11"
2307.04011,D:\Database\arxiv\papers\2307.04011.pdf,"How does the paper address the issue of transferring a model trained on idealized data to real-world robotic gripping scenarios, where conditions are likely to be more variable?","The paper identifies several issues that arise when transferring the model to real-world scenarios and proposes advanced data augmentation methods to address them. These methods generate synthetic data that mimics real-world variability in gripping, such as variations in slipping velocity, unloaded sensor pillars, and non-perpendicular forces.","(a)
 (b)
Fig. 3: (a) Illustration of how sequence data is input into a model to generate classiﬁcations for each window. The input is
represented by the red rectangle, which undergoes window reshaping, denoted by the dotted rounded rectangles. Trepresents the
number of data points in one data sequence, and in our case T= 40 samples at 1,000 Hz. The transparent circles represent
the hidden states; the initial hidden layer, h0, is initialized as a vector of zeros with the same shape as the hidden state of the
GRU cells. The encoder is shown as grey trapezoids connected to blue GRU cells, with purple circles representing recurrent
computations. The orange trapezoids represent the fully connected estimator, and the green items represent the output estimations.
(b) Illustration the aggregation process when the ensemble model is deployed, using Z= 5 classiﬁers in the emsemble.
C. Training data augmentation
1) Data augmentation by rotational symmetry: During the
data collection process, the sensor is placed at the origin of
the world coordinate frame. Its horizontal surface is parallel
to thex−yplane of the world frame of reference, and the
side edges align with the x−yaxis directions. Hence we use
a rotation transformation to augment the data; intuitively, it
can be understood as rotating the initial position of the sensor
around thezaxis by a random angle. For each data point in a
sequence, we perform the following mathematical calculations:
[
Fx′
Fy′]
=[
cos(θ)−sin(θ)
sin(θ) cos(θ)]
·[
Fx
Fy]
,θ∈[0,2π),(1)
whereFxandFyrepresent the force values along the original
x−yaxis, andFx′andFy′are the augmented force values
after virtual rotation of the sensor by a randomly sampled angle,
θ, from a uniform distribution of [0,2π).
2) Advanced data augmentation for domain adaptation: The
data used in our study was collected under idealized conditions,
where a hexapod robot was used to compress the sensor against
a ﬂat surface and move laterally in a controled manner. In this
setup, the force was nearly perpendicular or parallel to the
contact surface and the movement speed is nearly constant.
However, in real-world robotic gripping, the conditions are
expected to be quite different from this idealized setup, and
the performance of the model trained on such data is expected
to be poor. We identify several issues that may arise when
transferring the model trained on idealized data to real-world
gripping scenarios, and we propose a range of advanced data
augmentation methods to address these issues in the following
paragraphs. These methods are designed to generate synthetic
data that mimics the real-world variability of the gripping:
•Issue: The slipping velocity in real-world robotic gripping
is not constant, as it is inﬂuenced by various factors such asgravity, friction, and the shape of the object being gripped.
However, during the data collection process, the hexapod
induces slip at a constant velocity .Remedy: We employ
random sampling to sample a percentage of data points
from the raw data sequence, thereby generating a new
data sequence. And we maintain the frequency of the new
sequence at the same rate as the raw sequence (1,000 Hz).
This approach can simulate velocity variations to mimic
real-world gripping scenarios, as it changes the magnitude
differences of some temporally adjacent data points while
keeping the time interval unchanged.
•Issue: In some gripping scenarios, a portion of the sensor
pillars may not be in contact with the object. For instance,
this can occur when employing sensors to grip an object with
a rounded surface or when gripping an object smaller than
the sensor’s contact area .Remedy: To simulate an unloaded
pillar, we substitute a number of pillar data sequences with
zero sequences. Noise is then added to make the generated
sequence resemble a realistic sensor signal. The noise is
derived from a normal distribution with a mean of 0.0 N and
a standard deviation of 0.001 N.
•Issue: Unlike with the hexapod, the force generated by the
gripper may not be perfectly perpendicular to the x−yplane
of the sensor frame of reference, and the force leading to slip
may not be perfectly in this plane. This can occur when the
gripped object is not ﬂat or the mechanical linkage of the
gripper ﬂexes when applying force to the object .Remedy:
First, we sampled nine individual pillar sequences from raw
sensor sequences with different sensor compression levels
and hexapod movement types, and then combined them to
form a new sensor sequence. Secondly, we scaled (scale
factor ranging from 0.2 to 2.0) the magnitude of values for a
number of pillar sequences. Lastly, we randomly permuted the
position (by pillar index) of a nine-pillar sequence. Employing
these techniques can encourage the NN capture a broader and
more comprehensive pattern of incipient slip (see Section III-
B.3), rather than only learning the limited pattern introduced
by the hexapod.
D. Neural networks
The key decision making component of our incipient slip
detection approach is a binary classiﬁer. Initially, we trained
a NN capable of estimating the probability of incipient slip
for each time point in a sequence. Next, we set a threshold
to convert the continuous probability into a binary output. To
enhance the accuracy of the classiﬁer, we used an ensemble
technique that trains multiple independent classiﬁers concur-
rently and aggregates their output probabilities to produce the
ﬁnal decision (shown in Fig. 3(b)).
1) Architecture: Fig. 3(a) illustrates the process of inputting
a data sequence into the NN and obtaining the corresponding
slip classiﬁcation. The modiﬁed data sequence, as explained
in Section III-B.4, is input into an encoder. Subsequently, the
encoder output is passed to a speciﬁc type of RNN called a
gated recurrent unit (GRU) [24]. In our approach, we utilize a
single layer of GRU for each propagation step, and we refer
to it as a GRU cell. The hidden output from the GRU cell is
generated as a combination of the current input and historical
information. Moreover, an estimator is included that takes the
hidden layer output from the GRU cell and converts it into a
probability estimation. The ground truth label of each window
is determined by the label of the last sample in the window.
2) Training: The ensemble model consists of Z(Z= 5 in
our case) independently trained classiﬁer models. During each
training iteration of each classiﬁer model, a subset comprising
a proportion of λsequences (λ= 40% in our case) is randomly
sampled with replacement from the entire training set and used
for NN training. The ﬁnal layer of the estimator utilizes a two-
class softmax activation function, with its outputs interpreted
as probabilities for the occurrence of incipient slip and other .
Our chosen loss function is binary cross-entropy.
3) Decision making: We aggregate the output probability
from each classiﬁer model in the ensemble to convert the
continuous probability to binary prediction:
f:=1[∑Z
z=1Mz(x= [F(n−1)T+1,···,FnT])
Z>P th]
,
(2)
where 1[·]is an indicator function, Mzdonates thezthclassiﬁer
model in the ensemble, xdonates the input vector, and Pth
denotes the probability threshold, which is 50% in our work.
Zdonates the number of classiﬁers in the ensemble model.
IV. E XPERIMENTS AND RESULTS
We ﬁrst explicitly display our method’s high success rate in
detecting incipient slip, including ofﬂine and online scenarios.
Then, we illustrate the practical beneﬁts of our approach by
showcasing its ability to stabilize an insecure robotic grasp in
a number of practical gripping tasks.
A. Ofﬂine evaluation
The entire dataset is randomly split into two subsets: a
training set (∼80% of the entire dataset, comprising 160 data
(a)
(b)
 (c)
Fig. 4: The ofﬂine evaluation results. (a) Illustration of two
examples of successfully classiﬁed sequences, one correspond-
ing to slip event and the other to stop event. (b) Illustration
of the confusion matrix for the augmented test set, where TP
represent successful detection of incipient slip, FN represents
incipient slip is detected in a sequence of stop event, FP
represents nothing is detected in a sequence of slip event,
and TN represents the absence of detection in stop events,
i.e. successful classiﬁcation. (c) Illustration of the latency for
successful TP cases in predicting incipient slip. This ﬁgure was
created with reference to the moment when the ﬁrst of the nine
pillars slips (the ground truth for the onset of incipient slip).
sequences of slip event and 23 data sequences of stop event)
for model training, and a test set ( ∼20% of the entire dataset,
consisting of 40 data sequences of slip event and 5 data
sequences of stop event) for model evaluation. Both subsets are
expanded through the symmetry-based augmentation method
described in Section III-C.1, resulting in a ﬁve-fold increase
in the size of the training set and test set.
Fig. 4(a) displays two examples comparing the incipient
slip detection results over slip and stop events. As observed,
the algorithm’s conﬁdence in labeling incipient slip increases
rapidly as incipient slip starts and decreases as it progresses
toward gross slip. In comparison, the probability in the stop
case ﬂuctuates slightly but remains well below the threshold.
We deﬁne an incipient slip detection as successful if it occurs
within a 0.3 second window preceding the true labeled time
point of incipient slip (to accommodate the error of the ground
truth) and prior to the occurrence of the gross slip. For the
stop event, a successful estimation is deﬁned as a classiﬁcation
of the entire sequence as other .
Fig. 4(b) presents the confusion matrix, displaying the ﬁnal
classiﬁcation results over the entire test set; our algorithm
achieves an overall success rate of ∼95.6%. The results also
demonstrate its effectiveness in differentiating between the slip
and stop events; this indicates that our algorithm is not simply
detecting the changes in the force and yank of the pillars, as
mentioned earlier in Section III-B.2.
Our algorithm can effectively detect incipient slip in its early
stages. In Fig. 4(c), we present the latency between the moment
of incipient slip detected by the algorithm and the ground truth
onset of incipient slip. It is evident that, on average, incipient
slip can be detected within 10 ms of its initiation.
Fig. 5: Everyday objects for our online gripping task.
B. Online evaluation
In the online evaluation stage, we utilized the full data
set for training the ﬁnal deployed model. Again, to increase
the amount of training data, we applied both symmetry-based
(see Section III-C.1) and advanced data augmentation (see
Section III-C.2) techniques, resulting in a ﬁve-fold increase in
data amount (1140 data sequences).
The online evaluation was performed on six everyday objects,
depicted in Fig. 5. We include objects of varying surface
materials, curvatures, and hardness to ensure a broad range of
conditions are represented in our results.
1) Validating incipient slip detections: We cannot easily
validate incipient slip occurrences for everyday objects as
we cannot independently monitor individual pillar contacts.
Hence, we choose to perform the online evaluation based
on following well-founded assumptions. The incipient slip
detection is considered successful if it can be detected at any
time-point between the time when the robot’s movement begins
(Tm) and the time when gross slip occurs ( Tg); the criterion for
determining the occurrence of gross slip has been arbitrarily
deﬁned as the occurrence of relative translational movement
greater than 2 mm or relative rotational movement exceeding
2◦between the object and the robot’s frame of reference.
To induce a slip, the gripper ﬁrst grips the object with a
constant force. Then the robot moves the gripper downwards
towards a rigid and stationary table surface, eliciting the slip
between the sensor attached to the gripper tip and the object.
In each trial, the gripping force is selected from a range of
8 N to 30 N. The robot movement can be either translational,
rotational or a combination of translational and rotational. The
velocity (v) and acceleration ( a) of the robot movement have
three different levels: low ( v= 4 mm.s−1,a= 10 mm.s−2),
medium (v= 10 mm.s−1,a= 50 mm.s−2), and high ( v=
40mm.s−1,a= 100 mm.s−2). All robot movements were
performed using the built-in movel function of the UR script.The tool center position and orientation are obtained using the
built-in getlfunction of the UR robot. This function employs
forward kinematics calculations based on the read joint angles.
In accordance with the ofﬂine evaluation, control trails
are also conducted here for each vandacombination and
movement type. The purpose is to validate that the identiﬁed
behavior is indeed the incipient slip, rather the event with
similar pattern like the stop event we mentioned above. The
control data involves lifting the robot arm while maintaining a
secure grip using a pre-determined grip force that is sufﬁcient
to prevent any slippage. As a result, when lifting an object,
the pillars in contact undergo downward deformation due to
the force of gravity; subsequently, once the object is securely
held by the gripper and remains relatively motionless, these
pillars will remain stationary. Here, for the sake of convenient
explanation, we will also refer to this event as stop, and we label
the sequence as other . To ensure a fair experiment, we add extra
weight to lightweight objects to enhance their downward motion
when being lifted, aiming to make the pattern of the output
data sequence more like a slip event. In total, our experiment
consisted of 216 trials, including 162 sequences of slip event
(6 objects×3 movements×3 forces×3 velocity/acceleration
combinations) and 54 sequences of stop event (6 objects ×3
movements×1 force×3 velocity/acceleration combinations).
Fig. 6 illustrates the ﬁnal validation results. Fig. 6(a)
shows a confusion matrix, highlighting the high success rate
(∼96.8%) of our method in detecting incipient slip and its
ability to differentiate between slip and stop events. Fig. 6(b)
demonstrates that our algorithm can detect incipient slip almost
immediately upon the initiation of the movement that induces
slip, with a normalized displacement Dnorm range of 0.2 - 0.4,
within which the incipient slip can be detected (refer to the
caption for the deﬁnition of Dnorm ). These results provide
comprehensive validation of the effectiveness of our approach
in detecting incipient slipping in real-world gripping tasks.
2) Ablation study: This study aims to showcase the effec-
tiveness of our advanced augmentation method in bridging
the domain gap between the idealized data collected with
the hexapod and more realistic data encountered with the
robotic gripper. To accomplish this, we employed the model
training approach described in Section IV-A. However, instead
of splitting the data into separate train and test sets, we
trained the model using the entire dataset here, given the
different objective. Subsequently, we conducted online gripping
experiments, as described in Section IV-B.1, using this trained
model. Our ﬁndings, as illustrated in Fig. 7, indicate that the
model trained without our advanced augmentation method
exhibits a notably high false positive rate in the subsequent
online gripping task when compared to the results shown in
Fig. 6(a) where the model was trained using our advanced
augmentation method. In other words, the model trained without
our advanced augmentation is unable to effectively distinguish
patterns between slip and stop events. As a result, it incorrectly
detects incipient slip in many stop events.
C. Grasp stabilization after incipient slip detection
This experiment aims to show the beneﬁt of using our
incipient slip detection method in practical gripping tasks. This
(a)
 (b)
Fig. 6: (a) Illustration of a confusion matrix for the classiﬁcation. (b) Illustration of our validation of incipient slip in online testing.
Thexaxis in each sub-ﬁgure represents the normalized robot translational/angular displacement, calculated as Dnorm =D/D th,
whereDis the displacement before normalizing and Dth(2 mm for translation and 2◦for rotation) is an assumption that
indicates the reasonable minimum distance at which the gross slip must have happened or is likely to happen. To calculate Dnorm
for the compound Translation + Rotation sequences, we calculate the Dnorm for both the translation and rotation displacements
respectively and then take their mean value. Dnorm facilitates the intuitive visualization of when incipient slip is detected during
movement of the robot and also facilitates comparison across three different types of movement. The blue gradient ﬁll shows
increasing conﬁdence of gross slip, starting at 0.0 when the robot starts moving and reaching 1.0 at Dth.
Fig. 7: The ablation study results are to be compared with
those presented in Fig. 6(a). The model here is trained without
using data augmentation of the training data set leading to an
increased number of misclassiﬁcations.
involve lifting the robot arm while gripping the object with
a pre-determined small force to ensure that slip occurs. We
applied our incipient slip detection method and adjusted the
grip when incipient slip was ﬁrst detected to prevent the object
from slipping further. In this experiment, we simulate two
common scenarios that can trigger slips. The ﬁrst involves
gripping an object at its center of gravity with insufﬁcient
force and lifting it, causing a translational slip between the
gripper and the object. The second involves gripping an object
away from its center of gravity and lifting it, where rotational
slip is likely to occur. We implemented a simple grip force
adaptation that responds to incipient slip detection as follows:
if incipient slip is detected, the robot immediately stops, and
the gripper applies a pre-determined secure force to the object.
The objects used in the experiment are the same as those shown
in Fig. 5. The experiment was conducted 36 times (6 objects
×2 scenarios (translation or rotation) ×3 repetitions). We
ﬁx ArUco markers on the objects and gripper and use Python
OpenCV to track the positions and orientations of all.
We report the results in Table I, which demonstrate the
quickly and effective detection of incipient slip using our
algorithm. On average, our algorithm can timely detect incipient
slip and prevent the object from slipping when the relativeTABLE I: The translation and angular displacement between
the object and the gripper after being lifted in our online
gripping and lifting experiment, utilizing our incipient slip
detection and grip correction methods.
Object Translation (mm) Rotation (◦)
Hard paper box 4.5±0.6 1 .9±0.6
Wooden box 2.4±0.8 1 .5±0.9
Shampoo bottle 2.8±1.2 2 .1±0.4
Coffee jar 2.1±0.5 1 .6±0.2
Pringles can 1.4±0.4 1 .1±0.3
Thermos ﬂask 2.1±0.6 1 .2±0.4
Average 2.5±0.7 1 .9±0.5
translation between the object and the gripper reaches 2.5 mm
and the relative rotation reaches 1.9◦. Our algorithm showcases
its ability to facilitate timely corrective action, preventing object
falls; a demonstration video can be seen at our project website
given in the abstract.
V. D ISCUSSION
Our developed algorithm enables the NN to effectively learn
the incipient slip pattern from ofﬂine data and demonstrates
high accuracy in both ofﬂine and online test sets. Furthermore,
our algorithm enhances the security of robotic gripping.
Compared to previous related works [6], [13], our algorithm
offers several advantages. Firstly, our incipient slip detection
algorithm incorporates a data-driven learning-based approach,
minimizing the need for extensive human involvement in
investigating the complex patterns of incipient slip. Secondly,
the improved robustness of our algorithm enables the NN to
effectively adapt to diverse domains with various types of
PapillArray sensors and robotic gripping systems, despite being
trained solely on data lacking heterogeneity. Therefore, our
algorithm is more practical and possesses greater potential for
maximizing the utilization of valuable tactile data in real-world
scenarios. Thirdly, our algorithm has the ability to distinguish
between incipient slip and a closely related tactile pattern that
we refer to as a stop event. Notably, previous related work
[6], [7], [13], [19] has not adequately considered or addressed
the stop event; however, our investigation has revealed the
importance of including stop events when developing incipient
slip detection algorithms due to their similar patterns but entirely
different consequences.
There are limitations to our work that need consideration.
Firstly, the incipient slip detection could be improved by
transitioning from a binary signal to a continuous warning
signal. For instance, if incipient slip is detected in a small
portion of the contact surface, the remaining area may still
possess sufﬁcient fraction to prevent signiﬁcant slippage. In
such cases, the warning level of incipient slip is low and
corrective actions may not be necessary. Conversely, if a
signiﬁcant portion of the contact surface exhibits incipient slip,
the warning level should escalate and it becomes important to
for appropriate corrective actions. Moreover, our current choice
of force adaptation method for reacting to incipient slip falls
short when compared to the state-of-the-art gripping control
work [6]. However, it is important to note that force adjustment
is not the primary focus of our research in this paper, which
is focused on improving the incipient slip detection. In future
work, we will develop a more sophisticated force adaptation
technique that incorporates our incipient slip detection method.
VI. C ONCLUSION
In conclusion, this paper presents an incipient slip detection
method that employs deep learning and several data augmenta-
tion techniques to improve the robustness of the trained NN.
Our method is highly effective and reaches the state-of-art
performance, it enable a single pre-trained NN model to be
applied across various domains and tasks. In addition, our
method has the potential to be extended to other approaches
that use compliant tactile sensors.
APPENDIX
To train the NN parameters, we use stochastic gradient
descent with a momentum of 0.95 and a learning rate of 10−3,
with a batch size of 512. We also incorporate a weight decay of
10−3usingL2regularization during training. The encoder NN
consists of one hidden layer with 1024 units, and the output
dimension is 128. The GRU cell has a hidden layer dimension
of 128. The predictor network comprises two hidden layers
with 256 and 128 units, respectively. To all hidden layers, we
apply rectiﬁed non-linearity [25] and batch normalization [26].
We implement our NN using PyTorch (Version 1.12.1, Meta,
USA). All our experiments are conducted on a PC with an Intel
7-10875H CPU and an NVIDIA 2060 GPU. During the online
evaluation stage, e utilise ROS [27] to facilitate communication
between various components in our system.
REFERENCES
[1]R. S. Johansson and G. Westling, “Roles of glabrous skin receptors and
sensorimotor memory in automatic control of precision grip when lifting
rougher or more slippery objects,” Experimental Brain Research , vol. 56,
pp. 550–564, 1984.
[2]A.-S. Augurelle, A. M. Smith, T. Lejeune, and J.-L. Thonnard, “Im-
portance of cutaneous feedback in maintaining a secure grip during
manipulation of hand-held objects,” Journal of Neurophysiology , vol. 89,
no. 2, pp. 665–671, 2003.
[3]A. B. Vallbo, R. S. Johansson, et al. , “Properties of cutaneous mechanore-
ceptors in the human hand related to touch sensation,” Hum Neurobiol ,
vol. 3, no. 1, pp. 3–14, 1984.[4]W. Chen, H. Khamis, I. Birznieks, N. F. Lepora, and S. J. Redmond,
“Tactile sensors for friction estimation and incipient slip detection—toward
dexterous robotic manipulation: A review,” IEEE Sensors Journal , vol. 18,
no. 22, pp. 9049–9064, 2018.
[5]B. P. Delhaye, E. Jarocka, A. Barrea, J.-L. Thonnard, B. Edin, and
P. Lefevre, “High-resolution imaging of skin deformation shows that
afferents from human ﬁngertips signal slip onset,” Elife, vol. 10, p. e64679,
2021.
[6]H. Khamis, B. Xia, and S. J. Redmond, “Real-time friction estimation for
grip force control,” in 2021 IEEE International Conference on Robotics
and Automation (ICRA) , pp. 1608–1614, IEEE, 2021.
[7]J. W. James, N. Pestell, and N. F. Lepora, “Slip detection with a
biomimetic tactile sensor,” IEEE Robotics and Automation Letters , vol. 3,
no. 4, pp. 3340–3346, 2018.
[8]M. Schöpfer, C. Schürmann, M. Pardowitz, and H. Ritter, “Using a
piezo-resistive tactile sensor for detection of incipient slippage,” in ISR
2010 (41st International Symposium on Robotics) and ROBOTIK 2010
(6th German Conference on Robotics) , pp. 1–7, VDE, 2010.
[9]S. du Bois de Dunilac, D. Córdova Bulens, P. Lefèvre, S. J. Redmond,
and B. P. Delhaye, “Biomechanics of the ﬁnger pad in response to torsion,”
Journal of the Royal Society Interface , vol. 20, no. 201, p. 20220809,
2023.
[10] B. Delhaye, P. Lefevre, and J.-L. Thonnard, “Dynamics of ﬁngertip
contact during the onset of tangential slip,” Journal of The Royal Society
Interface , vol. 11, no. 100, p. 20140698, 2014.
[11] Z. Su, K. Hausman, Y . Chebotar, A. Molchanov, G. E. Loeb,
G. S. Sukhatme, and S. Schaal, “Force estimation and slip detec-
tion/classiﬁcation for grip control using a biomimetic tactile sensor,”
in2015 IEEE-RAS 15th International Conference on Humanoid Robots
(Humanoids) , pp. 297–303, IEEE, 2015.
[12] J. W. James, S. J. Redmond, and N. F. Lepora, “A biomimetic tactile
ﬁngerprint induces incipient slip,” in 2020 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS) , pp. 9833–9839,
IEEE, 2020.
[13] P. M. Ulloa, D. C. Bulens, and S. J. Redmond, “Incipient slip detection
for rectilinear movements using the papillarray tactile sensor,” in 2022
IEEE Sensors , pp. 1–4, 2022.
[14] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning internal
representations by error propagation,” tech. rep., California Univ San
Diego La Jolla Inst for Cognitive Science, 1985.
[15] B. E. Boser, I. M. Guyon, and V . N. Vapnik, “A training algorithm for
optimal margin classiﬁers,” in Proceedings of the Fifth Annual Workshop
on Computational Learning Theory , pp. 144–152, 1992.
[16] C. Chorley, C. Melhuish, T. Pipe, and J. Rossiter, “Development of a
tactile sensor based on biologically inspired edge encoding,” in 2009
International Conference on Advanced Robotics , pp. 1–6, IEEE, 2009.
[17] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE , vol. 86,
no. 11, pp. 2278–2324, 1998.
[18] W. Yuan, S. Dong, and E. H. Adelson, “Gelsight: High-resolution robot
tactile sensors for estimating geometry and force,” Sensors , vol. 17,
no. 12, p. 2762, 2017.
[19] W. Yuan, R. Li, M. A. Srinivasan, and E. H. Adelson, “Measurement of
shear and slip with a gelsight tactile sensor,” in 2015 IEEE International
Conference on Robotics and Automation (ICRA) , pp. 304–311, IEEE,
2015.
[20] S. Dong, D. Ma, E. Donlon, and A. Rodriguez, “Maintaining grasps
within slipping bounds by monitoring incipient slip,” in 2019 International
Conference on Robotics and Automation (ICRA) , pp. 3818–3824, IEEE,
2019.
[21] H. Khamis, B. Xia, and S. J. Redmond, “A novel optical 3d force and
displacement sensor–towards instrumenting the papillarray tactile sensor,”
Sensors and Actuators A: Physical , vol. 291, pp. 174–187, 2019.
[22] H. Khamis, R. I. Albero, M. Salerno, A. S. Idil, A. Loizou, and S. J.
Redmond, “Papillarray: An incipient slip sensor for dexterous robotic or
prosthetic manipulation–design and prototype validation,” Sensors and
Actuators A: Physical , vol. 270, pp. 195–204, 2018.
[23] R. S. Johansson and A. B. Vallbo, “Tactile sensibility in the human hand:
relative and absolute densities of four types of mechanoreceptive units in
glabrous skin.,” The Journal of Physiology , vol. 286, no. 1, pp. 283–300,
1979.
[24] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio, “Empirical evaluation of
gated recurrent neural networks on sequence modeling,” arXiv preprint
arXiv:1412.3555 , 2014.
[25] X. Glorot, A. Bordes, and Y . Bengio, “Deep sparse rectiﬁer neural
networks,” in Proceedings of the Fourteenth International Conference on
Artiﬁcial Intelligence and Statistics , pp. 315–323, JMLR Workshop and
Conference Proceedings, 2011.
[26] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
training by reducing internal covariate shift,” in International Conference
on Machine Learning , pp. 448–456, PMLR, 2015.
[27] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs, R. Wheeler,
A. Y . Ng, et al. , “Ros: an open-source robot operating system,” in ICRA
workshop on open source software , vol. 3, p. 5, Kobe, Japan, 2009."
2302.13934,D:\Database\arxiv\papers\2302.13934.pdf,"In the context of learning with distribution shift, how does the complexity of the function class G, which represents the nuisance function, affect the performance of the predictor when the shift in the marginal distribution of y is significantly smaller than the shift in the joint distribution of (x, y)?","The complexity of the function class G, particularly its ℓ∞ metric entropy, plays a crucial role in determining the predictor's performance under distribution shift. When the shift in the marginal distribution of y is much smaller than the shift in the joint distribution of (x, y), the predictor's performance is more sensitive to the complexity of G, as reflected in the squared convergence rate of G in the error bounds.","for which (x, y)7→Etrain[z|x=x,y=x]∈ F+G. This is typically referred to as being realizable or
well-specified.
Assumption 2.2 (Additive well-specification) .For some f⋆∈ F andg⋆∈ G, it holds thats
Ptrain[z|x=x,y=x]∼ N(f⋆(x) +g⋆(y), σ2) (2.1)
Via universality of Gaussian processes, our results can be extended to general subgaussian noise. Since
the model is well-specified, a natural performance measure of a predictor (f, g)is its excess square-loss risk,
denoted Re(f, g):
Re(f, g) :=Ee(f(x) +g(y)−z)2− inf
f′∈F,g′∈GEe(f′(x) +g′(y)−z)2
=Ee((f−f⋆)(x) + (g−g⋆)(y))2.
Empirical Risk Minimization. We study the excess risk under Ptestof square-loss empirical risk mini-
mizers, or ERMs, for Ptrain. Given a number n∈N, we collect (xi,yi,zi)i∈[n]i.i.d∼Ptrainsamples and let
(ˆfn,ˆgn)denote (any) empirical risk minimizer of the samples:
(ˆfn,ˆgn)∈arg min
(f,g)∈F×GˆLn(f, g),ˆLn(f, g) :=1
nnX
i=1(f(xi) +g(yi)−zi)2. (2.2)
Distribution Shift. Although we have samples from Ptrain, we are primarily interested in the excess square
loss under Ptest. For simplicity, the body of this paper focuses on when the density ratios between these
distributions are upper bounded; as discussed in Section 3.4, these conditions can be weakened considerably.
We introduce the density ratio coefficients for the joint distribution (x,y)as well as the marginal distribution
overy.
Definition 2.1. Define the density ratio coefficients νx,y, νy≥1to be the smallest scalars such that for all
measurable sets A⊂ X × Y andB⊂ Y,
Ptest[(x,y)∈A]≤νx,yPtrain[(x,y)∈A],Ptest[y∈B]≤νyPtrain[y∈B].
The interesting regime is where νx,y, νyare finite. A standard covariate shift argument upper bounds the
excess risk on Ptestby the joint density ratio, νx,y, times the excess risk on Ptrain. Our aim is to show that
much better bounds are possible. Specifically, if the class Fis “smaller” than the class G, then the excess
risk on Ptestisless sensitive to shifts in the joint distribution (i.e., νx,y) than it is to shifts in the y-marginal
(i.e.,νy). Such an improvement is most interesting in the regime where νx,y≫νy, which requires that xis
not a measurable function of y.
Controlling distribution shift via bounded density ratios is popular in the offline reinforcement learning,
where such terms are called concentrability coefficients [Xie and Jiang, 2020, Xie et al., 2022]. We stress
that the uniform density ratio bounds in this section are merely for convenience; we discuss generalizations
at length in Section 3.4.
4
Conditional Completeness. Notice that (f⋆, g⋆)may not be identifiable in the model Eq. (2.1). The most
glaring counterexample occurs when x=y, and f⋆+g⋆∈ F ∩ G . Then, (f, g) = ( f⋆+g⋆,0)and
(f, g) = ( 0, f⋆+g⋆)are both optimal pairs of predictors. However, this setting is uninteresting for our
purposes, since x=yimplies that νx,y=νy. On the other hand, when xandyare independent, the model
is identifiable up to a constant offset, i.e., (f⋆+c, g⋆−c)is an optimal pair. This line of reasoning suggests
that the indentifiable part of f⋆in Eq. (2.1) corresponds to the part of xthat is orthogonal to y. To capture
this effect, we introduce the conditional bias offgiven yunder the training distribution :
βf(·) =Etrain[(f−f⋆)(x)|y=·]. (2.3)
Note that this is a function of y, notx. One can check that Rtrain(f, g) = 0 if and only if (f(x), g(y)) =
(f⋆(x) +βf(y), g⋆(y)−βf(y))with probability one over (x,y)∼Ptrain. Note, in particular, that this
requires βfis almost surely (under Ptrain) equal to a measurable function of x. This allows, for example,
(f, g) = ( f⋆−c, g⋆+c)for constants c∈R, and, in particular, (f⋆, g⋆)meet these requirements since
βf⋆= 0.
We now introduce our final, and arguably only non-standard, assumption.
Assumption 2.3 (γ-Conditional Completeness) .There exists some γ >0such that, for any (f, g)∈ F ×G
satisfying Rtrain(f, g)≤γ2, it holds that g−βf∈ G.
Conditional completeness is somewhat non-intuitive but it is satisfied in some natural cases. We list
them here informally, and defer formal exposition to Appendix A.1. First, as aluded to above, when x⊥y,
βf(y)is constant in yand so conditional completeness holds as long as Gis closed under affine translation.
Second, it holds when FandGare linear classes and xandyare jointly Gaussian; this follows since
the conditional distribution Etrain[x|y=y]is linear in y. The latter example extends to nonparametric
settings: conditional completeness holds if the conditional expectations x|yare smooth and Gcontains
correspondingly smooth functions.
The restriction to Rtrain(f, g)≤γ2allows us to make the assumption compatible with the following,
standard boundedness assumption (for otherwise we would need to have g−kβf∈ G for all k∈N,
see Remark A.1.)
Assumption 2.4 (Boundedness) .We assume that for all f∈ F andg∈ G,|f(x)|and|g(y)|are uniformly
bounded by some B >0. For simplicity, we also assume FandGcontain the zero predictor.
Notation. We use a≲bto denote inequality up to universal constants, and use O(·)andeO(·)as informal
notation suppressing problem-dependent constants and logarithmic factors, respectively. A scalar-valued
random variable is standard normal if Z∼ N (0,1)and Rademacher if Zis uniform on {−1,1}. For
v= (v1, . . . , v n)∈Rnandq∈[1,∞), define the normalized q-norms ∥v∥q,n= (1
nPn
i=1|vi|q)1/qand
∥v∥∞,n=∥v∥∞= max i∈[n]|vi|. We let W=X × Y with elements w∈ W , so we can view classes
f∈ F, g∈ G, and βfas mappings with type W → R. Given h∈ H and a sequence w1:n∈ Wn, define
the evaluation vector h[w1:n] := ( h(w1), . . . , h (wn))∈Rnand evaluated class H[w1:n] :={h[w1:n] :h∈
H} ⊂ Rn.
3 Results
All of our results follow from the same schematic: we argue that if Fis simpler than G, it is much easier to
recover f⋆than it is to recover g⋆, subject to the identifiability issues introduced by βf. To express this, we
5
introduce the per-function risks, for e∈ {train,test}:
Re[f] :=Ee[(f−f⋆−βf)2],Re[g;f] :=Ee[(g−g⋆+βf)2]
Our schematic shows that Rtrain[ˆfn]≪ R train[ˆgn;ˆfn], with precise convergence rates. The expression
Re[f]reflects that fis identifiable only up to a bias, while Re[g;f]can be thought of as the residual error
after accounting for the bias in f. A straightforward consequence of these definitions is the following risk
decomposition:
Lemma 3.1. Let(f, g)∈ F × G . Then, under Assms. 2.1 and 2.2, Rtrain(f, g) =Rtrain[f] +Rtrain[g;f].
Morever, Rtest(f, g)≤2(Rtest[f] +Rtest[g;f]). Therefore,
Rtest(f, g)≤2(νx,yRtrain[f] +νyRtrain[g;f])≤2(νx,yRtrain[f] +νyRtrain(f, g)). (3.1)
Eq. (3.1) is the starting point for our results. By comparison, the standard distribution shift bound is
Rtest(f, g)≤νx,yRtrain(f, g). (3.2)
Hence, Eq. (3.1) leads to sharper estimates for ERM in the regime where νy≪νx,yandRtrain[ˆfn]≪
Rtest[ˆfn,ˆgn), i.e., when the shift in yis less than the shift in the joint distribution and when the estimate of
f⋆is more accurate than the estimate of f⋆+g⋆.
The bulk of the analysis involves obtaining sharp bounds on Rtrain[ˆfn], this is sketched in Section 4. In
the remainder of this section, we describe implications for various settings of interest.
3.1 Nonparametric Rates
We begin by demonstrating improvements in the non-parametric regime , where we measure the complexity
of function classes by their metric entropies. Recall that an ϵ-cover of a set Vin a norm ∥ · ∥ is a set V′⊂V
such that, for any v∈V, there exists v′∈V′for which ∥v−v′∥ ≤ϵ. The covering number ofVat scale ϵ
in norm ∥ · ∥ is the minimal cardinality of an ϵ-cover, denoted N(V,∥ · ∥, ε). Metric entropies of function
classes are defined via the logarithm of the covering number.
Definition 3.1 (Metric Entropy) .We define the q-norm metric entropy of a function class H:W → Ras
Mq(ϵ,H) := supnsupw1:nlogN(H[w1:n],∥ · ∥ q,n, ε).
As in classical results in statistical learning theory, rates of convergence depend on function class com-
plexity primarily through the growth rate of the metric entropy, i.e., how Mq(ϵ,H)scales as a function of
ϵ. We state our first main result informally, in line with this tradition.
Theorem 1 (Informal) .Under Assm. 2.1-Assm. 2.4, the error of (ˆfn,ˆgn)underPtestis bounded as follows
with high probability:
Rtest(ˆfn,ˆgn)≲eO
νx,y
rate n,2(F) + rate n,⋆(G)2+σ2
n
+νyrate n,2(G)
,
where above we define
rate n,q(H) =

d
nMq(ϵ,H) =O(dlog(1/ϵ))
n−2
2+pMq(ϵ,H) =O(ϵ−p), p≤2
n−1
pMq(ϵ,H) =O(ϵ−p), p > 2, (3.3)
andrate n,⋆(H) =n−(1/2∧1/p)forM∞(ϵ,H) =O(ϵ−p).
6
A formal statement is given in Appendix E. As a preliminary point of comparison, the naive analysis
would yield a bound of the form
Rtest(ˆfn,ˆgn)≤eO
νx,y(rate n,2(F) + rate n,2(G)) +σ2
n
, (naive analysis, covariate shift)
which can be worse than the above bound when νy≪νx,yandrate n,⋆(G)2≪rate n,2(G). The rate in
Theorem 1 is a consequence of the second result:
Theorem 2 (Faster recovery of f⋆up to bias, informal) .Adopt the notation of Theorem 1. With high
probability, it holds that
Rtrain[ˆfn] =Etrain[(ˆf−f⋆−βˆfn)2]≲eO
rate n,2(F) + rate n,⋆(G)2+σ2
n
, (3.4)
It is crucial to note that the interaction between the complexity of the class Gand the distribution shift pa-
rameter νx,yin Theorem 1, as well as the dependence of the bias-adjusted risk Rtrain[ˆfn]ofGin Theorem 2,
scales with the squared convergence rate for G.
Analogously, naively upper bounding Rtrain[ˆfn]≤ R train(f, g)would yield
Rtrain[ˆfn] =Etrain[(ˆf−f⋆−βˆfn)2]≲eO
rate n,2(F) + rate n,2(G) +σ2
n
, .
(naive analysis, recovery of f⋆)
Examining the definition of the rate functions in Theorem 1, we see that when the ℓ2andℓ∞metric entropies
ofGare comparable, we can see that rate ⋆,n(G)2≪rate n,2(G), and, when bounding the rate function with
exponent p≥2in (3.3) (above the so-called Donsker threshold), rate ⋆,n(G)2∼rate n,2(G)2. In these cases,
Theorems 1 and 2 yield substantial improvements of the naive counterparts.
3.2 Comparison with Orthogonal ML
The style of our results is similar to those appearing in the literature on Neyman orthogonalization (also re-
ferred to as Double/Debiased ML or orthogonal statistical learning) [c.f., Chernozhukov et al., 2017, Mackey
et al., 2018, Foster and Syrgkanis, 2019]. At a high level, orthogonal ML considers a situation with an un-
known pair (f⋆, g⋆), where we are primarily interested in learning f⋆, referring to g⋆as a nuisance function.
We describe two categories of differences: difference in problem specification and difference in statistical
rates .
Differences in problem specification. In orthogonal ML, the parameter g⋆is truly a nuissance whose
confounding effect on f⋆is to be removed. In our setting, however, the optimal predictor depends on both
f⋆andg⋆through their sum, and thus g⋆cannot be neglected in the prediction.
Moreover, orthogonal ML leverages an auxiliary supervision mechanism to learn g⋆in order to remove
it. In contrast, we reason about the statistical convergence of single-step ERM without access to auxilliary
information
7
Differences in statistical rates. In orthogonal ML with ERM, it is shown in Foster and Syrgkanis [2019]
that the dependence of recovery of f⋆on the class Gscales as
E[(ˆfOrthogonalML −f⋆)2]≲eO
rate n,2(F) + rate n,2(G)2+σ2
n
. (3.5)
Qualitatively, the rates are similar to those in Theorems 1 and 2, with the exception that we replace rate n,⋆(G)
withrate n,2(G). There are two comparative weakness in our bound:
(a) First, rate n,⋆(G)dependence on the ℓ∞covering numbers of G, whereas rate n,2(G)depends on the
ℓ2covering numbers.
(b) For p≤1/2(below the so-called Donsker threshold), rate n,2(G)2can decay to zero faster than
O(1/n), leaving the σ2/nterm to dominate it. On the other hand, rate n,⋆(G)2scales as 1/nwith
some constant factor prepended, and thus, can dominate the σ2/nterm when this constant factor is
large. Similarly, dependence on σ2may differ between the two. We partially address this limitation
for finite (and more generally, parameteric) function classes, as discussed in Section 3.3.
The dependence on ℓ∞covering numbers arises from our Hölder Inequality for the Dudley integral, Proposi-
tion 4.5, applied to bounding the cross-interactions between the FandGclasses. The suboptimal rate n,⋆(G) =
O(n−1/2)forp≤1/2arises from the same proposition, which incurs a dependence on the unlocalized com-
plexity of the class Grather than the localized complexities which determine rate n,2. By comparison, Foster
and Syrgkanis [2019] use independent data to learn g⋆beforehand, and thus do not need to decorrelate ˆf
andˆgin the same way. It is an open question if this discrepancy reflects a limitation in our analysis or is a
fundamental limitation of ERM.
Aside from the above situations, our rates coincide. We summarize this observation:
Observation 3.2. Letσ2≥1and suppose that the class Gsatisfies M∞(ϵ,G)≤CM2(ϵ,G)for all ϵ >0
and some constant C. Then, for some constant C′depending only on Gsuch that
rate n,⋆(G)2+σ2
n≤C′
rate n,2(G)2+σ2
n
. (3.6)
Orthogonal ML without Orthogonal ML Despite its limitations, our bound can be somewhat more
practical than what is found in the orthogonal machine learning literature, as it applies to ERM directly and
does not require algorithmic modifications or an auxiliary supervision signal. The key difference here is that
whereas orthogonal ML aims for inference – consistent recovery of f⋆– we care only about the prediction
error off⋆+g⋆. Thus, we need not address the identifiability challenges present in orthogonal ML. As
a consequence, we bypass algorithmic modifications that typically require more precise modeling of the
data generating process, and which typically render orthogonal ML more susceptible to misspecification
issues. Finally, we should note that in canonical settings for orthogonal learning, we can show that our main
assumption, conditional completeness, holds. In this sense, our work shows that, in typically settings for
orthogonal learning, one can obtain similar statistical improvements with ERM alone andwithout auxiliary
supervision .
Please see Appendix A for an even more detailed discussion.
8
3.3 Finite Function Classes
When FandGare finite function classes with log|F| ≤ d1andlog|G| ≤ d2, an application of Theorem 1
gives the rate of Rtest(ˆfn,ˆgn)≲νx,y·d1+d2
n, which is precisely what one obtains via naive change of
measure arguments. Although direct application of Theorem 1 does not yield improvements—precisely
because of the lack of localization as discussed above— we canimprove upon this bound with an additional
hypercontractivity assumption, often popular in the statistical learning literature [Mendelson, 2015]. We
defer formal definitions, a formal theorem statement, and proofs to Appendix D; the following informal
theorem summarizes our findings.
Theorem 3 (Informal) .Under certain hypercontractivity conditions detailed in Appendix D, it holds with
high probability that
Rtest(ˆfn,ˆgn)≲1
n(νx,yd1+νyd2+νx,yd2·ϕn(d1, d2)),
where ϕn(d1, d2) = (d2
n)c1+(d1
d2)c2, for constants c1, c2>0depending on the hypercontractivity exponents.
When d2≫d1, the bound replaces the dimension term d2νx,ywithd2ϕ(d1, d2)νx,y+νyd2, a strict
improvement when νy≪νx,yandϕ(d1, d2)≪1. The above bound can be extended to function classes
with “parametric” metric entropy (Remark D.1). In all cases, ϕn(d1, d2)≥d2
n, which is still weaker than an
idealized version of Theorem 1 where rate n,2(G)2replaces rate n,⋆(G)2.
3.4 Refined Measures of Distribution Shift
The decomposition in Lemma 3.1 and all subsequent guarantees can be refined considerably. First, we
can replace uniform bounds on the density ratios (Definition 2.1) with the following function-dependent
quantities:
ν1:= sup
f∈FEtest[(f−f⋆−βf)2]
Etrain[(f−f⋆−βf)2](3.7)
ν2:= sup
f∈F,g∈GEtest[(g−g⋆−βf)2]
Etrain[(g−g⋆−βf)2], (3.8)
Corollary 3.1. Immediately from Lemma 3.1, it holds that Rtest(f, g)≤2(ν1Rtrain[f] +ν2Rtrain(f, g))
Both Theorem 1 and Theorem 3 continue to hold using ν1andν2instead of νx,yandνy. Note that
ν1≤νx,yandν2≤νyalways, but they can be much smaller as demonstrated by the follow upper bounds
onν1.
Lemma 3.3. Suppose x⊥yunderPtrain. Then ν1≤νx:= supA⊂XPtest[x∈A]/Ptrain[x∈A].
Lemma 3.4. Assume (a) Xis a Hilbert space, (b) the functions f∈ F are linear in xand (c) there are
constants νlin>0such that, with βx(y) :=Etrain[x|y],
Etest[(x−βx(y))(x−βx(y))H]⪯νlin·Etrain[(x−βx(y))(x−βx(y))H]
Then, ν1≤νlin.
Appendix A.2 proves both lemmas. Importantly, νlincan be finite even when νx,yis infinite, e.g. if the
distribution over xis discrete under Ptrain, but continuous under Ptest.
9
Beyond Uniform Ratios. Eqs. (3.7) and (3.8) can be generalized further to allow for additive error.
Corollary 3.2. Suppose that, for all f∈ F andg∈ G,
Etest[(f−f⋆−βf)2]≤ν1Etrain[(f−f⋆−βf)2] + ∆ 1
Etest[(g−g⋆−βf)2]≤ν2Etrain[(g−g⋆−βf)2] + ∆ 2,
Then, immediately from Lemma 3.1,
Rtest(f, g)≤2(ν1Rtrain[f] +ν2Rtrain(f, g) + ∆ 1+ ∆ 2).
This deceptively simple modification allows for situations when the density ratios between the test and
train distributions are not uniformly bounded, or possibly even infinite. Appendix A.3 details the many
consequences of this observation. We highlight a key one here:
Lemma 3.5. Suppose Assm. 2.4 holds. Then, for Rtrain[f],Rtrain(f, g)sufficiently small,
Rtest(f, g)≤8Bp
Rtrain[f]·χ2(Ptest(x,y),Ptrain(x,y))
+ 8Bp
Rtrain(f, g)·χ2(Ptest(y),Ptrain(y)),
where χ2(Ptest(x,y),Ptrain(x,y))denotes the χ2divergence (see e.g. Polyanskiy and Wu [2022, Chapter
2]) between the joint distribution of (x,y)under test and train distributions, and χ2(Ptest(y),Ptrain(y))
denotes χ2divergence restricted to the marginals of y.
The above lemma is qualitatively similar to Corollary 3.1 and Lemma 3.1: If Rtrain[f]≪ R train(f, g)
(as ensured by our analysis, under appropriate assumptions), then we ensure more resilience to the χ2
divergence between the joint distributions of (x,y)than would naively be expected.
4 Analysis Overview
We begin this section with formal precursors to Theorems 1 and 2 in terms of Dudley integrals [Dudley,
1967], stated as Theorems 4 and 5. The rest of the section provides an overview of the proof. Section 4.1
contains the necessary preliminaries, notably Rademacher and Gaussian complexities and their associated
critical radii. Section 4.2 provides the roadmap for the proof of Theorem 4, focusing on our novel excess
risk bound for Rtrain[ˆfn]in terms of a “cross critical radius” term. We bound this term in Section 4.3 via a
Hölder style inequality for Rademacher complexity.
For convenience, define the centered classes
Fcnt:={f−βf−f⋆:f∈ F}
Gcnt:={g−g⋆+βf:f∈ F, g∈ G}
Hcnt:={f+g−(f⋆+g⋆) :f∈ F, g∈ G}.
Formal Main Result. We define the Dudley functional, a standard measure of statistical complexity.
10
Definition 4.1 (Dudley Functional) .Letradq(V) := supv∈V∥v∥q,nbe the q-norm radius and Mq(V;·)
be the metric entropy in the induced ℓqnorm (Definition E.1). Given V⊂Rndefine Dudley’s chaining
functional (in the q-norm) as
Dn,q(V) := inf
δ≤radq(V) 
2δ+4√nZradq(V)
δq
Mq(V;ε/2)dε!
.
Furthermore, given a function class Hand letting H[r, w1:n]denotes the empirically localized class (Defi-
nition 4.2 below), define the Dudley critical radius
δn,D(H, c) := inf
r: sup
w1:nDn,2(H[r, w1:n])≤r2
2c
,
We now state the formal version of our main results. Calculations in Appendix E obtain Theorem 1 and
Theorem 2 by bounding the Dudley functionals using standard statistical learning arguments. First, we state
the precursor to Theorem 1.
Theorem 4. Suppose Assms. 2.1 to 2.4 hold. Let σB:= max {B, σ}, letν1, ν2be as in Eqs. (3.7) and(3.8) ,
and let c1be a sufficiently small universal constant. Then if Eq. (4.1) holds, that probability at least 1−δ,
Rtest(ˆfn,ˆgn)≲ν1
δn,D(Fcnt, σB)2+ sup
w1:nDn,∞(Gcnt[w1:n])2
+ν2·δn,D(Hcnt, σB)2
+(ν1+ν2)σ2
Blog(1/δ)
n.
This is derived from the following precursor to Theorem 2.
Theorem 5. Suppose Assms. 2.2 to 2.4 hold. Let σB:= max {B, σ}, and let c1>0be a sufficiently small
universal constant. If nis sufficiently large that
δn,D(Hcnt, σB)2+σ2
Blog(1/δ)
n≤c1γ, (4.1)
then it holds that probability at least 1−δ/2,
Rtrain[ˆfn]≲δn,D(Fcnt, σB)2+ sup
w1:nDn,∞(Gcnt[w1:n])2+σ2
Blog(1/δ)
n.
Appendix E converts these results into the Theorems 1 and 2. The first step is to replace the dependence
on centered classes FcntandGcntwith terms depending only on FandG. Then, one computes the Dudley
critical radii for classes with bounded metric entropy.
4.1 Learning-Theoretic Preliminaries
We state all definitions for a general class of functions Hmapping W → R. We define two key notions of
localized andproduct classes .
Definition 4.2 (Product and Localized Classes) .LetH,H′:W →R.
• We define the empirically localized function class as H[r, w1:n] :={h∈ H :1
nPn
i=1h(wi)2≤r}
andpopulation localized classH(r) :={h∈ H:Etrain[h2]≤r}.
11"
2009.09266,D:\Database\arxiv\papers\2009.09266.pdf,The paper describes a method for optimizing human-generated sketches to improve their recognition accuracy while minimizing the effort required to create them.  What are the potential implications of this method for the design of human-computer interfaces that involve drawing or sketching?,"This method could lead to more intuitive and efficient interfaces for tasks involving sketching, as it allows for optimization of user input, potentially reducing the cognitive load and improving the overall user experience.","tion ˆXhas been classiﬁed correctly so far.1If a solution
candidateZfulﬁlls both constraints and it has lower loss
value according to the user determined objective obj, the
best solution ˆXis set to the solution candidate Z. Solution
candidates are generated for each operation as follows: For
removal of visible parts, we consider all stroke segments
si:= ((xi,yi,Ii),(xi+1,yi+1,Ii+1)). A solution candidate
Zbased on ˆXis obtained by removing point ifrom ˆX, ie.
Z:=X\si. In case, point i+1is the last point of a stroke, it
is also removed. Otherwise, additionally, we set Ii+1= 1to
indicate the start of a new stroke. The order in which the so-
lution candidates are created is important. That is, it matters
which stroke segments are removed ﬁrst. A natural choice
(denoted as CL) is to select stroke segments with smallest
classiﬁer loss LCas being removed ﬁrst. The idea being that
segments being only weakly indicative of the given class Y
can likely be removed without causing a mis-classiﬁcation.
Increasing the number of strokes due to removal of stroke
segments is not desirable. Therefore, we consider a variant
(CE), where only stroke segments at the beginning or end of
a stroke are removed. For comparison, we also consider re-
moval in reverse order(RO) in which points are created and in
the same order(SO). The motivation being that this procedure
constantly removes one stroke after the other without split-
ting strokes. Furthermore, humans might ﬁrst draw the most
important high level outline that might help in distinguishing
objects and add more details over time.
For changing the direction of strokes and changing the or-
der of strokes, we choose a solution candidate randomly. That
is, for changing direction of a stroke, we ﬂip the direction of
a randomly chosen stroke of ˆXby reversing the sequence of
points of the stroke. To change the order of strokes, we per-
form a cut-and-paste operation. That is, we choose a random
sequence consisting of subsequent strokes, remove it and
insert it either after a random stroke or at the very beginning
of the sequence. We also consider doing both that is for each
cut-and-paste operation, we ﬂip randomly the direction with
50% probability. This randomized approach is essentially
equally to trying all options given the number of strokes is
small.
For continuous optimization, we use gradient descent with
gradients obtained from the classiﬁer. That is, we maintain a
solution candidate Zthat is initialized with the original input
X. It is updated in each iteration using a gradient descent
step treating the classiﬁer weights as ﬁxed and the input Xas
variable. Otherwise the procedure is identical to Algorithm
1. The loss function LTotis a weighted combination of the
classiﬁer loss LC, the effort loss LMand the point-wise
difference loss LP:
LTot(Z) :=βCLC(Z) +βPLP(Z) +βELE(Z)
Evaluation
Datasets : We used two datasets. 1 Mio. samples of the Quick-
Draw (Ha and Eck 2017) dataset distributed equally among
1Note, this includes the original input Xdue to initialization of
ˆX:=X.the ﬁrst 30 classes. It consists of human sketches of an ob-
ject given its name. The data was created primarily using
mouse and touch devices. The second dataset consists of mu-
sical notes drawn using a pen(Calvo-Zaragoza and Oncina
2014). It consists of 32 classes but only of 15200 samples.
We padded or stripped sequences to be of ﬁxed length, ie. 104
points. For evaluation, we optimized 256 samples for each
class of the QuickDraw dataset that were not used for train-
ing, yielding 7680 test samples. For the Homus dataset we
used 20% of the data corresponding to 3040 samples. Note
that our optimization algorithms do not require any training
data. They optimize test samples directly.
Models : For classiﬁer C, we trained three instances of two
identical classiﬁer architectures for each dataset yielding 12
classiﬁers. For each instance, we ran our optimization algo-
rithms. We used PyTorch 1.6.0 and trained on 2080 TI Nvidia
GPUs. Our “LSTM” architecture is made of 3 Conv1D, 2
LSTM and 2 dense layers with dropout. The “Conv1D” net-
work consists of 3 Conv1D layers with stride 2 and a dense
layer. For comparing to prior work, we also implemented
an architecture based on (Yu et al .2017). Details on archi-
tectures can be found in the supplementary material. If not
stated differently we used d= 20% , ie. we allowed at most a
distortion of 20% for deletion and moving points by at most
20%.
Procedure : We conduct: i) a user study, where humans have
to redraw original and optimized samples, ii) a qualitative
evaluation, illustrating created samples, and iii) a quantitative
evaluation, discussing the impact of parameters and com-
paring various approaches. Results for all combinations of
models and datasets were very similar in nature. Therefore,
we focus on just one scenario, i.e. the LSTM-network on the
Quickdraw dataset, results for other setups are summarized
in the end and detaisl can be found in the supplementary
material.
Qualitative Evaluation
We discuss optimized samples for each alteration operation
separately. As seen in Figure 2 methods preferring removal
of segments in the same order as creation (SO) or reverse
order(RO) tend to remove entire strokes. This can lead to
unnatural sketches, e.g. angles without heads. Random re-
moval(RA) and classiﬁer loss based ordering (CL) increases
the number of strokes, which might be undesirable, when
it comes to reproducing the optimized sample. Removing
only end points based on classiﬁer loss (CE) tends to produce
samples that are well-recognizable without artifacts and it
does not split strokes.
Figure 3 shows outcomes for continuous optimization.
Changes appear more subtle than for removal, in particu-
lar, when optimizing for accuracy (second row). Continuous
optimization tends to shorten strokes, straighten them (best
seen for wings of the ﬁrst angle) and it might also rotate them
– as done for clock hands. When only optimizing for effort
(last row), which should lead to largest distortion, changes
seem to be the least noticeable. Our quantitative analysis
shows that this is not the case. Samples get scaled entirely in
a more uniform manner, making changes harder to spot (best
seen for third image from the left in the last row).
Figure 2: Original and generated samples for removal, mini-
mizing creation effort - more samples in supplementary ma-
terial
Samples for altering order and direction of strokes are in
the supplementary material.
Quantitative
We use priorly described metrics related to (i) the classiﬁer’s
capability to recognize samples ( Acc(uracy),AccNoi(se)),
(ii) the effort of creating a sample LEand (iii) the distortion
of the original sample LD,LP. To compute AccNoi,
we create for each input 10 noisy samples, where each
ϵi,j∈[−10,10]. For the Quickdraw dataset, where points
are within a range of [0,255] this means that the maximal
distance due to the addition of ϵi,jto each coordinate
between two points is about 28 pixels or 10% of the canvas
used for sketching. We report the accuracy on all created
samples, i.e. for the Quickdraw dataset with 7680 test
samples, we report the accuracy of 76800 samples.
Table 1 shows all metrics for continuous optimization vary-
ing loss weights β. All settings improve upon the original in
terms of accuracy. This is expected given the constraint that
a sample is no modiﬁcation is done that changes a correctly
classiﬁed sample into an incorrectly classiﬁed one. When
optimizing for effort, accuracy gains of optimized samples
vary. For noisy samples, accuracy can even be lower than for
the original. Reduced samples contain less (redundant) infor-
mation for classiﬁcation than the original ones, making them
somewhat more sensitive to noise. If optimizing effort loss
(βE= 1) only, effort loss is not lowest among all options.
Having a classiﬁer loss βC>0, not only strongly improves
accuracy, but interestingly also leads to lowest effort loss.
Without a classiﬁer loss, all parts of the sketch are altered
irrespective of whether they are relevant for classiﬁcation.Thus, signiﬁcant increase in loss is occurred due to small
movements of highly relevant points for classiﬁcation. This
is largely avoided using a classiﬁer loss.
ObjβC,βP,βEAccAccNoise LELP
Original 0.897±0.02 0.894±0.02 1527.4±161.4 0.0±0.0
Acc1.0,0,0 0.929±0.04 0.923±0.03 1539.6±158.8 10.8±11.5
0,0,1.0 0.904±0.02 0.9±0.02 1485.8±162.8 39.3±9.1
.9999,0,.0001 0.963±0.01 0.954±0.01 1461.9±160.1 63.0±10.2
.9998,.0001,.0001 0.966±0.01 0.958±0.01 1446.9±159.8 78.2±16.9
Eff1.0,0,0 0.962±0.01 0.927±0.01 1525.3±162.3 1.94±1.1
0,0,1.0 0.903±0.02 0.89±0.02 1407.1±152.3 113.2±13.4
.9999,0,.0001 0.963±0.01 0.934±0.01 1422.0±157.9 100.1±9.1
.9998,.0001,.0001 0.966±0.01 0.94±0.01 1417.1±160.4 107.6±24.4
Table 1: Results varying loss term weights βCL,βRE,βEF
Obj Meth.AccAccNoiLELD
Original 0.897±0.02 0.895±0.02 1527.4±161.4 0.0±0.0
AccCL 0.971±0.0 0.967±0.01 1444.2±157.1 180.7±15.7
CE 0.956±0.01 0.953±0.01 1395.4±151.7 123.7±11.3
RA 0.919±0.01 0.916±0.01 1503.2±159.0 60.9±8.7
RO 0.908±0.01 0.906±0.01 1497.7±160.7 24.6±4.5
SO 0.911±0.01 0.909±0.02 1499.0±160.0 29.3±3.8
EffCL 0.961±0.01 0.955±0.01 1346.2±141.0 220.5±20.0
CE 0.947±0.01 0.941±0.01 1257.3±131.0 219.3±19.8
RA 0.914±0.01 0.881±0.02 1381.8±143.4 220.7±19.8
RO 0.912±0.01 0.872±0.02 1191.1±122.9 219.1±19.7
SO 0.912±0.01 0.868±0.02 1288.6±140.4 220.2±20.0
Table 2: Results for removal
The results for all removal strategies (Table 2) indicate that
abandoning irrelevant stroke parts yields improvements for
both effort and accuracy at the same time. Using the classiﬁer
loss (CL or CE) for ordering removals yields best results in
terms of accuracy. When optimizing for effort, only these
methods also achieve much better noisy accuracy AccNoi
than the original samples. Both also do well in terms of effort.
When being allowed to split strokes (CL) accuracy is larger
than for removing parts at the end of strokes CE, but these
gains come at the expense of having more strokes. Remov-
ing samples in reverse order (RO) or in sorted order (SO)
gives lowest effort loss. Removing entire strokes from the
beginning (or end) yields beneﬁts, since we do not account
for moving to the ﬁrst point or from the last point to some
starting point and there is often a signiﬁcant distance between
the end point of one stroke and start point of the next stroke.
This distance is also gained when removing entire strokes. In
contrast, when removing strokes (segments) in the middle, a
transition between strokes remains.
Table 3 shows that the proposed optimization procedure
if only a ﬁxed percentage of average stroke segments of a
category is kept as proposed and described for GDSA in
(Muhammad et al .2019) for a smaller subset of QuickDraw.
In this setup, removal takes place even if it leads to erro-
neous classiﬁcation. Our classiﬁer guided methods CE and
CL achieve higher accuracy than prior work (DQSN(Zhou,
Xiang, and Cavallaro 2018) and GDSA) which is based on
Figure 3: Original and generated samples for continuous optimization for various βC,βPandβE
Method Original Acc. ∆if keep 50% ∆if keep 25%
DQSN 0.92 -0.12 -0.27
GDSA 0.92 -0.06 -0.20
CE(This paper) 0.95 -0.02 -0.16
CL(This paper) 0.95 0.01 -0.14
Table 3: Accuracy reduction for Sketch-a-Net architecture
when reducing visible elements; bold shows best
training a model using reinforcement learning signiﬁcantly.
We attribute this to the fact that we optimize samples individ-
ually in an iterative manner.
As shown in Table 4 both permuting strokes (P) and revers-
ing directions (R) or doing both (B) yield signiﬁcant gains in
terms of accuracy and also effort.
Obj Meth.AccAccNoiLELD
Original 0.897±0.02 0.895±0.02 1527.4±161.4 0.0±0.0
AccP 0.952±0.01 0.948±0.01 1595.6±162.1 0.096±0.07
R 0.951±0.01 0.948±0.01 1551.8±159.5 0.06±0.07
B 0.961±0.01 0.959±0.01 1580.1±159.6 0.094±0.04
EffP 0.911±0.02 0.892±0.02 1393.9±135.7 0.179±0.09
R 0.915±0.01 0.888±0.01 1306.6±120.5 0.193±0.1
B 0.913±0.01 0.88±0.02 1295.2±117.9 0.096±0.1
Table 4: Results for permuting strokes(R), reversing direction
(R) and doing both (B)
Obj Met.AccAccNoiLELDLP
Original 0.897±0.02 0.895±0.02 1527.4±161.4 0.0±0.0 0.0±0.0
AccB-C 0.976±0.0 0.972±0.0 1518.4±155.5 60.1±13.7 69.1±12.0
C-D 0.975±0.0 0.972±0.01 1388.4±150.0 167.2±25.6 107.2±18.5
D-B 0.98±0.01 0.979±0.01 1486.4±161.8 152.2±32.2 0.0±0.0
EffB-C 0.961±0.01 0.925±0.01 1203.3±120.0 101.4±22.2 131.1±14.1
C-D 0.973±0.0 0.964±0.01 1281.7±132.8 228.7±21.5 121.3±18.5
D-B 0.958±0.01 0.911±0.02 1086.1±94.2 219.9±19.2 0.0±0.0
Table 5: Results for applying multiple methods sequentially;
(C)ontinuous point movement, (B) Reverse and permute,
(D)eletion of parts
We also applied two and more methods sequentially (Ta-
ble 5). For continuous optimization of points Cwe used(βC,βP,βE) = (.9998,.0001,.0001) . Applying multiple
methods gives some more improvement. That is, both the
maximum accuracy and minimum effort loss are lower, when
applying multiple methods. We also investigated different
orderings, eg. B-C instead of C-B as well as performing mul-
tiple applications simulating an interwoven optimization. For
example, B-C-B-C with reduced iterations for each method.
This leads to some additional improvements.
Other networks and datasets: We found that qualitatively
all results were identical, meaning that if there was a clear
improvement for optimized samples for one dataset and one
network type this also held for others. But gains could vary
per dataset, network and operation considered. For exam-
ple, the highest accuracy gains relative to the original was
achieved for Conv1D on Quickdraw (13.4%) compared to
8.3% for LSTM on QuickDraw.
User study
We conducted an experiment to assess, if optimized samples
can be reproduced by humans and if these reproductions in-
deed yield gains according to the speciﬁed objective. While
the prior numerical investigation is highly suggestive, opti-
mized samples might be unnatural for humans. Thus, repro-
ductions of those samples might take longer and deviate more
strongly from the proposal than non-optimized samples, mak-
ing a user study necessary. We used generated samples for
method “D-B” (Table 5) optimized towards accuracy for the
QuickDraw dataset. The overall pool of sketches consisted of
10 original samples per class, where each sample consisted of
up to 7 strokes to ensure good readability of instructions, ie.
numbering and arrows. Since we are particularly interested
in the capability, whether errors in interaction can be miti-
gated, we chose 5 (of the 10) original samples per class that
were misclassiﬁed. Each participant had to copy an optimized
version of a human input and the original version for ﬁve
randomly selected sketches, yielding 10 sketches per user -
see Figure 4. It was decided randomly, whether a user was
ﬁrst presented the optimized or original version. Users were
advised to use the same number of strokes and draw them in
the order and direction as indicated by the numbering and
arrows. We recruited 200 English speaking participants on
Amazon Mechanical Turk. We removed reproduced sketches,
Figure 4: During the user study participants are shown a
sketch with numbered strokes and stroke start indicated (left
panel). They should reproduce it (right)
that did not match the instructed number of strokes or took
more than 60s to create sketch or that had only the original
or the corresponding optimized sample drawn adequately,
i.e. within 60s and with the correct number of strokes. The
(LSTM) classiﬁer had an accuracy of 54% on sketches re-
sembling the original and 68% on sketches based on the
optimized sample. The differences are statistically signiﬁcant
using a t-test, yielding p<0.02. Participants took on average
23.2s to (re)sketch an original sample. They were 1.7s faster
for optimized samples (though only with p= 0.21). Note
that we used samples optimized towards accuracy not effort.
Still, even those samples have (mostly) less visible strokes
(LD), while overall hand movements are typically similar to
original samples ( LE) - see Table 5.
Related Work
Human-AI interaction: (Rzepka and Berger 2018) summa-
rized the effects of various user and AI system character-
istics in general, while (Martins, Santos, and Dias 2019)
focused on digital AI assistants. Interaction between AI
and users has also been studied (Amershi et al .2019;
Janssen et al .2019; Bansal et al .2019; Carroll et al .2019;
Martins, Santos, and Dias 2019; Nocentini et al .2019;
Ghosh et al .2019; Shneiderman 2020) in various contexts
such as social robots (Martins, Santos, and Dias 2019;
Nocentini et al .2019). The primary focus has been on desir-
able AI behavior, eg. empathy, or strategies how AI can adapt
to user behavior((Ghosh et al .2019; Carroll et al .2019) with
few exceptions. (Bansal et al .2019) explicitly investigated
how users can alter the behavior, i.e. override decisions of the
AI, by understanding the error boundary of a classiﬁer, while
(Shneiderman 2020) provides general guidelines on human-
centered AI. Closest to our work is (Schneider 2020). (Schnei-
der 2020) introduced a human-to-AI coach based on an auto-
encoder that given a picture of a digit outputs a digit that has
lower classiﬁer loss and potentially consists of less pixels.
In contrast, we optimize samples individually in an iterative
manner, also provide instructions on how to create samples
and we are the ﬁrst to evaluate on actual users. Our work also
uses more complex datasets that are commonly studied in
other contexts, e.g. see (Xu 2020) for a survey on machine
learning and human sketches. Abstracting sketches using
removal of stroke segments and entire strokes, while preserv-
ing semantics was studied in (Riaz Muhammad et al .2018;Muhammad et al .2019). That is, an agent learns to se-
lect strokes that are relevant for a classiﬁer to maintain
the correct class. From the perspective of this paper, this
is similar to neglecting all constraints and focusing on
“time” with narrowing down on just one option for ab-
stracting: Removal. In this paper, we also consider a grad-
ual movement of points. (Riaz Muhammad et al .2018;
Muhammad et al .2019) used reinforcement learning while
having correct classiﬁcation as a reward (rather than as an
objective). The implementation using reinforcement learning
also differs from our approach improving individual sam-
ples directly. Combination of both approaches might lead to
better outcomes that is using RL with a planning and simu-
lation capability. (Liu et al .2019) used GANs to complete
artiﬁcially corrupted sketches, ie. through occlusion. They
achieved high-quality results comparable to other methods
such as image inpainting.
Explainability: This paper has strong ties to (personalized)
explanations (Schneider and Handali 2019; Guidotti et al .
2018) and explanations in the ﬁeld of human-AI inter-
action (Hois, Theofanou-Fuelbier, and Junk 2019). Coun-
terfactual explanations seek to identify a modiﬁcation of
the input to obtain another class (Dhurandhar et al .2018;
Goyal et al .2019). (Dhurandhar et al .2018) aims to identify
minimal changes to digits on a pixel level using perturbations.
Thus, in contrast to our work, they focus on mis-classiﬁed
samples. Moreover, the suggested changes commonly in-
volve adding or removing multiple pixels distributed across
the digit, which seems infeasible for humans, since they are
not able to reproduce digits on that level of detail. (Goyal et
al.2019) combine two images, the image to change and an
image from a class the image should be changed to.
Discussion and Conclusions
Humans will interact more and more with AI. This paper pro-
vided ﬁrst steps towards improving this interaction by show-
ing how human inputs to an AI can be optimized. Our results
indicate that optimized samples can be created faster and lead
to less mis-classiﬁcations, while still bearing similarity to the
original input. We chose to optimize samples individually,
which allows to personalize to a very high degree. We believe
that an approach using meta-learning or reinforcement learn-
ing with planning could lead to even better results. Our opti-
mized samples were also accompanied by instructions on how
to create them efﬁciently. While our human study conﬁrmed
that reconstructing proposed samples leads to saving in time
and reduces mis-classiﬁcations, more exploration of the ﬁeld
of human to AI interaction is needed: improve interaction on
a semantic level as needed for interaction with chatbots be-
yond making chatbots more human (Chaves and Gerosa 2019;
Ciechanowski et al .2019), consider other recognition prob-
lems such as speech(Zhang et al .2018), perform a joint op-
timization of human inputs and AI models, eg. interactive
modeling(Ware et al .2001), derive optimization algorithms
that use inputs of a human to provide general rules as feed-
back, assess additional concerns such as acceptance of tech-
nology by humans(Venkatesh et al .2003). We hope that the
community will pick up on these questions to foster seamless
use of AI and reduce risks due to miscommunication.
References
[Amershi et al. 2019] Amershi, S.; Weld, D.; V orvoreanu, M.;
Fourney, A.; Nushi, B.; Collisson, P.; Suh, J.; Iqbal, S.; Ben-
nett, P. N.; Inkpen, K.; et al. 2019. Guidelines for human-ai
interaction. In Proceedings of the 2019 chi conference on
human factors in computing systems , 1–13.
[Bansal et al. 2019] Bansal, G.; Nushi, B.; Kamar, E.;
Lasecki, W. S.; Weld, D. S.; and Horvitz, E. 2019. Be-
yond accuracy: The role of mental models in human-ai team
performance. In Proceedings of the AAAI Conference on
Human Computation and Crowdsourcing , volume 7, 2–11.
[Bastani et al. 2016] Bastani, O.; Ioannou, Y .; Lampropoulos,
L.; Vytiniotis, D.; Nori, A.; and Criminisi, A. 2016. Mea-
suring neural net robustness with constraints. In Advances in
neural information processing systems .
[Calvo-Zaragoza and Oncina 2014] Calvo-Zaragoza, J., and
Oncina, J. 2014. Recognition of pen-based music notation:
the homus dataset. In 22nd International Conference on
Pattern Recognition .
[Carroll et al. 2019] Carroll, M.; Shah, R.; Ho, M. K.; Grif-
ﬁths, T.; Seshia, S.; Abbeel, P.; and Dragan, A. 2019. On
the utility of learning about humans for human-ai coordina-
tion. In Advances in Neural Information Processing Systems ,
5174–5185.
[Chaves and Gerosa 2019] Chaves, A. P., and Gerosa, M. A.
2019. How should my chatbot interact? a survey on human-
chatbot interaction design. arXiv preprint arXiv:1904.02743 .
[Ciechanowski et al. 2019] Ciechanowski, L.; Przegalinska,
A.; Magnuski, M.; and Gloor, P. 2019. In the shades of the
uncanny valley: An experimental study of human–chatbot
interaction. Future Generation Computer Systems 92:539–
548.
[Dhurandhar et al. 2018] Dhurandhar, A.; Chen, P.-Y .; Luss,
R.; Tu, C.-C.; Ting, P.; Shanmugam, K.; and Das, P. 2018.
Explanations based on the missing: Towards contrastive ex-
planations with pertinent negatives. In Advances in Neural
Information Processing Systems .
[Ghosh et al. 2019] Ghosh, A.; Tschiatschek, S.; Mahdavi,
H.; and Singla, A. 2019. Towards deployment of robust
ai agents for human-machine partnerships. arXiv preprint
arXiv:1910.02330 .
[Goyal et al. 2019] Goyal, Y .; Wu, Z.; Ernst, J.; Batra, D.;
Parikh, D.; and Lee, S. 2019. Counterfactual visual explana-
tions. arXiv preprint arXiv:1904.07451 .
[Guidotti et al. 2018] Guidotti, R.; Monreale, A.; Turini, F.;
Pedreschi, D.; and Giannotti, F. 2018. A survey of methods
for explaining black box models.
[Ha and Eck 2017] Ha, D., and Eck, D. 2017. A neu-
ral representation of sketch drawings. arXiv preprint
arXiv:1704.03477 .
[Hois, Theofanou-Fuelbier, and Junk 2019] Hois, J.;
Theofanou-Fuelbier, D.; and Junk, A. J. 2019. How
to achieve explainability and transparency in human ai inter-
action. In International Conference on Human-Computer
Interaction , 177–183. Springer.[Janssen et al. 2019] Janssen, C. P.; Donker, S. F.; Brumby,
D. P.; and Kun, A. L. 2019. History and future of human-
automation interaction. International journal of human-
computer studies 131:99–107.
[Liu et al. 2019] Liu, F.; Deng, X.; Lai, Y .-K.; Liu, Y .-J.; Ma,
C.; and Wang, H. 2019. Sketchgan: Joint sketch completion
and recognition with generative adversarial network. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , 5830–5839.
[Martins, Santos, and Dias 2019] Martins, G. S.; Santos, L.;
and Dias, J. 2019. User-adaptive interaction in social robots:
A survey focusing on non-physical interaction. International
Journal of Social Robotics 11(1):185–205.
[Muhammad et al. 2019] Muhammad, U. R.; Yang, Y .;
Hospedales, T. M.; Xiang, T.; and Song, Y .-Z. 2019. Goal-
driven sequential data abstraction. In Proceedings of the
IEEE International Conference on Computer Vision , 71–80.
[Nocentini et al. 2019] Nocentini, O.; Fiorini, L.; Acerbi, G.;
Sorrentino, A.; Mancioppi, G.; and Cavallo, F. 2019. A
survey of behavioral models for social robots. Robotics
8(3):54.
[Poursaeed et al. 2018] Poursaeed, O.; Katsman, I.; Gao, B.;
and Belongie, S. 2018. Generative adversarial perturba-
tions. In Pro. of Conference on Computer Vision and Pattern
Recognition .
[Riaz Muhammad et al. 2018] Riaz Muhammad, U.; Yang,
Y .; Song, Y .-Z.; Xiang, T.; and Hospedales, T. M. 2018.
Learning deep sketch abstraction. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, 8014–8023.
[Rzepka and Berger 2018] Rzepka, C., and Berger, B. 2018.
User interaction with ai-enabled systems: a systematic review
of is research. In Int. Conf. on Information Systems (ICIS) .
[Schneider and Handali 2019] Schneider, J., and Handali, J.
2019. Personalized explanation in machine learning. In
European Conference on Information Systems (ECIS) .
[Schneider 2020] Schneider, J. 2020. Human-to-ai coach:
Improving human inputs to ai systems. In International
Symposium on Intelligent Data Analysis , 431–443. Springer.
[Shneiderman 2020] Shneiderman, B. 2020. Human-centered
artiﬁcial intelligence: Reliable, safe & trustworthy. Interna-
tional Journal of Human–Computer Interaction 36(6):495–
504.
[Venkatesh et al. 2003] Venkatesh, V .; Morris, M. G.; Davis,
G. B.; and Davis, F. D. 2003. User acceptance of information
technology: Toward a uniﬁed view. MIS quarterly 425–478.
[Ware et al. 2001] Ware, M.; Frank, E.; Holmes, G.; Hall, M.;
and Witten, I. H. 2001. Interactive machine learning: let-
ting users build classiﬁers. International Journal of Human-
Computer Studies 55(3):281–292.
[Xu 2020] Xu, P. 2020. Deep learning for free-hand sketch:
A survey. arXiv preprint arXiv:2001.02600 .
[Yu et al. 2017] Yu, Q.; Yang, Y .; Liu, F.; Song, Y .-Z.; Xiang,
T.; and Hospedales, T. M. 2017. Sketch-a-net: A deep neural
network that beats humans. International journal of computer
vision 122(3):411–425.
[Zhang et al. 2018] Zhang, Z.; Geiger, J.; Pohjalainen, J.;
Mousa, A. E.-D.; Jin, W.; and Schuller, B. 2018. Deep
learning for environmentally robust speech recognition: An
overview of recent developments. ACM Transactions on
Intelligent Systems and Technology (TIST) 9(5):1–28.
[Zhou, Xiang, and Cavallaro 2018] Zhou, K.; Xiang, T.; and
Cavallaro, A. 2018. Video summarisation by classiﬁ-
cation with deep reinforcement learning. arXiv preprint
arXiv:1807.03089 ."
2209.15165,D:\Database\arxiv\papers\2209.15165.pdf,"The paper describes a method for distilling style from image pairs for global forward and inverse tone mapping.  What are the limitations of existing deep learning tone mapping methods that the authors address, and how does their proposed approach overcome these limitations?","Existing deep learning tone mapping methods rely on analyzing global and local image features to find the right mapping, often employing large convolutional or fully connected networks.  The authors argue that this approach fails to capture the stylistic choices of a color artist, leading to inaccurate results.  Their proposed method, which operates at the pixel level, effectively distills the image-specific style vector without analyzing the image content, resulting in more accurate and faithful reproduction of manually color-graded or retouched images.","CVMP ’22, December 1–2, 2022, London, United Kingdom Mustafa et al.
4th degree PCC of input condition
R
G
B
xcCentroid  
SDR ImageHDR Image
u 2
u 1s( u 1 ,c) t( u 1 ,c)v 2
v 2
Conditional Af fine CouplingActNorm
Randompermutation  Invertible  
Block  z1
z2
z3Invertible  
Block  RG R2G2B2R4G4B4B
Figure 2: The network architecture used for our pixel-wise conditional INN. Here the sub-networks 𝑠(·)and𝑡(·)are fully
connected networks with 2 hidden layers each. We use 8 invertible blocks in our architecture.
v1=u1, v2=u2⊙exp(𝑠([u1,c]))+𝑡([u1,c]).(6)
Since the subnetworks in a coupling block are never inverted
themselves, we can append cwithout losing the invertibility of
theINN. Moreover, the operation has a lower-triangular jacobian
whose determinant is the product of diagonal elements.
3.4.2 Dimensionality of latent. Central to an efficient style map-
ping framework is the dimensionality of the latent vector. Mapping
the style of an image from the target domain to a low-dimensional
style vector allows easy user-interactive image manipulation and
editing. Our default architecture depicted in Fig. 2 encodes style
in 3 latent dimensions, matching the dimensionality of an input
pixel. However, some applications may require differently sized
style vectors. As alternatives, we demonstrate the adaptations that
enable the INN to operate with fewer or more latent dimensions.
To encode style in fewer dimensions, we split off some features
after 4 invertible blocks. As depicted in Fig. 3 (left), these features are
forced to follow the standard normal distribution using MLE . The
remaining features continue through 4 more invertible blocks. The
total number of invertible blocks is 8, to match our 3-dimensional
INN from Fig. 2. This splitting of intermediate features is similar to
the multiscale version of the normalizing flow described in [Dinh
et al. 2016].
For a higher-dimensional latent style vector, we construct an in-
vertible model on an augmented input space xaug. Similar to [Huang
et al.2020], each input vector xis appended with samples from a
standard normal distribution as shown in Fig. 3 (right). This allows
us to improve the expressibility of the latent space at the cost of an
increased dimension (from 3 to 4) for final image manipulation.
3.5 Optimization and inference
Similar to other normalizing flows, we train our INN byMLE where
the likelihood is given by Eq. 5. The network thus learns to con-
ditionally map a target pixel to a latent vector. However, when
presented with an entire frame, there is no easy way to extract a
single low-dimensional vector that captures style. For that, we aug-
ment MLE training with a reconstruction loss to force the per-pixelrepresentations of the same frame to lie closer in the latent space
(see Fig. 1).
Likelihood loss: To restrict the magnitude of gradients for
backpropagation, it is customary to minimize the negative log-
likelihood ( NLL) instead of likelihood since the logarithmic trans-
form is monotonic. The NLL loss is:
LNLL(x𝑝,y𝑝)=−log𝑝𝑍 𝑔−1
𝜃(x𝑝; c𝑝)−log∇𝑔−1
𝜃(x𝑝; c𝑝)
=log 2𝜋+ 𝑔−1
𝜃(x𝑝; c𝑝)2
2−log∇𝑔−1
𝜃(x𝑝; c𝑝)
(7)
This result follows since we choose a standard normal z𝑝∼N(0,1)
as the base distribution with the following log-likelihood:
log𝑝𝑍(𝑧)=−1
2log 2𝜋−1
2||𝑧||2
2. (8)
The first term is a constant w.r.t. z and can be dropped during
training. Due to this loss, the INN learns a bijective mapping from
the distribution of pixels to the chosen latent, conditioned on the
encoded input pixel. Through MLE , we encourage per-pixel latents
to follow a standard normal distribution as shown in Fig. 4 (right).
Reconstruction loss: Our secondary requirement is for pixels
of the same frame to cluster together in the latent space 𝑍. We
achieve this by first passing all 𝐾pixels of frame 𝑖through the INN
in reverse, and computing the centroid of their latent representa-
tions,
z𝑖=1
𝐾𝐾∑︁
𝑝=1𝑔−1
𝜃(x𝑖
𝑝;C(y𝑖
𝑝)), (9)
where z𝑖is the per-frame style vector. Then, we reconstruct the
frame with𝐾forward passes of the INN using the single extracted
style latent z𝑖but different conditional inputs. Finally, we compute
the reconstruction loss Lrecfor each pixel of an image as follows:
Lrec(x𝑖
𝑝,y𝑖
𝑝)=∥𝑔𝜃(z𝑖;C(y𝑖
𝑝))−x𝑖
𝑝∥2 (10)
The reconstruction loss enforces a similar value of the style vector
z𝑖for the entire frame. We also observe that this simple constraint
on the latent representation allows mapping of dissimilar frames
in the input space to distant and distinct regions in the latent space
(see Fig. 1).
Distilling Style from Image Pairs for Global Forward and Inverse Tone Mapping CVMP ’22, December 1–2, 2022, London, United Kingdom
Invertible  
Block  c
R
G
Bx augCentroid  Invertible  
Block  Invertible  
Block  
nz1
z2
z3
z4n 
~
Split  Invertible  
Block  c
R
G
Bx
Invertible  
Block  
n 
~z1
z2Invertible  
Block  Centroid  Invertible  
Block  
Figure 3: Changes in INN architecture to decrease the latent style vector to 2 dimensions (left) or increase to 4 dimensions
(right).
Optimization of our INN is done bi-directionally with the total
loss as the sum of the NLL lossLNLLand the reconstruction loss
Lrec. After successful training, the INN can be used to extract an
overall per-frame style vector. This is achieved by running a reverse
pass for each pixel of a given image and computing the centroid
according to Eq. 9. In Fig. 4, we show the change in the latent space
for the training samples due to the addition of Lrec.
4 RESULTS
In this section, we evaluate the efficacy of our method in encoding
the style of a target domain image into a low-dimensional latent
space for the task of forward and inverse tone mapping. First, we
compare our approach of conditioning color mapping on a style
with the traditional approach of conditioning on the input image
(Sec. 4.2, comparison with HDRNet). Then, we demonstrate that
the existing combination of PCC with dimensionality reduction
approaches (PCA and VAE) gives far inferior results as compared
to our INN (Sec. 4.3). We report results in terms of PSNR (for RGB
values) and FLIP [Andersson et al .2020] here, and CIELab in the
appendix. We choose these metrics because they are sensitive to
color differences (unlike SSIM).
4.1 Datasets
We rely on two sources of SDR-HDR pairs. For images, we use
Adobe-MIT 5K dataset [Bychkovsky et al .2011]. Each RAW image
in this dataset was tone-mapped (retouched) by 5 experts, who
produced results in different styles. We report the results for expert
C in the main paper, and for other expert in the supplementary ma-
terials. All images are rescaled to the height of 480pixels for faster
training and then split into a random 80/20% train/test sets. Due to
the pixel-wise formulation of our INN, the same trained model can
be employed on images of different resolutions at inference time.
Due to the lack of any publicly available manually color graded
video datasets, we decoded 3 Blu-ray movies, namely “BBC Planet
Earth II Episode 3 - Jungles"", “BBC Blue Planet II Episode 5 - Green
Seas"" and “The Lego Batman Movie"". 4K HDR content is often
sold with two disks — one color graded for 4K HDR and another
for 1080p SDR displays. We took advantage of that by extracting
content from both disks. The frames from SDR and HDR streams
were time-synchronized by finding the offset that maximized cross-
correlation. Finally, the frames were manually inspected to ensure
close correspondence. For good diversity, we construct a sequence
from each video by collecting every 120thframe. For each video
sequence, the first 80% of frames are used for training and theremaining 20% for testing. On average, we have 500-800 frames in
the training set per movie. Each frame is rescaled to a resolution
of half HD ( 960×540). Both SDR and HDR RGB pixel values are
display encoded (BT.2020 + PQ for HDR, BT.709 + sRGB for SDR).
Publicly available datasets proposed in methods like [Chen et al .
2021; Kim et al .2019] do not include manual color grading, but
instead rely on Youtube’s automatic HDR to SDR conversion pro-
cess. The primary objective of our work is to model manual color
grading, making such datasets unsuitable for the task of distilling
color artist’s style.
4.2 Conditioning on style vs. image content
The central assumption of current deep learning tone mapping
methods is that the right mapping can be found by analyzing global
and local image features [Gharbi et al .2017]. Based on that as-
sumption, the existing methods employ large convolutional and/or
fully connected networks operating on an entire image. In contrast,
our INN operates at the pixel level to effectively distill the image-
specific style vector without analyzing the image content. Here, we
test which approach can better predict the results of manual tone-
mapping and color grading. We train our INN separately on pairs of
HDR-SDR frames taken from each movie from the movies dataset
(see Sec. 4.1). Our pixel-wise training scheme allows us to operate
on high-resolution images. Inference for each frame of half HD
resolution ( 960×540) takes 0.025 seconds. Each model is trained
for 80 epochs with an initial learning rate of 5𝑒−4with gradual
learning rate scheduling. Additionally, for a more challenging set-
ting, we train our INN on the Adobe-MIT 5K dataset [Bychkovsky
et al. 2011].
As a representative example of existing learning-based tone map-
ping, we compare our results with HDRNet [Gharbi et al .2017]
(implemented in PyTorch [Ge 2021]), retrained on the same data as
our method. The numerical results and quality metric distributions,
shown in Fig. 6, demonstrate a dramatic improvement of 10-16 dB
as compared to HDRNet. This shows that information about the
style is necessary to faithfully reproduce manually color-graded
or retouched images. Although HDRNet is a much larger network,
consisting of 482K trainable parameters compared to 31K for our
INN, it cannot infer the target image based on the source image
alone. A few example images shown in Fig. 5 demonstrate that
HDRNet fails to reproduce accurate color and tones of the target
images. Similar to HDRNet, we see from Table 1 that other state-
of-the-art image enhancement methods that do not model style
struggle to reconstruct image-specific retouching with an accuracy
CVMP ’22, December 1–2, 2022, London, United Kingdom Mustafa et al.
z1z2
z1
Figure 4: The figure shows the distribution of the latent color representation for different frames from the training set with
(left) and without (right) the reconstruction loss Lrec. We see that the latent vectors of the training set follow a normal distri-
bution. Frames are taken from the BBC documentary Planet Earth “Jungles"" and the INN trained to encode the style of frames
to 2 dimensions is used for better visualization.
HDR Input PCA VAE HDRNet Ours Target
Jungles Green Seas MIT 5k
Figure 5: Qualitative comparisons with different methods on 3 datasets for the task of forward tone mapping. The target for
MIT5k dataset is the expert retouched image. Additional results are provided in the appendix.
Table 1: Comparison of our INN with state-of-the-art methods that assume one-to-one mapping for Expert C from the MIT5k
dataset [Bychkovsky et al. 2011]. The substantial improvement in performance clearly demonstrates how distilling style
can lead to almost perfect reconstruction accuracy. The values for our and HDRNet methods were obtained using the same
train/test splits, and the values from other methods are taken from the respective works.
HDRNet UPE GleNet 3DLUT StarEnh Curl DPE CRSNet DLPF Ours
[Gharbi et al.
2017][Wang et al.
2019][Kim et al.
2020a][Zeng et al.
2020][Song et al.
2021][Moran et al.
2021][Chen et al.
2018][He et al. 2020][Moran et al.
2020]
PSNR 22.49 23.24 25.88 24.92 25.46 24.04 23.76 24.23 23.93 39.22
Params. 482K 1M - <600K 14M 1.4M 3.34M 36K 1.8M 31.4K
higher than 25 dB. Please note that due to the novelty of our ap-
proach of encoding style as meta data, a direct comparison between
our work and the different image enhancement methods is not fair.
However, in Table 1, we show the results to delineate the need of
our approach of style conditioning over conventional CNN based
image-to-image mapping to achieve near perfect reconstruction.Next, we train our INN-based mapping for the task of inverse
tone mapping. While inverse tone-mapping often involves bit-depth
expansion and hallucination of over- and under-exposed pixels [Eil-
ertsen et al .2021], here we focus on the problem of learning global
SDR→HDR color mapping. We use the same movie datasets as for
the tone-mapping task but swap the source and target frames. The
Distilling Style from Image Pairs for Global Forward and Inverse Tone Mapping CVMP ’22, December 1–2, 2022, London, United Kingdom
HDRNet Ours PCA VAEPSNR FLIP
Green Seas  Lego Batman  MIT 5k  
 Jungles
Figure 6: Comparison of results on different datasets for the task of forward tone mapping. Note that our method achieves a
substantial improvement in performance compared to other dimensionality reduction methods across datasets. The purple ‘ +’
in the plots show the mean and black ‘ ×’ show the lowest 5𝑡ℎpercentiles. Note that the y-axis for FLIP metric has been reversed.
The results for Ours, PCA and VAE are reported for the model that encodes the style representation into 3 dimensional latent
vector.
PU21- PSNR
Jungles Green Seas  Lego Batman  
Figure 7: Comparison of results on different datasets for the task of inverse tone mapping on the movies dataset. To adapt
PSNR to HDR images, we use perceptually uniform PU21 transform [Mantiuk and Azimi 2021]. The labels for the violin plots
are consistent with Fig. 6. The results for Ours, PCA and VAE are reported for the model that encodes the style representation
into 3 dimensional latent vector.
results shown in Fig. 7 demonstrate a substantial improvement of
10-12 dB over HDRNet.
4.3 Other dimensionality reduction techniques
As explained in Sec. 3.1, the color mapping can be expressed as
PCC (Eq. 2) and then the size of the style matrix can be reduced
using standard dimensionality reduction methods, such as PCA or
VAE. Here, we compare those standard approaches with our INN.
Principal component analysis: We ran PCA on training pairs to
reduce the flattened style matrix Mflatinto the required number of
latent dimensions (2–4). During test time, ^Mflatis reconstructed
from the principal components and used to map the colors. We
observe from Fig. 6 (forward tone mapping) and Fig. 7 (inverse tone
mapping) that the strict linearity assumption of PCA results in poor
performance. Qualitative result comparisons are provided in Fig. 5.Variational autoencoder: Next, we replaced the linear projection
with a deep auto-encoder architecture. Since we are interested in an
interpretable latent space such as the one depicted in Fig. 8, we opted
for a VAE where the latent follows a normal distribution [Kingma
and Welling 2013]. The input to the VAE is the original matrix of
polynomial coefficients M. The goals is to train the VAE so that the
matrix can be predicted from a low-dimensional latent vector. The
training loss includes a reconstruction loss between predicted (after
matrix multiplication using decoded matrix ^M) and ground truth
pixels and the evidence lower-bound. For a fair comparison with our
method, we chose a fully connected network with approximately
the same number of parameters. We empirically found the best
results for a network with 6 hidden layers for the encoder and the
decoder, with a total of 31K trainable parameters. The network
was trained for 500 epochs with an initial learning rate of 5𝑒−4
with gradual learning rate scheduling. The weights given to the
reconstruction and the evidence lower-bound were 𝜆=1,𝛿=1𝑒−3,
CVMP ’22, December 1–2, 2022, London, United Kingdom Mustafa et al.
respectively. Although the VAE attains higher quality scores than
PCA and its results are comparable to HDRNet (which does not use
a style vector), VAE still performs much worse than our INN (see
Fig. 6 and 7).
5 APPLICATIONS
5.1 Assisted color grading
Color grading is a manual, labor-intensive process that requires
a substantial set of skills. Our method can be used to partially
automate this process. First, we ask the color artist to manually
color grade N scenes, which we use to train our model. Then, we
require the color artist to adjust only two or three parameters for
the remaining scenes. Such adjustment is much easier than using
color grading tools with dozens of different color adjustments. The
added benefit of using our mapping is that the style is likely to be
more consistent across the movie than if a manual color grading
tool was used.
Since we do not have access to RAW video frames, typically
used for color grading, we demonstrate this application using HDR
frames from Blu-ray movies as input and the SDR frames as the
color-graded target. Our results, from Sec. 4, have already demon-
strated that our mapping can faithfully reproduce the SDR target
frames. Figures 8 and 10 show example frames generated by adjust-
ing a 2-dimensional style vector. Each frame comes from a different
movie, for which a separate INN was trained. The style space allows
for convenient exploration of tone and color adjustments that have
been applied to previously color graded frames. The dimensions of
the space are easy to interpret: they represent the change of color
temperature, contrast and brightness. In a supplementary video, we
demonstrate a mock interface of the real-time color grading tool.
5.2 Transmission of SDR and HDR video
content
The current practice is to encode and distribute SDR and HDR
content separately (on Blu-ray or via streaming), which approxi-
mately doubles the required storage space. There exist methods for
concurrent SDR+HDR image [Artusi et al .2019] and video coding
[Mantiuk et al .2006], but they require transmitting a substantial
amount of additional data. Here we show that our color mapping
can substantially reduce, or even eliminate, the need for auxiliary
data.
We compare our inverse tone mapping INN from Sec. 4 with the
coding used in JPEG XT (Profile A with open-loop encoding). The
frames are encoded individually using either JPEG XT or a regular
JPEG + our learned color mapping. JPEG XT encodes HDR images
by storing a tone-mapped version of an HDR image (base layer), a
custom mapping function that predicts the HDR image from the
tone-mapped image, and the difference between the predicted and
the original HDR image (extension layer). Both base and extension
layers are encoded using a standard JPEG codec. We replicate such
encoding but replace the custom mapping used in JPEG XT with
our generative color mapping INN. Then, we measure the rate-
distortion curves for a test set from the “Jungles"" movie. The rate-
distortion curves, shown in Fig. 9, depict a consistent improvement
in performance when using our INN (employing both the base
and extension layers). At an extremely low bit rate of 0.5, the INNwithout an extension layer produces the best quality images (PU-
PSNR of over 30 dB).
5.3 Assisted dynamic range expansion for HDR
displays
The vast majority of video content has been color graded for SDR
displays and cannot take advantage of the higher luminance and
contrast offered by HDR displays. Here, we show that it is possible to
use our mapping function to expand SDR content for HDR displays.
It should be noted, however, that our mapping will not be able to
reconstruct details in the saturated parts of an image [Eilertsen
et al. 2017a].
This process is identical to color grading, explained in the pre-
vious section, except we infer the HDR frames from their SDR
counterpart. Similar to forward tone mapping, in this application
we require only a small portion of the HDR frames to be manually
color graded by color experts. Results of such assisted dynamic
range expansion are included in the appendix.
6 ABLATION STUDY
To achieve the best performance, we conduct an ablation study
over the choice of the conditioning vector c𝑝for training the INN.
Furthermore, we provide a study over the dimensionality of the
style vector.
6.1 Conditioning vector
First, we study the effect of using different degrees of polynomials
in PCC as our per-pixel conditioning vector. We conducted an
ablation study over the 1st(RGB), 2nd, 3rdand 4thdegree polynomial
expansion. Second, we study the effect of using image statistics as
the conditioning vector in addition to the 4thdegree polynomial. We
train our INN architecture in such that each pixel-wise conditioning
vector (PCC) is concatenated with high level features extracted
from the entire image using an additional feed-forward network
𝐻. For this task, we use a pre-trained VGG network, similar to
[Ardizzone et al .2020; Denker et al .2021] as the feed-forward
network. The final conditioning vector for a given pixel is given as
c𝑝=[C(y𝑝),𝐻(y)], where the weights of 𝐻(·)are simultaneously
being updated alongside the weights of the INN. Finally, we train
our INN using the 4thdegree polynomial concatenated with the
histogram of the luma channel as the conditioning vector. Table 2
(left) shows that the INN performs best when no additional image
statistics are added to the 4th-degree PCC conditioning vector. Note
that different conditioning vectors have different lengths, as shown
in the second row of Table 2 (left).
6.2 Dimensionality of the style vector
Next, we investigate the impact of changing the dimensionality
of the latent style vector. Mapping the style of a target domain
image to a low dimensional latent representation allows easy user-
interactive image manipulation. To this end, we provide additional
network architectures in Sec. 3.4.2 to train our INN for 2 and 4 latent
style encodings. In Table 2 (right) we further provide a comparison
of similar dimensions of the latent representations for PCA and
VAE.
Distilling Style from Image Pairs for Global Forward and Inverse Tone Mapping CVMP ’22, December 1–2, 2022, London, United Kingdom
z1z2
-0.50.00.5-0.50.0
0.250.5
Figure 8: Example mapping obtained by manipulating a 2-dimensional style vector. Both dimensions control brightness and
color temperature. Such style space enables assisted color grading, which mimics the range of styles found in the training
image pairs.
RGB PCC-2 PCC-3 PCC-4 PCC-4 +VGG PCC-4 +Hist
PSNR 28.98 31.52 33.02 41.68 30.130 35.69
len(c𝑝) 3 9 19 34 60 60Dim-2 Dim-3 Dim-4
PCA 17.67 18.24 18.35
VAE 20.21 28.06 31.52
Ours 37.85 41.68 41.99
Table 2: Ablation studies over the choice of conditioning vector (left) and over the dimensionality of the latent style vector for
different methods (right). All results are reported for the “BBC Planet Earth Jungles"" movie in terms of PSNR (dB).
1 2 3 4 5 6
Bit rate25283134374043PU21-PSNR
INN (base)
INN (base + extension)
JpegXT (base + extension)
Figure 9: Rate-distortion curves, comparing the learned INN
to JPEG XT. HDR Image quality (y-axis) is measured by com-
puting PSNR on PU21 encoded images [Mantiuk and Azimi
2021].7 CONCLUSIONS
This work highlights the importance of modeling style when learn-
ing global image transforms, such as those between differently
color-graded SDR-HDR images. We conclusively show that extract-
ing a style vector from a target image considerably improves the
reconstruction quality. This is due to the existence of an infinite
number of equally plausible solutions, each representing a unique
color artist’s choice. Our proposed conditional INN effectively mod-
els this one-to-many mapping by extracting and encoding this
artistic choice from examples of image pairs into a low-dimensional
style vector. We show that our method significantly outperforms
state-of-the-art deep architectures that ignore style, as well as al-
ternate dimensionality reduction methods that incorporate latent
style but cannot encode it efficiently. Moreover, our invertible frame-
work enables interactive style manipulation by adjusting the low-
dimensional latent vector. The main focus of our work is color
mapping for video content, which is global in nature (the same for
all pixels in an image, M:R3→R3). However, our method can
be easily extended to a manual local mapping by splitting frames
into a number of tiles, as done in [Eilertsen et al. 2015].
CVMP ’22, December 1–2, 2022, London, United Kingdom Mustafa et al.
z1z2
-1.00.0-1.00.0
1.01.0
z1z2
-1.00.0-1.00.0
1.01.0
Figure 10: Additional results showing assisted color grading by manipulating a 2-dimensional style vector on two datasets -
Planet Earth Episode 1 Islands (top) and Lego Batman movie (bottom). Both dimensions control brightness and color temper-
ature.
ACKNOWLEDGEMENTS
This project has received funding from the European Research
Council (ERC) under the European Union’s Horizon 2020 research
and innovation programme (grant agreement N◦725253–EyeCode).
Distilling Style from Image Pairs for Global Forward and Inverse Tone Mapping CVMP ’22, December 1–2, 2022, London, United Kingdom
APPENDIX
In this appendix, we further report quantitative results for both
forward and inverse tone-mapping on different datasets using ad-
ditional metrics (Sec. 1). We also investigate whether a single INN
can effectively capture all the expert styles from the MIT-Adobe
FiveK dataset (Sec. 1.3). Finally in Sec. 2, we show improvement in
performance due to the proposed bi-directional training with NLL
LNLLand the reconstruction loss Lrec.
1 ADDITIONAL RESULTS
1.1 Forward tone mapping
The violin plots in Fig. 11 compare our INN with HDRNet, PCA
and VAE using the CIE DE 2000 [Sharma et al .2005] metric. Similar
to Fig. 6 in the main document, there is a substantial improvement
in reconstruction quality due to correctly extracting and utilizing
style.
For the MIT-Adobe FiveK dataset, we provide results for all the
experts in Fig. 12. These are consistent with images retouched by
expert C reported in the other figures. Here, a separate network is
used for each expert to better compare with existing works.
1.2 Inverse tone mapping
For the task of inverse tone mapping, Fig. 13 shows similar violin
plots for 2 more metrics: FLIP and CIE DE 2000. Before running the
SDR metrics, we encode the reconstructed and ground truth HDR
images with PU21 encoding.
1.3 Image content vs. style conditioning
Additionally, we trained a single INN for all 5 experts of MIT-Adobe
FiveK, something that can not be done with HDRNet because of
the lack of conditioning on style. Since we learn a one-to-many
mapping, the same network produces outputs in different styles
by utilising different latent vectors. Fig. 14 shows that our single
INN successfully captures the style of all the experts. We further
report the quantitative comparison of our single trained INN on
the individual test set for each expert in Table 4. Refer to Fig. 15
and Fig. 16 for qualitative comparisons on selected scenes. The
deep-learning methods, like HDRNet, that model expert retouching
with one-to-one mappings are unsuitable for this task since a single
network cannot learn different styles corresponding to the experts.
We see that our INN performs much better than PCA and VAE and
produces artifact-free images that better match the required style.
2 BI-DIRECTIONAL TRAINING
When trained with the NLL lossLNLL, our INN learns to condition-
ally map a target pixel to a latent vector. However, when presented
with an entire frame, there is no easy way to extract a single low-
dimensional vector, that captures the style of the mapped frames.
Recall that we augment MLE training by forcing the per-pixel rep-
resentations of the same frame to lie closer in the latent space.
In Table 3, we show the effect of our proposed bi-directional
training, by addition of the reconstruction loss LrecalongsideLNLL.
The NLL loss makes the style vectors resemble a predetermined
latent distribution (the standard normal in our experiments), whilethe reconstruction loss ensures that pixels from the same frame
have similar style vectors.
Table 3: Ablation study on the effect of bi-directional training for
the task of forward tone-mapping for the “BBC Planet Earth II
Episode 3 - Jungles"" dataset.
LNLLLNLL+L rec
PSNR↑ 36.80 41.68
FLIP↓ 0.108 0.070"
1806.11538,D:\Database\arxiv\papers\1806.11538.pdf,How do the authors address the computational complexity of visual relationship detection when the number of possible object-predicate combinations becomes very large?,"The authors propose a factorizable connection graph that represents relationships between objects in a more concise way, reducing the number of intermediate representations and computational cost.","4 Yikang LI et al.
as an integrated whole, i.e. considering each distinct combination of object cate-
gories and relationship predicates as a distinct class. Such methods will become
intractable when the number of such combinations becomes very large.
As an alternative paradigm, considering relationship predicates and object
categories separately becomes more popular in recent works [36,41,63,64]. Generic
visual relationship detection was ﬁrst introduced as a visual task by Lu et al.
in [37]. In this work, objects are detected ﬁrst, and then the predicates between
object pairs are recognized, where word embeddings of the object categories
are employed as language prior for predicate recognition. Dai et al. proposed
DR-Net to exploit the statistical dependencies between objects and their re-
lationships for this task [6]. In this work, a CRF-like optimization process is
adopted to reﬁne the posterior probabilities iteratively [6]. Yu et al. presented
a Linguistic Knowledge Distillation pipeline to employ the annotations and ex-
ternal corpus ( i.e. wikipedia), where strong correlations between predicate and
⟨subject-object⟩pairs are learned to regularize the training and provide extra
cues for inference [62]. Plummer et al. designed a large collection of handcrafted
linguistic and visual cues for visual relationship detection and constructed a
pipeline to learn the weights for combining them [42]. Li et al. used the message
passing structure among subject, object and predicate branches to model their
dependencies [34].
The most related works are the methods proposed by Xu et al. [58] and
Liet al. [35], both of which jointly detect the objects and recognize their re-
lationships. In [58], the scene graph was constructed by reﬁning the object and
predicate features jointly in an iterative way. In [35], region caption was intro-
duced as a higher-semantic-level task for scene graph generation, so the objects,
pair-wise relationships and region captions help the model learn representations
from three diﬀerent semantic levels. Our method diﬀers in two aspects: (1) We
propose a more concise graph to represent the connections between objects in-
stead of enumerating every possible pair, which signiﬁcantly reduces the com-
putation complexity and allows us to use more object proposals; (2) Our model
could learn to leverage the spatial information embedded in the subgraph feature
maps to boost the relationship recognition. Experiments show that the proposed
framework performs substantially better and faster in all diﬀerent task settings.
3 Framework of the Factorizable Network
The overview of our proposed Factorizable Network (F-Net) is shown in Figure 2.
Detailed introductions to diﬀerent components will be given in the following
sections.
The entire process can be summarized as the following steps: (1) generate
object region proposals with Region Proposal Network (RPN) [48]; (2) group
the object proposals into pairs and establish the fully-connected graph, where
every two objects have two directed edges to indicate their relations; (3) cluster
the fully-connected graph into several subgraphs and share the subgroup fea-
tures for object pairs within the subgraph, then a factorized connection graph
Factorizable Net 5
(1) ImageandRPNproposals(2) Fully-connectedGraph(3) Subgraph-basedRepresentationpersonwearholdhelmetbat(4) ROI-pooling and Feature Preparation(6) Object and Relation RecognitionPredicateinference (SRI)(5)Spatial-weightedMessagePassing (SMP)ObjectinferenceObjectfeaturevectors
Subgraphfeaturemaps
Fig. 2: Overview of our F-Net. (1) RPN is used for object region proposals,
which shares the base CNN with other parts. (2) Given the region proposal,
objects are grouped into pairs to build up a fully-connected graph, where every
two objects are connected with two directed edges. (3) Edges which refer to
similar phrase regions are merged into subgraphs, and a more concise connection
graph is generated. (4) ROI-Pooling is employed to obtain the corresponding
features (2-D feature maps for subgraph and feature vectors for objects). (5)
Messages are passed between subgraph and object features along the factorized
connection graph for feature reﬁnement. (6) Objects are predicted from the
object features and predicates are inferred based on the object features and the
subgraph features. Green, red and yellow items refer to the subgraph, object and
predicate respectively.
is obtained by treating each subgraph as a node; (4) ROI pools [15, 21] the ob-
jects and subgraph features and transforms them into feature vectors and 2-D
feature maps respectively; (5) jointly reﬁne the object and subgraph features
by passing message along the subgraph-based connection graph for better rep-
resentations; (6) recognize the object categories with object features and their
relations (predicates) by fusing the subgraph features and object feature pairs.
3.1 Object Region Proposal
Region Proposal Network [48] is adopted to generate object proposals. It shares
the base convolution layers with our proposed F-Net. An auxiliary convolution
layer is added after the shared layers. The anchors are generated by clustering
the scales and ratios of ground truth bounding boxes in the training set [35].
3.2 Grouping Proposals into Fully-connected Graph
As every two objects possibly have two relationships in opposite directions, we
connect them with two directed edges (termed as phrases). A fully-connected
graph is established, where every edge corresponds to a potential relationship (or
6 Yikang LI et al.
background ). Thus, Nobject proposals will have N(N−1) candidate rela-
tions (yellow circles in Fig. 2 (2)). Empirically, more object proposals will bring
higher recall and make it more likely to detect objects within the image and
generate a more complete scene graph. However, large quantities of candidate
relations may deteriorate the model inference speed. Therefore, we design an
eﬀective representations of all these relationships in the intermediate stage to
adopt more object proposals.
3.3 Factorized Connection Graph Generation
By observing that many relations refer to overlapped regions (Fig. 1), we share
the representations of the phrase region to reduce the number of the intermedi-
ate phrase representations as well as the computation cost. For any candidate
relation, it corresponds to the union box of two objects (the minimum box con-
taining the two boxes). Then we deﬁne its conﬁdence score as the product of
the scores of the two object proposals. With conﬁdence scores and bounding
box locations, non-maximum-suppression (NMS) [15] can be applied to suppress
the number of the similar boxes and keep the bounding box with highest score
as the representative. So these merged parts compose a subgraph and share
an uniﬁed representation to describe their interactions. Consequently, we get a
subgraph-based representation of the fully-connected graph: every subgraph con-
tains several objects; every object belongs to several subgraphs; every candidate
relation refers to one subgraph and two objects.
Discussion In previous work, ViP-CNN [34] proposed a triplet NMS to pre-
process the relationship candidates and remove some overlapped ones. However,
it may falsely discard some possible pairs because only spatial information is
considered. Diﬀerently, our method just proposes a concise representation of
the fully-connect graph by sharing the intermediate representation. It does not
prune the edges, but represent them in a diﬀerent form. Every predicate will
still be predicted in the ﬁnal stage. Thus, it is no harm for the model potential
to generate the full graph.
3.4 ROI-Pool the Subgraph and Object Features
After the clustering, we have two sets of proposals: objects and subgraphs. Then
ROI-pooling [15, 21] is used to generate corresponding features. Diﬀerent from
the prior art methods [35,58] which use feature vectors to represent the phrase
features, we adopt 2-D feature maps to maintain the spatial information within
the subgraph regions. As the subgraph feature is shared by several predicate
inferences, 2-D feature map can learn more general representation of the region
and its inherit spatial structure can help to identify the subject/object and their
relations, especially the spatial relations. We continue employing the feature
vector to represent the objects. Thus, after the pooling, 2-D convolution layers
and fully-connected layers are used to transform the subgraph feature and object
features respectively.
Factorizable Net 7
(k×512×1×1)subgraphfeature(1	×	512×5×5)mergedfeatures(1	×	512×5×5)objectfeatures(k	×	512)attentionmaps(k×1×5×5)
refinedfeatures(1	×	512×5×5)SMP : SubgraphFeatureRefiningsubgraphfeatures(m	×	512×5×5)avg-pooledfeatures(m	×	515×1×1)
objectfeature(1	×	512)mergedfeatures(1	×	512)
refinedfeatures(1	×	512)attentionvector(m×1)SMP: ObjectFeatureRefiningconv kernel (1×512)Concatenatesubgraph feature(1×512×7×7)(1×512×7×7)(1×512×7×7)(1×1536×7×7)Spatial-sensitiveRelation Inference (SRI)predicate(1×512)subject
object
Fig. 3: Left:SMP structure for object/subgraph feature reﬁning. Right: SRI Mod-
ule for predicate recognition. Green, red and yellow refer to the subgraphs, ob-
jects and predicates respectively. ⊙denotes the dot product. ⊕and⊗denote
the element-wise sum and product.
3.5 Feature Reﬁning with Spatial-weighted Message Passing
As object and subgraph features involve diﬀerent semantic levels, where objects
concentrate on the details and subgraph focus on their interactions, passing
message between them could help to learn better representations by leveraging
their complementary information. Thus, we design a spatial weighted message
passing (SMP) structure to pass message between object feature vectors and
subgraph feature maps (left part of Fig. 3). Messages passing from objects to
subgraphs and from subgraphs to objects are two parallel processes. oidenotes
the object feature vector and Skdenotes the subgraph feature map.
Pass Message From Subgraphs to Objects This process is to pass several
2-D feature maps to feature vectors. Since objects only require the general in-
formation about the subgraph regions instead of their spatial information, 2-D
average pooling is directly adopted to pool the 2-D feature maps Skinto feature
vectors sk. Because each object is connected to various number of subgraphs,
we need ﬁrst aggregate the subgraph features and then pass them to the target
object nodes. Attention [59] across the subgraphs is employed to keep the scale
aggregated features invariant to the number of input subgraphs and determine
the importance of diﬀerent subgraphs to the object:
˜si=∑
Sk∈Sipi(Sk)·sk (1)
where Sidenotes the set of subgraphs connected to object i.˜sidenotes ag-
gregated subgraph features passed to object i.skdenotes the feature vector
average-pooled from the 2-D feature map Sk.pi(Sk) denotes the probability
thatskis passed to the target i-th object (attention vector in Fig. 3):
pi(Sk) =exp(
oi·FC(atts)(ReLU ( sk)))
∑
Sk∈Ciexp(
oi·FC(atts)(ReLU ( sk))) (2)
8 Yikang LI et al.
where FC(atts)transforms the feature skto the target domain of oi. ReLU
denotes the Rectiﬁed Linear Unit layer [40].
After obtaining message features, the target object feature is reﬁned as:
ˆoi=oi+ FC(s→o)(ReLU ( ˜si)) (3)
where ˆoidenotes the reﬁned object feature. FC(s→o)denotes the fully-connected
layer to transform merged subgraph features to the target object domain.
Pass Message From Objects to Subgraphs Each subgraph connects to sev-
eral objects, so this process is to pass several feature vectors to a 2-D feature
map. Since diﬀerent objects correspond to diﬀerent regions of the subgraph fea-
tures, when aggregating the object features, their weights should also depend on
their locations:
˜Ok(x, y) =∑
oi∈OkPk(oi)(x, y)·oi (4)
where Okdenotes the set of objects contained in subgraph k.˜Ok(x, y) denotes
aggregated object features to pass to subgraph kat location ( x, y).Pk(oi)(x, y)
denotes the probability map that the object feature oiis passed to the k-th
subgraph at location ( x, y) (corresponding to the attention maps in Fig. 3):
Pk(oi)(x, y) =exp(
FC(atto)(ReLU ( oi))·Sk(x, y))
∑
Sk∈Ciexp(
FC(atto)(ReLU ( oi))·Sk(x, y)) (5)
where FC(atto)transforms oito the target domain of Sk(x, y). The probabilities
are summed to 1 across all the objects at each location to normalize the scale
of the message features. But there are no such constraints along the spatial
dimensions. So diﬀerent objects help to reﬁne diﬀerent parts of the subgraph
features.
After the aggregation in Eq. 4, we get a feature map where the object features
are aggregated with diﬀerent weights at diﬀerent locations. Then we can reﬁne
the subgraph features as:
ˆSk=Sk+ Conv(o→s)(
ReLU(
˜Ok))
(6)
where ˆSidenotes the reﬁned subgraph features. Conv(o→s)denotes the convolu-
tion layer to transform merged object messages to the target subgraph domain.
Discussion Since subgraph features embed the interactions among several ob-
jects and objects are the basic elements of subgraphs, message passing between
object and subgraph features could: (1) help the object feature learn better rep-
resentations by considering its interactions with other objects and introduce the
contextual information; (2) reﬁne diﬀerent parts of subgraph features with cor-
responding object features. Diﬀerent from the message passing in ISGG [58] and
MSDN [35], our SMP (1) passes message between “points” (object vectors) and
“2-D planes” (subgraph feature maps); (2) adopts attention scheme to merge
Factorizable Net 9
diﬀerent messages in a normalized scale. Besides, several SMP modules can be
stacked to enhance the representation ability of the model.
3.6 Spatial-sensitive Relation Inference
After the message passing, we have got reﬁned representations of the objects
oiand subgraph regions Sk. Object categories can be predicted directly with
the object features. Because subgraph features may refer to several object pairs,
we use the subject and object features along with their corresponding subgraph
feature to predict their relationship:
p⟨i,k,j⟩=f(oi,Sk,oj) (7)
As diﬀerent objects correspond to diﬀerent regions of subgraph features, sub-
ject and object features work as the convolution kernels to extract the visual cues
of their relationship from feature map.
S(i)
k= FC (ReLU ( oi))⊗ReLU ( Sk) (8)
where S(i)
kdenotes the convolution result of subgraph feature map Skwithi-th
object as convolution kernel. ⊗denotes the convolution operation. As learning a
convolution kernel needs large quantities of parameters, Group Convolution [29]
is adopted. We set group numbers as the number of channels, so the group
convolution can be reformulated as element-wise product.
Then we concatenate S(i)
kandS(j)
kwith the subgraph feature Skand predict
the relationship directly with a fully-connected layer:
p⟨i,k,j⟩= FC(p)(
ReLU([
S(i)
k;Sk;S(j)
k]))
(9)
where FC(p)denotes the fully-connected layer for predicate recognition. [ ·] de-
notes the concatenation.
Bottleneck Layer Directly predicting the convolution kernel leads to a lot of
parameters to learn, which makes the model huge and hard to train. The number
of parameters of FC(p)equals:
#FC(p)=C(p)×C×W×H (10)
where C(p)denotes the number of predicate categories. Cdenotes the channel
size. WandHdenote the width and height of the feature map. Inspired by
the bottleneck structure in [22], we introduce an additional 1 ×1 bottleneck
convolution layer prior to FC(p)to reduce the number of channels (omitted in
Fig. 3). After adding an bottleneck layer with channel size equalling to C′, the
parameter size gets:
#Conv(bottleneck )+ #FC(p)=C×C′+C(p)×C′×W×H (11)
If we take C′=C/2, as #Conv(bottleneck )is far less than #FC(p), we almost half
the number of parameters.
10 Yikang LI et al.
Discussion In previous work, spatial features have been extracted from the
coordinates of the bounding box or object masks [6,36,62]. Diﬀerent from these
methods, ours embeds the spatial information in the subgraph feature maps.
Since FC(p)has diﬀerent weights at diﬀerent locations, it could learn to decide
whether to leverage the spatial feature and how to use that by itself from the
training data.
4 Experiments
In this section, implementation details of our proposed method and experiment
settings will be introduced. Ablation studies will be done to show the eﬀec-
tiveness of diﬀerent modules. We also compare our F-Net with state-of-the-art
methods on both accuracy and testing speed.
4.1 Implementation details
Model details ImageNet pretrained VGG16 [54] is adopted to initialize the
base CNN, which is shared by RPN and F-Net. ROI-align [21] is used to gener-
ated 5×5 object and subgraph features. Two FC layers are used to transform
the pooled object features to 512-dim feature vectors. Two 3 ×3 Conv layers
are used to generate 512-dim subgraph feature maps. For SRI module, we use
a 256-dim bottleneck layer to reduce the model size. All the newly introduced
layers are randomly initialized.
Training details During training, we ﬁx Conv 1and Conv 2of VGG16, and set
the learning rate of the other convolution layers of VGG as 0.1 of the overall
learning rate. Base learning rate is 0.01, and get multiplied by 0.1 every 3 epochs.
RPN NMS threshold is set as 0.7. Subgraph clustering threshold is set as 0.5. For
the training samples, 256 object proposals and 1024 predicates are sampled 50%
foregrounds. There is no sampling for the subgraphs, so the subgraph connection
maps are identical from training to testing. The RPN part is trained ﬁrst, and
then RPN, F-Net and base VGG part are jointly trained.
Inference details During testing phase, RPN NMS threshold and subgraph
clustering threshold are set as 0.6 and 0.5 respectively. All the predicates (edges
of fully-connected graph) will be predicted. Top-1 categories will be used as
the prediction for objects and relations. Predicated relationship triplets will be
sorted in the descending order based on the products of their subject, object
and predicate conﬁdence probabilities. Inspired by Li et al. in [34], triplet NMS
is adopted to remove the redundant predictions if the two triplets refer to the
identical relationship.
Factorizable Net 11
Table 1: Dataset statistics. VG-MSDN andVG-DR-Net are two cleansed-
version of raw Visual Genome dataset. #Img denotes the number of images.
#Rel denotes the number of subject-predicate-object relation pairs. #Object
and#Predicate denotes the number of object and predicate categories respec-
tively..
DatasetTraining Set Testing Set#Object #Predicate#Img #Rel #Img #Rel
VRD [37] 4,000 30,355 1,000 7,638 100 70
VG-MSDN [28,35] 46,164 507,296 10,000 111,396 150 50
VG-DR-Net [6,28] 67,086 798,906 8,995 26,499 399 24
4.2 Datasets
Two datasets are employed to evaluate our method, Visual Relationship Detec-
tion (VRD) [37] and Visual Genome [28]. VRD is a small benchmark dataset
where most of the existing methods are evaluated. Compared to VRD, raw Vi-
sual Genome contains too many noisy labels, so dataset cleansing should be done
to make it available for model training and evaluation. For fair comparison, we
adopt two cleansed-version Visual Genome used in [35] and [6] and compare
with their methods on corresponding datasets. Detailed statistics of the three
datasets are shown in Tab. 1.
4.3 Evaluation Metrics
Models will be evaluated on two tasks, Visual Phrase Detection (PhrDet) and
Scene Graph Generation (SGGen) . Visual Phrase Detection is to detect the
⟨subject-predicate-object ⟩phrases, which is tightly connected to the Dense Cap-
tioning [25]. Scene Graph Generation is to detect the objects within the image
and recognize their pair-wise relationships. Both tasks recognize the ⟨subject-
predicate-object⟩triplets, but scene graph generation needs to localize both the
subject and the object with at least 0.5 IOU (intersection over union) while
visual phrase detection only requires one bounding box for the entire phrase.
Following [37], Top-K Recall (denoted as Rec@K ) is used to evaluate how
many labelled relationships are hit in the top K predictions. The reason why
we use Recall instead of mean Average Precision (mAP) is that annotations of
the relationships are not complete. mAP will falsely penalize the positive but
unlabeled relations. In our experiments, Rec@50 andRec@100 will be reported.
The testing speed of the model is also reported. Previously, only accuracy
is reported in the papers. So lots of complicated structure and post-processing
methods are used to enhance the Recall. As scene graph generation is getting
closer to the practical applications and products, testing speed become a critical
metric to evaluate the model. If not speciﬁed, testing speed is evaluated with
Titan-X GPU."
2211.03216,D:\Database\arxiv\papers\2211.03216.pdf,"In the context of machine learning, what are the challenges associated with removing data from a model trained on graph-structured data, and how do these challenges differ from those encountered when removing data from models trained on unstructured data?","Removing data from a model trained on graph-structured data is more challenging than removing data from models trained on unstructured data due to the interconnected nature of graph data. Removing a single node or edge can affect the embeddings of multiple other nodes, making it difficult to isolate the impact of the removed data.","WWW ’23, May 1–5, 2023, Austin, TX, USA Pan et al.
Figure 2: The embedding procedure of GST with 𝐿=𝐽=3.
The scalar scattering coefficients 𝜙𝑝𝑗(𝑙)(S,x)(red blocks) are
concatenated to form the vector representation zof a graph.
Certified approximate removal. Let𝐴be a (randomized) learn-
ing algorithm that trains on D, the set of data points before removal,
and outputs a model ℎ∈H, whereHrepresents a chosen space
of models. The data removal requests leads to a change from Dto
D′. Given a pair of parameters (𝜖,𝛿), an unlearning algorithm 𝑀
applied to𝐴(D) is said to guarantee an (𝜖,𝛿)-certified approximate
removal for 𝐴, where𝜖,𝛿>0andXdenotes the space of possible
datasets, if∀T⊆H,D⊆X :
P 𝑀(𝐴(D),D,D′)∈T≤𝑒𝜖P 𝐴(D′)∈T+𝛿,
P 𝐴(D′)∈T≤𝑒𝜖P 𝑀(𝐴(D),D,D′)∈T+𝛿. (3)
This definition is related to (𝜖,𝛿)-DP [ 15] except that we are now
allowed to update the model based on the updated dataset D′un-
der the certified approximate removal criterion. An (𝜖,𝛿)-certified
approximate removal method guarantees that the updated model
𝑀(𝐴(D),D,D′)is approximately the same from a probabilistic
point of view as the model 𝐴(D′)obtained by retraining from
scratch onD′. Thus, any information about the removed data is
approximately (but with provable guarantees) eliminated from the
model. Note that exact removal corresponds to (0,0)-certified ap-
proximate removal. Ideally, we would like to design 𝑀so that it
satisfies Equation (3) with a predefined (𝜖,𝛿)pair and has a com-
plexity that is significantly smaller than that of complete retraining.
Nonlinearities in the graph learning framework. Previous
work [ 9] on analyzing approximate unlearning of linearized GNNs
focuses on node classification tasks within a single training graph.
There, the training samples are node embeddings and the embed-
dings are correlated because of the graph convolution operation.
The analysis of node classification proved to be challenging since
propagation on graphs “mixes” node features, and thus the removal
of even one feature/edge/node could lead to the change of embed-
dings for multiple nodes. The same technical difficulty exists for
GNNs tackling graph classification tasks (graph embeddings are
correlated), which limits the scope of theoretical studies of unlearn-
ing to GNNs that do not use nonlinear activations, such as SGC [ 56].
Meanwhile, if we use nontrainable graph feature extractors (i.e.,
GSTs) for graph classification tasks to obtain the graph embeddings,
the removal requests arising for one graph will not affect the em-
beddings of other graphs. This feature of GSTs not only makes the
analysis of approximate unlearning tractable, but also allows one tointroduce nonlinearities into the graph embedding procedure (i.e.,
the nonlinear activation function used in GSTs), which significantly
improves the model performance in practice.
4 UNLEARNING GRAPH CLASSIFIERS
We now turn our attention to describing how the inherent stability
properties of GSTs, which are nonlinear graph embedding methods,
can aid in approximate unlearning without frequent retraining (a
detailed discussion regarding how to replace GST with GNNs is
available in Section 5).
Motivated by the unlearning approach described in [ 28] for un-
structured data, we design an unlearning mechanism 𝑀that up-
dates the trained model from w★tow′, the latter of which repre-
sents an approximation of the unique optimizer of 𝐿(w,D′). Denote
the Hessian of 𝐿(·,D′)atw★asHw★=∇2𝐿(w★,D′)and denote
the gradient difference by Δ=∇𝐿(w★,D)−∇𝐿(w★,D′). The up-
date rule is w′=w★+H−1
w★Δ, which can be intuitively understood
as follows. Our goal is to achieve ∇𝐿(w′,D′)=0for the updated
model. Using a Taylor series expansion we have
∇𝐿(w′,D′)≈∇𝐿(w★,D′)+∇2𝐿(w★,D′)(w′−w★)=0.
Therefore, we have
w′−w★=
∇2𝐿(w★,D′)−1
0−∇𝐿(w★,D′)
w′=w★+
∇2𝐿(w★,D′)−1
∇𝐿(w★,D)−∇𝐿(w★,D′)
.(4)
The last equality holds due to the fact that ∇𝐿(w★,D)=0. When
∇𝐿(w′,D′)=0,w′is the unique optimizer of 𝐿(·,D′)due to
strong convexity. If ∇𝐿(w′,D′)≠0, some amount of information
about the removed data point remains. One can show that the
gradient residual norm ∥∇𝐿(w′,D′)∥determines the error of w′
when used to approximate the true minimizer of 𝐿(·,D′)again via
Taylor series expansion.
We would like to point out that the update approach from [ 28]
originally designed for unstructured unlearning can be viewed
as a special case of Equation (4) when the graph G𝑛needs to be
unlearned completely. In this case, we have Δ=∇𝐿(w★,D)−
∇𝐿(w★,D′)=𝜆w★+∇ℓ((w★)𝑇z𝑛,𝑦𝑛), which is the same expres-
sion as the one used in [ 28]. However, when we only need to un-
learn part of the nodes in G𝑛,Δbecomes Δ=∇ℓ((w★)𝑇z𝑛,𝑦𝑛)−
∇ℓ((w★)𝑇z′𝑛,𝑦𝑛), where z′𝑛is obtained via GST computed on the
remaining nodes in G𝑛. The unlearning mechanism shown in Equa-
tion (4) can help us deal with different types of removal requests
within a unified framework, and the main analytical contribution
of our work is to establish bounds of the gradient residual norm for
the generalized approach in the context of graph classification.
As discussed above, Equation (4) is designed to minimize the
gradient residual norm ∥∇𝐿(w′,D′)∥. However, the direction of
the gradient residual 𝐿(w′,D′)may leak information about the
training sample that was removed, which violates the goal of ap-
proximate unlearning. To address this issue, [ 28] proposed to hide
the real gradient residual by adding a linear noise term b𝑇wto the
training loss, a technique known as loss perturbation [ 6]. When tak-
ing the derivative of the noisy loss, the random noise bis supposed
to “mask” the true value of the gradient residual so that one can-
not infer information about the removed data. The corresponding
Unlearning Graph Classifiers with Limited Data Resources WWW ’23, May 1–5, 2023, Austin, TX, USA
approximate unlearning guarantees for the proposed unlearning
mechanism can be established by leveraging Theorem 4.1 below.
Theorem 4.1 (Theorem 3 from [ 28]).Denote the noisy training
loss by𝐿b(w,D)=Í𝑛
𝑖=1
ℓ(w𝑇z𝑖,𝑦𝑖)+𝜆
2∥w∥2
+b𝑇w, and let
𝐴be the learning algorithm that returns the unique optimum of
𝐿b(w,D). Suppose that w′is obtained by the unlearning procedure
𝑀and that∥∇𝐿(w′,D′)∥≤𝜖′for some computable bound 𝜖′>0. If
b∼N( 0,𝑐𝜖′/𝜖)𝑑is normally distributed with some constant 𝜖,𝑐>0,
then𝑀satisfies Equation (3) with (𝜖,𝛿)for algorithm 𝐴applied to
D′, where𝛿=1.5·𝑒−𝑐2/2.
Hence, if we can appropriately bound the gradient residual norm
∥∇𝐿(w′,D′)∥for graph classification problems, we can show that
the unlearning mechanism ensures an (𝜖,𝛿)-certified approximate
removal. For the analysis, we need the following assumptions on
the loss function ℓ. These assumptions naturally hold for commonly
used linear classifiers such as linear regression and logistic regres-
sion (see Section 5).
Assumption 4.2. There exist constants 𝐶1,𝐶2,𝛾1,𝛾2such that for
∀𝑖∈[𝑛]andw∈R𝑑: 1)∥∇ℓ(w𝑇z𝑖,𝑦𝑖)∥≤𝐶1; 2)|ℓ′(w𝑇zi,𝑦𝑖)|≤
𝐶2; 3)ℓ′is𝛾1-Lipschitz; 4) ℓ′′is𝛾2-Lipschitz; 5) it is always possible
to rescale x𝑖for graphG𝑖so that|[x𝑖]𝑗|≤1,∀𝑗∈[𝑔𝑖].
We show next that the gradient residual norm for graph classifi-
cation can be bounded for both types of removal requests.
Theorem 4.3. Suppose that Assumptions 4.2 hold, and that the
difference between the original dataset D=(Z,y)and the updated
datasetD′=(Z′,y)is in the embedding of the 𝑛-th training graph,
which equals z′𝑛=Φ(S𝑛,x′𝑛)with[x′𝑛]𝑔𝑛=0. Let𝐵be the frame
constant for the graph wavelets used in GST (see Equation (2)). Then
∥∇𝐿(w′,D′)∥≤𝛾2𝐹3
𝜆2𝑛min
4𝐶2
1,(𝛾1𝐶1𝐹2+𝜆𝐶2𝐹)2
𝜆2𝑔𝑛
,(5)
where𝐹=√︃Í𝐿−1
𝑙=0𝐵2𝑙. For tight energy preserving wavelets we have
𝐵=1,𝐹=√
𝐿.
The proof of Theorem 4.3 can be found in Appendix A. The
key idea is to use the stability property of GSTs, as we can view
the change of graph signal from x𝑛tox′𝑛as a form of signal
perturbation. The stability property ensures that the new embed-
dingΦ(S𝑛,x′𝑛)does not deviate significantly from the original one
Φ(S𝑛,x𝑛), and allows us to establish the upper bound on the norm
of gradient residual. Note that the second term on the RHS in Equa-
tion (5) decreases as the size 𝑔𝑛of the graphG𝑛increases. This is
due to the averaging operator 𝑈used in GSTs, and thus the graph
embedding is expected to be more stable under signal perturbations
for large rather than small graphs.
Next, we consider a more common scenario where an entire
node inG𝑛needs to be unlearned. This type of request frequently
arises in graph classification problems for social networks, where
unlearning one node corresponds to one user withdrawing from
one or multiple social groups. In this case, we have the following
bound on the gradient residual norm.
Theorem 4.4. Suppose that Assumptions 4.2 hold, and that both
the features and all edges incident to the 𝑔𝑛-th node inG𝑛have to beunlearned. Then
∥∇𝐿(w′,D′)∥≤4𝛾2𝐶2
1𝐹3
𝜆2𝑛,𝐹=vut𝐿−1∑︁
𝑙=0𝐵2𝑙. (6)
Remark. In this case, the norm ∥z′𝑛−z𝑛∥capturing the change in
the graph embeddings obtained via GST is proportional to the norm of
the entire graph signal ∥x𝑛∥. The second term within the min function
is independent on 𝑔𝑛and likely to be significantly larger than the first
term. Thus, we omit the second term in Equation (6). More details are
available in Appendix B.
Batch removal. The update rule in Equation (4) naturally sup-
ports removing multiple nodes from possibly different graphs at
the same time. We assume that the number of removal requests 𝑚
at one time instance is smaller than the minimum size of a training
graph, i.e.,𝑚<min𝑖𝑔𝑖, to exclude the trivial case of unlearning an
entire graph. In this setting, we have the following upper bound on
the gradient residual norm, as described in Corollary 4.5 and 4.6.
The proofs are delegated to Appendix C.
Corollary 4.5. Suppose that Assumptions 4.2 hold, and that 𝑚
nodes from𝑛graphs have requested feature removal. Then
∥∇𝐿(w′,D′)∥≤𝛾2𝑚2𝐹3
𝜆2𝑛min
4𝐶2
1,(𝛾1𝐶1𝐹2+𝜆𝐶2𝐹)2
𝜆2𝑔𝑛
,(7)
where𝐹=√︃Í𝐿−1
𝑙=0𝐵2𝑙.
Corollary 4.6. Suppose that Assumptions 4.2 hold, and that 𝑚
nodes from𝑛graphs have requested entire node removal. Then
∥∇𝐿(w′,D′)∥≤4𝛾2𝑚2𝐶2
1𝐹3
𝜆2𝑛,𝐹=vut𝐿−1∑︁
𝑙=0𝐵2𝑙. (8)
Data-dependent bounds. The upper bounds in Theorems 4.3
and 4.4 contain a constant factor 1/𝜆2which may be large when
𝜆is small and 𝑛is moderate. This issue arises due to the fact that
those bounds correspond to the worst case setting for the gradient
residual norm. Following an approach suggested in [ 28], we also
investigated data-dependent gradient residual norm bounds which
can be efficiently computed and are much tighter than the worst-
case bound. Note that these are the bounds we use in the online
unlearning procedure of Algorithm 2 for simulation purposes.
Theorem 4.7. Suppose that Assumptions 4.2 hold. For both single
and batch removal setting, and for both feature and node removal
requests, one has
∇𝐿 w′,D′≤𝛾2𝐹Z′H−1
w★ΔZ′H−1
w★Δ, (9)
where Z′is the data matrix corresponding to the updated dataset D′.
Algorithmic details. The pseudo-codes for training unlearning
models, as well as the single node removal procedure are described
below. During training, a random linear term b𝑇wis added to
the training loss. The choice of standard deviation 𝛼determines
the privacy budget 𝛼𝜖/√︁
2 log(1.5/𝛿)that is used in Algorithm 2.
During unlearning, 𝛽tracks the accumulated gradient residual
norm. If it exceeds the budget, then (𝜖,𝛿)-certified approximate
removal for 𝑀is no longer guaranteed. In this case, we completely
retrain the model using the updated dataset D′.
WWW ’23, May 1–5, 2023, Austin, TX, USA Pan et al.
Algorithm 1 Training Procedure
1:input: Training datasetD={z𝑖∈R𝑑,𝑦𝑖}𝑛
𝑖=1, lossℓ, parame-
ters𝛼,𝜆>0.
2:Sample the noise vector b∼N( 0,𝛼2)𝑑.
3:w★=arg minw∈R𝑑Í𝑛
𝑖=1
ℓ(w𝑇z𝑖,𝑦𝑖)+𝜆
2∥w∥2
+b𝑇w.
4:return w★.
Algorithm 2 Unlearning Procedure
1:input: Training graphsG𝑖with features x𝑖, graph shift opera-
torS𝑖and label𝑦𝑖, lossℓ, removal requests R𝑚={𝑟1,𝑟2,...},
parameters 𝜖,𝛿,𝛾 2,𝛼,𝜆,𝐹 >0.
2:Compute the graph embeddings z𝑖via GST(x𝑖,S𝑖). Initiate the
training setD={z𝑖∈R𝑑,𝑦𝑖}𝑛
𝑖=1.
3:Compute wusing Algorithm 1 ( D,ℓ,𝛼,𝜆 ).
4:Set the accumulated gradient residual norm to 𝛽=0.
5:for𝑟∈R𝑚do
6: Inx′𝑟, set the feature of a node to be removed to 0. Update
S′𝑟if the entire node is to be removed.
7: Compute the new graph embedding z′𝑟with GST(x′𝑟,S′𝑟).
8: Update the training set D′andZ′.
9: Compute Δ=∇𝐿(w,D)−∇𝐿(w,D′),H=∇2𝐿(w,D′).
10: Update the accumulated gradient residual norm as 𝛽=𝛽+
𝛾2𝐹∥Z′∥∥H−1Δ∥∥Z′H−1Δ∥.
11: if𝛽>𝛼𝜖/√︁
2 log(1.5/𝛿)then
12: Recompute wusing Algorithm 1 ( D′,ℓ,𝛼,𝜆 ),𝛽=0.
13: else
14: w=w+H−1Δ.
15: end if
16:D=D′.
17:end for
18:return w.
5 DISCUSSION
Commonly used loss functions. For linear regression, the loss
function isℓ(w𝑇z𝑖,𝑦𝑖)=(w𝑇z𝑖−𝑦𝑖)2, while∇2ℓ(w𝑇z𝑖,𝑦𝑖)=z𝑖z𝑇
𝑖,
which does not depend on w. Therefore, it is possible to have
∥∇𝐿(w′,D′)∥=0based on the proof in Appendix A. This obser-
vation implies that our unlearning procedure 𝑀is a(0,0)-certified
approximate removal method when linear regression is used as the
linear classifier module. Thus, the exact values of 𝐶1,𝐶2,𝛾1,𝛾2are
irrelevant for the performance guarantees for 𝑀.
For binary logistic regression, the loss function is defined as
ℓ(w𝑇z𝑖,𝑦𝑖)=−log(𝜎(𝑦𝑖w𝑇z𝑖)), where𝜎(𝑥)=1/(1+exp(−𝑥))
denotes the sigmoid function. As shown in [ 28], the assumptions
(1) and (4) in 4.2 are satisfied with 𝐶1=1and𝛾2=1/4. We only
need to show that (2) and (3) of 4.2 hold as well. Observe that
ℓ′(𝑥,𝑦)=𝜎(𝑥𝑦)−1. Since the sigmoid function 𝜎(·)is restricted
to lie in[0,1],|ℓ′|is bounded by 1, which means that our loss
satisfies (2) in 4.2 with 𝐶2=1. Based on the Mean Value Theorem,
one can show that 𝜎(𝑥)ismax𝑥∈R|𝜎′(𝑥)|-Lipschitz. With some
simple algebra, one can also prove that 𝜎′(𝑥)=𝜎(𝑥)(1−𝜎(𝑥))⇒
max𝑥∈R|𝜎′(𝑥)|=1/4. Thus the loss satisfies assumption (3) in 4.2
as well, with 𝛾1=1/4. For multiclass logistic regression, one can
adapt the one-versus-all strategy which leads to the same result.Note that it is also possible to use other loss functions such as lin-
ear SVM in Equation (1) with regularization. Choosing appropriate
loss function for different applications could be another interesting
future direction of this work.
Reducing the complexity of recomputing graph embed-
dings. Assume that the removal request arises in graph G𝑛. For the
case of feature removal, since we do not need to update the graph
shift operator S𝑛, we can reuse the graph wavelets {H𝑗(S𝑛)}𝐽
𝑗=1
computed before the removal to obtain the new embedding z′𝑛,
which is with complexity 𝑂(𝑑𝑔2𝑛).
For the case of complete node removal, we do need to update
the graph wavelets {H𝑗(S′𝑛)}𝐽
𝑗=1based on the updated S′𝑛. In gen-
eral, the complexity of computing z′𝑛in this case equals 𝑂(𝑑𝑔3𝑛),
as we need to compute the eigenvalue decomposition of S′𝑛and
perform matrix multiplications multiple times. This computational
cost may be too high when the size 𝑔𝑛ofG𝑛is large. There are
multiple methods to reduce this computational complexity, which
we describe next.
If the wavelet kernel function ℎ(𝜆)is a polynomial function, we
can avoid the computation of the eigenvalue decomposition of S′𝑛by
storing the values of all {S𝑘𝑛}𝐾
𝑘=1in advance during initial training,
where𝐾is the degree of ℎ(𝜆). For example, if ℎ(𝜆)=𝜆−𝜆2, we
have H(S′𝑛)=V′(Λ′−Λ′2)V′𝑇=S′𝑛−S′𝑛2. Note that we can write
the new graph shift operator as S′𝑛=S𝑛+ES𝑛+S𝑛E, where Eis a
diagonal matrix (i.e., if we remove the 𝑔𝑛-th node inG𝑛, we have
E=diag[0,..., 0,−1]). In this case, S′𝑛2can be found as
S′
𝑛2=S2
𝑛+2S𝑛ES𝑛+S2
𝑛E+ES2
𝑛+ES𝑛ES𝑛+ES2
𝑛E+S𝑛E2S𝑛+S𝑛ES𝑛E.
Thus, if we can store the values of all {S𝑘𝑛}𝐾
𝑘=1in advance dur-
ing initial training, we can reduce the complexity of computing
{S′𝑛𝑘}𝐾
𝑘=1to𝑂(𝑔2𝑛), due to the fact that whenever Eis involved in a
matrix multiplication (i.e., S𝑛ES𝑛), the computation essentially re-
duces to matrix-vector multiplication which is of complexity 𝑂(𝑔2𝑛).
Therefore, the complexity of computing {S′𝑛𝑘}𝐾
𝑘=1is𝑂(𝑔2𝑛)and the
overall computational complexity of obtaining z′𝑛is𝑂(𝑑𝑔2𝑛).
Lastly, ifℎ(𝜆)is an arbitrary function, and we need to recom-
pute the eigenvalue decomposition of S′𝑛, the problem is related
to a classical problem termed “downdating of the singular value
decomposition” of a perturbed matrix [ 27]. The overall complex-
ity of obtaining z′𝑛then becomes 𝑂(𝑔2𝑛(log2𝜉+𝑑)), where𝜉is a
parameter related to machine precision.
It is worth pointing out that 𝑂(𝑔2𝑛)is order-optimal with respect
to the unlearning complexity of removing nodes from a graph G𝑛,
since the complexity of the basic operation, graph convolution (i.e.,
Sx), is𝑂(𝑔2𝑛). As we will show in Section 6, the unlearning com-
plexity of using nontrainable GSTs is significantly smaller than that
of using GNNs when constructing graph embeddings in the worst
case. This is due to the fact that we may need to retrain GNNs
frequently to eliminate the effect of removed nodes on the embed-
ding procedure; on the other hand, we only need to recompute
the embeddings of affected training graphs when using GSTs. The
GSTs for different training graphs are computed independently,
which may be seen as a form of sharding with small components.
However, unlike traditional sharding-based methods [ 3,7], we do
not need to carefully select the partition, and the sizes of the shards
do not affect the performance of the final model.
Unlearning Graph Classifiers with Limited Data Resources WWW ’23, May 1–5, 2023, Austin, TX, USA
Using differentially-private GNNs for graph embeddings.
To ensure that the gradient residual norm does not grow excessively,
we need to have control over the graph embedding procedure so that
the embedding is stable with respect to small perturbations in the
graph topology and features. The nontrainable GST is one choice,
but DP-GNNs can also be used for generating the graph embed-
dings as they may improve the overall performance of the learner.
Based on Theorem 5 from [ 28], the overall learning framework still
satisfies the certified approximate removal criterion, and thus can
be used as an approximate unlearning method as well. However,
most DP-GNNs focus on node classification instead of graph classi-
fication tasks, and it remains an open problem to design efficient
GNNs for graph classification problems while preserving node-level
privacy. Moreover, it has been shown in [ 9] that DP-GNNs often
require a high “privacy cost” ( 𝜖≥5) (see Equation (3)) to unlearn
one node without introducing significant negative effects on model
performance. In contrast, we find that in practice, our proposed
unlearning approach based on GSTs only requires 𝜖=1. Therefore,
using DP-GNNs for graph embedding in unlearning frameworks
may not offer any advantages compared to alternatives.
6 EXPERIMENTAL RESULTS
Settings. We test our methods on five benchmarking datasets for
graph classification, including two real social networks datasets
IMDB, COLLAB [ 43], and three other standard graph classification
benchmarking datasets MNIST, CIFAR10, PROTEINS [ 11,12,14,34,
59]. As we focus on the limited training data regime, we use 10ran-
dom splits for all experiments with the training/validation/testing
ratio 0.1/0.1/0.8. Following [ 28], we use LBFGS as the optimizer
for all non-GNN methods due to its high efficiency on strongly
convex problems. We adopt the Adam [ 33] optimizer for GNNs
following the implementation of Pytorch Geometric library bench-
marking examples [ 18]. We compare our unlearning approach (Fig-
ure 1 (a)) with a naive application of [ 28] (Figure 1 (b)) as well as
complete retraining. The tested backbone graph learning models
include GST, GFT, linear-GST (i.e., GST without nonlinear activa-
tions) and GIN [ 58]. For all approximate unlearning methods we
use(𝜖,𝛿)=(1,10−4)and noise𝛼=0.1as the default parameters
unless specified otherwise. The shaded area in all plots indicates
one standard deviation. Additional details are in Appendix F.
Performance of the backbone models. We first test the per-
formance of all backbone graph learning models on the standard
graph classification problem. The results are presented in Tables 1
and 2. We observe that GST has consistently smaller running times
compared to GIN while offering matching or better accuracy. This
validates the findings of [ 22] and confirms that GST is indeed effi-
cient and effective in the limited training data regime. Compared
to linear-GSTs, we find that the nonlinearity of GST is important
to achieve better accuracy, with an average increase of 3.5%in test
accuracy over five datasets. In general, GST also significantly out-
performs GFT with respect to accuracy with a significantly smaller
running time. This is due to the fact that GST (with polynomial
wavelet kernel functions as in [ 22]) does not require an eigenvalue
decomposition as GFT does, which is computationally expensive
to perform for large datasets.
Performance of different unlearning methods. Next, we
test various unlearning schemes combined with GST. In this set ofTable 1: Test accuracy ( %) of the backbone graph learning
methods for standard graph classification in the limited
training data regime. The results report the mean accuracy
and standard deviation. Bold numbers indicate the best re-
sults.
IMDB PROTEINS COLLAB MNIST CIFAR10
GST 68.56±3.52 68.26±2.28 74.42±0.81 47.59±0.25 33.12±0.40
linear-GST 68.30±3.67 62.79±4.67 73.84±0.70 38.52±0.26 31.07±0.21
GFT 50.81±1.32 49.67±1.45 34.58±0.79 10.13±0.22 10.00±0.15
GIN 66.63±4.29 65.12±1.55 73.11±1.43 48.17±0.45 30.05±0.59
Table 2: Running time (s) of the backbone graph learning
methods for standard graph classification in the limited
training data regime. The results report the mean accuracy
and standard deviation. Bold numbers indicate the best re-
sults.
IMDB PROTEINS COLLAB MNIST CIFAR10
GST 6.47±0.89 7.57±1.79 10.89±1.08 82.94±5.75 75.36±1.17
linear-GST 6.92±1.50 7.59±1.25 10.94±1.40 82.23±0.83 74.98±1.07
GFT 4.43±1.04 9.00±0.96 137.69±1.29 1307.43±1.10 4240.62±2.56
GIN 23.13±1.32 21.94±0.92 949.06±63.68 1279.26±30.92 1239.03±33.06
experiments, we sequentially unlearn one node from each of the
selected 10%training graphs. We compare our Algorithm 2 with
the unstructured unlearning method [ 28] and complete retraining
(Retrain). Note that in order to apply unstructured unlearning to the
graph classification problem, we have to remove the entire training
graph whenever we want to unlearn even one single node from it.
The results are depicted in Figure 3. We can see that our unlearning
scheme with GSTs has accuracy comparable to that of complete
retraining but much lower time complexity. Also, note that a naive
application of [ 28] (indexed “UU” for unstructured unlearning) re-
sults in complete retraining in almost all the cases (see Table 3). In
addition, the method requires removing the entire training graph
instead of just one node as requested, thus the accuracy can drop
significantly when unlearning many requests (Figure 4).
Table 3: Number of retraining from scratch during sequen-
tial unlearning one node from 10%of training graphs.
IMDB PROTEINS COLLAB MNIST CIFAR10
GST 3.3 7.2 7.7 65.9 113.0
GST UU 10.0 11.0 50.0 550.0 450.0
GST Retrain 10 11 50 550 450
linear-GST 3.0 6.8 6.3 54.1 91.6
linear-GST UU 10.0 11.0 49.6 532.5 450.0
We further examine the performance of these unlearning ap-
proaches in the “extreme” graph unlearning setting: Now we un-
learn one node from each of the 90%training graphs sequentially.
Due to the high time complexity of baseline methods, we conduct
this experiment only on one small dataset, IMDB, and one medium
dataset, COLLAB. We also compare completely retraining GIN on
IMDB. The results are shown in Figure 4. We observe that retraining
GIN is indeed prohibitively complex in practice. A naive applica-
tion of [ 28] (indexed “UU”) leads to a huge degradation in accuracy
despite the high running complexity. This is due to the fact that one
WWW ’23, May 1–5, 2023, Austin, TX, USA Pan et al.
6466687072Accuracy/uni00A0(%)IMDB
GST/uni00A0Unlearn
GST/uni00A0Unlearn/uni00A0UU
GST/uni00A0Retrain
linear/uni00ADGST/uni00A0Unlearn
linear/uni00ADGST/uni00A0Unlearn/uni00A0UU5560657075PROTEINS
65.067.570.072.575.077.5COLLAB
253035MNIST
2829303132CIFAR10
2 4 6 8 10
#/uni00A0removal/uni00A0request0204060Accumulate/uni00A0
/uni00A0removal/uni00A0time/uni00A0(sec)
2 4 6 8 10
#/uni00A0removal/uni00A0request050100150200250
0 10 20 30 40 50
#/uni00A0removal/uni00A0request0100200300400
0 200 400
#/uni00A0removal/uni00A0request0250050007500100001250015000
0 100 200 300 400
#/uni00A0removal/uni00A0request0100002000030000
Figure 3: Sequential unlearning results. We unlearn one node in each of the 10%of the selected training graphs. The shaded
area indicates one standard deviation. All approximate unlearning methods satisfy (1,10−4)-certified approximate removal.
has to remove the entire training graph for each node unlearning
request, which is obviously wasteful. Overall, our proposed strategy
combined with GST gives the best results regarding time complexity
and offers comparable test accuracy and affordable privacy costs.
55606570Accuracy/uni00A0(%)IMDB
GST/uni00A0Unlearn
GST/uni00A0Unlearn/uni00A0UU
GST/uni00A0Retrain
GIN/uni00A0Retrain505560657075COLLAB
GST/uni00A0Unlearn
GST/uni00A0Unlearn/uni00A0UU
GST/uni00A0Retrain
0 20 40 60 80
#/uni00A0removal/uni00A0request05001000150020002500Accumulate/uni00A0
/uni00A0removal/uni00A0time/uni00A0(sec)
0 100 200 300 400
#/uni00A0removal/uni00A0request01000200030004000
Figure 4: Unlearning results for the extreme setting. We un-
learn one node in each of the 90%training graphs.
Performance of the proposed method with abundant train-
ing data. Next, we use the IMDB dataset as an example to demon-
strate the performance of our unlearning method compared to
complete retraining; the training/validation/testing ratio is set to
0.6/0.2/0.2, and out of 600training graphs, we unlearn 500samples
in terms of each removing a single node. The results (see Figure 5)
show that our approach still offers roughly a 10-fold decrease in
unlearning time complexity compared to retraining GIN, with com-
parable test accuracy. Note that due to the high complexity of re-
training GIN in this setting, we only performed complete retraining
for the number of removal requests indicated using green marks in
Figure 5. The green lines correspond to the interpolated results.
Bounds on the gradient residual norm. We also examine the
worst-case bounds (Theorem 4.4) and the data-dependent bounds
(Theorem 4.7) of Algorithm 2 computed during the unlearning
0 200 400
#/uni00A0removal/uni00A0request707274767880Accuracy/uni00A0(%)
IMDB
GST/uni00A0Unlearn
GIN/uni00A0Retrain
0 200 400
#/uni00A0removal/uni00A0request050001000015000Accumulate/uni00A0
/uni00A0removal/uni00A0time/uni00A0(sec)IMDBFigure 5: Unlearning results when training data is abundant.
We unlearn one node in each of the 83%training graphs.
process, along with the true value of the gradient residual norm
(True norm) to validate our theoretical findings from Section 4. For
simplicity, we set 𝛼=0during training. Figure 6 confirms that the
worst-case bounds are looser than the data-dependent bounds.
0 20 40 60 80
#/uni00A0removal/uni00A0request102
101104107Gradient/uni00A0residual/uni00A0normIMDB
True/uni00A0norm
Worst/uni00ADcase/uni00A0bound/uni00A0(Theorem/uni00A04.4)
Data/uni00ADdependent/uni00A0bound/uni00A0(Theorem/uni00A04.7)
0 100 200 300 400
#/uni00A0removal/uni00A0request102
101104107COLLAB
Figure 6: Comparison of the worst-case and data-dependent
bounds on the gradient residual norm, and the true value of
the gradient residual norm.
7 CONCLUSION
We studied the first nonlinear graph learning framework based on
GSTs that accommodates an approximate unlearning mechanism
with provable performance guarantees on computational complex-
ity. With the adoption of the mathematically designed transform
GSTs, we successfully extended the theoretical analysis on linear
models to nonlinear graph learning models. Our experimental re-
sults validated the remarkable unlearning efficiency improvements
compared to complete retraining.
Unlearning Graph Classifiers with Limited Data Resources WWW ’23, May 1–5, 2023, Austin, TX, USA
ACKNOWLEDGMENTS
This work was funded by NSF grants 1816913 and 1956384.
WWW ’23, May 1–5, 2023, Austin, TX, USA Pan et al.
REFERENCES
[1]Martin Anthony and Peter L Bartlett. 2009. Neural network learning: Theoretical
foundations . cambridge university press.
[2]Filippo Maria Bianchi, Daniele Grattarola, Lorenzo Livi, and Cesare Alippi. 2021.
Graph neural networks with convolutional arma filters. IEEE Transactions on
Pattern Analysis and Machine Intelligence (2021).
[3]Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hen-
grui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. 2021.
Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP) . IEEE,
141–159.
[4]Joan Bruna and Stéphane Mallat. 2013. Invariant scattering convolution networks.
IEEE transactions on pattern analysis and machine intelligence 35, 8 (2013), 1872–
1886.
[5]Yinzhi Cao and Junfeng Yang. 2015. Towards making systems forget with machine
unlearning. In 2015 IEEE Symposium on Security and Privacy . IEEE, 463–480.
[6]Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. 2011. Differen-
tially private empirical risk minimization. Journal of Machine Learning Research
12, 3 (2011).
[7]Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert,
and Yang Zhang. 2022. Graph unlearning. In Proceedings of the 2022 ACM SIGSAC
Conference on Computer and Communications Security . 499–513.
[8]Eli Chien, Chao Pan, and Olgica Milenkovic. 2022. Certified Graph Unlearning.
InNeurIPS 2022 Workshop: New Frontiers in Graph Learning .
[9]Eli Chien, Chao Pan, and Olgica Milenkovic. 2023. Efficient Model Updates for
Approximate Unlearning of Graph-Structured Data. In International Conference
on Learning Representations .
[10] Ameya Daigavane, Gagan Madan, Aditya Sinha, Abhradeep Guha Thakurta,
Gaurav Aggarwal, and Prateek Jain. 2021. Node-Level Differentially Private
Graph Neural Networks. arXiv preprint arXiv:2111.15521 (2021).
[11] Li Deng. 2012. The mnist database of handwritten digit images for machine
learning research. IEEE Signal Processing Magazine 29, 6 (2012), 141–142.
[12] Paul D Dobson and Andrew J Doig. 2003. Distinguishing enzyme structures from
non-enzymes without alignments. Journal of molecular biology 330, 4 (2003),
771–783.
[13] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell,
Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. 2015. Convolutional
networks on graphs for learning molecular fingerprints. Advances in neural
information processing systems 28 (2015).
[14] Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and
Xavier Bresson. 2020. Benchmarking graph neural networks. arXiv preprint
arXiv:2003.00982 (2020).
[15] Cynthia Dwork. 2011. Differential privacy. Encyclopedia of cryptography and
security.
[16] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin.
2019. Graph neural networks for social recommendation. In The world wide web
conference . 417–426.
[17] Li Fei-Fei, Robert Fergus, and Pietro Perona. 2006. One-shot learning of object
categories. IEEE transactions on pattern analysis and machine intelligence 28, 4
(2006), 594–611.
[18] Matthias Fey and Jan Eric Lenssen. 2019. Fast graph representation learning with
PyTorch Geometric. arXiv preprint arXiv:1903.02428 (2019).
[19] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion
attacks that exploit confidence information and basic countermeasures. In Pro-
ceedings of the 22nd ACM SIGSAC conference on computer and communications
security . 1322–1333.
[20] Fernando Gama, Alejandro Ribeiro, and Joan Bruna. 2019. Diffusion Scattering
Transforms on Graphs. In International Conference on Learning Representations .
[21] Fernando Gama, Alejandro Ribeiro, and Joan Bruna. 2019. Stability of graph
scattering transforms. Advances in Neural Information Processing Systems 32
(2019).
[22] Feng Gao, Guy Wolf, and Matthew Hirn. 2019. Geometric scattering for graph
data analysis. In International Conference on Machine Learning . 2122–2131.
[23] Thomas Gaudelet, Ben Day, Arian R Jamasb, Jyothish Soman, Cristian Regep,
Gertrude Liu, Jeremy BR Hayter, Richard Vickers, Charles Roberts, Jian Tang, et al .
2021. Utilizing graph machine learning within drug discovery and development.
Briefings in bioinformatics 22, 6 (2021), bbab159.
[24] Antonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou. 2019. Making
ai forget you: Data deletion in machine learning. Advances in Neural Information
Processing Systems 32 (2019).
[25] David F Gleich. 2015. PageRank beyond the Web. siam REVIEW 57, 3 (2015),
321–363.
[26] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. 2020. Eternal sunshine
of the spotless net: Selective forgetting in deep networks. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9304–9312.
[27] Ming Gu and Stanley C Eisenstat. 1995. Downdating the singular value decom-
position. SIAM J. Matrix Anal. Appl. 16, 3 (1995), 793–810.[28] Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. 2020.
Certified Data Removal from Machine Learning Models. In International Confer-
ence on Machine Learning . PMLR, 3832–3842.
[29] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30
(2017).
[30] David K Hammond, Pierre Vandergheynst, and Rémi Gribonval. 2011. Wavelets
on graphs via spectral graph theory. Applied and Computational Harmonic
Analysis 30, 2 (2011), 129–150.
[31] Chao Huang, Huance Xu, Yong Xu, Peng Dai, Lianghao Xia, Mengyin Lu, Liefeng
Bo, Hao Xing, Xiaoping Lai, and Yanfang Ye. 2021. Knowledge-aware coupled
graph neural network for social recommendation. In Proceedings of the AAAI
Conference on Artificial Intelligence , Vol. 35. 4115–4122.
[32] Vassilis N Ioannidis, Siheng Chen, and Georgios B Giannakis. 2020. Pruned graph
scattering transforms. In International Conference on Learning Representations .
[33] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[34] Alex Krizhevsky, Geoffrey Hinton, et al .2009. Learning multiple layers of features
from tiny images. (2009).
[35] Jia Li, Yu Rong, Hong Cheng, Helen Meng, Wenbing Huang, and Junzhou Huang.
2019. Semi-supervised graph classification: A hierarchical graph perspective. In
The World Wide Web Conference . 972–982.
[36] Maosen Li, Siheng Chen, Zihui Liu, Zijing Zhang, Lingxi Xie, Qi Tian, and Ya
Zhang. 2021. Skeleton graph scattering networks for 3d skeleton-based human
motion prediction. In Proceedings of the IEEE/CVF International Conference on
Computer Vision . 854–864.
[37] Ruirui Li, Xian Wu, Xian Wu, and Wei Wang. 2020. Few-shot learning for new
user recommendation in location-based social networks. In Proceedings of The
Web Conference 2020 . 2472–2478.
[38] Xiaoxiao Li, Nicha C Dvornek, Yuan Zhou, Juntang Zhuang, Pamela Ventola, and
James S Duncan. 2019. Graph neural network for interpreting task-fmri biomark-
ers. In International Conference on Medical Image Computing and Computer-
Assisted Intervention . Springer, 485–493.
[39] Stéphane Mallat. 2012. Group invariant scattering. Communications on Pure and
Applied Mathematics 65, 10 (2012), 1331–1398.
[40] Chengsheng Mao, Liang Yao, and Yuan Luo. 2019. Medgcn: Graph convolutional
networks for multiple medical tasks. arXiv preprint arXiv:1904.00326 (2019).
[41] Yimeng Min, Frederik Wenkel, and Guy Wolf. 2020. Scattering gcn: Overcoming
oversmoothness in graph convolutional networks. Advances in Neural Information
Processing Systems 33 (2020), 14498–14508.
[42] Mohammadreza Mohammadrezaei, Mohammad Ebrahim Shiri, and Amir Masoud
Rahmani. 2018. Identifying fake accounts on social networks based on graph
analysis and classification algorithms. Security and Communication Networks
2018 (2018).
[43] Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel,
and Marion Neumann. 2020. Tudataset: A collection of benchmark datasets for
learning with graphs. arXiv preprint arXiv:2007.08663 (2020).
[44] Tamara T Mueller, Johannes C Paetzold, Chinmay Prabhakar, Dmitrii Usynin,
Daniel Rueckert, and Georgios Kaissis. 2022. Differentially Private Graph Classi-
fication with GNNs. arXiv preprint arXiv:2202.02575 (2022).
[45] Chao Pan, Siheng Chen, and Antonio Ortega. 2021. Spatio-Temporal Graph
Scattering Transform. In International Conference on Learning Representations .
[46] Chao Pan, Jin Sima, Saurav Prakash, Vishal Rana, and Olgica Milenkovic. 2023.
Machine Unlearning of Federated Clusters. In International Conference on Learn-
ing Representations .
[47] Sina Sajadmanesh, Ali Shahin Shamsabadi, Aurélien Bellet, and Daniel Gatica-
Perez. 2022. GAP: Differentially Private Graph Neural Networks with Aggregation
Perturbation. arXiv preprint arXiv:2203.00949 (2022).
[48] Aliaksei Sandryhaila and José MF Moura. 2013. Discrete signal processing on
graphs: Graph Fourier transform. In 2013 IEEE International Conference on Acous-
tics, Speech and Signal Processing . IEEE, 6167–6170.
[49] Victor Garcia Satorras and Joan Bruna Estrach. 2018. Few-shot learning with
graph neural networks. In International conference on learning representations .
[50] Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh.
2021. Remember what you want to forget: Algorithms for machine unlearning.
Advances in Neural Information Processing Systems 34 (2021).
[51] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre
Vandergheynst. 2013. The emerging field of signal processing on graphs: Ex-
tending high-dimensional data analysis to networks and other irregular domains.
IEEE signal processing magazine 30, 3 (2013), 83–98.
[52] David I Shuman, Christoph Wiesmeyr, Nicki Holighaus, and Pierre Van-
dergheynst. 2015. Spectrum-adapted tight graph wavelet and vertex-frequency
frames. IEEE Transactions on Signal Processing 63, 16 (2015), 4223–4235.
[53] Michael Veale, Reuben Binns, and Lilian Edwards. 2018. Algorithms that remem-
ber: model inversion attacks and data protection law. Philosophical Transactions
of the Royal Society A: Mathematical, Physical and Engineering Sciences 376, 2133
(2018), 20180083.
Unlearning Graph Classifiers with Limited Data Resources WWW ’23, May 1–5, 2023, Austin, TX, USA
[54] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. 2020. Generalizing
from a few examples: A survey on few-shot learning. ACM computing surveys
(csur) 53, 3 (2020), 1–34.
[55] Max Welling and Thomas N Kipf. 2017. Semi-supervised classification with graph
convolutional networks. In International Conference on Learning Representations .
[56] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. 2019. Simplifying graph convolutional networks. In International
conference on machine learning . PMLR, 6861–6871.
[57] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2020. Graph neural
networks in recommender systems: a survey. ACM Computing Surveys (CSUR)
(2020).
[58] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Powerful
are Graph Neural Networks?. In International Conference on Learning Representa-
tions .
[59] Pinar Yanardag and SVN Vishwanathan. 2015. Deep graph kernels. In Proceedings
of the 21th ACM SIGKDD international conference on knowledge discovery and
data mining . 1365–1374.
[60] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,
and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale
recommender systems. In Proceedings of the 24th ACM SIGKDD international
conference on knowledge discovery & data mining . 974–983.
[61] Dongmian Zou and Gilad Lerman. 2020. Graph convolutional neural networks via
scattering. Applied and Computational Harmonic Analysis 49, 3 (2020), 1046–1074."
2306.10134,D:\Database\arxiv\papers\2306.10134.pdf,"In the context of multi-agent reinforcement learning, how does the proposed communication method address the challenge of balancing individual agent needs with the limited bandwidth available for communication?","The proposed method utilizes a centralized scheduler that dynamically allocates bandwidth based on the importance of each agent's message, considering factors like information gain and potential utility for other agents, ensuring that agents with more critical information receive a larger share of the bandwidth.","interpretation and decision-making. The architecture of DSMS is illustrated in Figure 3. The training
process follows the CTDE paradigm, employing parameter sharing to reduce the number of neural
networks that need to be learned and using a centralized critic. To enable agent-speciﬁc behavior, the
networks are conditioned on an agent ID.
3.1 Encoding the history
The ﬁrst component of DSMS involves utilizing an LSTM to compress the history of each agent
into a ﬁxed-size vector. In the context of POMGs, the agent’s history typically consists of the
previous observation-action pairs. However, in scenarios where agents can communicate, they also
need to remember the messages they have received and sent to avoid redundant communication. To
address this, we augment the agent’s history with two additional elements: the received message
at the previous time step, denoted as mt−1
i, and the bandwidth weight assigned by the scheduler,
represented as wt−1
i. By incorporating these elements into the history representation, agents can
effectively track the relevant information for communication within the LSTM framework. We denote
asht
ithe output of the LSTM for agent iat time-step t.
3.2 Weight-based bandwidth allocation
The allocation of bandwidth among agents poses a signiﬁcant challenge due to the complex nature of
assessing the importance of a message, which depends on various parameters that are not directly
accessible to the agents. To determine the importance of a message, an agent needs to consider
factors such as the information gain compared to previous messages and whether the information
is useful to other agents (which is not directly available). Additionally, the agents need to compare
the utility of their messages to determine the optimal allocation. To address this challenge, we
propose a two-step solution involving individual message utility assessment by each agent followed
by centralized scheduling.
Individual message utility assessment In the ﬁrst step, each agent independently assesses the utility
of its own message. By leveraging its individual history, which includes the previously allocated
bandwidths and the messages received from other agents, an agent can evaluate the potential value of
the information it can share with others. To quantify this utility, we employ an MLP that takes the
agent’s history hias input and computes a scalar utility value denoted as ui.
Scheduler The scheduler plays a crucial role in allocating the available bandwidth to each agent
based on their message utilities. Given the vector of agent message utilities u, the scheduler employs
the Gumbel-Softmax technique [ 24] to transform these utilities into importance weights w. Although
a classic softmax operation could be used for this transformation, we found that the addition of noise
in the Gumbel-Softmax improves the training process.
The next step involves converting these importance weights into individual bandwidth sizes bifor
each agent. The equation for computing biis as follows:
bi= 2×soft_ceil((B
2−n)
×wi)
=⇒∑
ibi≤B (1)
In this equation, Brepresents the total available bandwidth, and nis the number of agents. The
−nensures that there will not be bandwidth overﬂow from using the ceil operator, and the factor 2
is required to send complex numbers as explained in the next sub-section. The soft_ceil operation
ensures that gradients can ﬂow during training, and it is deﬁned as:
soft_ceil (x) =x+⌈x⌉−stop_grad (x)
Importantly, this method guarantees that each agent will have a minimum of two bytes for commu-
nication, as the softmax operation ensures that the weights are strictly positive. Although a ﬂoor
operation could be used instead of ceil, we found out that ceil simpliﬁes the training process.
By applying this procedure, the scheduler effectively assigns bandwidth to each agent based on their
message utilities while ensuring that the overall bandwidth constraint is not exceeded.
4
0 4 8 12 16 20 24 28
dimension−1.0−0.50.00.51.0valuemo
1 mo
2(a) Original messages mo
0.0 0.1 0.2 0.3 0.4 0.5
frequency0.000.060.120.180.240.30value
mf
1 mf
2(b) Frequency messages mf
Figure 4: Fourier Transform analysis of messages
3.3 Fourier-based dynamic message resizing
Based on their history hi, each agent will create a message mo
i∈Rpof sizepusing an MLP. To
match the allocated bandwidth bi, agents employ a transformation process that involves Discrete
Fourier Transform and clipping.
Discrete Fourier Transform (DFT) The motivation behind using Fourier Transform is to map mo
i
into the frequency domain, yielding a condensed representation where the magnitude of the ﬁrst
frequencies contains most of the essential information. The DFT transformation of the original
messagemo
ito the frequency message mf
iis deﬁned as follows, with mf
i,kdenoting the k-th
component of the message.
∀i∈[1,N],∀k∈[0,p−1], mf
i,k=p−1∑
q=0mo
i,qexp−2πkqj
p∈C (2)
In this equation, the symbol jrepresents the imaginary unit, which denotes the square root of -1, and
Crefers to the space of complex numbers.
It is important to note that the DFT of a real-valued signal, such as mf
i, exhibits Hermitian symmetry.
This symmetry implies that the frequency components at negative frequencies can be derived from
their corresponding symmetric components at positive frequencies. As a result, the frequency
components of mf
i,kfork>⌈p−1
2⌉are redundant. Therefore, we only retain the components mf
i,k
corresponding to the positive frequencies which corresponds to elements in the range of 0to⌊p
2⌋.
This allows for a more efﬁcient representation of the frequency message, reducing redundancy and
facilitating the subsequent resizing step.
Clipping. In order to match the allocated bandwidth bi, the frequency message mf
istill needs to
undergo clipping. Although clipping results in the removal of some information from the message,
the DFT yields an inherent ordering of the importance of each element in the frequency domain.
Sincemf
iis a vector of complex numbers, transmitting each element requires 2bandwidth units —
one for the real part and another for the complex part. Hence, we retain only the ﬁrst bi/2components
ofmf
i, resulting in the clipped message mc
i, which is sent to the other agents.
Figure 4 illustrates two messages mo, of sizep= 32 , along with their frequency counterparts, mf.
These messages are obtained from trained policies. Notably, the frequency message mf
2exhibits
an interesting property where the magnitudes after the 9th frequency are very close to zero. This
demonstrates that by combining the DFT with clipping, the message can be effectively compressed,
resulting in minimal loss of information in certain cases.
3.4 Message interpretation and decision making
Inverse Discrete Fourier Transform (IDFT) is the mathematical operation that enables the recon-
struction of the original message mr
iof agentifrom its clipped frequency version, mc
i. Although
5
the reconstruction may not be perfect, it retains the most signiﬁcant components of the message.
Importantly, the ﬁdelity of the reconstruction improves as the allocated bandwidth increases.
Decision making The incoming messages received by agent iare concatenated into miby averaging
them:
mi=1
N−1∑
j̸=imr
i (3)
The aggregated message miis then incorporated into agent i’s decision-making process by condi-
tioning its policy on the concatenation of its history hiand the aggregated message mi, denoted as
⟨hi,mi⟩. Furthermore, the message miis also included in the input of the LSTM in the next timestep,
ensuring that agents memorize the important information contained in the received messages. This
allows agents to make informed decisions based on both their individual history and the aggregated
messages from other agents.
3.5 Training
The training of all the components in DSMS is conducted end-to-end using the MADDPG algorithm.
In the DSMS framework, the critic network plays a crucial role, as it takes as input the weights wof
the scheduler. This allows the critic to provide valuable feedback to both the scheduler and the agents,
enabling them to learn and improve their communication strategies collaboratively. By training the
entire DSMS system end-to-end, the components can jointly optimize their performance and adapt to
the dynamics of the environment, resulting in more effective and efﬁcient communication.
Table 1: Results on Predator Prey
Methods Collisions
DSMS 11.42
MD-MADDPG 1.97
SchedNet 0.97
Table 2: Results on Cooperative Navigation
Methods Avg.Dis Collisions(%)
DSMS 0.799 1.45
MD-MADDPG 1.074 1.72
SchedNet 1.581 1.16
(a) Predator Prey
 (b) Cooperative Navigation
Figure 5: Environments
4 Experiments
4.1 Environments
We evaluate the performance of DSMS using two scenarios from the widely-used multi-agent particle
environments [ 11]: Predator Prey and Cooperative Navigation. These scenarios involve agents
operating in a two-dimensional world with continuous state space. To add complexity and simulate
real-world conditions, we modiﬁed these scenarios to impose partial observability.
Predator Prey In the Predator Prey scenario, our system consists of four predator agents (red)
pursuing a single prey agent (green) as shown in Figure 5(a). The environment includes one landmark
obstacle and two ""forests"" represented by large green balls, which can provide concealment for the
agents. The prey agent has full observability and moves at the same velocity as the predators. It
follows a ﬁxed policy obtained by training it against DSMS predators with full observability. For
the predators, partial observability is introduced, where each agent’s observation area is limited to a
square centered on itself, with a length equal to half the size of the environment. This means that
the maximum observation area is 25% of the total environment when a predator is positioned at the
center. Within their observation area, the predators can observe the relative positions and velocities
of other agents. The predators receive a shared reward, composed of positive components for every
6
0 10k 20k 30k 40k 50k
episode40
20
02040mean global returnDSMS
MD-MADDPG
SchedNet(a) Predator Prey
0 30k 60k 90k 120k 150k
episode270
210
150
90
mean total returnDSMS
MD-MADDPG
SchedNet (b) Cooperative Navigation
Figure 6: Evolution of the mean total return (sum of the agent’s return) for Cooperative Navigation
and mean global return (agent’s receive the same reward) for Predator Prey during the training for
DSMS, SchedNet and MD-MADDPG.
collision with the prey (+5) and a negative component based on the distance between each predator
agent and the prey.
Cooperative Navigation In the Cooperative Navigation scenario, our system consists of three agents
(red) and three landmark objects (black) in the environment as shown in Figure 5(b). The objective is
for the agents to cooperate with each other to cover all the landmarks without collisions. Each agent
has a partial ﬁeld of observation, which allows them to perceive the relative positions of other agents
and landmarks within their observation area. The ""leader"" agent, represented by a darker shade of red,
has a larger observation area that covers up to 50% of the environment when positioned at the center.
The other two agents have observation areas that cover up to 25% of the environment. Individual
rewards are assigned to each agent based on their distance to the closest landmark, penalties for
collisions with other agents, and the distances between each landmark and the closest agent.
4.2 Results
Baselines In our comparative analysis, we evaluated DSMS in comparison to two recent existing
works: MD-MADDPG [ 10] and SchedNet [ 8], which serve as baselines for our study. These
approaches speciﬁcally address the challenge of multi-agent communication over a single shared
medium. MD-MADDPG introduces a shared memory space that agents can access sequentially,
effectively reducing conﬂicts but granting all agents full access to the communication channel. On the
other hand, SchedNet incorporates a scheduler that manages the shared communication medium and
allocates limited bandwidth to agents, thereby determining which agents can utilize the channel for
message transmission. By comparing DSMS with these baselines, we aim to assess its effectiveness
and improvements in enhancing communication and coordination among agents.
In this subsection, we trained the models with a bandwidth of B= 64 , which allowed SchedNet to
allocate two agents at each timestep. We conducted training for 50,000 episodes for the Predator-Prey
scenario and 150,000 episodes for the Cooperative Navigation scenario. The results, averaged over 5
different seeds, are presented in Figure 6. For the Cooperative Navigation scenario, we measured
the sum of the individual returns for each agent. This metric provides an indication of the overall
performance of the team in completing the task. In the case of Predator-Prey, as the agents share
the same reward, we plotted the global return. Figure 6 illustrates the learning progress of DSMS
compared to the baselines, MD-MADDPG and SchedNet. Tables 1 and 2 present different statistics
of the ﬁnal policies for the two scenarios. Those statistics were obtained by running 200 episodes for
each seed (1,000 episodes in total).
Predator Prey In the Predator-Prey scenario, DSMS exhibited faster learning compared to the other
baselines as illustrated in Figure 6(a). It achieved a ﬁnal performance with an average total return
of 40, while MD-MADDPG and SchedNet failed to obtain positive rewards, with respective scores
of -10 and -20. Table 1 provides insights into the ﬁnal performance by presenting the frequency of
capturing the prey during an episode. Both MD-MADDPG and SchedNet captured the prey less
than 2.5 times per episode on average. In contrast, DSMS signiﬁcantly outperformed them with an
7
1 5 16 21 25 36 47 50
step0.00.20.40.60.81.0importance weight
predator1 predator2 predator3 predator4(a) Bandwidth allocation
1 5 16 21 25 36 47 50
step0.00.30.60.91.2predator-prey dist
predator1 predator2 predator3 predator4 (b) Distances between predators and prey
Figure 7: Communication analysis for Predator Prey
average capture rate of 11.42. This highlights the superior performance and effectiveness of DSMS
in the Predator-Prey scenario.
Cooperative Navigation In the Cooperative Navigation scenario, Figure 6(b) illustrates the learning
curves of DSMS and the baselines. It is evident that DSMS and MD-MADDPG exhibit similar
learning patterns in the initial episodes, but DSMS outperforms MD-MADDPG in terms of the ﬁnal
result. Notably, DSMS continues to improve even after 50,000 episodes, indicating its potential
for further learning, while MD-MADDPG appears to have converged. In contrast, SchedNet faces
difﬁculties in learning and falls behind the other algorithms in terms of performance. This suggests
that the communication mechanism employed by DSMS is more effective and yields better results.
Table 2 provides additional insights into the policies obtained after training. It reveals that the
agents trained with DSMS approach the landmarks more closely. In contrast to our DSMS approach,
SchedNet’s agents keep some distance from the landmarks leading to less collision, but this cautious
behavior results in a signiﬁcant hindrance to its ﬁnal performance. These results validate the efﬁciency
of our communication method and highlight the promising potential of our ﬁne-grained bytewise
scheduling approach.
4.3 Communication analysis
In this subsection, we analyze the division of the bandwidth between the agents.
Predator Prey For this scenario, we compare the bandwidth repartition (Figure 7(a)) with the
distance between the agents and the prey (Figure 7(b)) during one episode of 50 steps. We separated
the episode into seven blocks (1-4, 5-15, 16-20, 21-24, 25-35, 36-46, and 47-50). This separation
reveals a correlation between the allocated bandwidth and the distance to the prey. In particular, the
agent closer to the prey receives on average more bandwidth than the other, but it is not always the
case. This shows that DSMS does indeed learn to attribute the bandwidth based on the importance of
the message, and that the scheduling takes not only into account the current observation but also the
novelty of the message which explains why agents more distant can receive more bandwidth.
1 5 9 13 17 20
step0.00.20.40.60.81.0importance weight
agent1 agent2 agent3
(a) Bandwidth allocation
Figure 8: Communication analysis for
Cooperative NavigationCooperative Navigation For the Cooperative Navigation
scenario, we focus on the allocation of bandwidth between
the leader agent (Agent 1 in red) and the other two agents.
In Figure 8, we observe that the leader agent is allocated
more bandwidth than the other two agents in 10 out of the
20 steps of the episode. This demonstrates that the DSMS
scheduling mechanism is able to learn the importance of
the leader’s messages and allocate more bandwidth ac-
cordingly. By dynamically adjusting the bandwidth allo-
cation based on the agent’s role and observation window,
DSMS effectively prioritizes the communication needs of
the leader agent, leading to improved coordination and
performance in cooperative navigation tasks.
8
0 10k 20k 30k 40k 50k
episode40
20
0204060mean global returnfull_comm
non_comm
bandwidth(16+8)bandwidth(32+8)
bandwidth(64+8)
bandwidth(96+8)(a) Predator Prey
0 30k 60k 90k 120k 150k
episode130
120
110
100
90
mean total returnfull_comm
non_comm
bandwidth(16+6)
bandwidth(32+6)
bandwidth(64+6) (b) Cooperative Navigation
Figure 9: Evolution of the mean return during the training for DSMS with different bandwidth sizes
including no communication and full communication.
4.4 Ablations
In addition to comparing DSMS with existing works, we conducted an ablative study to assess
the impact of bandwidth on performance. We considered two extreme cases: no communication
(bandwidth of 0) and full communication (bandwidth equal to the product of the number of agents
with the size of a full message). Additionally, we tested bandwidth values of 24, 40, 72, and 104
for the Predator-Prey scenario, and 22, 38, and 68 for the Cooperative Navigation scenario. The
Cooperative Navigation scenario features one agent less, hence the slightly lower bandwidth values.
The results of this ablative study, shown in Figure 9, revealed several interesting ﬁndings. As
expected, full communication achieved the best performance, while no communication yielded the
worst performance. All non-zero bandwidth values demonstrated performance close to the full
communication setting with a natural ordering based on the bandwidth size. This suggests that
full communication is not necessary, and our approach performs well even with a small bandwidth
allocation. These ﬁndings provide valuable insights into the optimal bandwidth requirements for
effective multi-agent communication.
5 Conclusions
In this paper, we have presented a novel approach, Dynamic Sparse Message Scheduling, for ad-
dressing the challenges of communication in MARL systems. DSMS leverages Fourier Transform to
dynamically resize messages, enabling a ﬁne-grained scheduling mechanism to allocate bandwidth
based on the importance of information. We evaluated DSMS in two scenarios: Predator-Prey
and Cooperative Navigation, with modiﬁcations to impose partial observability. In both scenarios,
DSMS outperformed existing baselines, demonstrating faster learning and achieving higher ﬁnal
performance. One important aspect of DSMS is its bandwidth allocation strategy, which dynamically
adjusts the message sizes based on the importance of the information. Our analysis revealed that
DSMS prioritizes agents with better observations or new information, leading to more efﬁcient
communication and coordination among agents. Furthermore, an ablative study on different band-
width sizes demonstrated that DSMS performs well even with reduced bandwidth. This highlights
the robustness of our approach and its ability to adapt to varying communication constraints. In
conclusion, DSMS presents a promising solution to the communication challenges in MARL, offering
improved coordination, faster learning, and effective bandwidth utilization. It paves the way for
enhanced collaboration and communication among agents, enabling more sophisticated and efﬁcient
multi-agent systems in various domains.
Future work includes conducting additional experiments on different environments to further evaluate
the performance of DSMS. We would also like to test DSMS with off-policy value-based methods
such as VDN, QMIX or LAN [ 25,26,21]. Additionally, we plan to explore the replacement of the
centralized scheduler with a decentralized approach, where agents reserve a subset of the bandwidth
to share their next message utility. While this would improve decentralization, it would require agents
to estimate their utility before taking an action and receiving the new observation, creating an offset
in the communication protocol.
9
Acknowledgements
This research was supported by funding from the Flemish Government under the “Onderzoek-
sprogramma Artiﬁciële Intelligentie (AI) Vlaanderen” program. This work was supported in part
by the National Natural Science Foundation of China under Grants 62032018 and 61876151, in
part by the Industry-University-Research Innovation Fund for the China Universities under Grant
2021ZYA09001, and by the China Scholarship Council. R. Avalos is supported by the Research
Foundation – Flanders (FWO), under grant number 11F5721N.
References
[1]David Baldazo, Juan Parras, and Santiago Zazo. Decentralized multi-agent deep reinforcement
learning in swarms of drones for ﬂood monitoring. In 2019 27th European Signal Processing
Conference (EUSIPCO) , pages 1–5. IEEE, 2019.
[2]Floyd D’Souza, João Costa, and J Norberto Pires. Development of a solution for adding a
collaborative robot to an industrial agv. Industrial Robot: the international journal of robotics
research and application , 47(5):723–735, 2020.
[3]Rihab Kouki, Alexandre Boe, Thomas Vantroys, and Faouzi Bouani. Autonomous internet of
things predictive control application based on wireless networked multi-agent topology and
embedded operating system. Proceedings of the Institution of Mechanical Engineers, Part I:
Journal of Systems and Control Engineering , 234(5):577–595, 2020.
[4]Tong Zhang, Yu Gou, Jun Liu, Tingting Yang, Shanshan Song, and Jun-Hong Cui. A scalable and
fair power allocation scheme based on deep multi-agent reinforcement learning in underwater
wireless sensor networks. In Proceedings of the 15th International Conference on Underwater
Networks & Systems , pages 1–5, 2021.
[5]Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learn-
ing to communicate with deep multi-agent reinforcement learning. In Advances in neural
information processing systems , pages 2137–2145, 2016.
[6]Sainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropa-
gation. In Advances in neural information processing systems , pages 2244–2252, 2016.
[7]Jiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent coopera-
tion. In Advances in neural information processing systems , pages 7254–7264, 2018.
[8]Daewoo Kim, Sangwoo Moon, David Hostallero, Wan Ju Kang, Taeyoung Lee, Kyunghwan
Son, and Yung Yi. Learning to schedule communication in multi-agent reinforcement learning.
InICLR 2019: International Conference on Representation Learning . International Conference
on Representation Learning, 2019.
[9]Hangyu Mao, Zhengchao Zhang, Zhen Xiao, Zhibo Gong, and Yan Ni. Learning agent
communication under limited bandwidth by message pruning. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence , volume 34, pages 5142–5149, 2020.
[10] Emanuele Pesce and Giovanni Montana. Improving coordination in small-scale multi-agent
deep reinforcement learning through memory-driven communication. Machine Learning ,
109(9-10):1727–1747, 2020.
[11] Ryan Lowe, YI WU, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch.
Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in Neural
Information Processing Systems , 30:6379–6390, 2017.
[12] Sai Qian Zhang, Qi Zhang, and Jieyu Lin. Succinct and robust multi-agent communication
with temporal message control. Advances in Neural Information Processing Systems , 33:17271–
17282, 2020.
[13] Guangzheng Hu, Yuanheng Zhu, Dongbin Zhao, Mengchen Zhao, and Jianye Hao. Event-
triggered communication network with limited-bandwidth constraint for multi-agent reinforce-
ment learning. IEEE Transactions on Neural Networks and Learning Systems , 2021.
[14] Rundong Wang, Xu He, Runsheng Yu, Wei Qiu, Bo An, and Zinovi Rabinovich. Learning
efﬁcient multi-agent communication: An information bottleneck approach. In International
Conference on Machine Learning , pages 9908–9918. PMLR, 2020.
10
[15] Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994 , pages 157–163. Elsevier, 1994.
[16] Martin L Puterman. Markov decision processes. Handbooks in operations research and
management science , 2:331–434, 1990.
[17] Frans A Oliehoek and Christopher Amato. A concise introduction to decentralized POMDPs .
Springer, 2016.
[18] Frans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. Optimal and approximate q-value
functions for decentralized pomdps. Journal of Artiﬁcial Intelligence Research , 32:289–353,
2008.
[19] Felix A Gers, Jürgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction
with lstm. Neural computation , 12(10):2451–2471, 2000.
[20] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI conference on artiﬁcial
intelligence , volume 32, 2018.
[21] Raphaël Avalos, Mathieu Reymond, Ann Nowé, and Diederik M. Roijers. Local Advantage
Networks for Cooperative Multi-Agent Reinforcement Learning. In AAMAS ’22: Proceedings
of the 21st International Conference on Autonomous Agents and MultiAgent Systems (Extended
Abstract) , 2022.
[22] Boualem Boashash. Note on the use of the wigner distribution for time-frequency signal analysis.
IEEE Transactions on Acoustics, Speech, and Signal Processing , 36(9):1518–1521, 1988.
[23] Marina Bosi and Richard E Goldberg. Introduction to digital audio coding and standards ,
volume 721. Springer Science & Business Media, 2002.
[24] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.
InInternational Conference on Learning Representations , 2016.
[25] Amanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Learning when to communicate at
scale in multiagent cooperative and competitive tasks. In International Conference on Learning
Representations , 2018.
[26] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. QMIX: Monotonic value function factorisation for deep multi-agent
reinforcement learning. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th
International Conference on Machine Learning , volume 80 of Proceedings of Machine Learning
Research , pages 4295–4304. PMLR, 10–15 Jul 2018.
11"
1909.11864,D:\Database\arxiv\papers\1909.11864.pdf,How does the model address the issue of potential noise in the energy function when dealing with paths of varying lengths?,"The model utilizes different margin values (γi) for paths with different step numbers, acknowledging that noise in the energy function increases with path length. This helps to maintain accuracy in path-based inference.","Figure 3: The representation of path p(hr1−→t′r2−→t). The top part of the ﬁgure depicts the process of
space transition of t′andt, while the bottom part illustrates the generated continuous path from hptotpafter the
transition.
Note that a triple (h,r,t )in the KG can be seen
as a one-step path between handt. Thus, the value
ofE(h,r,t )is able to be obtained by substituting
direct relation rasps=1into Equation (2).
From Equation (2) we could observe that the se-
quence matrix Si
pbefore each relation riis dif-
ferent. If the order of several relations in a path
is altered, the value of energy function will also
change at the same time. Therefore, paths with the
same relation set but different relation order will
infer out distinct direct relations in our model. The
speciﬁc representation of the ordered relation path
will be demonstrated in the following contents.
To keep the order information of relations in
paths, we project the head and tail entities of a
relation into different spaces by introducing two
matrices for each relation. Let Wr,1∈Rd×dand
Wr,2∈Rd×ddenote the projection matrices of the
head entity and the tail entity for relation r, respec-
tively. With these two matrices, we will project the
head and tail entities into distinct spaces with re-
spect to the same relation. Suppose there is a path
r1,r2,...,r nfromhtot, ideally, we deﬁne the
following equations


Wr1,1h+r1=Wr1,2t(1)
Wr2,1t(1)+r2=Wr2,2t(2)
...
Wrn,1t(n−1)+rn=Wrn,2t, (5)
where t(i)indicates the i-th passing node on the
path.For the entity pair with a relation path, we get
their representations after eliminating the passing
nodes from Equation (5). Thus, the concrete forms
of the variables in Equation (2) are shown as fol-
lows,
hp=Wr1,1h,tp=Wps=nt, (6)
Si
p=i∏
k=1Tk, (7)
where
Wps=n=Sn
pWrn,2, (8)
Tk={
Ik= 1
M(rk,rk−1)k>1. (9)
Wps=n∈Rd×dindicates the projection matrix
for pathps=n, which aims to project the tail en-
tity in a path to the space of ps=n. Moreover,
Iin Equation (9) denotes the identity matrix and
M(rk,rk−1)∈Rd×dmeans the space transition
matrix from the head entity space of rkto the tail
entity space of rk−1, i.e., M(rk,rk−1)Wrk,1=
Wrk−1,2.
Figure 3 illustrates the representation of the re-
lation path in our model. Suppose there is a 2-step
path fromhtotpassingt′, i.e.,hr1−→t′r2−→t. It
is obvious that t′acts as the tail entity of relation
r1and as the head entity of relation r2at the same
time, which is shown on the top part of Figure 3.
To connect relations in different spaces, we try to
unify the passing node in the path into the same
space. As deﬁned in Equation (9), T2is utilized
to transfer the passing node t′from the head entity
space ofr2to the tail entity space of r1. More-
over,T2is also assigned to the relation r2and the
tail entityt. Note that the tail entity twill be pro-
jected into the space of path pwhich is deﬁned in
Equation (6). Finally, the path from hptotpwill
pass through r1andT2r2as shown on the bottom
part of Figure 3.
3.2 Pooling Strategy
We design a two layer pooling strategy to fuse the
information from different paths. First, we utilize
a minimum pooling method to extract feature in-
formation from paths with isteps and deﬁne an
energy function as follows,
E(
h,Ps=i
r,t)
=Min[E(
h,ps=i,t)
|ps=i∈Ps=i
r],
(10)
wherePs=i
rindicates the set of all i-step paths
which are relevant to the relation rfrom the head
entityhto the tail entity t. To obtain Ps=i
r, we
introduce a conditional probability Pr(r|ps=i)to
represent the reliability of a path ps=iassociated
with the given relation r,
Pr(
r⏐⏐ps=i)
= Pr(r,ps=i)/Pr(ps=i)
=N(r,ps=i)/N(p)
N(ps=i)/N(p)
=N(r,ps=i)
N(ps=i),(11)
where Pr(r,ps=i)denotes the joint probability of
randps=i,Pr(ps=i)denotes the marginal proba-
bility ofps=i. In addition, N(r,ps=i)denotes the
number of cases where randps=ilink the same
entity pair in the KG, N(ps=i)denotes the num-
ber of the path ps=iin the KG and N(p)denotes
the total number of paths in the KG. Since N(p)
can be removed from both the numerator and de-
nominator of the fractional expression, we ﬁnally
convert the probability into frequency for compu-
tation.
We ﬁlter the paths by choosing all ps=ifromh
totwhose Pr(
r⏐⏐ps=i)
>0. Thus,Ps=i
ris the
set of all ﬁltered ps=i. Sometimes we could in-
fer the fact not from the direct relation rbut from
the path, which means the value of E(
h,Ps=i
r,t)
could possibly be less than that of E(h,r,t ).
Furthermore, we utilize a minimum pooling
method to fuse information from paths with dif-ferent lengths and deﬁne an energy function as fol-
lows,
Efinal(h,r,t ) =Min[E(h,r,t ), E(
h,Ps=1
r,t)
,
E(
h,Ps=2
r,t)
,..., E (h,Ps=n
r,t)],
(12)
whereE(h,r,t )indicates the energy value of di-
rect relation rand it is calculated by substituting
rasps=1into Equation (2). E(
h,Ps=i
r,t)
is ini-
tialized as inﬁnite , thus it will not inﬂuence the
outcome of ﬁnal energy function if there is no i-
step path between handt.
In summary, we adopt the min-pooling strat-
egy twice in our model. For E(
h,Ps=i
r,t)
, min-
pooling aims to choose the most matched path
withramong all i-step paths. And for the ﬁnal en-
ergy function, min-pooling tries to extract nonlin-
ear features from paths of various lengths. In addi-
tion, the min-pooling method addresses the prob-
lem that there may be no relation paths between h
andt.
3.3 Objective Function
The objective function for the proposed model
OPTransE is formalized as
L(S) =∑
(h,r,t )∈S{
L(h,r,t ) +λ·∑
i[1
Zi
·∑
ps=i∈Ps=irPr(
ps=i⏐⏐⏐h,t)
·Pr(
r⏐⏐⏐ps=i)
·L(
h,ps=i,t)]}
,
(13)
whereL(h,r,t )indicates the loss function for
the triple (h,r,t ), andL(
h,ps=i,t)
represents the
loss value with respect to the relation path ps=i.
The probability Pr(
ps=i⏐⏐h,t)
indicates the reli-
ability of the relation path ps=igiven the entity
pair(h,t), and Pr(r|ps=i)denotes the reliabil-
ity of a path ps=iassociated with the given rela-
tionr. The details of Pr(
ps=i⏐⏐h,t)
are shown
in (Lin et al., 2015a), which is computed by
a path-constraint resource allocation algorithm.
Zi=∑
ps=i∈Ps=irPr(
ps=i⏐⏐h,t)
Pr(
r⏐⏐ps=i)
is a
normalization factor, and λis utilized to balance
the triple loss and paths losses.
We adopt the margin-based loss in our model,
i.e.,
L(h,r,t ) =∑
(h′,r,t′)∈S′[γ+E(h,r,t )−E(h′,r,t′)]+,
(14)
L(
h,ps=i,t)
=∑
(h′,r,t′)∈S′[γi+E(h,p,t )−E(h′,p,t′)]+,
(15)
wherepis the simple form of ps=i.[x]+=
max(x,0)returns the higher one between xand
0.γiis the margin to separating positive and nega-
tive samples. It is noteworthy that we employ dif-
ferent margin γifor paths with different step num-
ber because the noise of energy function will be
magniﬁed as the number of steps increases. The
corrupted triple set S′for(h,r,t )is denoted as
follows:
S′={(h′,r,t)∪(h,r,t′)}. (16)
We replace the head entity or the tail entity in the
triple randomly and guarantee that the new triple
is not an existing valid triple.
Our goal is to minimize the total loss. Valid re-
lation paths will obtain lower energy value after
the optimization, so that paths can sometimes re-
place directed relations when performing the pre-
diction.
3.4 Parameter Learning
We utilize stochastic gradient descent (SGD) to
optimize the objective function in Equation (13)
and learn parameters of the model. To ensure the
convergence of the model, we impose limitations
to the norm of vectors, i.e., ||h||2≤1,||r||2≤
1,||t||2≤1,||Wr,1h||2≤1,||Wr,2t||2≤1.
Moreover, we note that the objective function de-
ﬁned in Equation (13) has two parts. The ﬁrst part
is for the basic triple and the second part is for
the relation paths. To focus on the representation
of ordered relation paths in the second part, we
only update the parameters of relation vectors in
the path when conducting the optimization of the
model.
In addition, we follow PTransE (Lin et al.,
2015a) to generate reverse relation r-1to enlarge
the training set, and the inference in KGs can be
through the reverse paths. For instance, for the
fact ( Honolulu ,CapitalOf ,Hawaii ), we will also
add a fact with the reverse relation to the KG, i.e.,
(Hawaii ,CapitalOf−1,Honolulu ).
3.5 Complexity Analysis
Letddenote the dimension of entities and rela-
tions, NeandNrdenote the number of entities andrelations, respectively. The number of model pa-
rameters for OPTransE is ( Ned+Nrd+ 2Nrd2),
which is the same as that of STransE.
Moreover, let Npdenote the expected number
of relation paths between the entity pair, Ntde-
note the number of triples for training, kdenote the
maximum length of relation paths. According to
the objective function shown in Equation (13) and
details of parameter learning stated in Section 3.4,
the time complexity of OPTransE for optimization
isO(k2d3NpNt), which is on the same magnitude
as that of RPE(MCOM) (Lin et al., 2018).
4 Experiments
4.1 Datasets
To evaluate the proposed model OPTransE, we use
two benchmark datasets: WN18 and FB15K as ex-
perimental data. They are subsets of the knowl-
edge graph WordNet (Miller, 1995) and Free-
base (Bollacker et al., 2008), respectively (Bordes
et al., 2013). These two datasets have been widely
employed by researchers for KG completion (Jia
et al., 2018; Lin et al., 2018). The statistic details
of the two datasets are shown in Table 1. In our ex-
periments, as we add triples of reverse relations to
the datasets, the number of relations and training
triples are doubled.
Table 1: Statistics of datasets
Dataset #Rel #Ent #Train #Valid #Test
WN18 18 40,943 141,442 5,000 5,000
FB15K 1345 14,951 483,142 50,000 59,071
4.2 Experimental Settings
We adopt the idea from TransR (Lin et al., 2015b)
and initialize the vectors and matrices of OP-
TransE by an existing method STransE (Nguyen
et al., 2016). Following TransH (Wang et al.,
2014), Bernoulli method is applied for generat-
ing head or tail entities when sampling corrupted
triples.
As the length of paths increases, the reliability
of the path will decline accordingly. To better de-
termine the maximum length of paths for exper-
iment, before the test on FB15K, we had evalu-
ated OPTransE with 3-step paths on WN18. How-
ever, OPTransE (3-step) performs comparably as
OPTransE (2-step) with a higher computational
cost. This indicates that longer paths hardly con-
tain more useful information and it is unnecessary
Table 2: Evaluation results on link prediction
ModelWN18 FB15K
Mean Rank Hits@10(%) Mean Rank Hits@10(%)
Raw Filtered Raw Filtered Raw Filtered Raw Filtered
SE 1011 985 68.5 80.5 273 162 28.8 39.8
SME 545 533 65.1 74.1 274 154 30.7 40.8
TransE 263 251 75.4 89.2 243 125 34.9 47.1
TransH 318 303 75.4 86.7 212 87 45.7 64.4
TransR 238 225 79.8 92.0 198 77 48.2 68.7
TranSparse 223 211 80.1 93.2 187 82 53.5 79.5
STransE 217 206 80.9 93.4 219 69 51.6 79.7
ITransF - 205 - 94.2 - 65 - 81.0
HolE - - - 94.9 - - - 73.9
ComplEx - - - 94.7 - - - 84.0
ANALOGY - - - 94.7 - - - 85.4
ProjE 277 260 79.4 94.9 124 34 54.7 88.4
RTransE - - - - - 50 - 76.2
PTransE (ADD, 2-step) 235 221 81.3 92.7 200 54 51.8 83.4
PTransE (MUL, 2-step) 243 230 79.5 90.9 216 67 47.4 77.7
PTransE (ADD, 3-step) 238 219 81.1 94.2 207 58 51.4 84.6
PaSKoGE - - 81.3 95.0 - - 53.1 88.0
RPE (ACOM) - - - - 171 41 52.0 85.5
RPE (MCOM) - - - - 183 43 52.2 81.7
RotatE - 309 - 95.9 - 40 - 88.4
OPTransE 211 199 83.2 95.7 136 33 58.0 89.9
to enumerate longer paths. Therefore, considering
the computational efﬁciency, we limit the maxi-
mum length of relation paths as 2 steps.
In our experiments, we utilize the grid search to
choose the best parameters for the two datasets, re-
spectively. The best conﬁgurations for OPTransE
are as follows: the dimension of entity and rela-
tion vectors d= 50 , the learning rate α= 0.0001 ,
the marginγ= 5.0,γ1= 5.0,γ2= 5.5, the bal-
ance factor λ= 0.01on WN18; and d= 100 ,
α= 0.0005 ,γ= 4.0,γ1= 4.5,γ2= 5.0,
λ= 0.01on FB15K. In addition, L1 norm is
employed for scoring and we run SGD for 2000
epochs in the training procedure.
4.3 Evaluation Metrics and Baselines
The same as in previous work (Bordes et al., 2013;
Nguyen et al., 2016), we evaluate the proposed
model OPTransE on the link prediction task. This
task aims to predict the missing entity in a triple
(h,r,t ), i.e., predicting hwhenrandtare given,
or predicting tgivenhandr. When testing a fact
(h,r,t ), we replace either head or tail entity with
all entities in the dataset and calculate scores ofgenerated triples according to Equation (12). And
then we sort the entities with their scores in as-
cending order to locate the rank of the target en-
tity.
For speciﬁc evaluation metrics, we employ the
widely used mean rank (MR) and Hits@10 in
the experiments. Mean rank indicates the aver-
age rank of correct entities and Hits@10 means
the proportion of correct entities ranked in top 10.
Higher Hits@10 or lower value of mean rank im-
plies the better performance of the model on the
link prediction task. Moreover, it is noted that the
generated triple for test may exist in the dataset as
a fact, thus such triples will affect the ﬁnal rank of
the target entity to some extent. Hence, we could
ﬁlter out these generated triples which are facts in
the dataset before ranking. If we have performed
ﬁltering, the result will be denoted as ”Filtered”,
otherwise it will be denoted as ”Raw”.
Moreover, Bordes et al. (2013) deﬁned four
categories of relations in KGs by mapping their
properties such as 1-to-1, 1-to-N, N-to-1 and N-
to-N. Thus, experimental results of distinguish-
ing the four different relation types have also been
Table 3: Filtered evaluation results on FB15K by mapping properties of relations(%)
Tasks Predicting Head Entities (Hits@10) Predicting Tail Entities (Hits@10)
Relation Category 1-to-1 1-to-N N-to-1 N-to-N 1-to-1 1-to-N N-to-1 N-to-N
SE 35.6 62.6 17.2 37.5 34.9 14.6 68.3 41.3
SME (linear) 35.1 53.7 19.0 40.3 32.7 14.9 61.6 43.3
SME (bilinear) 30.9 69.6 19.9 38.6 28.2 13.1 76.0 41.8
TransE 74.6 86.6 43.7 70.6 71.5 49.0 85.0 72.9
TransH 66.8 87.6 28.7 64.5 65.5 39.8 83.3 67.2
TransR 78.8 89.2 34.1 69.2 79.2 37.4 90.4 72.1
TranSparse 86.8 95.5 44.3 80.9 86.6 56.6 94.4 83.3
STransE 82.8 94.2 50.4 80.1 82.4 56.9 93.4 83.1
PTransE(ADD, 2-step) 91.0 92.8 60.9 83.8 91.2 74.0 88.9 86.4
PTransE(MUL, 2-step) 89.0 86.8 57.6 79.8 87.8 71.4 72.2 80.4
PTransE(ADD, 3-step) 90.1 92.0 58.7 86.1 90.7 70.7 87.5 88.7
PaSKoGE 89.7 94.8 62.3 86.7 89.3 72.9 93.4 88.9
RPE (ACOM) 92.5 96.6 63.7 87.9 92.5 79.1 95.1 90.8
RPE (MCOM) 91.2 95.8 55.4 87.2 91.2 66.3 94.2 89.9
RotatE 92.2 96.7 60.2 89.3 92.3 71.3 96.1 92.2
OPTransE 93.1 97.4 69.0 89.8 92.8 87.4 96.7 92.3
recorded for comparison.
In the link prediction task, several competitive
KG completion methods are utilized as baselines,
including SE (Bordes et al., 2011), SME (Bor-
des et al., 2014), TransE (Bordes et al., 2013),
TransH (Wang et al., 2014), TransR (Lin et al.,
2015b), TranSparse (Ji et al., 2016), STransE
(Nguyen et al., 2016), ITransF (Xie et al., 2017),
HolE (Nickel et al., 2016), ComplEx (Trouillon
et al., 2016), ANALOGY (Liu et al., 2017), ProjE
(Shi and Weninger, 2017), RTransE (Garc ´ıa-
Dur´an et al., 2015), PTransE (Lin et al., 2015a),
PaSKoGE (Jia et al., 2018), RPE (Lin et al., 2018)
and RotatE (Sun et al., 2019). Among them,
RTransE, PTransE, PaSKoGE and RPE exploit the
information of paths between entity pairs.
4.4 Results
Table 2 shows the performances of different meth-
ods on the link prediction task according to vari-
ous metrics. Numbers in bold mean the best re-
sults among all methods and the underlined ones
mean the second best. The evaluation results of
baselines are from their original work, and ”-” in
the table means there is no reported results in prior
work. Note that we implement ProjE and PTransE
on WN18 using the public codes.
From Table 2 we could observe that: (1)
PTransE performs better than its basic model
TransE, and RPE outperforms its original method
TransR. This indicates that additional informa-tion from relation paths between entity pairs is
helpful for link prediction. Note that OPTransE
outperforms baselines which do not take relation
paths into consideration in most cases. These re-
sults demonstrate the effectiveness of OPTransE
to take advantage of the path features in the KG.
(2) OPTransE performs better than previous path-
based models like RTransE, PTransE, PaSKoGE
and RPE on all metrics. This implies that the order
of relations in paths is of great importance for rea-
soning, and learning representations of ordered re-
lation paths can signiﬁcantly improve the accuracy
of link prediction. Moreover, the proposed pool-
ing strategy which aims to extract nonlinear fea-
tures from different relation paths also contributes
to the improvements of performance.
Speciﬁc evaluation results on FB15K by map-
ping properties of relations (1-to-1, 1-to-N, N-
to-1, and N-to-N) are shown in Table 3. Sev-
eral methods which have reported these results are
listed as baselines. OPTransE achieves the high-
est scores in all sub-tasks. We note that it is more
difﬁcult to predict head entities of N-to-1 relations
and tail entities of 1-to-N relations, since the pre-
diction accuracy on these two sub-tasks is gen-
erally lower than those of other sub-tasks. Sur-
prisingly, OPTransE has achieved signiﬁcant im-
provements on these two sub-tasks. Especially
when predicting tail entities of 1-to-N relations,
OPTransE promotes Hits@10 to 87.4% which is
8.3% higher than the best performance among
baselines. Meanwhile, since the average predic-
tion accuracy for N-to-N relations of OPTransE
on the two datasets has reached 91.1%, we can
also infer that our model has strong ability to deal
with N-to-N relations. OPTransE projects the head
and tail entities of a triple into different relation-
speciﬁc spaces, thus, it is able to better discrimi-
nate the relevant entities. Furthermore, these re-
sults also conﬁrm that ordered relation paths be-
tween entity pairs which are exploited by OP-
TransE contain useful information and can help
to perform more accurate inference when facing
complex relations.
5 Conclusion and Future Work
In this paper, we propose a novel KG completion
model named OPTransE, which aims to address
the issue of relation orders in paths. In our model,
we project the head entity and the tail entity of
each relation into different spaces to guarantee the
order of the path. In addition, a pooling method
is applied to extract complex and nonlinear fea-
tures from numerous relation paths. Finally, we
evaluate our proposed model on two benchmark
datasets and experimental results demonstrate the
effectiveness of OPTransE.
In the future, we will explore the following re-
search directions: (1) we will study the appli-
cations of the proposed models in various do-
mains, like personalized recommendation (Liu
et al., 2018); (2) we will explore other techniques
to fuse the ordered relation information from dif-
ferent paths (Liu et al., 2019).
Acknowledgments
This work was partially sponsored by Na-
tional Key R&D Program of China (grant no.
2017YFB1002000).
References
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data , SIGMOD’08, pages 1247–1250. ACM.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2014. A semantic matching energy
function for learning with multi-relational data. Ma-
chine Learning , 94(2):233–259.Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in neural information
processing systems , NIPS’13, pages 2787–2795.
Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured embed-
dings of knowledge bases. In Twenty-Fifth AAAI
Conference on Artiﬁcial Intelligence , AAAI’11,
pages 301–306.
Li Dong, Furu Wei, Ming Zhou, and Ke Xu.
2015. Question answering over freebase with multi-
column convolutional neural networks. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers) , volume 1 of ACL-
IJCNLP’15 , pages 260–269.
Alberto Garc ´ıa-Dur ´an, Antoine Bordes, and Nicolas
Usunier. 2015. Composing relationships with trans-
lations. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP’15, pages 286–290.
Kelvin Guu, John Miller, and Percy Liang. 2015.
Traversing knowledge graphs in vector space. In
Proceedings of the 2015 Conference on Empir-
ical Methods in Natural Language Processing ,
EMNLP’15, pages 318–327.
Guoliang Ji, Kang Liu, Shizhu He, and Jun Zhao.
2016. Knowledge graph completion with adap-
tive sparse transfer matrix. In Proceedings of the
Thirtieth AAAI Conference on Artiﬁcial Intelligence ,
AAAI’16, pages 985–991.
Yantao Jia, Yuanzhuo Wang, Xiaolong Jin, and Xueqi
Cheng. 2018. Path-speciﬁc knowledge graph em-
bedding. Knowledge-Based Systems , 151:37–44.
Jens Lehmann, Robert Isele, Max Jakob, Anja
Jentzsch, Dimitris Kontokostas, Pablo N Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick
Van Kleef, S ¨oren Auer, et al. 2015. Dbpedia–a
large-scale, multilingual knowledge base extracted
from wikipedia. Semantic Web , 6(2):167–195.
Xixun Lin, Yanchun Liang, Fausto Giunchiglia, Xi-
aoyue Feng, and Renchu Guan. 2018. Relation path
embedding in knowledge graphs. Neural Comput-
ing and Applications , pages 1–11.
Yankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun,
Siwei Rao, and Song Liu. 2015a. Modeling rela-
tion paths for representation learning of knowledge
bases. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP’15, pages 705–714.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015b. Learning entity and relation em-
beddings for knowledge graph completion. In Pro-
ceedings of the Twenty-Ninth AAAI Conference on
Artiﬁcial Intelligence , AAAI’15, pages 2181–2187.
Hanxiao Liu, Yuexin Wu, and Yiming Yang. 2017.
Analogical inference for multi-relational embed-
dings. In Proceedings of the 34th International Con-
ference on Machine Learning-Volume 70 , ICML’17,
pages 2168–2178. JMLR. org.
Hongzhi Liu, Yingpeng Du, and Zhonghai Wu. 2019.
Aem: Attentional ensemble model for personal-
ized classiﬁer weight learning. Pattern Recognition ,
96:106976.
Hongzhi Liu, Zhonghai Wu, and Xing Zhang. 2018.
Cplr: Collaborative pairwise learning to rank for
personalized recommendation. Knowledge-Based
Systems , 148:31–40.
George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM , 38(11):39–
41.
Dat Quoc Nguyen, Kairit Sirts, Lizhen Qu, and Mark
Johnson. 2016. Stranse: a novel embedding model
of entities and relationships in knowledge bases. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
HLT-NAACL’16, pages 460–466.
Maximilian Nickel, Lorenzo Rosasco, and Tomaso
Poggio. 2016. Holographic embeddings of knowl-
edge graphs. In Proceedings of the Thirtieth AAAI
Conference on Artiﬁcial Intelligence , AAAI’16,
pages 1955–1961.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies , HLT-
NAACL’13, pages 74–84.
Baoxu Shi and Tim Weninger. 2017. Proje: embed-
ding projection for knowledge graph completion. In
Proceedings of the Thirty-First AAAI Conference on
Artiﬁcial Intelligence , AAAI’17, pages 1236–1242.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international con-
ference on World Wide Web , WWW’07, pages 697–
706. ACM.
Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian
Tang. 2019. Rotate: Knowledge graph embedding
by relational rotation in complex space. ICLR’19.
Kristina Toutanova, Victoria Lin, Wen-tau Yih, Hoi-
fung Poon, and Chris Quirk. 2016. Compositional
learning of embeddings for relation paths in knowl-
edge base and text. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , volume 1 of
ACL’16 , pages 1434–1444.Th´eo Trouillon, Johannes Welbl, Sebastian Riedel, ´Eric
Gaussier, and Guillaume Bouchard. 2016. Complex
embeddings for simple link prediction. In Interna-
tional Conference on Machine Learning , ICML’16,
pages 2071–2080.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph embedding by trans-
lating on hyperplanes. In Proceedings of the Twenty-
Eighth AAAI Conference on Artiﬁcial Intelligence ,
AAAI’14, pages 1112–1119.
Qizhe Xie, Xuezhe Ma, Zihang Dai, and Eduard Hovy.
2017. An interpretable knowledge transfer model
for knowledge base completion. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers) ,
volume 1 of ACL’17 , pages 950–962."
1803.08006,D:\Database\arxiv\papers\1803.08006.pdf,How do the authors address the issue of temporal inconsistency in object localization when using referring expression grounding models designed for images on video data?,"The authors propose a temporal consistency technique that re-ranks object proposals based on their overlap with proposals in neighboring frames, their original objectness score, and their matching score from the grounding model, resulting in more temporally coherent predictions.","4 A. Khoreva et al.
on seven classes of actors (adult, baby, etc.) and mostly action-oriented descriptions. In
contrast, we consider arbitrary objects and unconstrained referring expressions.
2.2 Video Object Segmentation
Video object segmentation has witnessed considerable progress [37,48,47,21,3,50]. In
the following, we group the related work into unsupervised and semi-supervised.
Unsupervised methods. Unsupervised methods assume no human input on the video
during test time. They aim to group pixels consistent in both appearance and mo-
tion and extract the most salient spatio-temporal object tube. Several techniques ex-
ploit object proposals [54,21], saliency [9] and optical ﬂow [37]. Convnet-based ap-
proaches [6,17,47] cast video object segmentation as a foreground/background classi-
ﬁcation problem and feed to the network both appearance and motion cues. Because
these methods do not have any knowledge of the target object, they have difﬁculties in
videos with multiple moving and dominant objects and cluttered backgrounds.
Semi-supervised methods. Semi-supervised methods assume human input for the ﬁrst
frame, either by providing a pixel-accurate mask [48,3], clicks [32] or scribbles [41],
and then propagate the information to the successive frames. Existing approaches focus
on leveraging superpixels [53], constructing graphical models [48], utilizing object pro-
posals [40] or employing optical ﬂow and long-term trajectories [52]. Lately, convnets
have been considered for the task [3,39,50]. These methods usually build the archi-
tecture upon the semantic segmentation networks [29] and process each frame of the
video individually. [3] proposes to ﬁne-tune a pre-trained generic object segmentation
network on the ﬁrst frame mask of the test video to make it sensitive to the target ob-
ject. [39] employs a similar strategy, but also provides a temporal context by feeding
the previous frame mask to the network. Several methods extend the work of [3] by in-
corporating the semantic information [33] or by integrating online adaptation [50]. [15]
proposes to employ a recurrent network to exploit the long-term temporal information.
The above methods employ a pixel-level mask on the ﬁrst frame. However, for many
applications, particularly on small touchscreen devices, it can be prohibitive to provide
a pixel-accurate segmentation. Hence, there has been a growing interest to integrate
cheaper forms of supervision, such as point clicks [2,32] or scribbles [41], into convnet-
based techniques. In spirit with these approaches, we aim to reduce the annotation effort
on the ﬁrst frame by using language referring expressions to specify the object. Our
approach also builds upon convnets and exploits both linguistic and visual modalities.
3 Method
In this section we provide an overview of the proposed approach. Given a video V=
{f1,...,f N}with N frames and a textual query of the target object Q, our aim is to
obtain a pixel-level segmentation mask of the target object in every frame that it appears.
We leverage recent advances in grounding referring expressions in images [58,56]
and pixel-level segmentation in videos [39,17]. Our method consists of two main steps
(see Figure 2). Using as input the textual query Qprovided by the user, we ﬁrst generate
target object bounding box proposals for every frame of the video by exploiting refer-
ring expression grounding models, designed for images only. Applying these models
off-the-shelf results in temporally inconsistent and jittery box predictions (see Figure
Video Object Segmentation with Language Referring Expressions 5
Query: ""A girl 
in white dancing""
Referring Expression GroundingTemporal Consistency
Pixel-wise Segmentation
Figure 2: System overview. We ﬁrst localize the target object via grounding model us-
ing the given referring expression and enforce temporal consistency of bounding boxes
across frames. Next we apply a segmentation convnet to recover detailed object masks.
3). Therefore, to mitigate this issue and make them more applicable for video data,
we next employ temporal consistency, which enforces bounding boxes to be coherent
across frames. As a second step, using as guidance the obtained box predictions of the
target object on every frame of the video we apply a convnet-based pixel-wise segment-
ation model to recover detailed object masks in each frame.
3.1 Grounding objects in video by referring expressions
As discussed in §2, the task of natural language grounding is to automatically localize
a region described by a given language expression. It is typically formulated as meas-
uring the compatibility between a set of object proposals O={oi}M
i=1and a given
textual query Q. The grounding model provides as output a set of matching scores
S={si}M
i=1between a box proposal and a textual query Q. The box proposal with the
highest matching score is selected as the predicted region.
We employ two state-of-the-art referring expression grounding models – DBNet
[58] and MattNet [56], to localize the object in each frame. Mask R-CNN [12] bounding
box proposals are exploited as an initial set of proposals for both models, although
originally DBNet has been designed to utilize EdgeBox proposals [8]. However, using
the grounding models designed for images and picking the highest scoring proposal
for each video frame lead to temporally incoherent results. Even with simple textual
queries for adjacent frames that from a human perspective look very much alike, the
referring model often outputs inconsistent predictions (see Figure 3). This indicates the
inherent instability of the grounding models trained on the image domain. To resolve
this problem we propose to re-rank the object proposals by exploiting temporal structure
along with the original matching scores given by a grounding model.
Temporal consistency. The goal of the temporal smoothing step is to improve tem-
poral consistency and to reduce id-switches for target object predictions across frames.
Since objects tend to move smoothly through space and in time, there should be little
changes from frame to frame and the box proposals should have high overlap between
neighboring frames. By ﬁnding temporally coherent tracks of an object that are spread-
out in time, we can focus on the predictions that consistently appear throughout the
video and give less emphasis to objects that appear for only a short period of time.
The grounding model provides the likeliness of each box proposal to be the target
object by outputting a matching score si. Then each box proposal is re-ranked based
on its overlap with the proposals in other frames, the original objectness score given by
[12] and its matching score from the grounding model. Speciﬁcally, for each proposal
6 A. Khoreva et al.
we compute a new score: ˆsi=si∗(∑M
j=1,j ̸=irij∗dj∗sj/tij), whererijmeasures an
intersection-over-union ratio between box proposals iandj,tijdenotes the temporal
distance between two proposals ( tij=|fi−fj|) anddjis the original objectness
score. Then, in each frame we select the proposals with the highest new score. The
new scoring rewards temporally coherent predictions which likely belong to the target
object and form a spatio-temporal tube. This step allows to improve temporal coherence
boosting grounding and video segmentation performance (see Table 1 in §5 and Table
5 in §6) while being computational efﬁcient (takes only a fraction of second).
3.2 Pixel-level video object segmentation
We next show how to output pixel-level object masks, exploiting the bounding boxes
from grounding as a guidance for the segmentation network. The boxes are used as
the input to the network to guide the network towards the target object, providing
its rough location and extent. The task of the network is to obtain a pixel-level fore-
ground/background segmentation mask using appearance and motion cues.
Approach. We model pixel-level segmentation as a box reﬁnement task. The bounding
box is transformed into a binary image (255 for the interior of the box, 0 for the back-
ground) and concatenated with the RGB channels of the input image and optical ﬂow
magnitude, forming a 5-channel input for the network. Thus we ask the network to learn
to reﬁne the provided boxes into accurate masks. Fusing appearance and motion cues
allows to better exploit video data and handle better both static and moving objects.
We make one single pass over the video, applying the model per-frame. The network
does not keep a notion of the speciﬁc appearance of the object in contrast to [39,3],
where the model is ﬁne-tuned during the test time to learn the appearance of the target
object. Neither do we do an online adaptation as in [50], where the model is updated
on its previous predictions while processing video frames. This makes the system more
efﬁcient during the inference time, which is more suitable for real-world applications.
Similar to [39], we train the network on static images, employing the saliency seg-
mentation dataset [7] which contains a diverse set of objects. The bounding box is
obtained from the ground truth masks. To make the system robust during test time to
sloppy boxes from the grounding model, we augment the ground truth box by ran-
domly jittering its coordinates (uniformly, ±20% of the original box width and height).
We synthesize optical ﬂow from static images by applying afﬁne transformations for
both background and foreground object to simulate the camera and object motion in the
neighboring frames, as in [20]. This simple strategy allows us to train on diverse set of
static images, while exploiting motion information during test time. We train the net-
work on many triplets of RGB images, synthesized ﬂow magnitude images and loose
boxes in order for the model generalize well to different localization quality of boxes
given by the grounding model and different dynamics of the object.
During inference we use the state-of-the-art optical ﬂow estimation method Flow-
Net2.0 [16]. We compute the optical ﬂow magnitude by subtracting the median motion
for each frame and averaging the magnitude of the forward and backward ﬂow. The ob-
tained image is further scaled to [0; 255] to maintain the same range as RGB channels.
Network. As our network architecture we use ResNet-101 [13]. We adapt the network
to the segmentation task following the procedure of [29] and employing atrous convo-
lutions [5] with hybrid rates [51] within the last two blocks of ResNet to enlarge the
Video Object Segmentation with Language Referring Expressions 7
Query: ""A woman with a stroller.""
Query: ""A girl riding a horse.""
W/o temporal consistency With temporal consistency
Figure 3: Qualitative results of language grounding with and w/o temporal consistency
on DA VIS 17. The results are obtained using MattNet [56] trained on RefCOCO [57].
receptive ﬁeld as well as to alleviate the ""gridding"" issue. After the last block, we apply
spatial pyramid pooling [5], which aggregates features at multiple scales by applying at-
rous convolutions with different rates, and augment it with the image-level features [28]
to exploit better global context. The network is trained using a standard cross-entropy
loss (all pixels are equally weighted). The ﬁnal logits are upsampled to the ground truth
resolution to preserve ﬁner details for back-propagation.
For network initialization we use a model pre-trained on ImageNet [13]. The new
layers are initialized using the ""Xavier"" strategy [11]. The network is trained on MSRA
[7] for segmentation. To avoid the domain shift we ﬁne-tune the model on the training
sets of DA VIS 16[38] and DA VIS 17[42] respectively. We employ SGD with a polyno-
mial learning policy with initial learning rate of 0.001, crop size of 513×513, random
scale data augmentation (from 0.5to2.0) and left-right ﬂipping during training. The
network is trained for 20kiterations on MSRA and 20kiterations on the training set of
DA VIS 16/DA VIS 17. During inference we employ test time augmentation as in [5].
Other sources of supervision. Additionally we consider variants of the proposed
model using different sources of supervision. Our approach is ﬂexible and can take
advantage of the ﬁrst frame mask annotation as well as language. We describe how
language can be used on top of the mask supervision, improving the robustness of the
system against occlusions and dynamic backgrounds (see §6 for results).
Mask. Here we discuss a variant that uses only the ﬁrst frame mask supervision during
test time. The network is initialized with the bounding box obtained from the object
mask in the 1st frame and for successive frames uses the prediction from the preceding
frame warped with the optical ﬂow (as in [39]) to get the input box for the next frame.
Following [39,3] we ﬁne-tune the model for 1kiterations on an augmented set obtained
from the ﬁrst frame image and mask, to learn the speciﬁc properties of the object.
Mask + Language. We show that using language supervision is complementary to the
ﬁrst frame mask. Instead of relying on the preceding frame prediction as in the previous
paragraph, we use the bounding boxes obtained from the grounding model after the
temporal consistency step. We initialize with the ground truth box in the ﬁrst frame and
ﬁne-tune the network on the 1st frame.
4 Collecting referring expressions for video
Our task is to localize and provide a pixel-level mask of an object on all video frames
given a language referring expression obtained either by looking at the ﬁrst frame only
8 A. Khoreva et al.
ID 1: ""A man in a grey t-shirt and yellow trousers"" ID 1: ""A man in a grey shirt walking through the crossing""
ID 2: ""A woman in a black shirt"" ID 2: ""A woman walking through the crossing""
ID 3: ""A white truck on the road"" ID 3: ""A white truck moving from the left to right""
First frame annotation Full video annotation
Figure 4: Example of annotations provided for the 1st frame vs. the full video. Full
video annotations include descriptions of activities and overall are more complex.
or the full video. To validate our approach we employ two popular video object seg-
mentation datasets, DA VIS 16[38] and DA VIS 17[42]. These two datasets introduce
various challenges, containing videos with single or multiple salient objects, crowded
scenes, similar looking instances, occlusions, camera view changes, fast motion, etc.
DA VIS 16[38] consists of 30 training and 20 test videos of diverse object categories
with all frames annotated with pixel-level accuracy. Note that in this dataset only a
single object is annotated per video. For the multiple object video segmentation task
we consider DA VIS 17. Compared to DA VIS 16, this is a more challenging dataset, with
multiple objects annotated per video and more complex scenes with more distractors,
occlusions, smaller objects, and ﬁne structures. Overall, DA VIS 17consists of a training
set with 60videos, and a validation/test-dev/test-challenge set with 30sequences each.
As our goal is to segment objects in videos using language speciﬁcations, we aug-
ment all objects annotated with mask labels in DA VIS 16and DA VIS 17with non-ambigu-
ous referring expressions. We follow the work of [34] and ask the annotator to provide a
language description of the object, which has a mask annotation, by looking only at the
ﬁrst frame of the video. Then another annotator is given the ﬁrst frame and the corres-
ponding description, and asked to identify the referred object. If the annotator is unable
to correctly identify the object, the description is corrected to remove ambiguity and
to specify the object uniquely. We have collected two referring expressions per target
object annotated by non-computer vision experts (Annotator 1, 2).
However, by looking only at the 1st frame, the obtained referring expressions may
potentially be invalid for an entire video. (We actually quantiﬁed that only ∼15% of the
collected descriptions become invalid over time and it does not affect strongly segment-
ation results as temporal consistency step helps to disambiguate some of such cases, see
the supp. material for details.) Besides, in many applications, such as video editing or
video-based advertisement, the user has access to a full video. Providing a language
query which is valid for all frames might decrease the editing time and result in more
coherent predictions. Thus, on DA VIS 17we asked the workers to provide a description
of the object by looking at the full video. We have collected one expression of the full
video type per target object. Future work may choose to use either setting.
The average length for the ﬁrst frame/full video expressions is 5.5/6.3words. For
DA VIS 17ﬁrst frame annotations we notice that descriptions given by Annotator 1 are
longer than the ones by Annotator 2 ( 6.4vs.4.6words). We evaluate the effect of
description length on the grounding performance in §5. Besides, the expressions rel-
evant to a full video mention verbs more often than the ﬁrst frame descriptions ( 44%
vs.25%). This is intuitive, as referring to an object which changes its appearance and
Video Object Segmentation with Language Referring Expressions 9
ID 1: ""A girl with blonde hair dressed in blue"".
ID 1: ""A brown camel in the front"".
ID 1: ""A black scooter ridden by a man"". ID 2: ""A man in a suit riding a scooter"".
Figure 5: Video object segmentation qualitative results using only referring expressions
as supervision on DA VIS 16and DA VIS 17, val sets. Frames sampled along the video.
position over time may require mentioning its actions. Adjectives are present in over
50% for all annotations. Most of them refer to colors (over 70%), shapes and sizes ( 7%)
and spatial/ordering words ( 6%ﬁrst frame vs. 13% full video expressions). The full
video expressions also have a higher number of adverbs and prepositions, and overall
are more complex than the ones provided for the ﬁrst frame, see Figure 4 for examples.
Overall augmented DA VIS 16/17contains∼1.2k referring expressions for more than
400objects on 150videos with∼10k frames. We believe the collected data will be
of interest to segmentation as well as vision and language communities, providing an
opportunity to explore language as alternative input for video object segmentation.
5 Evaluation of natural language grounding in video
In this section we discuss the performance of natural language grounding models on
video data. We experiment with DBNet [58] and MattNet [57]. DBNet is trained on
Visual Genome [22] which contains images from MS COCO [26] and YFCC100M
[45], and spans thousands of object categories. MattNet is trained on referring expres-
sions for MS COCO images [26], speciﬁcally RefCOCO and RefCOCO+ [57]. Unlike
RefCOCO which has no restrictions on the expressions, RefCOCO+ contains no spa-
tial words and rather focuses on object appearance. Both aforementioned models rely
on external bounding box proposals, such as EdgeBox [8] or Mask R-CNN [12].
We carry out most of our evaluation on DA VIS 16and DA VIS 17with the refer-
ring expressions introduced in §4. To evaluate the localization quality we employ the
intersection-over-union overlap (IoU) of the top scored box proposal with the ground
truth bounding box, averaged across all queries.
5.1 DA VIS 16/DA VIS 17referring expression grounding
Table 1 reports performance of the grounding models on DA VIS 16and DA VIS 17refer-
ring expressions. In the following we summarize our key observations.
(1) We see the effect of replacing EdgeBox with Mask R-CNN object proposals
for DBNet model ( 54.1to64.9). Employing better proposals signiﬁcantly improves the
quality of this grounding method, thus we rely on Mask R-CNN proposals in all the
following experiments. (2) We note the stability of grounding performance across two
annotations (see ∆(A1,A2)), showing that the grounding methods are quite robust to
10 A. Khoreva et al.
MethodObject
proposalsTrain.
dataTemp.
cons.DA VIS 16 DA VIS 17
1st frame 1st frame Full video
mIoU∆(A1,A2) mIoU∆(A1,A2) mIoU
DBNetEdgeBoxVis.Gen.- 54.1 1.0 - - -
Mask R-CNN - 64.9 2.1 48.4 1.3 49.6
MattNet Mask R-CNNRefCOCO - 67.1 2.2 51.6 1.6 50.3
RefCOCO+ - 69.1 3.2 50.8 1.2 50.1
DBNet Mask R-CNN Vis.Gen. ! 68.8 0.6 49.6 1.6 50.2
MattNet Mask R-CNNRefCOCO ! 71.4 0.2 52.8 0.5 51.3
RefCOCO+ ! 72.5 0.3 52.3 0.0 51.2
Table 1: Comparison of the DBNet[58] and MattNet [56] models on DA VIS 16training
set and DA VIS 17val set.∆(A1,A2) denotes the difference between Annotator 1 and 2.
variations in language expressions. (3) The grounding models trained on images are
not stable across frames, even when small changes in appearance occur (e.g. see Fig-
ure 3). We see that our proposed temporal consistency technique beneﬁts both methods
(e.g. DBNet: 64.9vs.68.8on DA VIS 16, MattNet 51.6vs.52.8on DA VIS 17). (4) On
both datasets MattNet performs better than DBNet. The gap is particularly large on
DA VIS 16(72.5vs.68.8), as DA VIS 16contains videos of a single foreground moving
object, while DBNet is trained on a densely labeled Visual Genome dataset with many
foreground and background objects. (5) On DA VIS 16MattNet trained on RefCOCO+
outperforms MattNet trained on RefCOCO ( 72.5vs.71.4), while both perform similar
on DA VIS 17. As RefCOCO+ contains no spatial words, MattNet trained on this data-
set is more accurate in localizing queries mentioning object appearance. (6) Compared
to DA VIS 16, DA VIS 17is signiﬁcantly more challenging, as it contains cluttered scenes
with multiple moving objects (e.g. for MattNet 71.4vs.52.8). (7) When comparing res-
ults on expressions provided for the ﬁrst frame versus expressions provided for the full
video, we observe diverging trends. While DBNet is able to improve its performance
(48.4vs.49.6), MattNet performance decreases ( 52.8vs.51.3). We attribute this to the
fact that DBNet is trained on the more diverse Visual Genome descriptions.
Attribute-based analysis. Next we perform a more detailed analysis of the ground-
ing models on DA VIS 17. We split the textual queries/videos into subsets where a cer-
tain attribute is present and report the averaged results for the subsets. Table 2 presents
attribute-based grounding performance on ﬁrst-frame based expressions averaged across
annotators. To estimate the upper bound performance and the impact of imperfect
bounding box proposals we add an Oracle comparison, where performance is repor-
ted on the ground-truth object boxes. We summarize our ﬁndings in the following.
(1) As MattNet is trained on MS COCO images and both models rely on MS COCO-
based Mask R-CNN proposals, we compare performance for expressions which include
COCO versus non-COCO objects. Both models drop in performance on non-COCO ex-
pressions, showing the impact of the domain shift to DA VIS 17(e.g. for MattNet 59.6
vs.36.9). Even DBNet which is trained on a larger training corpus suffers from the
same effect ( 55.5vs.37.3). (2) We label the DA VIS 17expressions as “spatial” if they
include some of the spatial words (e.g. left, right). Such queries are signiﬁcantly harder
for all models (e.g. for MattNet 33.8vs.58.5). (3) Verbs are important as they allow
to disambiguate an object in a video based on its actions. Presence of verbs in expres-
Video Object Segmentation with Language Referring Expressions 11
MethodTrain.
dataObj.
prop.mIoU
CO. ˜CO. Sp. ˜Sp. Ve. ˜Ve.Expr. length Num. obj.
S M L 1 2-3 >3
DBNet Vis.Gen. Mask
R-CNN55.5 37.3 36.5 55.7 37.4 52.0 61.8 49.2 33.6 79.5 49.3 22.6
MattNet RefCOCO 59.6 36.9 33.8 58.5 55.8 51.7 63.9 50.2 49.1 86.1 51.2 16.1
DBNet Vis.Gen.Oracle79.3 59.0 47.7 81.7 70.3 77.6 84.8 69.9 67.9 10073.8 37.2
MattNet RefCOCO 73.2 46.6 42.2 72.5 74.7 62.9 79.0 61.1 59.0 100 64.5 23.2
Table 2: Grounding performance breakdown for different attributes on DA VIS 17, val set.
Results obtained after the temporal consistency, using average between two annotators
(1st frame based). Attributes: COCO/non-COCO, Spatial/non-Spatial, Verbs/no Verbs,
Expression length (Short, Medium, Long) and Number of objects.
sions is a challenging factor for DBNet trained on Visual Genome, while MattNet does
signiﬁcantly better ( 37.4vs.55.8). (4) Expression length is also an important factor.
We quantize our expressions into Short (<4 words), Medium (4–6 words) and Long (>6
words). All models demonstrate similar drop in performance as expression length in-
creases (e.g. for MattNet 63.9→50.2→49.1). (5) Videos with more objects are more
difﬁcult, as these objects also tend to be very similar, such as e.g. ﬁsh in a tank (e.g.
for MattNet 86.1→51.2→16.1). (6) From the Oracle performance on COCO versus
non-COCO expressions, we see that all models are able to signiﬁcantly improve their
performance even for non-COCO objects (e.g. for DBNet 37.3to59.0). DBNet beneﬁts
more than MattNet from Oracle boxes, showing its higher potential to generalize to a
new domain given better proposals.
6 Video object segmentation results
In this section we present single and multiple video object segmentation results using
natural language referring expressions on two datasets: DA VIS 16[38] and DA VIS 17
[42]. In addition, we experiment with fusing two complementary sources of informa-
tion, employing both the pixel-level mask and language supervision on the ﬁrst frame.
All results here are obtained using the bounding boxes given by the MattNet model [56]
trained on RefCOCO [57] after the temporal consistency step (see §3.1).
For evaluation we use the IoU measure (also called Jaccard index - J) between the
ground truth and the predicted segmentation, averaged across all video sequences and
all frames. For DA VIS 17we also employ the J&Fmeasure proposed in [42].
6.1 DA VIS 16single object segmentation
Table 3 compares our results to previous work on DA VIS 16[38]. As we employ MattNet
[56], which exploits Mask R-CNN [12] box proposals, we also would like to compare to
its segments. We report the oracle Mask R-CNN results, where on each frame the seg-
ment with the highest ground truth overlap was chosen. Even with the oracle assignment
of segments, [12] under-performs compared to our segmentation model ( 71.5vs.83.1).
This shows that for very detailed mask annotations (as in DA VIS 16/17) a more com-
plex segmentation module than the Mask R-CNN segmentation head is required (which
itself is a shallow FCN with reduced output resolution, resulting in coarse masks).
Our method, while only exploiting language, shows competitive performance, on
par with techniques which use a pixel-level mask on the ﬁrst frame ( 82.8vs.81.7for"
2110.00816,D:\Database\arxiv\papers\2110.00816.pdf,How does the coverage rate of a quantile region constructed using directional quantile regression (DQR) vary with the dimension of the response variable and the directional quantile level? What are the implications of this variation for practical applications?,"The coverage rate of a DQR quantile region decreases as the dimension of the response variable increases and as the directional quantile level decreases. This means that DQR regions often achieve a coverage level significantly lower than the nominal level, even for reasonable quantile levels. This poses a challenge for practical applications, as users may not be able to accurately estimate the true coverage level of their DQR regions without further calibration.","Feldman, Bates, and Romano
Method Quantile region
x= 1.5 x= 2.5
Naïve QR
NPDQR
VQR
Our method
Figure 2: Quantile region obtained by each of the methods: Naïve QR ,NPDQR,VQR, and our
method. See more details about the synthetic data in Appendix B.1.
4
Flexible Multiple-Output Quantile Regression
Theα-th quantile function of Yis deﬁned as:
qα(x) := inf{y∈R:F(y|X=x)≥α},
whereFis the CDF of Y|X=x. One application of conditional quantiles is to obtain a prediction
interval for a one-dimensional response, as presented next. Denote by αlo=α/2,αhi= 1−α/2the
lower and upper quantile levels, respectively. Given the lower and upper quantiles qαlo(x),qαhi(x),
the prediction interval for YgivenX=xis deﬁned as:
C(x) = [qαlo(x),qαhi(x)].
By construction, the interval satisﬁes the requirement in (1). While the true conditional quantiles are
unknown, they can be estimated empirically by solving an optimization problem, e.g., by minimizing
the pinball loss (Koenker and Bassett, 1978; Koenker and Hallock, 2001; Steinwart and Christmann,
2011). This process is known to yield estimations that are asymptotically consistent under some
regularity conditions (Meinshausen, 2006; Takeuchi et al., 2006; Steinwart and Christmann, 2011).
Even though the estimated quantiles are not perfectly accurate, they have been shown to be adaptive
to local variability (Hunter and Lange, 2000; Taylor, 2000; Koenker and Hallock, 2001; Meinshausen,
2006; Takeuchi et al., 2006; Steinwart and Christmann, 2011).
2.3 Naïve Multivariate Quantile Regression
The idea presented in the previous section can be extended to build a quantile region for a multivariate
response. The naïve approach regresses to the upper and lower quantile for each dimension separately.
The nominal coverage level for each dimension is set to be 1−β, whereβ=α/d. This process results
in a prediction interval Cjfor each feature in the response vector that attains the right coverage rate
in the population level:
P[Yn+1∈Cj(x)] = 1−β.
The prediction intervals are used to construct the quantile region in the following way:
R(x) =C1(x)×C2(x)×...×Cd(x).
Notice that the resulted quantile region is a rectangle, for any distribution of Y|X. Furthermore, in
the ideal inﬁnite-samples case, the produced quantile region satisﬁes the coverage requirement (1),
as proved in Appendix A.1. Even though this method converges to the desired coverage level with
inﬁnite data, the quantile regions it produces are not ﬂexible, and too conservative. This problem
is illustrated in Figure 2, where we see that the regions produced by this naïve approach do not
reﬂect the true distribution of the response, as opposed to the other methods: the true distribution
is v-shaped, whereas the estimated regions are rectangles. Moreover, further experiments (Section 5)
reveal that this method produces the largest quantile regions among all existing methods, indicating
poor statistical eﬃciency.
2.4 Directional Quantile Regression
The next approach, which we refer to as directional quantile regression (DQR) (Kong and Mizera, 2012;
Paindaveine and Šiman, 2011; Boček and Šiman, 2017), is not restricted to produce rectangle-shaped
quantile regions, in contrast to the naïve approach, and thus can improve the statistical eﬃciency.
However, DQRis also limited as it can only produce convexquantile regions, as it coincides with
Tukey depth (Tukey, 1975; Kong and Mizera, 2012). Observe that the naïve method from Section 2.3
estimates the boundaries of the quantile region in four directions u∈{(0,1),(1,0),(0,−1),(−1,0)}.
TheDQRmethod extends this procedure, and estimates the boundaries in all directions, as displayed
in Figure 3. Formally, DQRﬁrst projects Y∈Rdusing a direction u∈Sd−1, where Sd−1:={u∈
Rd:∥u∥2= 1}is the unit sphere of Rd. The quantile region boundaries are deﬁned as the following
5
Feldman, Bates, and Romano
hyperplanes. The order- αquantile of YgivenX=xin a direction u∈Sd−1is any element of the
collection of hyperplanes:
παu,x:={(x,y)∈Rp×Rd:uTy=fθ(x,u)},
with
θ= argmin
θ′1
nn∑
i=1ρα(uTYi,fθ′(Xi,u)),
whereθare the parameters of the regression model, fθ(x,u) :Rp+d→Ris the regression function
andραis the pinball loss, expressed as:
ρα(y,ˆy) ={
α(y−ˆy)y−ˆy>0,
(1−α)(ˆy−y)otherwise.
Paindaveine and Šiman (2011) deﬁned fθ(x,u)as a linear function, i.e., fθ(x,u) =θ(u)Tx, where
the coeﬃcients θ(u)∈Rpare a function of the direction u. A solution for each direction deﬁnes the
following half-space:
H+
u(x) ={y∈Rd:uTy≥fθ(x,u)}.
Figure 3 illustrates the half-spaces obtained from diﬀerent directions. As the ﬁgure implies, the
quantile region is deﬁned as the intersection of all half-spaces obtained from all directions:
R(x) =∩u∈Sd−1H+
u(x). (2)
As shown by Boček and Šiman (2017), the conditional quantile regions are closed, convex and nested,
i.e., for any x∈X,Rα1(x)⊆Rα2(x)forα1≥α2. The convexity of the constructed regions is also
illustrated in Figure 2. However, the quantile region achieves coverage lower than the nominal level in
the population level. See Section 4.1 for more details. We note that there are more recent methods to
compute the same quantile regions described in this section (Hallin et al., 2010, 2015; Charlier et al.,
2020). These methods require estimating only a ﬁnite set of half-spaces, but they are all limited to
construct convex regions. In fact, the method of Hallin et al. (2010) is the common way to compute
these directional regions, although in this work we focus on the version proposed by Paindaveine and
Šiman (2011), for simplicity.
Figure 3: Half-spaces obtained from DQRon unconditional data. The left and right panels contain 8
and 512 diﬀerent half-spaces, respectively. Figure credit: Hallin et al. (2013).
6
Flexible Multiple-Output Quantile Regression
2.5 Additional Related Work
The problem of estimating the quantile region for Y|X=xwas also tackled by Hallin et al. (2015);
Charlier et al. (2020). However, their proposed methods can only construct convex regions and
are based on kernel functions or on a quantization grid, which are infeasible for high-dimensional
regressors. A similar technique is the one by Liu et al. (2019), which is a fast algorithm to construct
half-space regions. A diﬀerent approach to multivariate quantiles, called vector quantile regression
(VQR) (Carlier et al., 2016, 2017, 2020), addresses the estimation of the conditional distribution
ofY|X=x, and can produce non-convex quantile regions. Nevertheless, it assumes that Y
depends linearly on X, and Figure 2 shows that this method fails on the synthetic data, in which this
assumption is not satisﬁed. This approach relates to the one proposed by Chernozhukov et al. (2017)
that is based on statistical depth. More recent approaches to construct distribution-free quantile
regions, based on geometric tools, are proposed by Hallin (2017); Hallin et al. (2021). However, these
methods can only handle data without covariates.
2.6 Our Contribution
We state three features of our method, where each addresses a diﬀerent limitation of the vanilla DQR
method.
Flexible quantile regions. TheDQRmethod can only construct convex regions, whereas the true
distribution of the response might not be convex. As illustrated in Figure 2, the quantile regions
produced by this method are too conservative and uninformative. In contrast, as indicated by this
ﬁgure, the regions produced by our algorithm are non-convex, reﬂecting the true distribution of the
response.
Feasible for high-dimensional responses. Due to the curse of dimensionality, the quantile
regression problem becomes more diﬃcult when increasing the dimension of the response. In addition,
the discrepancy between the nominal coverage level and the empirical coverage rate achieved by DQR
worsens as the dimension of the response increases. See Section 4.1 for more details. Moreover, the
time complexity of the methods proposed by Carlier et al. (2016, 2017, 2020) grows exponentially as
the dimension of the response increases. Our method overcomes these limitations by computing the
directional quantiles in space Z, whose dimension can be determined regardless of the dimension of
the response. As a result, our method is feasible for higher-dimensional response settings.
Guaranteed coverage rate. The quantile regions constructed by DQRandVQRare not guaranteed
to achieve the desired coverage rate. This problem is more severe with the DQR, whose coverage rate
is signiﬁcantly lower than the nominal level, even with inﬁnite data. See Section 4.1 for additional
details. To overcome this limitation, we develop a calibration scheme that guarantees the coverage
requirement (1). This process is generic and can also be applied to our proposed method, DQR,VQR,
and other methods. Our numerical experiments in Section 5 show that, after calibration, all methods
achieve the right marginal coverage.
3. Proposed Method
In this section, we introduce the proposed algorithm, but ﬁrst, extend DQRbeyond linear settings.
3.1 Non-parametric Directional Quantile Regression
Before describing our contribution, we pause to extend the formulation of conditional directional
quantiles as deﬁned by Kong and Mizera (2012); Paindaveine and Šiman (2011); Boček and Šiman
(2017) beyond linear models. This extension of DQRwill be used as a subroutine in our main
algorithm. To this end, we follow the original DQRfrom Section 2.4, however, use a non-parametric
7
Feldman, Bates, and Romano
function class forfθ, formulated as neural networks in this work. In such a case, the quantile region
forYconditional on X=xis given by
R(x) ={y∈Rd:uTy≥fθ(x,u),∀u∈Sd−1}.
The latter stands in contrast with the methods proposed by Paindaveine and Šiman (2011); Boček
and Šiman (2017) that allow R(x)in(2)to depend only linearly on x. We refer to this new method
as non-parametric DQR(NPDQR) throughout this work.
3.2 Our Method: Going Beyond Convex Quantile Regions
In this section, we present a general approach to construct quantile regions of an arbitrary shape,
overcoming the convexity restriction of NPDQR. Our method relies on the following observation.
When the distribution of Y|Xhas level sets of the density that are convex, NPDQR(which must
create convex regions) is still appropriate. This motivates us to transform an arbitrary response
into a space where it has level sets of the density that are convex. Then, we will apply NPDQRand
construct a convex quantile region in that space. Lastly, we will transform it back to the original
space of the response, using the inverse of the mapping. By applying a non-linear mapping, this
process will result in a quantile region that is not restricted to have a convex shape, having an
arbitrary structure.
We now describe this procedure in detail. We start by learning a mapping that transforms a
general distribution Y|X=xinto a latent distribution Zxwhose level sets are convex. In this
work, we focus on mapping Y|Xto ar-dimensional standard normal distribution, which is not only
spherical, but also has convex level sets. To learn such a mapping, we ﬁt a conditional variational
auto-encoder (CVAE) (Sohn et al., 2015) on the training set {(Xi,Yi)}i∈I1, and obtain the non-linear
transformation between space Yto spaceZ. For technical details regarding CVAE, see Appendix F.4.
For our purposes, an ideal CVAE (E(y;x),D(z;x))should satisfy the following:
Zx=E(Y;X=x)∼N(0,1)r,D(Zx;X=x) =Y.
SinceZxis spherically distributed, the conditional distribution Zx|Xn+1=xfor a new test point
Xn+1has a convex level sets. Figure 4 illustrates this process. The top panel visualizes the non-linear
mappingY|X→Zx∼N (0,1)3obtained by the CVAE model. Observe how the distribution
Zx|X=xhas approximately a spherical shape. Observe also that the inverse transformation is
fairly accurate, so it can map samples from space Zback to spaceY.
Since the distribution of Z|Xis approximately spherical, NPDQRcan estimate eﬀectively its
quantile region. We therefore ﬁt NPDQRin spaceZ. First, we map the response vectors of the
training set to space Z, and obtain the transformed training set {(Xi,E(Yi;Xi))}i∈I1. Next, we ﬁt a
NPDQRmodel on the transformed training samples, as described in Section 2.4. This process results
in a model that can construct a quantile region RZ(x)⊆Z, for any given feature vector x. Notice
that even though the constructed regions are convex, they are appropriate, since the distribution of
Z|Xis approximately spherical. That is, NPDQRis applied in a space for which it is well-suited.
The procedure of ﬁtting the NPDQRmodel is summarized in the bottom panel of Figure 4, displaying
(in red) the quantile region constructed in space Zduring training, for a speciﬁc feature vector x.
At test time, given a test point Xn+1, we (i) construct a quantile region RZ(Xn+1)⊆Zby
applying the ﬁtted NPDQRmodel; and (ii) transform the estimated region to the original space Y,
forming the desired quantile region:
RY(Xn+1) :=D(RZ(Xn+1);Xn+1)⊆Y. (3)
Observe that RY(Xn+1)is the quantile region of Xn+1inY. From a practical point of view, the
functionDcan only map a discrete set of points from RZ(Xn+1), and therefore the resulting set
RY(Xn+1)is a discretization of the quantile region. We address this important issue in Section 4.2
8
Flexible Multiple-Output Quantile Regression
CVAE
training
NPDQR
training
Figure 4: CVAE and NPDQRtraining schemes on the synthetic data. For further details regarding
the synthetic data, see Appendix B.1.
9
Feldman, Bates, and Romano
Algorithm 1: Spherically transformed DQR(ST-DQR)
Input:
Data (Xi,Yi)∈Rp×Rd,i∈I1.
Miscoverage level α∈(0,1).
Directional quantile regression algorithm, e.g., NPDQRfrom Section 3.1.
Conditional variational auto-encoder algorithm (E(y;x),D(z;x)); see Section F.4.
A test point Xn+1=x.
Training time:
Fit a CVAE model on the data {(Xi,Yi)}i∈I1. See (Sohn et al., 2015).
Transform the response values Yito the spaceZ:Zi=E(Yi;Xi),i∈I1.
Fit a directional quantile regression model on the training set in space Z{(Xi,Zi) :i∈I1}to
obtain a method to construct quantile regions in Z, denoted by RZ(x).
Test time:
Construct the quantile region in ZRZ(Xn+1=x).
Transform the quantile region to Y:RY(Xn+1=x) =D(RZ(Xn+1=x);Xn+1=x).
Output:
A quantile region RY(Xn+1=x)for the unseen input Xn+1=x.
and show how to construct a continuous region from the discretized RY(Xn+1). The test procedure
is illustrated in Figure 5, in which, we can see that while the quantile region in space Zhas a convex
shape, the transformed one (in space Y) has the desired non-convex structure. The whole procedure
is summarized in Algorithm 1, which we refer to as Spherically Transformed DQR(ST-DQR).
Figure 5: The test procedure on the synthetic data, given a new test point Xn+1=xnew.
We pause here to highlight several features of the proposed algorithm. First, we use a CVAE
model which is nonlinear and non-convex, so we can obtain arbitrary quantile regions, unlike previous
approaches. Second, since we apply NPDQRin a latent r-dimensional space, where ris a hyper-
parameter, our method can eﬀectively treat high-dimensional response variables by choosing r<d.
We demonstrate this in Section 5.1, in which we display the results obtained by diﬀerent methods on
synthetic data sets with higher-dimensional responses.
3.3 Theoretical Results
We explain a formal property satisﬁed by our proposed algorithm that supports the behavior we
observed in Figure 2. We would like our quantile region to reﬂect the true distribution of the response
variable. For example, in the synthetic data from Figure 2, the quantile region should only cover
blue points, i.e., areas where the response can be present. Formally, we ask that the quantile region
10
Flexible Multiple-Output Quantile Regression
will be contained in the support of Y|X=x. We now show that a quantile region constructed by
our method satisﬁes this property.
Theorem 1 SupposeY|X=xhas a continuous distribution for all x. Suppose (E(y;x),D(z;x))
is a CVAE model that satisﬁes:
∀x∈supp(X) :Zx=E(Y;X=x)∈Rr,D(Zx;X=x)d=Y|X=x,
whereEandDare continuous functions. Suppose RZ(x)is a quantile region in space Z. Deﬁne the
quantile region in space Yas:RY(x) =D(RZ(x);x).Then the quantile region RY(x)satisﬁes:
RY(x)⊆supp(Y|X=x). (4)
All proofs are given in Appendix A. Even though the requirement in (4)is a modest bar, we see
in Figure 2 that, unlike the proposed method, the other methods do not satisfy this property. In
conclusion, we have shown that a quantile region in Yconstructed by our method does not contain
spurious portions, since it does not cover areas outside the support of Y|X=x. As a complementary
result, we also give a lower bound for the coverage rate of a quantile region constructed by our
method in Appendix A.4. To achieve the exact nominal coverage level, we propose a calibration
procedure, described in Section 4.
4. Calibration
In this section, we introduce a procedure to calibrate quantile regions to exactly achieve 1−α
coverage. The procedure is modular and can be used with any quantile region algorithm, such as
DQR,VQR, or our proposed method from the previous section. At a technical level, the calibration
scheme instantiates split conformal prediction (Vovk et al., 2005) in a way that is compatible with
multi-dimensional quantile regions.
4.1DQRRequires Estimating Extreme Quantiles
To motivate our calibration scheme, we ﬁrst point out that the parameter αinDQRdoes not
correspond to the coverage level. This phenomenon is known in the literature (Zuo and Serﬂing, 2000;
Tukey, 1975), and in this section, we provide an intuitive explanation and an example illustrating
this problem. As a result, the DQRregions have a coverage level unknown to the user without further
calibration, such as the one described in this section. This problem arises from the deﬁnition of
theDQRquantile region as an intersection of inﬁnite half-spaces, where each covers 1−αof the
distribution; see Figure 3. As a result, their intersection, i.e., the quantile region output by DQR, covers
strictly less than 1−αof the distribution. To make this precise, we now analyze the coverage rate of
a quantile region constructed with the DQRmethod, in the setting in which Y|X=x∼N(0,1)r
(see Appendix F.5 for the full calculation). The left panel of Figure 6 displays the coverage rate of a
quantile region constructed by DQRas a function of the dimension r, when the nominal coverage
level is set to 90%. The right panel in that ﬁgure presents the coverage of a DQRquantile region as
a function of the directional quantile level 1−αforr= 3. We see that the achieved coverage is
far below the nominal rate. For example, to construct regions that truly have coverage 90%in a
three-dimensional response setting, one would need the 99.38%directional quantiles. Unfortunately,
such extreme quantiles are impractical to estimate, as shown by Diebolt et al. (2000). In summary,
theDQRregions do not achieve the nominal coverage rate, even for reasonable quantile levels. The
coverage level is the scaling of interest to the user, so we turn to formulate a calibration scheme that
guarantees the desired coverage level.
11"
2306.05587,D:\Database\arxiv\papers\2306.05587.pdf,"Given the prevalence of influenza viruses and their potential for causing pandemics, what are the limitations of current methods for predicting viral host and subtype, and how might these limitations be addressed?","Current methods for predicting viral host and subtype rely heavily on supervised learning algorithms and require correctly labeled data, which can lead to poor predictive ability for labels with insufficient data.  Further research is needed to address these limitations, including exploring alternative approaches to handle insufficient data and investigating the prediction of cross-species transmissibility.","Springer Nature 2021 L ATEX template
4 MC-NN for IAV Prediction
Table 1 Summary statistics of data sets.
Data Set (alias) # Total Pairs # Seqs from IRD # Seqs from GISAID
<2020 (pre-20 ) 33,159 41,940 24,378
2020 - 2022 (post-20 ) 4,488 3,232 5,744
Incomplete (incomplete ) 8,525 11,111 5,939
A/American Pelican/Kansas/W22-200/2022 (isolated ID: EPI ISL14937098)
was inaccurately labelled as ‘host’. Subsequently, the final outcome comprised
46,172 unique pairs of complete and partial HA and NA sequences
The criterion for defining the completeness of A sequence was consid-
ered complete if its length was equivalent to that of the actual genomic
sequence [32] or the complete coding region defined by The National Center for
Biotechnology Information (NCBI) [31]. The completeness annotation cannot
be explicitly obtained from the metadata of the strain. Therefore, incomplete
sequences were obtained by filtering the complete sequences from the full
influenza database, which comprises both complete and incomplete sequences
(all sequences =complete sequences ∪incomplete sequences ).
The pre-trained model was trained using a training data set comprising
sequences of strains isolated before 2020. Conversely, the sequences of strains
isolated from 2020 to 2022 were used solely to evaluate the performance of the
models during testing; the testing data set also included incomplete sequences.
The characteristics of the data sets used in this study are presented in Table 1.
3.1.2 Label Reassignment
While the GISAID and IRD databases recorded >300 hosts, only 30% were
consistent across both databases. This issue could be attributed to the blended
use of animals’ common and scientific names. We regrouped the viral hosts
into 25 categories based primarily on the biological family classification of the
animals; the distribution of reassigned hosts is presented in Fig. 1. We also
moved a few subtypes in the data set into other subtypes (i.e. H15, H17, H18,
N10, and N11), as shown in Fig. 2.
0100101102103104No. of Data Points
Birds of Prey
Canidae
Columbidae
Ducks
Environment
Geese
Gulls
Horses
Human
Lab
* Avian
* Game Birds
* Mammals
* Passerines
* Seabirds
* Wading Birds
* Water Birds
Poultry
Sandpipers
Sea Mammals
Swans
Swine
T eals
Turkeys
Wild Birds2636
14100
41631
0194825
2681227
689
20126394
88119309
3691111
43013825
21674420
39
311111
9763347
2901202
55
81246
11834
625100
629129
33143
710388
3384
9
3380
45297392
1348
710862
32139155
476959
24
15Integrated Dataset (< 2020) Integrated Dataset (2020  2022)
 IRD Dataset (Incomplete Sequences)
Fig. 1 Data distribution (hosts)
Springer Nature 2021 L ATEX template
MC-NN for IAV Prediction 5
0100101102103104No. of Data Points
H1
H2
H3
H4
H5
H6
H7
H8
H9
H10
H11
H12
H13
H14
H16
mixed HA
N1
N2
N3
N4
N5
N6
N7
N8
N9
mixed NA12234
17572876
352
5439991
18132528
807
312403268
59110261219
502751200
43442
105
6202241
132670
438
25122450
1656171
734139
736
15
0190
415439
114111646
1610345315420
24053550
820
51133189
1649293
19631585
100335447
151481374
246425934
25227451
1142Integrated Dataset (< 2020) Integrated Dataset (2020  2022)
 IRD Dataset (Incomplete Sequences)
Fig. 2 Data distribution (subtypes)
3.2 Protein Sequence Representation
Neural networks are mathematical operators that operate on inputs and gener-
ate numerical outputs. However, the raw input sequences must be represented
as numerical vectors before the neural network can process them. One popular
method of vectorising sequences is one-hot encoding. In natural language pro-
cessing (NLP), the length of the one-hot vector for each word is determined
by the size of the vocabulary, which comprises all unique words or tokens in
the data. When representing amino acids, the length of the one-hot vector for
each amino acid depends on the number of unique amino acids. This results in
a sparse matrix for large vocabularies, which is computationally inefficient. An
alternative and more powerful approach are to represent each word as a dense
vector through word embedding. Word embedding learns the representation of
a word by considering its context, allowing similar words to have similar rep-
resentations. It has been used successfully in the extraction of features from
biological sequences [33].
The word embedding process can be incorporated into a deep learning
model without relying on manually-crafted feature extraction techniques. A
protein’s amino acid sequence is usually written as a string of letters but can
also be represented as a set of tripeptides, also known as 3-grams. In NLP,
N-grams refer to Nconsecutive words in a text, and similarly, N-grams of
a protein sequence refer to Nconsecutive amino acids. For example, the 3-
grams of the sequence ”AAADADTICIG” would be ‘AAA’, ‘AAD’, ‘ADA’,
‘DAD’, ‘ADT’, ‘DTI’, ‘TIC’, ‘ICI’, and ‘CIG’. N was set to 3 based on previous
research findings [34, 35].
4 Neural Network Architectures
In this study, we propose a multi-channel neural network (MC-NN) archi-
tecture that incorporates two inputs, namely HA trigrams and NA trigrams,
and produces three outputs, specifically host, HA subtypes, and NA subtypes.
The neural network models utilized in this research encompass bidirectional
Springer Nature 2021 L ATEX template
6 MC-NN for IAV Prediction
gated recurrent unit (BiGRU), convolutional neural network (CNN), and
transformer.
4.1 Bidirectional Gated Recurrent Unit
The Bidirectional Gated Recurrent Unit (BiGRU) is a model designed to han-
dle sequential data by considering both past and future information at each
time step. This model is composed of two separate Gated Recurrent Unit
(GRU) layers, one for processing the input sequence in the forward direction
and the other for processing the input sequence in the backward direction. The
outputs of these two layers are then concatenated and utilised for prediction
purposes.
GRUs, similar to Long Short-Term Memory (LSTM) units, possess a reset
gate and an update gate [36]. The reset gate determines the amount of previous
information that needs to be forgotten, while the update gate decides the
proportion of information to discard and the proportion of new information
to incorporate. Due to fewer tensor operations, GRUs are faster in terms of
training speed when compared to LSTMs.
The utilisation of a BiGRU provides the advantage of considering both the
past and future context at each time step, thereby leading to more informed
predictions. This is particularly useful in sequential data processing where
context plays a crucial role in prediction accuracy.
4.2 Transformer
The Transformer neural network architecture has had a significant impact in
the field of NLP [37]. It was initially designed to facilitate machine transla-
tion, however, the scope of its application can be broadened to encompass
other areas such as addressing protein folding dilemmas [38]. The Transformer
architecture serves as the cornerstone for the advancement of contemporary
natural language processing models, including BERT [39], T5 [40], and GPT-
3 [41]. One of the most significant benefits that a Transformer possesses over
conventional Recurrent Neural Networks (RNNs) is its capability to process
data in a parallel manner. This attribute allows for the utilisation of Graphics
Processing Units (GPUs) to optimise the speed of processing and effectively
handle extensive text sequences.
The Transformer neural network presents a breakthrough in the field
of deep learning through its incorporation of positional encoding and self-
attention mechanism. The positional encoding feature serves as a means of
preserving the word order information in the data, thereby enabling the neural
network to learn and understand the significance of the order. The atten-
tion mechanism, on the other hand, allows the model to effectively translate
words from the source text to the target text by determining their relative
importance. The self-attention mechanism, as implied by its name, allows
the neural network to focus on its own internal operations and processes.
Through this mechanism, the neural network can comprehend the contextual
Springer Nature 2021 L ATEX template
MC-NN for IAV Prediction 7
meaning of words by analysing their relationships and interactions with sur-
rounding words. Furthermore, the self-attention mechanism enables the neural
network to not only differentiate between words but also reduce computational
requirements, thus improving its efficiency.
4.3 Convolutional Neural Network
A Convolutional Neural Network (CNN) was designed to work with image and
video data. It is an artificial neural network that uses convolutional layers to
extract features from raw data. These convolutional layers analyse the spatial
relationship between pixels and learn to recognise patterns in the data. The
concept behind Convolutional Neural Networks (CNNs) is based on the visual
processing mechanism of the human brain, where neurons are selectively acti-
vated in response to various features present in an image, such as edges. In
CNNs, two primary types of layers are utilised, namely convolution layers and
pooling layers. Convolution layers are the core of the CNN architecture, per-
forming convolution operations on the input image and filters. On the other
hand, pooling layers perform down-sampling on the image in order to minimise
the number of learnable parameters. This study implements one-dimensional
convolution layers to process sequence data.
5 Implementation and Evaluation Methods
All of the models in this study were built using Keras and trained on pre-20
data sets. They were then tested on both post-20 and incomplete data sets.
The architecture of the multi-channel neural network used in this study is
illustrated in Figure 3. The Transformer architecture used here is the encoder
presented in [37].
In some cases, there is confusion regarding the role of validation and test
sets, leading to the tuning of model hyperparameters using the testing set
instead of a separate validation set. This increases the risk of data leakage
and reduces the credibility of the results. To avoid this issue, nested cross-
validation (CV) is used instead of classic K-fold CV. In nested CV, an outer
CV is used to estimate the generalisation error of the model and an inner CV
is used for model selection and hyperparameter tuning. The outer CV splits
the data into a training outer set and a testing set, while the inner CV splits the
training outer set into a training inner set and a validation set. The model is
trained only on the training inner set, its hyperparameters are tuned based on
its performance on the validation set, and its overall performance is evaluated
on the testing set. In this study, the outer fold kouter was set to 5 and the inner
foldkinner was set to 4. The hyperparameters settings for the neural network
architectures used in this study are presented in Table 2.,
The present study utilises data sets that exhibit a high degree of imbal-
ance, and as such, the application of conventional evaluation metrics such
as accuracy and receiver operating characteristic (ROC) curves can lead to
misleading results, as demonstrated in prior research [42, 43]. Precision-recall
Springer Nature 2021 L ATEX template
8 MC-NN for IAV Prediction
InputEmbeddingHA sequencesInputEmbeddingNA sequencesConcatenate
Dense Block
Positional Encoding
FCSoftmaxFCSoftmaxFCSoftmaxPositional EncodingConv-256Max-PoolConv-128Max-PoolConv-64Multi-HeadAttentionAdd & NormFeedForwardAdd & NormBiGRU-128BiGRU-64
HA SubtypesNA SubtypesHosts
Fig. 3 The multi-channel neural network architecture: positional encoding is only employed
along with Transformer.
Table 2 Hyperparameter Settings
Models Hyperparameters
CNNkernel size = 3, 4, 5
embedding size = 50, 100, 150, 200
learning rate = 0.01, 0.005, 0.001, 0.0001
BiGRUembedding size = 50, 100, 150, 200
learning rate = 0.01, 0.005, 0.001, 0.0001
Transformerembedding size = 32, 64, 128
learning rate = 0.01, 0.005, 0.001, 0.0001
num heads = 1, 2, 3, 4, 5
curve (PRC), on the other hand, has been demonstrated to be more informa-
tive when addressing highly imbalanced data sets and has been widely adopted
in the research [44–47].
The utilisation of linear interpolation to calculate the area under the
precision-recall curve (AUPRC) has been shown to be inappropriate [43]. An
alternative approach that has been demonstrated to be effective in such cases
is the calculation of the average precision (AP) score[48]. Furthermore, this
study also employs conventional evaluation metrics F 1score, with the formulas
for these metrics provided below:
Precision =TP
TP+FP(1)
Recall =TP
TP+FN(2)
Springer Nature 2021 L ATEX template
MC-NN for IAV Prediction 9
F1= 2×Precision ×Recall
Precision +Recall(3)
AP=X
n(Recall n−Recall n−1Precision n) (4)
where TP, FP, TN, FN stand for true positive, false positive, true negative
and false negative. If positive data is predicted as negative, then it counts as
FN, and so on for TN, TP and FP.
The evaluation of the overall performance of the models was conducted
using the results obtained from the Basic Local Alignment Search Tool
(BLAST) as a baseline because BLAST is a commonly employed benchmark
in computational biology and bioinformatics.
6 Results
6.1 Overall Performance
The model’s performance on various data sets is shown in Figures 4 to 6.
The metrics used, such as average precision (AP), have been developed for
binary classification but can be adapted to multi-class classification using a
one-vs-all approach. This approach involves designating one class as positive
and all others as negative. AP and F 1score were used to compare the models
to a baseline model, the Basic Local Alignment Search Tool (BLAST), with
its default parameters. The BLAST results were obtained through five-fold
cross-validation and are indicated by the solid black line in the figures.
The models were trained solely on the pre-20 data set and tested on the
post-20 and incomplete data sets. The pre-20 and post-20 data sets only con-
tained complete sequences, while the incomplete data set included incomplete
sequences. All models outperformed the baseline model on the pre-20 data
but there was no significant difference in the performance of all models. The
results also showed that the host classification task was more challenging than
the subtype classification task with all models.
The MC-CNN model exhibited outstanding performance on the pre-20 set
across all classification tasks, achieving an average AP of 94.61% (94.22%,
94.99%), and an average F 1score of 93.20% (92.86%, 93.54%). The MC-CNN
model sustained its superior performance on the post-20 set, with an average
AP of 95.24% (95.12%, 95.36%), and an average F 1score of 94.38% (94.17%,
94.58%). The MC-Transformer performed best on the incomplete data set,
achieving an average AP of 91.63% (91.41%, 91.85%), and an average F 1score
of 89.29% (88.80%, 89.78%).
6.2 Performance on Single Sequence Input
The proposed MC-NN uses two inputs. However, it cannot be guaranteed that
the required HA and NA pairs will always be obtainable for every strain. We
conducted additional experiments on two data sets, one comprising 23,802 HA
protein sequences and the other containing 5,142 NA protein sequences. The
Springer Nature 2021 L ATEX template
10 MC-NN for IAV Prediction
BiGRU CNN Transformer0.50.60.70.80.91.0AP
BiGRU CNN Transformer0.50.60.70.80.91.0F1< 2020
2020 - 2022
Incomplete
Fig. 4 Comparison of Overall Performance Between Models (Hosts): the baseline results
with BLAST are framed by the black solid line.
BiGRU CNN Transformer0.600.650.700.750.800.850.900.951.00AP
BiGRU CNN Transformer0.600.650.700.750.800.850.900.951.00F1< 2020
2020 - 2022
Incomplete
Fig. 5 Comparison of Overall Performance Between Models (HA subtypes): the baseline
results with BLAST are framed by the black solid line.
BiGRU CNN Transformer0.700.750.800.850.900.951.00AP
BiGRU CNN Transformer0.700.750.800.850.900.951.00F1< 2020
2020 - 2022
Incomplete
Fig. 6 Comparison of Overall Performance Between Models (NA Subtypes): the baseline
results with BLAST are framed by the black solid line.
results of these experiments are presented in Table 3. The results indicated
reduced performance for all models when corresponding H/N sequence pairs
were missing. However, the MC-Transformer model outperformed the MC-
CNN and MC-BiGRU models on both data sets.
7 Conclusion and Discussion
The rapid mutation of influenza viruses leads to frequent seasonal outbreaks,
although they infrequently result in pandemics. However, these viruses can
exacerbate underlying medical conditions, elevating the risk of mortality. In
this study, we present a novel approach to predict the viral host at a lower
taxonomic level and subtype of the Influenza A virus (IAV) by utilising multi-
channel neural networks.
Springer Nature 2021 L ATEX template
MC-NN for IAV Prediction 11
Table 3 The overall performance of MC-NN on data sets with single HA or NA sequences.
AlgorithmsSingle HA Single NA
AP % (95% CI) F1 % (95% CI) AP % (95% CI) F1 % (95% CI)
CNN 76.38 (56.90, 95.86) 65.04 (55.21, 74.87) 79.60 (74.84, 84.36) 62.10 (54.66, 69.55)
BiGRU 80.52 (77.26, 83.79) 56.26 (46.20, 66.31) 76.79 (74.96, 78.62) 59.47 (53.37, 65.57)
Transformer 89.75 (87.96, 91.54) 76.61(69.79, 83.42) 83.56 (81.22, 85.91) 70.96 (66.14, 75.78)
Our approach differs from traditional methods, as it employs a neural net-
work architecture that can learn the embedding of protein trigrams instead
of manually encoding protein sequences into numerical vectors. The multi-
channel nature of our network eliminates the need for separate models for
similar tasks, as it can take multiple inputs and produce multiple outputs. We
evaluated the performance of our approach using various algorithms, includ-
ing CNN, BiGRU, and Transformer, and found that Transformer performed
better than the other algorithms. In addition to our previous experiments,
we carried out further evaluations to assess the performance of the models
in the absence of matching H/N sequence pairs. The results showed that the
MC-Transformer model consistently displayed superior performance.
This method could greatly benefit resource-poor regions where labora-
tory experiments are cost-prohibitive. However, our approach is limited by its
reliance on supervised learning algorithms and the need for correctly labelled
data, which may result in the poor predictive ability for labels with insuffi-
cient data. Further research is needed to address these limitations, including
the prediction of cross-species transmissibility and leveraging insufficient data.
Acknowledgments. The work is supported by the University of Liverpool.
Declarations
Conflict of interest The authors have no conflicts of interest to declare.
All co-authors have seen and agreed with the contents of the manuscript. We
certify that the submission is original work and is not under review at any
other publication.
Ethics approval This article does not contain any studies with human
participants or animals performed by any of the authors.
References
[1] Lafond, K. E. et al. Global burden of influenza-associated lower respi-
ratory tract infections and hospitalizations among adults: A systematic
review and meta-analysis. PLoS Medicine 18(3), e1003550 (2021) .
[2] Lau, L. L. et al. Viral shedding and clinical illness in naturally acquired
influenza virus infections. The Journal of infectious diseases 201 (10),
1509–1516 (2010) ."
2207.08192,D:\Database\arxiv\papers\2207.08192.pdf,How does the paper's proposed method for learning inter-object functional relationships differ from prior work in the field of causal discovery from visual observations?,"The paper's method leverages the agent's interaction policy to collect data that explicitly demonstrates the causal relationships between objects, allowing for more accurate inference of functional relations compared to prior work that relies solely on passive observation of the environment.","Reasoning Initial State Position Scores Direction Scores Execution 
Interaction Sequence Functional Relations Current+Action Future Interaction 
Policy 
Current 
Target Scene Graph Dynamics 
Network 
Possible Futures Inference 
Network Dynamics 
Network Position 
Network Direction 
Network 
Inference 
Network 
Compare and Select Final Action Interaction 
Planning max
max
Light on 
Action Candidates 
…
Figure 3: BusyBot Overview. [Interaction] infers a sequence of actions from visual input to efﬁciently interact
with a given scene. [Reasoning] infers a functional scene graph (i.e., inference network) and predicts future
states (i.e., dynamics network). [Planning] uses the trained manipulation policy network (learned from multiple
boards), relation inference and dynamics network (extracted from the speciﬁc board) to plan actions for reaching
the target state.
policy is considered effective if it can 1) successfully trigger changes in responder objects, and 2)
interact with different objects to explore novel states of the board.
Interaction policy. The interaction policy is modeled by two neural networks: a position network
and a direction network, which follows a similar formulation as UMP-Net [ 20] to jointly learn the
action position and direction. The position network takes a depth image as input and outputs per-pixel
position affordance P∈[0,1]W×H, which indicates the likelihood of an effective interact position.
The direction network takes in the depth image and the selected action position (represented as a 2-D
Gaussian distribution centered around the corresponding pixel location of the 3-D action position)
and outputs a score for each direction candidate r(adir
t)∈[0,1]. We uniformly sample 18 directions in
SO(3) as direction candidates. Since it is often hard to identify the state of small-displacement objects
from visual observations, the agent executes both the selected direction and its opposite direction.
Supervision. Unlike UMP-Net [ 20] that requires object state from simulation to compute reward,
BusyBot uses a simple self-supervised reward computed from image difference, which is enabled by
the ampliﬁed effects in the BusyBoard environments:
rimg(adir
t) ={
1 if∑H
i=1∑W
j=1I(oi j,o′
i j)>δ
0 if∑H
i=1∑W
j=1I(oi j,o′
i j)≤δI(oi j,o′
i j) ={
1 if oi j=o′
i j
0 if oi j̸=o′
i j(1)
where oando′denote RGB image observations before and after the action execution. δis a
threshold speciﬁes the minimum number of different pixels. We use binary cross-entropy (BCE) loss
between the inferred action score r(adir
t)∈[0,1]and the ground-truth reward computed from image
observations rimg(adir
t)∈[0,1].
Exploration. At the early stage of training the position and direction inference network, we use the
epsilon-greedy method to encourage random exploration. Additionally, in order to prevent the model
from only selecting the position that has the highest affordance score, we apply the Upper Conﬁdence
Bound (UCB) Bandit algorithm on the inferred position affordance. Given the per-pixel position
affordance score P(i,j), the updated score is P′(i,j) =P(i,j)+c√
ln(t)
N, where c=0.5,tis the number
of past steps, and Nis the numeber of times when the pixel (i,j)falls in the M×M(M=10)window
centered around each previously selected pixels.
4.2 Reason: Learning to Discover Inter-object Relations by Predicting the Future
The reasoning module takes in RGB image sequences of the agent’s interactions ( §4.1), infers the
inter-object relations, and predicts future dynamics, which would guide goal-conditioned planning
(§4.3). To accomplish this goal, we adopt and modify the V-CDN model [25].
The inference network is implemented with three Graph Neural Networks (GNNs) to extract functional
relations as a scene graph. Each object Oicorresponds to a node iin the graph, with a node input n1:T
i,
where Tis the ﬁrst Tframes in an interaction sequence. Unlike the V-CDN model [ 25] that uses
keypoint locations as node features, BusyBot uses both objects’ visual features and locations, which
allows the network to represent both motion change (e.g., door opens) and appearance change (e.g.,
light turns on) of the responders. The ﬁrst GNN learns spatial node and edge embeddings at each
step, which are concatenated with 256-dimensional embeddings of the executed actions a1:T
ioutput
from an MLP layer. The combined embeddings are then aggregated over temporal dimension using a
4
1-D convolution network and input to the second GNN which predicts a probabilistic distribution
over edge types ed={ed
i j|ed
i j∈R2}N
i,j=1(index 0 indicates no relation and index 1 indicates has
relation). Conditioned on the edge types, the third GNN predicts 32-dimensional edge embeddings
eh={eh
i j|eh
i j∈R32}N
i,j=1which store history dynamics associated with each edge.
The dynamics network is a Graph Recurrent Network (GRN) that predicts the next state nt+1given the
current observation nt, the executed action at, and the edges E={ed,eh}from the inferred functional
scene graph. The inference and dynamics network are jointly trained under the objective to minimize
the mean squared error (MSE) between predicted and ground-truth object features. fI
φdenotes the
inference model parameterized by φ,fD
ψdenotes the dynamics model parameterized by ψ.
L=min
φ,ψ∑
tMSE(nt+1,fD
ψ(nt,at,fI
φ(n1:T,a1:T))) ( t≥T) (2)
Data collection. The interaction dataset used in the reasoning module is collected the using learned
interaction policy: at each step, positions with affordance scores above a threshold are grouped into
clusters using the K-means clustering algorithm (with kbeing the maximum possible number of
movable links on all busyboards), and positions with the highest score in each cluster are selected as
position candidates. Conditioned on each position candidate, direction with the highest affordance
score will be selected to form the ﬁnal action candidates, from which a candidate will be randomly
chosen to execute. For each board environment, RGB image observations of 30 interaction steps are
generated. In addition, to prevent the model from overﬁtting on board appearances, we ensure that
every 20 board environments share the same initial visual appearance but with different inter-object
functional relations.
4.3 Plan: Goal-conditioned Manipulation with Relation Predictive Agent
Given an initial and target image of a BusyBoard, the task is to have BusyBot infer 1) which object(s)
to manipulate; 2) what action(s) to execute in order to successfully reach the target state.
Using the data collection method as discussed in the reasoning module, the agent infers the action
candidates and generate an interaction sequence of 30 images, which is input to the inference network
to obtain the functional scene graph. Then we consider three options to plan for goal-conditioned
tasks: 1) Relation agent , at each step, identify a responder that needs to be changed, and ﬁnd the
corresponding trigger based on the functional scene graph. This method is similar to the idea of Li
et al. [ 26]. However, the agent might have trouble handling one-to-many relations. To solve this
issue, we propose 2) Predictive agent that uses the dynamics network from the reasoning module
and choose the action that minimizes the L2 distance of the predicted next state and the target state.
However, the predictive agent may have difﬁculty generalizing to novel object instances, due to
the difﬁculty of predicting unseen dynamics. 3) Unlike piror works that only use either predicted
dynamics or functional scene graph for planning, our ﬁnal method BusyBot combines the relation
and predictive agent, where action candidates are ﬁrst ﬁltered based on the functional scene graph
and then selected based on dynamic predictions. More discussions are provided in Sec. 5.
5 Evaluation
We evaluate BusyBot with both simulated (Fig. 4) and real-world busyboards (Fig. 6). In simulation
experiments, we set up the following environments: a) Training Board : for training interaction
and reasoning module. b) Novel Conﬁg : testing board with training object instances but in new
conﬁgurations, which include new inter-object functional relations, position and orientation of objects,
board color and texture; c) Novel Object : both object instances and board conﬁgurations are novel.
In total, we generate 10,000 training boards, 2,000 boards with novel conﬁgurations, and 2,000 boards
with novel object instances. We generate 30 interaction images for each board, where 23 images are
reserved for relation inference and the rest for future predictions. As for object instances, we use 41
switches, 10 doors, 5 lamps, and 2 tracktoy objects, split into training / testing with ratio: 32/9, 5/5,
3/2, 1/1. The setup for real-world evaluation is described in Sec 5.4
5.1 Interaction Module Evaluation
To evaluate the interaction policy network, we compute the average precision and recall of the inferred
actions for the boards, where precision = # successful actions / # total proposed actions, and recall =
# successfully interacted objects / # total interactable objects. We compare the following methods:
5
(a) Interaction 
      Board           Position                       Direction 
(b) Reasoning 4
 8
 28…Steps    Action     Pred (c) More Configurations 
Novel Object 
 Novel Config 
1-to-many 1-to-1 1-to-many 1-to-1 
…
 16…
Figure 4: Qualitative Results , (a) action affordances (b) interaction steps and corresponding reasoning results
(c) More reasoning results →: inferred inter-object functional relations. →: ground truth.
•Oracle (joint state supervision): Interaction policy supervised on joint states. This is considered
as the oracle because changes in joint states are directly obtained from simulation.
•RGB: A baseline that takes RGB images instead of depth images as input to the model.
•w/o responder: Interaction policy supervised on visual feedback but no responder effects.
•w/o exploration: An ablated version of BusyBot without using UCB for exploration.
Novel Conﬁg Novel Object
Prec Recall Prec Recall
Oracle 91.8 82.4 79.5 90.2
RGB 62.3 63.1 9.50 17.0
w/o responder 0.71 0.65 0.24 0.49
w/o exploration 94.2 33.7 81.9 38.6
BusyBot 90.1 80.1 82.6 84.8
Table 1: Performance of Interaction PolicyResults and Analysis. From Tab. 1, we can see that
[w/o responder] achieves poor performence since vi-
sual feedback of small-displacement objects alone
is insufﬁcient for learning. In contrast, [BusyBot] is
able to achieve comparable performance as the or-
acle, which validates our hypothesis that triggered
responder effects can assist the model in learning
good interaction policy by amplifying the visual feed-
back. We also demonstrate the effectiveness of our
exploration method by showing the recall of [w/o
exploration] drops by more than 45% than that of [BusyBot]. From the [RGB] baseline, we ﬁnd that
color observations alone struggle to learn interaction policy for instances such as switches that are
all-white. Moreover, the performance of [RGB] drops signiﬁcantly for novel objects, implying that
the model trained with RGB observations overﬁts on colors and lacks generalizability.
5.2 Relation Reasoning Module Evaluation
The reasoning module is evaluated by the following metrics: 1) Relation inference accuracy, measured
by the precision ( Edge-P ) and recall ( Edge-R ) of the inferred functional relation pairs. 2) Future
state prediction accuracy ( Pred-A ), measured by the percentage of correct future state predictions.
We compare the following alternatives:
•w/o inference: An ablated version of the model without the inference network. The dynamics
network takes in all history interaction data and directly predicts the next state.
•w/o exploration: An ablated version of our method, where the input of the reasoning network
are data collected under an inferior interaction policy.
Training Board Novel Conﬁg Novel Object
Edge-P Edge-R Pred-A Edge-P Edge-R Pred-A Edge-P Edge-R Pred-A
w/o inference - - 79.2 - - 36.2 - - 7.04
w/o exploration 55.6 51.0 89.6 74.6 3.10 14.3 75.1 0.96 11.6
BusyBot 95.8 100 88.1 95.5 99.7 73.8 85.0 99.5 31.0
Table 2: Performance of Reasoning Module. For BusyBot, while the future state prediction accuracy (Pred-A)
decreases for unseen board appearances (novel conﬁg, novel object), the reasoning module is still able to reliably
infer the inter-object functional relations (Edge-P, Edge-R) in novel scenarios.
Results and Analysis. Compared to [w/o inference], we see that without inferring the inter-object
relations, the model overﬁts on the training data and generalizes poorly to novel boards. We also
observe that with bad interactions [w/o exploration], the reasoning model is not able to uncover the
relations accurately and make correct future state predictions. In comparison, our model generalizes
well to novel board conﬁgurations and achieves performance comparable to that of the training board.
This demonstrates that a good interaction policy helps the agent uncover the correct inter-object
6
functional relations, which then helps the agent to understand scene dynamics. For boards with novel
object instances, even though the future state prediction accuracy drops by around 40% than the seen
instances (which is expected since the object features are never seen by the dynamics model), the
relation inference accuracy is still comparable. The performance on novel boards veriﬁes that the
model’s ability to infer inter-object functional relationship can transfer to new scenes and objects.
5.3 Goal-conditioned Manipulation
obj not in target state gt edge gt action 
wrong direction 
wrong link Novel object 1-to-many 
Initial Goal Relation Predictive BusyBot 
inferred edge inferred action 
repeat 
Figure 5: Goal-conditioned Manipulation. Compared to the pre-
dictive agent, the relation agent generalizes better on novel objects,
while struggles in handling one-to-many relations. Our method
BusyBot combines the advantages of both agents.We generate 50 one-to-one tasks
and 50 one-to-many tasks for each
type of board (training, novel conﬁg,
novel object). One-to-one tasks con-
tain only two-state triggers, and thus
only require the algorithm to iden-
tify the correct trigger (similar task
studied in IFRexplore [ 26]). One-
to-many tasks contain both multi-
direction and multi-link triggers that
require the agent to not only identify
the correct trigger, but also infer the
correct action to manipulate the trig-
ger (e.g., the correct button position
or pushing direction).
Training Novel Conﬁg Novel Object
1-to-1 1-to-m 1-to-1 1-to-m 1-to-1 1-to-m
PPO 91.2 88.5 62.3 54.3 59.9 55.7
BC 95.4 94.9 57.0 51.8 63.8 54.8
Relation 98.3 61.1 93.7 60.0 92.0 62.8
Predictive 97.7 67.5 91.0 67.0 89.0 58.2
BusyBot 98.3 71.0 93.7 69.4 92.3 64.9
Table 3: Goal-conditioned Manipulation ResultMetrics & Baselines. We measure object-
level success rate on both one-to-one and one-
to-many manipulation tasks for each type of
board. The success rate is deﬁned at object
level at the end of an interaction sequence
with a maximum of 8 steps. Success rate =
# affectable responders in goal state / # total
affectable responders. We compare the three
agents as discussed in 4.3, along with two
learning-based agents using behavior cloning
(BC) and proximal policy optimization (PPO). More details can be found in supp.
Results and Analysis. All agents achieve good performances on one-to-one tasks. This means
that both the relation and dynamics learned by the reasoning module can generalize to novel board
conﬁgurations and objects. The [predictive] agent achieves better performance on one-to-many tasks
with seen object instances by leveraging future predictions to select the correct action to apply on the
trigger object. In contrast, the [relation] agent can only identify the trigger object but not the exact
action (e.g., which link to interact with or which direction to push). On the other hand, the [relation]
agent performs slightly better than the [predictive] agent on all one-to-one tasks and boards with
novel objects, when the dynamics model sometimes fails to predict the correct next state. This shows
that inter-object functional relationship can generalize to scenarios when future predictions are not
reliable enough to assist planning. The result of [PPO] and [BC] indicate that RL-based agents fail to
generalize to boards with novel conﬁgurations or objects, and it is thus critical to learn an explicit
representation of inter-object functional relations.
5.4 Real-world Experiments
Setup. We test the trained model on a busyboard in real world with robot interactions (Fig. 6). The
board consists of 3 trigger objects (switches) and 3 responder objects (LEDs). Objects outside the
effective region are ignored. We manually modiﬁed the underlying inter-object functional relations of
the board by rewiring the objects. We test with 6 different conﬁgurations including 4 one-to-one and
2 one-to-many conﬁgurations. For each conﬁguration, the robot interacts with the board for 30 steps
and the rollout is grouped into 6 overlapping and continual sub-sequences, each of which has a length
of 25. In total, we generate a real-world dataset of 36 sequences with 108 inter-object functional
relation pairs for evaluating the reasoning module.
Results. Fig. 6 (c) shows that the algorithm is able to reﬁne inter-object functional relations
(reducing additional edges) through interactions. The precision and recall of inferred relations are
7
(a) Setup (b) Interaction (c) Reasoning Position Direction 
BusyBoard End-effector 8
9
13
16
17…Obs&Action Prediction Steps ……
………
………
(d) More Configurations 
Ground Truth Prediction 
one-to-one 
one-to-many 
effective region Figure 6: Real-world Busyboard. We test the trained models on a real-world busyboard with robot interactions
(a), and show that our algorithm is able to discover the inter-object relations (c) through interactions (b).
93.9% and100% , respectively. All inter-object functional relations can be discovered by our model,
with only a few additional pairs predicted. The result shows that the relation reasoning ability of the
model is transferable to real-world scenarios. More results can be found in supp.
5.5 Application in Simulation Home Environments
(a) Stove with controls 
(b) Room with multiple objects 
… 
… 
Figure 7: Application in AI2THOR Home Environments. The
ﬁgure shows the interactions (in yellow), corresponding state
changes, ground truth inter-object functional relationships (in green),
and inferred relationships (in red).To demonstrate the learned skills can
be applied beyond the BusyBoard en-
vironment, we further test our reason-
ing model in 2 kitchen scenes from
AI2THOR [ 15]: a) a stove with mul-
tiple controls and b) a room with mul-
tiple objects. Following the same
evaluation protocol, we let the agent
interact with the environments for
a few steps and use the trained rea-
soning model to infer the functional
scene graphs. We observe that the
algorithm achieves similar performance to boards with novel objects (shown in 5.2): while the
algorithm cannot perfectly predict the future states of the object (due to out-of-distribution object
visual appearance), it is able to infer the correct edges through interactions, without the need of
ﬁne-tuning. The result demonstrates the generalization ability of our proposed environment and
algorithm to new domains and applications.
5.6 Limitation and Future Work
While the BusyBoard environment is inspired by toys, it still lacks certain diversity and complexity in
real-world toys. For example, real-world toys are often designed with multi-sensory feedback such as
sound and tactile, while our environment focuses on visual effects only. Several assumptions made by
BusyBot could also be relaxed for more general applications. First, BusyBot assumes full observability
of objects in a single image. Future work may consider using a 3D scene representation that integrates
multi-view observations to handle larger-scale environments. In addition, the interaction module
assumes trigger objects can reach all states through single-step actions. Future work could consider
learning a more general manipulation policy [ 20] to accomodate objects that require a sequence of
actions to manipulate. Finally, the relation reasoning module assumes objects are detected. Though
the assumption is valid in our setup, it may not hold for cluttered scenes.
6 Conclusion
We propose a toy-inspired relational environment, BusyBoard, and a learning framework, BusyBot,
for embodied AI agents to acquire interaction, reasoning, and planning abilities. Our experiments
demonstrate that the rich sensory feedback in BusyBoard helps the agent learn a policy to efﬁciently
interact with the environment; using the data collected under this interaction policy, inter-object
functional relations can be inferred through predicting future states; and by combining the ability to
interact and reason, the agent is able to perform goal-conditioned manipulation tasks. We verify the
effectiveness and generalizability of our method in both simulation and real-world setups.
8
Acknowledgments: The authors would like to thank Huy Ha, Cheng Chi, Samir Gadre, Neil Nie, and
Zhanpeng He for their valuable feedback and support. This work was supported in part by National
Science Foundation under 2143601, 2037101, and 2132519, and Microsoft Faculty Fellowship. We
would like to thank Google for the UR5 robot hardware. The views and conclusions contained herein
are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies,
either expressed or implied, of the sponsors.
References
[1]R. Held and A. Hein. Movement-produced stimulation in the development of visually guided
behavior. Journal of comparative and physiological psychology , 56, 1963. doi:10.1037/
h0040546.
[2]G. E. Roberson, M. T. Wallace, and J. A. Schirillo. The sensorimotor contingency of multisen-
sory localization correlates with the conscious percept of spatial unity. Behavioral and Brain
Sciences , 24(5), 2001. doi:10.1017/S0140525X0154011X.
[3]R. Baillargeon. Infants’ physical world. Current Directions in Psychological Science , 13(3),
2004. doi:10.1111/j.0963-7214.2004.00281.x.
[4]D. Perille, A. Truong, X. Xiao, and P. Stone. Benchmarking metric ground navigation. In
2020 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR) , pages
116–121. IEEE, 2020.
[5]N. Tsoi, M. Hussein, O. Fugikawa, J. Zhao, and M. V ´azquez. An approach to deploy interactive
robotic simulators on the web for hri experiments: Results in social robot navigation. In
2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pages
7528–7535. IEEE, 2021.
[6]Manolis Savva*, Abhishek Kadian*, Oleksandr Maksymets*, Y . Zhao, E. Wijmans, B. Jain,
J. Straub, J. Liu, V . Koltun, J. Malik, D. Parikh, and D. Batra. Habitat: A Platform for Embodied
AI Research. In Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV) , 2019.
[7]C. Li, F. Xia, R. Mart ´ın-Mart ´ın, M. Lingelbach, S. Srivastava, B. Shen, K. E. Vainio, C. Gokmen,
G. Dharan, T. Jain, A. Kurenkov, K. Liu, H. Gweon, J. Wu, L. Fei-Fei, and S. Savarese. igibson
2.0: Object-centric simulation for robot learning of everyday household tasks. In 5th Annual
Conference on Robot Learning , 2021. URL https://openreview.net/forum?id=
2uGN5jNJROR .
[8]T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A
benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on
robot learning , pages 1094–1100. PMLR, 2020.
[9]S. James, Z. Ma, D. R. Arrojo, and A. J. Davison. Rlbench: The robot learning benchmark &
learning environment. IEEE Robotics and Automation Letters , 5(2):3019–3026, 2020.
[10] O. Ahmed, F. Tr ¨auble, A. Goyal, A. Neitz, M. W ¨uthrich, Y . Bengio, B. Sch ¨olkopf, and S. Bauer.
Causalworld: A robotic manipulation benchmark for causal structure and transfer learning. In
International Conference on Learning Representations , 2021.
[11] Y . Zhu, J. Wong, A. Mandlekar, and R. Mart ´ın-Mart ´ın. robosuite: A modular simulation
framework and benchmark for robot learning. In arXiv preprint arXiv:2009.12293 , 2020.
[12] F. Xiang, Y . Qin, K. Mo, Y . Xia, H. Zhu, F. Liu, M. Liu, H. Jiang, Y . Yuan, H. Wang, L. Yi,
A. X. Chang, L. J. Guibas, and H. Su. SAPIEN: A simulated part-based interactive environment.
InThe IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2020.
[13] A. Bakhtin, L. van der Maaten, J. Johnson, L. Gustafson, and R. Girshick. Phyre: A new
benchmark for physical reasoning. Advances in Neural Information Processing Systems , 32,
2019.
9
[14] A. Jain, A. Szot, and J. Lim. Generalization to new actions in reinforcement learning. In
H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on Machine
Learning , volume 119 of Proceedings of Machine Learning Research , pages 4661–4672. PMLR,
13–18 Jul 2020. URL https://proceedings.mlr.press/v119/jain20b.html .
[15] E. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, D. Gordon, Y . Zhu,
A. Gupta, and A. Farhadi. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint
arXiv:1712.05474 , 2017.
[16] S. Brahmbhatt, C. Ham, C. C. Kemp, and J. Hays. Contactdb: Analyzing and predicting grasp
contact via thermal imaging. In Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 8709–8719, 2019.
[17] T. Nagarajan, C. Feichtenhofer, and K. Grauman. Grounded human-object interaction hotspots
from video. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,
pages 8688–8697, 2019.
[18] T. Nagarajan, Y . Li, C. Feichtenhofer, and K. Grauman. Ego-topo: Environment affordances
from egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 163–172, 2020.
[19] K. Mo, L. J. Guibas, M. Mukadam, A. Gupta, and S. Tulsiani. Where2act: From pixels to
actions for articulated 3d objects. In Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 6813–6823, 2021.
[20] Z. Xu, H. Zhanpeng, and S. Song. Umpnet: Universal manipulation policy network for
articulated objects. IEEE Robotics and Automation Letters , 2022.
[21] B. Eisner, H. Zhang, and D. Held. Flowbot3d: Learning 3d articulation ﬂow to manipulate
articulated objects. RSS, 2022.
[22] S. Y . Gadre, K. Ehsani, and S. Song. Act the part: Learning interaction strategies for articulated
object part discovery. In Proceedings of the IEEE/CVF International Conference on Computer
Vision , pages 15752–15761, 2021.
[23] P. Battaglia, R. Pascanu, M. Lai, D. Jimenez Rezende, et al. Interaction networks for learning
about objects, relations and physics. Advances in neural information processing systems , 29,
2016.
[24] C. Mitash, A. Boularias, and K. Bekris. Physics-based scene-level reasoning for object pose
estimation in clutter. The International Journal of Robotics Research , page 0278364919846551,
2019.
[25] Y . Li, A. Torralba, A. Anandkumar, D. Fox, and A. Garg. Causal discovery in physical systems
from videos. Advances in Neural Information Processing Systems , 33, 2020.
[26] Q. Li, K. Mo, Y . Yang, H. Zhao, and L. Guibas. IFR-Explore: Learning inter-object functional
relationships in 3d indoor scenes. In International Conference on Learning Representations
(ICLR) , 2022.
[27] S. Nair, Y . Zhu, S. Savarese, and L. Fei-Fei. Causal induction from visual observations for goal
directed tasks. arXiv preprint arXiv:1910.01751 , 2019.
[28] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, and T. Lillicrap.
A simple neural network module for relational reasoning. Advances in neural information
processing systems , 30, 2017.
[29] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zambaldi, M. Malinowski,
A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep
learning, and graph networks. arXiv preprint arXiv:1806.01261 , 2018.
[30] M. Ze ˇcevi´c, D. S. Dhami, P. Veli ˇckovi ´c, and K. Kersting. Relating graph neural networks to
structural causal models. arXiv preprint arXiv:2109.04173 , 2021.
10
[31] W. Lin, H. Lan, and B. Li. Generative causal explanations for graph neural networks. In
International Conference on Machine Learning , pages 6666–6679. PMLR, 2021.
[32] N. R. Ke, O. Bilaniuk, A. Goyal, S. Bauer, H. Larochelle, B. Sch ¨olkopf, M. C. Mozer, C. Pal,
and Y . Bengio. Learning neural causal models from unknown interventions. arXiv preprint
arXiv:1910.01075 , 2019.
[33] Y . Zhu, J. Tremblay, S. Birchﬁeld, and Y . Zhu. Hierarchical planning for long-horizon manipu-
lation with geometric and symbolic scene graphs. In 2021 IEEE International Conference on
Robotics and Automation (ICRA) , pages 6541–6548. IEEE, 2021.
[34] D. Hafner, K.-H. Lee, I. Fischer, and P. Abbeel. Deep hierarchical planning from pixels. arXiv
preprint arXiv:2206.04114 , 2022.
[35] J. Li, C. Tang, M. Tomizuka, and W. Zhan. Hierarchical planning through goal-conditioned
ofﬂine reinforcement learning. arXiv preprint arXiv:2205.11790 , 2022.
[36] A. Agostini, C. Torras, and F. W ¨org¨otter. Efﬁcient interactive decision-making framework
for robotic applications. Artiﬁcial Intelligence , 247:187–212, 2017. ISSN 0004-3702. doi:
https://doi.org/10.1016/j.artint.2015.04.004. URL https://www.sciencedirect.com/
science/article/pii/S0004370215000661 . Special Issue on AI and Robotics.
[37] B. Quack, F. W ¨org¨otter, and A. Agostini. Simultaneously learning at different levels of
abstraction. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS) , pages 4600–4607, 2015. doi:10.1109/IROS.2015.7354032.
11"
2110.04887,D:\Database\arxiv\papers\2110.04887.pdf,How might the effectiveness of adversarial patches be improved in scenarios where the target object is viewed from multiple angles?,"By incorporating geometric transformations or multi-view data into the patch training process, adversarial patches could be made more robust to changes in viewing angle, potentially increasing their effectiveness in real-world scenarios.","Figure 3. Sample of a successful adversarial attack on the reference view
of LATIS-MV APE, while it fails to fool the detector at a different angle.
efﬁciency compared to the one applied to the reference view.
In the tests performed on our database, the projected patch
was at best half as effective as the reference view patch,
losing nearly all of its attack capabilities in some cases.
These ﬁndings have the potential to impact both the
attacking side and the defending side in adversarial patch
attacks: On the attack side, this study highlights the need
to incorporate view angle into the patch creation process,
as the patch loses a signiﬁcant amount of its effectiveness
when viewed from an angle. This integration can be achieved
by adding geometric transformations in the patch training
process, or including multi-view data in the training dataset.
As for the defense side, these ﬁndings suggest interesting
pathways to stronger defenses, such as adopting multi-view
detection techniques. These multi-view detectors combine
data from all of the available views to locate objects within
them, and as such can detect objects in a certain view that
a monocular detector failed to detect using the extra multi-
view data. Therefore, to attack such a potential defense, an
adversary has to defeat the detector on all of the views,
reducing the strength of the adversarial patch to be only
as strong as the weakest view, and in the case of a patch
untrained against view angle variations, this effectiveness
loss can be severe.
IV. C ONCLUSION
The threat of adversarial attacks has spurred increasing in-
terest in designing practical adversarial attacks and defenses
in the real world. To better understand these attacks, an
investigation of the interactions between adversarial patches
and the factors that affect object detection is necessary. This
paper unprecedentedly studies the effect of view angle on
the effectiveness of an adversarial patch. To emulate the
angle view deformation on the patch, we apply a perspective
geometric transformation to an existing attack. We compared
the original patch results with the transformed patch on other
views, and we notice a signiﬁcant effect on attack success
rates. This difference has implications for adversarial patches
on both tasks of attacking a detector or immunizing detectors
from such attacks.REFERENCES
[1] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards
real-time object detection with region proposal networks,” in
Advances in Neural Information Processing Systems , 2015.
[2] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You
only look once: Uniﬁed, real-time object detection,” in 2016
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2016, pp. 779–788.
[3] B. Tarchoun, A. Ben Khalifa, S. Dhifallah, I. Jegham, and
M. A. Mahjoub, “Hand-crafted features vs deep learning for
pedestrian detection in moving camera,” Traitement du Signal ,
vol. 37, pp. 209–216, 2020.
[4] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus, “Intriguing properties of neural
networks,” 2014.
[5] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and
harnessing adversarial examples,” 2015.
[6] A. Ben Khalifa, I. Alouani, M. A. Mahjoub, and A. Rivenq,
“A novel multi-view pedestrian detection database for collab-
orative intelligent transportation systems,” Future Generation
Computer Systems , vol. 113, pp. 506–527, 2020.
[7] B. Tarchoun, I. Jegham, A. B. Khalifa, I. Alouani, and M. A.
Mahjoub, “Deep cnn-based pedestrian detection for intelli-
gent infrastructure,” in 2020 5th International Conference
on Advanced Technologies for Signal and Image Processing
(ATSIP) , 2020, pp. 1–6.
[8] S. Thys, W. V . Ranst, and T. Goedem ´e, “Fooling automated
surveillance cameras: Adversarial patches to attack person de-
tection,” in 2019 IEEE/CVF Conference on Computer Vision
and Pattern Recognition Workshops (CVPRW) , 2019, pp. 49–
55.
[9] X. Liu, H. Yang, Z. Liu, L. Song, H. Li, and Y . Chen,
“Dpatch: An adversarial patch attack on object detectors,”
2019.
[10] D. Karmon, D. Zoran, and Y . Goldberg, “LaV AN: Localized
and visible adversarial noise,” in Proceedings of the 35th
International Conference on Machine Learning , ser. Proceed-
ings of Machine Learning Research, vol. 80. PMLR, 2018.
[11] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, and
M. Pietik ¨ainen, “Deep learning for generic object detection: A
survey,” International Journal of Computer Vision , vol. 128,
no. 2, pp. 261–318, Feb 2020.
[12] T. Chavdarova, P. Baqu ´e, S. Bouquet, A. Maksai, C. Jose,
T. Bagautdinov, L. Lettry, P. Fua, L. Van Gool, and F. Fleuret,
“Wildtrack: A multi-camera hd dataset for dense unscripted
pedestrian detection,” in 2018 IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2018.
[13] Y . Hou, L. Zheng, and S. Gould, “Multiview detection with
feature perspective transformation,” in Computer Vision –
ECCV 2020 , 2020, pp. 1–18.
[14] A. L ´opez-Cifuentes, M. Escudero-Vi ˜nolo, J. Besc ´os, and
P. Carballeira, “Semantic driven multi-camera pedestrian de-
tection,” 2021.
[15] J. Ferryman and A. Shahrokni, “Pets2009: Dataset and chal-
lenge,” in 2009 Twelfth IEEE International Workshop on
Performance Evaluation of Tracking and Surveillance , 2009."
2204.05864,D:\Database\arxiv\papers\2204.05864.pdf,"What are the limitations of using a single 3D model to represent an object in a pose estimation system, and how does the paper address these limitations?","Using a single 3D model can limit the system's ability to handle variations in object shape and pose. The paper addresses this by incorporating a deformable shape model that allows for variations in the object's shape, making the system more robust to real-world scenarios.","on the object shape (Muja et al., 2011; Hinterstoisser et al., 2012; Rios-Cabrera and Tuytelaars, 2013;
Xie et al., 2013; Cao et al., 2016). While impressive results in terms of accuracy and speed have been
demonstrated, holistic template-based approaches are limited to instance-based object detection and are not
robust to occlusions. To address class variability and viewpoint, various approaches have used a collection
of 2D appearance-based part templates trained separately on discretized views (Gu and Ren, 2010; Fidler
et al., 2012; Pepik et al., 2012; Xiang et al., 2014; Zhu et al., 2014).
Convolutional networks or convnets (LeCun et al., 1989; Krizhevsky et al., 2012) have emerged as the
method of choice for a variety of vision problems. Closest to the current work is their application in camera
viewpoint and keypoint prediction. Convnets have been used to predict the camera’s viewpoint with respect
to the object by way of direct regression or casting the problem as classiﬁcation into a set discrete views
(Massa et al., 2014; Tulsiani and Malik, 2015; Su et al., 2015a). While these approaches allow for object
category pose estimation they do not provide ﬁne-grained information about the 3D layout of the object.
Convnet-based keypoint prediction for human pose estimation (e.g., (Toshev and Szegedy, 2014; Zhou et al.,
2015b; Newell et al., 2016; Wei et al., 2016)) has attracted considerable study, while limited attention has
been given to their application with generic object categories (Long et al., 2014; Tulsiani and Malik, 2015).
Their success is due in part to the high discriminative capacity of the network. Furthermore, their ability
to aggregate information over a wide ﬁeld of view allows for the resolution of ambiguities (e.g., symmetry)
and for localizing occluding joints.
Statistical shape-based models tackle recognition by aligning a shape subspace model to image features.
While originally proposed in the context of 2D shape (Cootes et al., 1995) they have proven useful for
modelling the 3D shape of a host of object classes, e.g., faces (Cao et al., 2013), cars (Zia et al., 2013; Murthy
et al., 2017), and human pose (Ramakrishna et al., 2012). In (Zhu et al., 2015), data-driven discriminative
landmark hypotheses were combined with a 3D deformable shape model and a weak perspective camera
model in a convex optimization framework to globally recover the shape and pose of an object in a single
image. Here, we adapt this approach and extend it with a perspective camera model, in cases where the
camera intrinsics are known.
Since our initial work on pose estimation from semantic keypoints (Pavlakos et al., 2017), several other works
have been inspired by or have used our method. Wang et al. demonstrate the eﬀectiveness of using learned
keypoints for object tracking (Wang et al., 2019). Qin et al. show a pipeline that learns task speciﬁc keypoints
for manipulation (Qin et al., 2019). Manuelli et al. use semantic keypoints as their object representation for
manipulation (Manuelli et al., 2019). Peng et al. propose a reﬁnement of the keypoint detection for pose
estimation using pixel-wise voting (Peng et al., 2019). Hu et al. use object segmentations rather than object
bounding boxes to input detected objects into their semantic keypoint based pose estimation pipeline (Hu
et al., 2019). Zhou et al. learn a set of category agnostic keypoints for object pose estimation (Zhou et al.,
2018).
Other works directly use our work or similar approaches. Zuo et al. show that a keypoint based pose
estimation pipeline can be used to track the position of a low cost robotic arm that does not have its
own sensors, enabling cheap manipulation (Zuo et al., 2019). The keypoint-based pose estimation pipeline
was used by (Bowman et al., 2017) as the ﬁrst step in their semantic SLAM pipeline. Our pipeline has
been used by (Vasilopoulos and Koditschek, 2018; Vasilopoulos et al., 2020b; Vasilopoulos et al., 2020a) to
detect and localize objects for reactive planning for navigation, and was used by the Robotics Collaborative
Techonology Alliance (RCTA) program to rapidly collect and annotate data for pose estimation for mobile
manipulation (Narayanan et al., 2020; Kessens et al., 2020).
RGB-based instance-level pose estimation has also made improvements since our method was initially pro-
posed, largely due to deep network based pipelines (Kehl et al., 2017; Xiang et al., 2017; Mousavian et al.,
2017; Kehl et al., 2017; Tekin et al., 2018; Tremblay et al., 2018; Do et al., 2018; Li et al., 2018; Montserrat
et al., 2019; Labb´ e et al., 2020; Ke et al., 2020; Hou et al., 2020) and higher quality training data (Zeng et al.,
2017; Hodaˇ n et al., 2018; Hodaˇ n et al., 2020; Sundermeyer et al., 2020; Wang et al., 2020). Interestingly,
while several single-shot methods have focused on speed (Kehl et al., 2017; Tekin et al., 2018; Hou et al.,
2020), many state of the art approaches still follow the two-stage approach, where 2D-3D correspondences
are detected with a deep network in the ﬁrst stage, and 6D pose is solved in the second stage with PnP
methods. Diﬀerent intermediate representations have been explored to establish correspondences, such as
3D bounding box corners (Rad and Lepetit, 2017), dense object coordinates (Park et al., 2019), dense frag-
ments (Hodan et al., 2020), pixel-wise voting (Peng et al., 2019) and their hybrids (Song et al., 2020). We
will show through new experiments in Section 6.2 that our simple semantic keypoint based representation is
still eﬀective and competitive, even in challenging occluded cases.
3 Pose estimation from Semantic Keypoints
Our pipeline includes object detection, keypoint localization, and pose optimization. As object detection
has been a well studied problem, we assume that a bounding box around the object has been provided by
an oﬀ-the-shelf object detector, e.g., Faster R-CNN (Ren et al., 2015), and focus on the later two processes.
3.1 Pose optimization
Given the keypoint locations on the 3D model as well as their correspondences in the 2D image, one naive
approach is to simply apply an existing PnP algorithm to solve for the 6-DoF pose. This approach is
problematic because the keypoint predictions can be rendered imprecise due to occlusions and false detections
in the background. Moreover, the exact 3D model of the object instance in the testing image is often
unavailable. To address these diﬃculties, we ﬁt a deformable shape model to the 2D detections while
considering the uncertainty in keypoint predictions. This approach additionally allows the optimzation to
include the uncertainty of the detections, making it better able to handle false or imprecise detections.
We build a deformable shape model for each object category using 3D CAD models with annotated keypoints.
More speciﬁcally, the pkeypoint locations on a 3D object model are denoted by S∈R3×pand
S=B0+k∑
i=1ciBi, (1)
where B0is the mean shape of the given 3D model and B1,...,Bkare several modes of possible shape
variability computed by Principal Component Analysis (PCA).
Given detected keypoints in an image, which are denoted by W∈R2×p, the goal is to estimate the rotation
R∈R3×3and translation T∈R3×1between the object and camera frames as well as the coeﬃcients of the
shape deformation c= [c1,···,ck]⊤.
The inference is formulated as the following optimization problem:
min
θ1
2ξ(θ)D1
22
F+λ
2∥c∥2
2, (2)
whereθis the set of unknowns, ξ(θ) denotes the ﬁtting residuals dependent on θ, and the Tikhonov regularizer
∥c∥2
2is introduced to penalize large deviations from the mean shape.
To incorporate the uncertainty in 2D keypoint predictions, a diagonal weighting matrix D∈Rp×pis intro-
duced:
D=
d10··· 0
0d2··· 0
............
0 0···dp
, (3)
wherediindicates the localization conﬁdence of the ith keypoint in the image. In our implementation, di
is assigned the peak value in the heatmap corresponding to the ith keypoint. As shown previously (Newell
et al., 2016), the peak intensity of the heatmap provides a good indicator for the visibility of a keypoint in
the image.
The ﬁtting residuals, ξ(θ), measure the diﬀerences between the given 2D keypoints, provided by the previous
processing stage, and the projections of 3D keypoints. Two camera models are next considered.
3.1.1 Weak perspective model
If the camera intrinsic parameters are unknown, the weak perspective camera model is adopted, which is
usually a good approximation to the full perspective case when the camera is relatively far away from the
object. In this case, the reprojection error is written as
ξ(θ) =W−s¯R(
B0+k∑
i=1ciBi)
−¯T1⊤, (4)
wheresis a scalar, ¯R∈R2×3and ¯T∈R2denote the ﬁrst two rows of RandT, respectively, and
θ={s,c,¯R,¯T}.
The problem in (2) is continuous and in principal can be locally solved by any gradient-based method.
We solve it with a block coordinate descent scheme because of its fast convergence and the simplicity in
implementation. We alternately update each of the variables while ﬁxing the others. The updates of s,c
and ¯Tare simply solved using closed-form least squares solutions. The update of ¯Rshould consider the
SO(3) constraint. Here, the Manopt toolbox (Boumal et al., 2014) is used to optimize ¯Rover the Stiefel
manifold. As the problem in (2) is non-convex, we further adopt a convex relaxation approach (Zhou et al.,
2015a) to initialize the optimization. More speciﬁcally, we only estimate the pose parameters while ﬁxing
the 3D model as the mean shape in the initialization stage. By setting c=0and replacing the orthogonality
constraint on ¯Rby the spectral norm regularizer, the problem in (2) can be converted to a convex program
and solved with global optimality (Zhou et al., 2015a).
3.1.2 Full perspective model
If the camera intrinsic parameters are known, the full perspective camera model is used, and the residuals
are deﬁned as
ξ(θ) = ˜WZ−R(
B0+k∑
i=1ciBi)
−T1⊤, (5)
where ˜W∈R3×prepresents the normalized homogeneous coordinates of the 2D keypoints and Zis a
diagonal matrix:
Z=
z10··· 0
0z2··· 0
............
0 0···zp
, (6)
whereziis the depth for the ith keypoint in 3D. Intuitively, the distances from the 3D points to the rays
crossing the corresponding 2D points are minimized. In this case, the unknown parameter set θis given by
{Z,c,R,T}.
The optimization here is similar to the alternating scheme in the weak perspective case. The update of Zalso
admits a closed-form solution and the update of Rcan be analytically solved by the orthogonal Procrustes
analysis. To avoid local minima, the optimization is initialized by the weak perspective solution.
Image Intermediate heatmaps Output heatmaps
Figure 3: Overview of the stacked hourglass architecture (Newell et al., 2016). Here, two hourglass modules
are stacked together. The symmetric nature of the design allows for bottom-up processing (from high to
low resolution) in the ﬁrst half of the module, and top-down processing (from low to high resolution) in the
second half. Intermediate supervision is applied after the ﬁrst module. The heatmap responses of the second
module represent the ﬁnal output of the network that is used for keypoint localization.
3.2 Keypoint localization
The keypoint localization step employs the “stacked hourglass” network architecture (Newell et al., 2016)
that has been shown to be particularly eﬀective for 2D human pose estimation. Motivated by this success,
we use the same network design and train the network for object keypoint localization.
Network architecture A high level overview of the main network components is presented in Figure 3.
The network takes as input an RGB image, and outputs a set of heatmaps, one per keypoint, with the
intensity of the heatmap indicating the conﬁdence of the respective keypoint to be located at this position.
The network consists of two hourglass components, where each component can be further subdivided into
two main processing stages. In the ﬁrst stage, a series of convolutional and max-pooling layers are applied
to the input. After each max-pooling layer, the resolution of the feature maps decreases by a factor of two,
allowing the next convolutional layer to process the features at a coarser scale. This sequence of processing
continues until reaching the lowest resolution (4 ×4 feature maps), which is illustrated by the smallest layer
in the middle of each module in Figure 3. Following these downsampling layers, the processing continues
with a series of convolutional and nearest-neighbor upsampling layers. Each upsampling layer increases the
resolution by a factor of two. This process culminates with a set of heatmaps at the same resolution as the
input of the hourglass module. A second hourglass component is stacked at the end of the ﬁrst one to reﬁne
the output heatmaps. The groundtruth labels used to supervise the training are synthesized heatmaps based
on a 2D Gaussian centered at each keypoint with a standard deviation set to one. The ℓ2loss is minimized
during training. Additionally, intermediate supervision is applied at the end of the ﬁrst module, which
provides a richer gradient signal to the network and guides the learning procedure towards a better optimum
(Lee et al., 2015). We evenly weight the the loss from the intermediate heatmaps with the heatmaps from the
last module. We train the model using the RMSProp optimizer (Tieleman and Hinton, 2012). The heatmap
responses of the last module are considered as the ﬁnal output of the network and the peak in each heatmap
indicates the most likely location for the corresponding keypoint. For more details of our architecture, please
see our oﬃcial implementation.1
Design beneﬁts The most critical design element of the hourglass network is the symmetric combination
of bottom-up and top-down processing that each hourglass module performs. Given the large appearance
changes of objects due to in-class and viewpoint variation, both local and global cues are needed to eﬀectively
decide the locations of the keypoints in the image. The consolidation of features across diﬀerent scales in
1Our implementation of the keypoint detector network is available at https://github.com/kschmeckpeper/
keypoint-detection
the hourglass architecture allows the network to successfully integrate both local and global appearance
information, and commit to a keypoint location only after this information has been made available to the
network. Moreover, the stacking of hourglass modules provides a form of iterative processing that has been
shown to be eﬀective with several other recent network designs (Carreira et al., 2016; Wei et al., 2016)
and oﬀers additional reﬁnement of the network estimates. Additionally, the application of intermediate
supervision at the end of each module has been validated as an eﬀective training strategy, particularly
ameliorating the practical issue of vanishing gradients when training a deep neural network (Lee et al.,
2015). Finally, residual layers are introduced (He et al., 2016), which have achieved state-of-the-art results
for many visual tasks, including object classiﬁcation (He et al., 2016), instance segmentation (Dai et al.,
2016), and 2D human pose estimation (Newell et al., 2016).
4 Data Collection and Annotation
With most modern machine learning frameworks, the data quantity and quality is as important, if not more
so, than details of a particular learning algorithm. While unlabeled data can be rapidly obtained, high
quality labeled data is time consuming and costly to acquire. Recent eﬀorts (Castrejon et al., 2017; Sohn
et al., 2020; Xie et al., 2019) leverage machine learning to assist with manual annotation or to provide
semi-supervised labels from unlabeled data. Similarly, (Hua et al., 2016) and (Marion et al., 2018) leverage
multi-view scene reconstructions to eﬃciently annotate 3D objects, then reproject the annotation masks
to individual frames based on estimated camera viewpoints. (Osteen et al., 2019) combined geometric
reconstructions with hierarchical metric segmentations as well as learned object-segmentation proposals to
annotate 3D scene objects, reprojecting label masks to individual frames. Here, we extend this work to add
rapid keypoint annotation with depth-based keypoint reﬁnement to enable downstream applications such as
object pose estimation from color images. The work is most closely related to (Marion et al., 2018), which
also uses object reconstructions to annotate objects and their poses. While their work requires 3D meshes
of the objects as a prerequisite to using the annotation tool, we assume no existing models and emphasize
the scenario of needing to rapidly annotate a completely unknown object. In addition, our tool speciﬁcally
produces annotations for training keypoint detectors, while the alternative tool does not. As a result, we can
leverage the structure of the object given by the 3D keypoint annotations to perform automatic keypoint
reﬁnement with no additional human work beyond keypoint labeling.
To label a new object class with keypoint annotations, we collect sequences of streaming data from diﬀerent
viewpoints, environments, and lighting conditions using an RGB-D sensor. We annotate the resulting images
by performing 3D reconstruction, manually annotating the resulting models, projecting the keypoints into
the image frame, and reﬁning the resulting keypoints.
4.1 3D Reconstruction
We leverage state-of-the-art scene reconstruction from RGB-D sequences (Whelan et al., 2016) as well as
surface generation algorithms (Kazhdan and Hoppe, 2013) to pre-process the collected data. In previous work
(Osteen et al., 2019), we evaluated the performance of modern scene reconstruction algorithms, speciﬁcally
Elastic Fusion (Whelan et al., 2016), Elastic Reconstruction (Choi et al., 2015), and Kintinuous (Whelan
et al., 2015). Kintinuous is an extension to the original KinectFusion reconstruction algorithm (Izadi et al.,
2011) which supports operation over larger scales with loop closure. Kintinuous was designed for corridor-like
motion with infrequent loop closures over large distances, rather than the potentially frequent loop-closures
that can arise from the context of overlapping camera motion for object reconstruction (Whelan et al.,
2016). Elastic Reconstruction is an oﬄine reconstruction algorithm that constructs sets of scene fragments
(or sub-models), each consisting of a small number (50) of integrated sequential RGB-D frames, and then
globally registers the fragments and optimizes the associated sensor poses for a ﬁnal reconstruction.
In contrast to these algorithms, Elastic Fusion does not perform pose-graph optimization. Instead, Elastic
Fusion is designed with a focus on the quality of the reconstructed model, formulating the problem using a
deformation graph. A surfel map is used to model the environment, where surfels store properties such as
color, normal, radius, etc. Frame-to-model sensor tracking is performed using the portion of the model that
has been recently observed ( active ), while global loop-closure candidates are determined by attempting to
register new data with portions of the model that have not been recently observed ( inactive ). During loop
closure, the model is non-rigidly deformed to align surfels in the deformation graph, in contrast to pose-
graph optimization approaches which do not perform non-rigid deformation of the data, instead optimizing
the sensor trajectory given the measured data and associated uncertainty. After deforming sets of matching
surfels according to surface correspondences, the camera poses are updated by applying the relative transform
which brings the surfaces into alignment.
The evaluation from (Osteen et al., 2019) on sequences from ﬁve classes of the Redwood dataset (Choi et al.,
2016) found that the reconstructions of Elastic Fusion were comparable in terms of accuracy to the oﬄine
Elastic Reconstruction approach at a fraction of the runtime. Therefore, for this work we use Elastic Fusion
to create scene models, though the annotation system is agnostic to the choice of reconstruction algorithm.
Once reconstruction has taken place, the resulting 3D model as well as the estimated camera poses from
each frame are input to the annotation tool. The only phase of the process that requires human input is
the placement of keypoints on the surface of the 3D reconstruction, after which keypoint annotations are
automatically projected back to the original input image stream.
4.2 Annotation Reﬁnement
An ideal reconstruction system would correctly estimate the camera poses for each frame of a sequence,
such that the generated depth images match corresponding real depth images for all frames. In practice,
pose-graph based SLAM techniques such as Elastic Reconstruction as well as deformation techniques such
as Elastic Fusion attempt to minimize accumulated pose or model errors across the sequence. While this is
optimal for the purposes of estimating sensor trajectory over a sequence, the global pose estimate for a given
frame depends the accuracy of the estimates of other poses in the sequence.
While we only annotate sequences for which qualitatively good reconstructions are produced, we observe
the presence of drift in the camera pose estimates over the course of a sequence. In many cases, this drift
is mitigated by loop closure in the reconstruction algorithm, but we still develop techniques to account for
potentially inaccurate camera pose estimates at any given frame.
Speciﬁcally, we use the 3D model and camera trajectory to generate depth images at each camera pose,
then perform data association between the generated depth images and the corresponding true depth images
from the sensor. This technique ensures that the reconstruction results, integrated over the course of an
entire sequence, are updated to align with the real data at each individual frame. While we could also
compare input color images with their generated counterparts, we choose to focus on depth comparisons on
the assumption that 3D reconstructions preserve object shape better than object appearance for any given
viewpoint and lighting condition. Therefore we use the depth images to reﬁne annotations, and we compare
two alternative approaches to reﬁning annotations: those that update keypoints individually versus those
that update all keypoints together.
4.2.1 Keypoint-level Reﬁnement
The most noticeable artifact of keypoint projection with camera trajectory drift is pixels which project
to the background rather than the object of interest. More subtly, visible keypoints may project to the
correct object but be a few pixels away from the correct location, which may also be obvious to the observer
depending on the choice of keypoint location.
To address the issues of keypoints projecting to background objects (”jump-edge” pixels), as well as keypoints
that are visible but not correctly located, we create an approach to reﬁne individual keypoint locations. In
order to determine if a keypoint is eligible for reﬁnement, we ﬁrst determine whether it is occluded or not for
the current camera pose TF
Cin ﬁxed frame F. GivenNkeypoints speciﬁed for an object, the set of object
keypoints is deﬁned as K={Fk0,Fk1,..,FkN}.
For eachCki∈R3inK, transformed from ﬁxed frame Fto camera frame CbyCki=TF
CFki, the occlusion
test is performed by comparing the depth of the predicted 3D keypoint locationCkiwith the depth value
of the generated depth image at the pixel which the keypoint projects to, eﬀectively performing a z-buﬀer
test to identify which keypoints are occluded. Also, with knowledge of the 3D object geometry, local
normals for each keypointCniare compared to the camera viewpointCv, whereCv∈R3represents a unit
vector along the principal axis of the camera, and keypoints are identiﬁed as occluded if their local normals
are nearly orthogonal to or face away from the camera. More speciﬁcally, a keypoint is determined to be
occluded ifCv·Cni>τ, whereτis an approximate orthogonality threshold that is set to -0.15 radians for
this evaluation.
The following keypoint-level reﬁnement methods are mutually exclusive, such that no two reﬁnements will
be applied to the same keypoint.
Jump-edge adjustment : Often, corners and edges of objects are good candidates for keypoint locations.
For even small camera pose estimation errors, the projected keypoint location can jump past the object
of interest to the background. This is addressed by an adjustment approach that takes advantage of the
fact that our 3D keypoint positions are known for each estimated camera pose, and that we can easily
compare the depth values of the generated depth image and real depth image at the keypoint’s projected
pixel location to identify jump-edges. Speciﬁcally, we use our reconstructed model to project the keypoints
into the estimated camera frame. Any keypoint where the projected depth is less than the measured depth
at its projected pixel coordinates is deemed a jump edge. If a jump edge is detected, the reﬁned keypoint
location is updated to be the projection of the closest point in the real depth image to the estimated 3D
keypoint location.
Feature matching : For keypoints that correctly project to the desired object, 3D feature matching is em-
ployed to improve the keypoint location estimate. If a keypoint is clearly visible for the given camera pose,
then a 3D descriptor is extracted from the generated depth image and compared to descriptors extracted
in the true depth image around the estimated keypoint location. For this study, we use the FPFH feature
descriptor (Rusu et al., 2009), which was shown by (Choi et al., 2015) to be highly eﬀective at geometric
pairwise registration, but have also tested with the SHOT descriptor (Salti et al., 2014). The updated key-
point is deﬁned by a weighted sum of the Euclidean distance and the matched feature vector distances(with
weight values of 0.7 and 0.3, respectively) between the real and generated depth images. The closest match
in the real depth image is then chosen as the reﬁned keypoint location.
4.2.2 Object-level Reﬁnement
Since the goal of annotation reﬁnement is to maximize the alignment of the annotated model and an individ-
ual frame, regardless of any other camera pose estimates in the trajectory, we perform iterative closest point
(ICP) on the generated and real depth images to ensure the model is best aligned with each depth frame.
For this work, our primary concern is to align points belonging to the object of interest, so we limit our ICP
reﬁnement to the bounding volumes that contain the annotated objects. We use the volume that minimally
bounds the annotated keypoints to deﬁne the volume in the generated image, and since there may be error
in the pose estimate, the volume is dilated for the real depth image. Then, point-to-plane ICP (Chen and
Medioni, 1992) is performed on the cropped points from the generated and real point clouds derived from
Class Barrel Barrier Crate Gas Can Hedgehog Robot
Number of Keypoints 6 9 12 10 6 11
Table 1: Number of keypoints for each class in the RCTA Object Keypoints Dataset
the respective depth images. The camera pose estimate is adjusted based on the ICP alignment, and all
keypoints are transformed accordingly. This means that all keypoints undergo the same rigid transformation,
which preserves the relative poses of the keypoints, and allows us to reﬁne the positions of both visible and
occluded keypoints.
5 RCTA Object Keypoints Dataset
While large scale datasets such as PASCAL3D+ contain many types of objects, there are still many objects
and environments for which there is no corresponding dataset. This is especially true for the Robotics
Collaborative Technology Alliance (RCTA) program, which is a consortium of government, industry, and
academic researchers focusing on ﬁelding embodied autonomous systems in challenging and unstructured
environments. The scope of the work includes many unique objects that are not well-represented in existing
datasets, such as the Czech Hedgehog.
As part of this work, we have therefore created a dataset containing 101 sequences split across six of the
objects used by the RCTA (Narayanan et al., 2020) in both indoor and outdoor environments: barrel ,
barrier ,crate ,gas can ,Czech Hedgehog , and robot . The number of keypoints for each object class are shown
in Table 1.
We collected many data sequences for each object, not knowing ahead of time how many would yield
accurate 3D reconstructions. Indeed, many reconstructions did fail or were not accurate enough to include
in the analysis. To avoid adding the burden to a user of ﬁltering poor reconstructions manually, we defer
the determination of reconstruction viability to the actual annotation process and allow the annotator to
trivially skip a poor reconstruction. In total, we collected 283 sequences across the six object classes, and
determined that 101 were suitable for annotation, some of which are shown in Figure 4. For each object class,
a random subset of sequences and frames were annotated by hand. The total number of frames analyzed for
this work is 10281; of those, 951 were manually annotated.
With the public release of this dataset, we hope to increase diversity in the types of environments and
objects used for keypoint annotation. Furthermore, we release the raw data collection sequences as well as
the reconstructed models, with the hope of testing new reconstruction algorithms as they are released.
The full dataset is available for download at https://sites.google.com/view/
rcta-object-keypoints-dataset .
6 Results
We present an analysis of our architecture on existing benchmark datasets as well as custom data collections.
In both qualitative and quantitative output, we show the eﬀectiveness of the approach, and demonstrate the
challenges of acquiring accurate manual annotations for occluded keypoints."
2012.15115,D:\Database\arxiv\papers\2012.15115.pdf,"How does the performance of the proposed model change when using different numbers of retrieved tables, and what insights can be gained from this observation?","The model's performance improves as the number of retrieved tables increases, particularly when the TF-IDF retriever fails to identify the correct table. This suggests that the model can leverage correlations between tables to infer information even when the gold table is not explicitly present.","as an extension of the Table-BERT algorithm in-
troduced by Chen et al. (2020b) to the multi-table
setting, using an attention function to fuse together
the information from different tables.
4.1 Training & Testing
Relying on a closed-domain dataset provides a ta-
ble with appropriate information for answering
each query; namely, the table against which the
claim is to be checked in the closed setting. Al-
though this information is not available at test time,
we can construct a training regime that allows us
to exploit it to improve model performance. We ex-
periment with two different strategies: jointly mod-
eling reranking of tables along with veriﬁcation of
the claim, and modeling for each table a ternary
choice between indicating truth, falsehood, or giv-
ing no relevant information. Later, we demonstrate
how the former leads to increased performance on
veriﬁcation, while the latter gives access to a strong
predictor for cases where no appropriate table has
been retrieved.
Joint reranking and veriﬁcation For the joint
reranking and veriﬁcation approach, we assume
that a best table for answering each query is given
and can be used to learn a ranking function. We
model this as selecting the right table from Dq,
e.g., through a categorical variable sthat indicates
which table should be selected. We then learn a
joint probability of sand the truth value of the
claimvover the tables for a given query. Assuming
thatsandvare independent, p(s,v|q,Dq)is also
a categorical distribution with one correct outcome
that can be optimized for (that is, one correct pair
of table and truth value). As such, we let:
p(s,v|q,Dq) =σ(W(F∗(Dq)s)v) (5)
WhereW:R2n→R2is an MLP and σis the
softmax function. At train time, we obtain one
cross-entropy term corresponding to p(s,v|q,Dq)
per query. At test time, we marginalize over sto
obtain a ﬁnal truth value:
pv(v|q,Dq) =∑
t∈Dqp(v,s=t|q,Dq)(6)
This formulation has the additional beneﬁt of also
allowing us to make a prediction on which table
matches the query. We can do so by marginalizingoverv:
ps(s|q,Dq) =∑
vq∈{true,false}p(s,v=vq|q,Dq)
(7)
With this loss, we train the model by substituting
forDqa setD∗
qcontaining wherein the gold table is
guaranteed to appear. We ensure this by replacing
the lowest-scored retrieved table in Dqwith the
gold table whenever it has not been retrieved.
Ternary veriﬁcation At test time, there may be
cases where a table refuting or verifying the fact
is not contained in Dq. For some applications, it
could be useful to identify these cases. We there-
fore design an alternative variant of our system
better suited for this scenario. Intuitively, each ta-
ble can represent three outcomes – the query is true,
the query is false, or the table is irrelevant. We can
model this through a ternary variable isuch that
for tablet:
p(i|q,t,D q) =σ(W′(F∗(Dq)t)i) (8)
WhereW′:R2n→R3is an MLP and σis the
softmax function. During training, we assign true
orfalse to the gold table depending on the truth
of the query, and irrelevant to every other table.
We then use the mean cross-entropy over the tables
associated with each query as the loss for each
example. At test time, we compute the truth value
vof each query as:
∑
t∈Dqp(i=true|q,t)>∑
t∈Dqp(i=false|q,t)
(9)
5 Experiments
We apply our model to the TabFact dataset (Chen
et al., 2020b), which consists of 92,283 training,
12,792 validation and 12,792 test queries over
16,573 tables. The task is binary classiﬁcation of
claims as true or false, with an even proportion of
the two classes in each split. To benchmark our
open-domain models and construct performance
bounds, we begin by evaluating in the closed do-
main. As an upper bound, we can then compare
against the performance of the closed-domain sys-
tem scored using a single table retrieved through
an oracle. As a lower bound, we can again use
the closed-domain system, but using the highest-
ranked table according to our TF-IDF retriever. The
evaluation metric is simply prediction accuracy.
Retrieval method H@1 H@3 H@5 H@10
Query-matching word-level TF-IDF 41.7 54.2 59.0 65.3
Query-matching character-level (2,3)-gram TF-IDF 34.7 45.5 50.2 56.8
Entity-matching word-level exact match 48.2 57.9 64.2 67.3
Entity-matching word-level TF-IDF 56.0 65.6 74.1 81.2
Entity-matching character-level (2,3)-gram TF-IDF 69.6 78.8 82.3 86.6
Entity-matching character-level (1,2,3)-gram TF-IDF 62.3 75.2 80.1 86.1
Table 1: Retrieval accuracy for our entity-based TF-IDF retrieval along with several baselines for the TabFact
validation set, computed using all 16,573 TabFact tables. We experiment with matching the entire query against
table cells (above), and matching individual entities in the query against table cells using Equation 1 (below). For
all subsequent experiments we rely on character-level (2,3)-grams with entity-matching for retrieval.
Model Dev Test Simple Test Complex Test Small Test
Table-BERT (Chen et al., 2020b) 66.1 65.1 79.1 58.2 68.1
LogicalFactChecker (Zhong et al., 2020) 71.8 71.7 85.4 65.1 74.3
ProgVGAT (Yang et al., 2020) 74.9 74.4 88.3 67.6 76.2
TAPAS (Eisenschlos et al., 2020)* 81.0 81.0 92.3 75.6 83.9
Ours (Oracle retrieval) 78.2 77.6 88.9 72.1 79.4
Ours (1 retrieved table) 74.1 73.2 86.7 67.8 76.6
Ours (Ternary loss, 3 tables) 73.8 73.5 86.9 68.1 76.9
Ours (Ternary loss, 5 tables) 74.1 73.7 87.1 67.9 76.5
Ours (Ternary loss, 10 tables) 73.9 73.1 86.5 67.9 77.3
Ours (Joint loss, 3 tables) 74.6 73.8 87.0 68.3 78.1
Ours (Joint loss, 5 tables) 75.9 75.1 87.8 69.5 77.8
Ours (Joint loss, 10 tables) 73.9 73.8 86.9 68.1 76.9
Table 2: Prediction accuracy of our RoBERTa-based model on the ofﬁcial splits from the TabFact dataset. We
include closed-domain performance of several models from the literature, as well as the performance of our model
in both the closed and the open domain, using both proposed loss functions. The ﬁrst section of the table contains
closed-domain results, the second open-domain. * employs intermediary pretraining on additional synthetic data.
5.1 Retrieval
We choose bi- and tri-gram TF-IDF as the retrieval
strategy empirically. To address the comparative
performance of this choice, we compute and rank
in Table 1 the retrieval scores obtained through
our strategy on the TabFact test set. We compare
against several alternative strategies: bi- and tri-
gram TF-IDF vectors for all words in the query
(rather than just the entities), word-level TF-IDF
vectors for entities, and entity-level exact match-
ing. Our bi- and tri-gram TF-IDF strategy yields
by far the strongest performance. We furthermore
demonstrate how the exclusion of unigrams from
the TF-IDF vectors slightly increases performance.
5.2 Veriﬁcation
In Table 2, we compare our best-performing mod-
els to the closed-setting system from Chen et al.(2020b), as well as to several recent models from
the literature (Zhong et al., 2020; Yang et al., 2020;
Eisenschlos et al., 2020). We include results with
both losses as discussed in Section 4, using varying
numbers of tables.
With an accuracy of 75.1%, we obtain the best
open-domain results with our model using the joint
reranking-and-veriﬁcation loss and ﬁve tables. We
see performance improvements when increasing
the number of tables, both from 1 to 3 and from
3 to 5. In the closed domain, the 77.6%accuracy
our model achieves is a signiﬁcant improvement
over the 74.4%the strongest comparable baseline
reached. This may be due to our use of RoBERTa,
which has previously been found to perform well
for linearised tables (Gupta et al., 2020).
Relying purely on TF-IDF for retrieval — that
is, using our system with only one retrieved table
Model R@1 R@2-3 R@4-5
Oracle retrieval 80.6 74.1 75.0
1 table 80.6 55.6 53.9
3 tables 78.8 66.7 58.2
5 tables 79.4 73.1 71.7
Table 3: Peformance of our RoBERTa-based model on
the parts of the TabFact test set where our TF-IDF re-
triever assigns the gold table rank respectively 1, 2-3,
or 4-5.
— yields a performance of 73.2%. This is a sur-
prisingly small decrease compared to the closed
domain, given that an incorrect table is provided in
approximately a third of all cases (see Table 1). We
suspect that many cases for which the retriever fails
are also cases for which the closed-domain model
fails. To make sure we are not seeing the effect of
false negatives (e.g., tables which are not the gold
table, but which nevertheless have the information
to verify the claim), we run the model in a setting
where one retrieved table is used, but the gold ta-
ble is removed from the retrieval results; here, the
model achieves an accuracy of only 56.2%. We fur-
thermore test a system relying on a random table
rather than a retrieved table; with a performance
drop to 53.1, we ﬁnd that the information in the
retrieved table is indeed crucial to obtain high per-
formance (rather than the performance being purely
a consequence of, say, RoBERTa weights).
To understand how our model derives improve-
ment from the addition of more tables, we compute
in Table 3 the performance of our reranking-and-
veriﬁcation model when TF-IDF returns the correct
table at rank 1, rank 2-3, or rank 4-5. Immediately,
we notice a much stronger improvement from us-
ing multiple tables when TF-IDF fails to correctly
identify the gold table. This is natural, as those are
exactly the cases where our model (as opposed to
the baseline) has access to the appropriate informa-
tion to verify or refute the claim.
Interestingly, using three tables improves on us-
ing one table even when the gold table is not in-
cluded among the top three (from 53.9%to58.2%),
and using ﬁve tables improves on using three ta-
bles also when the gold table isincluded among
the top three (from 66.7%to73.1%). Manual in-
spection reveals that our model in some cases relies
on correlations between tables — if a sports team
loses games in three tables, then that may give aModel Accuracy
Full model 75.1
- Attention 73.6
- Joint objective 72.9
- Both 71.2
Table 4: Ablation study for our model, performing ver-
iﬁcation with the ﬁve-table version on the TabFact test
set. We remove respectively our cross-attention func-
tion, the reranking component in the loss, and both.
higher probability of that team also losing in an
unretrieved, hypothetical fourth table. To test this,
we apply the model in a setting where we retrieve
the top ﬁve tables excluding the gold table, and a
setting where we use ﬁve random tables. Using
highly scored (but wrong) tables, we achieve a per-
formance of 59.4%, a signiﬁcant improvement on
the53.1%we achieve using random tables. This
supports our hypothesis that other good tables can
provide useful background context for veriﬁcation.
It should be noted that such inferences, while
increasing model performance, may also increase
the degree to which the model exhibits biases. De-
pending on the application, this may as such not
be a desirable basis for veriﬁcation. Returning to
the example in Figure 1, inferring ownership on
the basis of political afﬁliation when no other in-
formation is available may increase accuracy on
average , but it can also lead to erroneous or biased
decisions (indeed, for the claim in the example, the
prediction would be wrong).
5.3 Ablation Tests
Our best-performing model from Table 2 relies
on two innovations: The cross-attention function
which contextualizes retrieved tables in relation to
each other, and the joint reranking-and-veriﬁcation
loss. In Table 4, we evaluate the model without
either of these. Leaving the attention function out
is simple — we use f(dk
q)for each table directly
for predictions. We model performance without
the reranking component of our loss function by
assuming a uniform distribution over the tables.
As can be seen, the combination of both is
strictly necessary to obtain strong performance
— indeed, without our joint objective, the model
performs worse than simply applying the baseline
model to the top table returned by TF-IDF as in
Table 2. The ability for the model to express the
relative relatedness of tables to the query is crucial.
0 0.20.40.60.8 10.80.850.90.951
RecallPrecision
Figure 3: Precision-recall curve for determining
whether a set of ﬁve retrieved tables in the TabFact vali-
dation set contains the gold table, using respectively en-
tropy of the reranking scores with our joint loss ( )
or the maximum probability of some table being the
gold table with our ternary loss, ( ). We also include
a most frequent class baseline ( ).
We include further investigation of the role our
cross-attention mechanism plays in Appendix E.
5.4 Predicting Insufﬁcient Information
In realistic settings, some claims will not be directly
answerable from any retrieved table. In such cases,
it can be valuable to explicitly inform the user —
giving false veriﬁcations or refutations when suf-
ﬁcient information is not available is misleading,
and can decrease user trust. To model a scenario
where the lack of relevant information must be de-
tected, we create a classiﬁcation task wherein the
model must predict for all examples, whether the
gold table is among the kdocuments in Dq.
Using the ternary loss, our model directly gives
the probability of each table containing appropri-
ate information as (1−p(It=irrelevant|q,t)).
We can estimate the suitability of the best retrieved
table for verifying the claim as max
t(1−p(It=
irrelevant|q,t)), and apply a threshold τ1to clas-
sifyDqas suitable or unsuitable. For the joint
loss, a more indirect approach is necessary. Intu-
itively, if our model is too uncertain about which
table answers the query, there is a high likelihood
that no suitable table has been retrieved. This cor-
responds to the entropy of the reranking compo-
nentHs(s|q,Dq)after marginalizing over the truth
value of the claim exceeding some threshold τ2.
We compare these strategies in Figure 3, obtain-
ing Precision-Recall curves by measuring at vary-
ingτ1andτ2. We ﬁnd that while both approachesoutperform a most frequent class baseline by a sig-
niﬁcant margin, the ternary loss performs better
than the joint loss. As such, the choice between
the two losses represents a tradeoff between raw
performance (see Tables 2 and 5) and the ability to
identify missing or incomplete information.
5.5 Wikipedia-scale Table Veriﬁcation
In our experiments so far, we have relied on the
16,573 TabFact tables as the knowledge source.
The tables selected for TabFact were taken from
WikiTables (Bhagavatula et al., 2013), and ﬁltered
so as to exclude “overly complicated and huge ta-
bles” (Chen et al., 2020b). Moving beyond the
scope of that dataset, a fully open fact veriﬁcation
system should be able to verify claims over even
larger collections of tables — for example, the full
set of tables available on Wikipedia. To make a
preliminary exploration of that larger-scale setting,
we include in Table 5 the performance of our ap-
proach evaluated using roughly 3 million tables
automatically extracted from Wikipedia.
Model Accuracy
RoBERTa only 52.1
Ours (1 table) 53.6
Ternary loss, 3 tables 55.8
Ternary loss, 5 tables 57.5
Joint loss, 3 tables 56.1
Joint loss, 5 tables 58.1
Table 5: Performance of our RoBERTa-based model on
the TabFact test set, using all Wikipedia tables rather
than just the TabFact tables as a backend.
As can be seen, our approach improves on the
naive strategy of using a single table and a closed-
domain veriﬁcation component also in this more
complex setting. To verify that the inference hap-
pens on the basis of the retrieved tables and not
simply the RoBERTa-weights, we include also the
performance of a model which simply uses clas-
siﬁcation on top of a RoBERTa-encoding of the
claim. Similar to our previous experiments, the
joint-loss model with ﬁve retrieved tables performs
the strongest. We note that it is unclear whether
the performance we observe here originates from
correlations obtained through background informa-
tion (as we see in Section 5.2 when the retriever
fails to ﬁnd the appropriate table), or due to veri-
ﬁcation against a single entirely appropriate table
happening at a lower rate than when using TabFact.
6 Related Work
Semantic querying against large collections of ta-
bles has previously been studied for question an-
swering. Sun et al. (2016) used string matching
between aliases of linked entities to search mil-
lions of tables crawled from the Web, with re-
trieved table cells providing evidence for a ques-
tion answering task. Jauhar et al. (2016) demon-
strated strong results with a Lucene index and a
Markov Logic Network-based model for answer-
ing scientiﬁc questions. Recently, Chakrabarti et al.
(2020a,b) developed an improved model for table
retrieval combining neural representations of the
table and the query with a BM25 index.
Cafarella et al. (2008, 2009) employed
keyphrase-based table retrieval by reranking a list
of tables returned by a search engine. Pimplikar
and Sarawagi (2012) used a graphical model to
perform retrieval on the basis of co-occurence
statistics, table metadata, and column headers. In
(Ghasemi-Gol and Szekely, 2018), non-parametric
clustering was employed as a strong heuristic for
table retrieval. Zhang and Balog (2018) introduced
a ranking method based on mapping available
features into several semantic spaces. Recently,
Zhang et al. (2019) introduced a neural method
for table retrieval and completion using word- and
entity-embeddings of table elements.
Neural modeling of tables has been the subject
of several recent papers. Aside from the original
BERT-based model in (Chen et al., 2020b), the clos-
est to our work is (Yin et al., 2020). In these paper,
a pretrained BERT-based encoder for tables is in-
troduced and demonstrated to yield strong improve-
ments on several semantic parsing tasks. Chen et al.
(2019) introduced a model to automatically predict
and compare column headers for tables in order to
ﬁnd semantically synonymous schema attributes.
Similarly, Zhang and Balog (2019) introduced an
autoencoder for predicting table relatedness.
Closed-domain semantic parsing over tables has
been studied extensively in the context of ques-
tion answering (e.g., Pasupat and Liang (2015);
Khashabi et al. (2016); Yu et al. (2018)). In Zhong
et al. (2020), a logic-based fact veriﬁcation system
was introduced to improve on the model presented
in the initial TabFact paper (Chen et al., 2020b).
Yang et al. (2020) builds on the program induc-
tion model also introduced in Chen et al. (2020b),
using a graph neural network to verify generated
programs. Orthogonally, a similar dataset for table-based natural language inference was introduced
by Gupta et al. (2020) — interestingly, like in our
experiments, they found RoBERTa-large to work
extremely well for linearised tables. Finally, Herzig
et al. (2020); Eisenschlos et al. (2020) introduced
BERT-based models for various table semantic
tasks, extending BERT with additional position
embeddings denoting columns and rows.
Open-domain fact veriﬁcation and question an-
swering over unstructured, textual data has been
studied in a series of recent papers. Early work re-
sulted in several highly sophisticated full pipeline
systems (Brill et al., 2002; Ferrucci et al., 2010;
Sun et al., 2015). These provided inspiration for
the inﬂuential DrQA model (Chen et al., 2017),
which like ours relies on a TF-IDF-based heuristic
retrieval model, and a complex reading model. Re-
cent work (Karpukhin et al., 2020; Lewis et al.,
2020) has built on this approach, developing
learned dense retrieval models with dot-product
indexing (Johnson et al., 2017), and increasingly
advanced pretrained transformer-models for read-
ing. The development of similarly fast, reliable and
learnable indexing techniques for tables as well as
text is an important direction for future work.
Concurrently with our work, Chen et al. (2020a)
have introduced a BERT-based model to perform
question answering over open collections of data
including tables. Like ours, their model consists
of separate retriever- and reader-steps. Their best-
performing reader employs a long-range sparse at-
tention transformer (Ainslie et al., 2020) to jointly
summarize all retrieved data. As in our case, their
model demonstrates signiﬁcant improvements from
using multiple retrieved tables.
7 Conclusion
We have introduced a novel model for fact veriﬁca-
tion over large collections of tables, along with two
strategies for exploiting closed-domain datasets to
increase performance. Our approach performs on
par with the current closed-domain state of the art,
with larger gains the more tables we include. When
using an oracle to retrieve a reference table, our ap-
proach also represents a new closed-domain state
of the art. Finally, we have made an initial foray
into Wikipedia-scale open-domain table fact veriﬁ-
cation, demonstrating improvements from multiple
tables also when using a full set of Wikipedia ta-
bles as the knowledge source. Our results indicate
that the use of multiple tables can provide contex-
tual clues to the model even when those tables do
not explicitly verify or refute the claim, because
they can provide evidence for the probability of the
claim. This is a double-edged sword, as reliance
on such clues can increase performance while also
inducing biased claims of truthfulness. Care will
be needed in future work to disentangle the positive
and negative aspects of this phenomenon.
Acknowledgments
We would like to thank Fabio Petroni and Nicola
De Cao for helpful discussions and comments.
References
Joshua Ainslie, Santiago Ontanon, Chris Alberti, Va-
clav Cvicek, Zachary Fisher, Philip Pham, Anirudh
Ravula, Sumit Sanghai, Qifan Wang, and Li Yang.
2020. ETC: Encoding long and structured inputs
in transformers. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP) , pages 268–284, Online. Asso-
ciation for Computational Linguistics.
Tariq Alhindi, Savvas Petridis, and Smaranda Mure-
san. 2018. Where is your evidence: Improving fact-
checking by justiﬁcation modeling. In Proceedings
of the First Workshop on Fact Extraction and VER-
iﬁcation (FEVER) , pages 85–90, Brussels, Belgium.
Association for Computational Linguistics.
Chandra Sekhar Bhagavatula, Thanapon Noraset, and
Doug Downey. 2013. Methods for exploring and
mining tables on wikipedia. In Proceedings of the
ACM SIGKDD Workshop on Interactive Data Explo-
ration and Analytics , pages 18–26.
Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
InProceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing , pages
632–642, Lisbon, Portugal. Association for Compu-
tational Linguistics.
Eric Brill, Susan Dumais, and Michele Banko. 2002.
An analysis of the AskMSR question-answering sys-
tem. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP 2002) , pages 257–264. Association for
Computational Linguistics.
Michael J Cafarella, Alon Halevy, and Nodira Khous-
sainova. 2009. Data integration for the rela-
tional web. Proceedings of the VLDB Endowment ,
2(1):1090–1101.
Michael J Cafarella, Alon Halevy, Daisy Zhe Wang, Eu-
gene Wu, and Yang Zhang. 2008. Webtables: ex-
ploring the power of tables on the web. Proceedings
of the VLDB Endowment , 1(1):538–549.Kaushik Chakrabarti, Zhimin Chen, Siamak Shakeri,
and Guihong Cao. 2020a. Open domain ques-
tion answering using web tables. arXiv preprint
arXiv:2001.03272 .
Kaushik Chakrabarti, Zhimin Chen, Siamak Shakeri,
Guihong Cao, and Surajit Chaudhuri. 2020b. Table-
qna: Answering list intent queries with web tables.
arXiv preprint arXiv:2001.04828 .
Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading Wikipedia to answer open-
domain questions. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 1870–
1879, Vancouver, Canada. Association for Computa-
tional Linguistics.
Jiaoyan Chen, Ernesto Jim ´enez-Ruiz, Ian Horrocks,
and Charles Sutton. 2019. Colnet: Embedding the
semantics of web tables for column type prediction.
InThe Thirty-Third AAAI Conference on Artiﬁcial
Intelligence, AAAI 2019, The Thirty-First Innova-
tive Applications of Artiﬁcial Intelligence Confer-
ence, IAAI 2019, The Ninth AAAI Symposium on Ed-
ucational Advances in Artiﬁcial Intelligence, EAAI
2019, Honolulu, Hawaii, USA, January 27 - Febru-
ary 1, 2019 , pages 29–36. AAAI Press.
Wenhu Chen, Ming-Wei Chang, Eva Schlinger,
William Wang, and William W Cohen. 2020a. Open
question answering over tables and text. arXiv
preprint arXiv:2010.10439 .
Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai
Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and
William Yang Wang. 2020b. Tabfact: A large-scale
dataset for table-based fact veriﬁcation. In 8th Inter-
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020 . OpenReview.net.
Sarah Cohen, Chengkai Li, Jun Yang, and Cong Yu.
2011. Computational journalism: A call to arms to
database researchers. In CIDR 2011, Fifth Biennial
Conference on Innovative Data Systems Research,
Asilomar, CA, USA, January 9-12, 2011, Online Pro-
ceedings , pages 148–151. www.cidrdb.org.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Machine Learning Challenges Work-
shop , pages 177–190. Springer.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Julian Eisenschlos, Syrine Krichene, and Thomas
M¨uller. 2020. Understanding tables with interme-
diate pre-training. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages
281–296, Online. Association for Computational
Linguistics.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, et al. 2010. Building watson: An overview
of the deepqa project. AI magazine , 31(3):59–79.
Terry Flew, Christina Spurgeon, Anna Daniel, and
Adam Swift. 2012. The promise of computational
journalism. Journalism Practice , 6(2):157–171.
Majid Ghasemi-Gol and Pedro Szekely. 2018. Tabvec:
Table vectors for classiﬁcation of web tables. arXiv
preprint arXiv:1802.06290 .
Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek
Srikumar. 2020. INFOTABS: Inference on tables as
semi-structured data. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics , pages 2309–2324, Online. Association
for Computational Linguistics.
Jonathan Herzig, Pawel Krzysztof Nowak, Thomas
M¨uller, Francesco Piccinno, and Julian Eisenschlos.
2020. TaPas: Weakly supervised table parsing via
pre-training. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics , pages 4320–4333, Online. Association for
Computational Linguistics.
Matthew Honnibal, Ines Montani, Soﬁe Van Lan-
deghem, and Adriane Boyd. 2020. spaCy:
Industrial-strength Natural Language Processing in
Python.
Gautier Izacard and Edouard Grave. 2020. Lever-
aging passage retrieval with generative models for
open domain question answering. arXiv preprint
arXiv:2007.01282 .
Sujay Kumar Jauhar, Peter Turney, and Eduard Hovy.
2016. Tables as semi-structured knowledge for ques-
tion answering. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 474–
483, Berlin, Germany. Association for Computa-
tional Linguistics.
Jeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.
Billion-scale similarity search with GPUs. arXiv
preprint arXiv:1702.08734 .
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for
open-domain question answering. In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP) , pages 6769–
6781, Online. Association for Computational Lin-
guistics.Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Pe-
ter Clark, Oren Etzioni, and Dan Roth. 2016. Ques-
tion answering via integer programming over semi-
structured knowledge. In Proceedings of the Twenty-
Fifth International Joint Conference on Artiﬁcial In-
telligence, IJCAI 2016, New York, NY, USA, 9-15
July 2016 , pages 1145–1152. IJCAI/AAAI Press.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings .
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
tus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich K ¨uttler, Mike Lewis, Wen-tau Yih,
Tim Rockt ¨aschel, Sebastian Riedel, and Douwe
Kiela. 2020. Retrieval-augmented generation for
knowledge-intensive NLP tasks. In Advances in
Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Sys-
tems 2020, NeurIPS 2020, December 6-12, 2020,
virtual .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Yixin Nie, Haonan Chen, and Mohit Bansal. 2019.
Combining fact extraction and veriﬁcation with neu-
ral semantic matching networks. In The Thirty-
Third AAAI Conference on Artiﬁcial Intelligence,
AAAI 2019, The Thirty-First Innovative Applications
of Artiﬁcial Intelligence Conference, IAAI 2019,
The Ninth AAAI Symposium on Educational Ad-
vances in Artiﬁcial Intelligence, EAAI 2019, Hon-
olulu, Hawaii, USA, January 27 - February 1, 2019 ,
pages 6859–6866. AAAI Press.
Panupong Pasupat and Percy Liang. 2015. Compo-
sitional semantic parsing on semi-structured tables.
InProceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers) , pages
1470–1480, Beijing, China. Association for Compu-
tational Linguistics.
Rakesh Pimplikar and Sunita Sarawagi. 2012. Answer-
ing table queries on the web using column keywords.
Proceedings of the VLDB Endowment , 5(10):908–
919.
Huan Sun, Hao Ma, Xiaodong He, Wen-tau Yih, Yu Su,
and Xifeng Yan. 2016. Table cell search for question
answering. In Proceedings of the 25th International
Conference on World Wide Web, WWW 2016, Mon-
treal, Canada, April 11 - 15, 2016 , pages 771–782.
ACM.
Huan Sun, Hao Ma, Wen-tau Yih, Chen-Tse Tsai,
Jingjing Liu, and Ming-Wei Chang. 2015. Open do-
main question answering via semantic enrichment.
InProceedings of the 24th International Conference
on World Wide Web, WWW 2015, Florence, Italy,
May 18-22, 2015 , pages 1045–1055. ACM.
James Thorne and Andreas Vlachos. 2018. Automated
fact checking: Task formulations, methods and fu-
ture directions. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics ,
pages 3346–3359, Santa Fe, New Mexico, USA. As-
sociation for Computational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-
9, 2017, Long Beach, CA, USA , pages 5998–6008.
Andreas Vlachos and Sebastian Riedel. 2014. Fact
checking: Task deﬁnition and dataset construction.
InProceedings of the ACL 2014 Workshop on Lan-
guage Technologies and Computational Social Sci-
ence, pages 18–22, Baltimore, MD, USA. Associa-
tion for Computational Linguistics.
Xiaoyu Yang, Feng Nie, Yufei Feng, Quan Liu, Zhi-
gang Chen, and Xiaodan Zhu. 2020. Program en-
hanced fact veriﬁcation with verbalization and graph
attention network. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP) , pages 7810–7825, Online. As-
sociation for Computational Linguistics.
Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-
bastian Riedel. 2020. TaBERT: Pretraining for joint
understanding of textual and tabular data. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 8413–
8426, Online. Association for Computational Lin-
guistics.
Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li,
Qingning Yao, Shanelle Roman, Zilin Zhang,
and Dragomir Radev. 2018. Spider: A large-
scale human-labeled dataset for complex and cross-
domain semantic parsing and text-to-SQL task. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing , pages
3911–3921, Brussels, Belgium. Association for
Computational Linguistics.
Li Zhang, Shuo Zhang, and Krisztian Balog. 2019. Ta-
ble2vec: Neural word and entity embeddings for ta-
ble population and retrieval. In Proceedings of the
42nd International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
SIGIR 2019, Paris, France, July 21-25, 2019 , pages
1029–1032. ACM.
Shuo Zhang and Krisztian Balog. 2018. Ad hoc table
retrieval using semantic similarity. In Proceedings
of the 2018 World Wide Web Conference on World
Wide Web, WWW 2018, Lyon, France, April 23-27,
2018 , pages 1553–1562. ACM.Shuo Zhang and Krisztian Balog. 2019. Auto-
completion for data cells in relational tables. In Pro-
ceedings of the 28th ACM International Conference
on Information and Knowledge Management, CIKM
2019, Beijing, China, November 3-7, 2019 , pages
761–770. ACM.
Wanjun Zhong, Duyu Tang, Zhangyin Feng, Nan
Duan, Ming Zhou, Ming Gong, Linjun Shou, Daxin
Jiang, Jiahai Wang, and Jian Yin. 2020. Logical-
FactChecker: Leveraging logical operations for fact
checking with graph module network. In Proceed-
ings of the 58th Annual Meeting of the Association
for Computational Linguistics , pages 6053–6065,
Online. Association for Computational Linguistics.
A Performance using RoBERTa-base
In our paper, we have reported results using the
larger version of RoBERTa with additional hidden
layers and greater dimensionality. Liu et al. (2019)
also include a smaller version, RoBERTa-base, cor-
responding to the BERT-base model of Devlin et al.
(2019). In Table 6, we report results correspond-
ing to those of Table 2 for our joint model using
RoBERTa-base instead of RoBERTa-large.
Interestingly, the performance gain from using
multiple tables is even larger for RoBERTa-base
(an increase of 4.1rather than 1.9point accuracy
from using ﬁve tables, for example). One ex-
planation could be that some information neces-
sary to verify certain facts may be encoded in the
weights of the larger RoBERTa. We attempt to in-
vestigate this using random tables; however, with
ﬁve random tables, RoBERTa-large and RoBERTa-
base reach an almost equivalent respective perfor-
mance of 52.1and52.0. As such, we believe that
RoBERTa-large exploits the correlations between
tables which we discuss in Section 5.2 better than
RoBERTa-base.
B Hyperparameters
Our model uses RoBERTa (Liu et al., 2019) to
encode each table into vectors. On top of RoBERTa
we employ key-value self-attention (Vaswani et al.,
2017) with two attention heads. We then use an
MLP consisting of a linear transformation to h=
3072 hidden units, followed by tanh -activation
and linear projection to the output space. During
training, we employ dropouts with probability 0.1
before each linear transformation in the MLP. The
hyperparameters for all experiments were selected
using the TabFact development set (with TabFact
tables as the backend)."
2004.11245,D:\Database\arxiv\papers\2004.11245.pdf,"The research explores the effectiveness of a method for transferring knowledge between domains with different data distributions.  What are the potential limitations of this approach when applied to real-world scenarios, particularly in the context of the specific domain addressed in the paper?","The method's reliance on a cycle-consistent generative adversarial network (CycleGAN) for domain adaptation might be hindered by the complexity and variability of real-world remote sensing data, potentially leading to inaccuracies in the generated images and impacting the overall performance of the knowledge transfer.","Fig. 3 . On the left, diagram of the HDAsource method. In the middle, diagram of the HDAtarget method. On the right,
diagram of the HDAfull method. This diagrams are showing the origin of the data use for the ﬁnal classiﬁer.
#nytbaseline HDAsource HDAtarget HDAfull
650 77.00 71.25 80.00 75.00
325 70.75 70.00 77.00 72.00
130 64.00 50.75 70.00 66.50
65 62.00 43.00 68.50 64.25
1 19.00 32.25 66.75 62.00
0 - 30.75 65.75 57.50
Table 1 . Rate of accuracy scores for classiﬁcation on Eu-
roSAT dataset
main, theCsclassiﬁer is very good, and helps in training a
performantGt2stransformation. A contrario, the transfor-
mation from source to target is less performant, and when
combined in the HDAfull setting, it only worsen the classiﬁ-
cation accuracy. Also, the best performance 66% in the fully
unsupervised setting is very good (only 11 points behind the
golden score (77%), which indicates that our method is capa-
ble of transferring knowledge even in the absence of labelled
samples in the target domain.
4. CONCLUSIONS
In this paper, we presented a novel approach for classiﬁcation
with heterogeneous domain adaptation using a cycle GAN
based approach. Our contribution lies in the addition of spe-
ciﬁc classiﬁcation and metric alignment losses, that also helps
in the generation process. Experimental results on a Land Use
classiﬁcation problem involving very different remote sens-
ing images indicate the power of our method, in both semi-
supervised and unsupervised settings. Future works will con-
sider ablation studies to fully undertstand the role of each
loss independently, and compare with existing unsupervised
or semi-supervised deep HDA methods.
5. ACKNOWLEDGEMENT
This work was supported by Centre National d’ ´Etudes Spa-
tiales (CNES) and Thales Alenia Space.6. REFERENCES
[1] Yuguang Yan, Wen Li, Hanrui Wu, Huaqing Min, Mingkui Tan, and
Qingyao Wu, “Semi-supervised optimal transport for heterogeneous
domain adaptation.,” in IJCAI , 2018, pp. 2969–2975.
[2] Wen Li, Lixin Duan, Dong Xu, and Ivor W Tsang, “Learning with
augmented features for supervised and semi-supervised heterogeneous
domain adaptation,” IEEE transactions on pattern analysis and ma-
chine intelligence , vol. 36, no. 6, pp. 1134–1148, 2013.
[3] Yao-Hung Hubert Tsai, Yi-Ren Yeh, and Yu-Chiang Frank Wang,
“Learning cross-domain landmarks for heterogeneous domain adapta-
tion,” in Proceedings of the IEEE conference on computer vision and
pattern recognition , 2016, pp. 5081–5090.
[4] Yuguang Yan, Wen Li, Michael KP Ng, Mingkui Tan, Hanrui Wu,
Huaqing Min, and Qingyao Wu, “Learning discriminative correlation
subspace for heterogeneous domain adaptation.,” in IJCAI , 2017, pp.
3252–3258.
[5] Anoop Cherian and Alan Sullivan, “Sem-gan: Semantically-consistent
image-to-image translation,” in 2019 IEEE Winter Conference on Ap-
plications of Computer Vision (WACV) . IEEE, 2019, pp. 1797–1806.
[6] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros, “Un-
paired image-to-image translation using cycle-consistent adversarial
networks,” in Proceedings of the IEEE international conference on
computer vision , 2017, pp. 2223–2232.
[7] Paolo Russo, Fabio M Carlucci, Tatiana Tommasi, and Barbara Ca-
puto, “From source to target and back: symmetric bi-directional adap-
tive gan,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 2018, pp. 8099–8108.
[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio,
“Generative adversarial nets,” in Advances in neural information pro-
cessing systems , 2014, pp. 2672–2680.
[9] Emmanuel de B ´ezenac, Ibrahim Ayed, and Patrick Gallinari, “Optimal
unsupervised domain translation,” CoRR , vol. abs/1906.01292, 2019.
[10] Gong Cheng, Junwei Han, and Xiaoqiang Lu, “Remote sensing image
scene classiﬁcation: Benchmark and state of the art,” Proceedings of
the IEEE , vol. 105, no. 10, pp. 1865–1883, 2017.
[11] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth,
“Eurosat: A novel dataset and deep learning benchmark for land use
and land cover classiﬁcation,” arXiv preprint arXiv:1709.00029 , 2017.
[12] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth,
“Introducing eurosat: A novel dataset and deep learning benchmark for
land use and land cover classiﬁcation,” in IGARSS 2018-2018 IEEE In-
ternational Geoscience and Remote Sensing Symposium . IEEE, 2018,
pp. 204–207.
[13] Christian Ledig, Lucas Theis, Ferenc Husz ´ar, Jose Caballero, Andrew
Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Jo-
hannes Totz, Zehan Wang, et al., “Photo-realistic single image super-
resolution using a generative adversarial network,” in Proceedings of
the IEEE conference on computer vision and pattern recognition , 2017,
pp. 4681–4690."
2006.12634,D:\Database\arxiv\papers\2006.12634.pdf,"While the paper highlights the potential of the dataset for various applications, what specific challenges might arise when applying adversarial training techniques to this dataset, considering its unique characteristics?","The dataset's fine-grained nature and large number of categories pose challenges for adversarial training, as the visual differences between classes are subtle, making it difficult to distinguish between genuine and adversarial examples.","in our benchmark experiment (Sec. 4) we focus on the prod-
uct type meta category.
Data collections. The data collection and annotation
pipeline is summarized in Fig. 3. The shelf images were
collected in 10 cities from 500 different stores. The data col-
lectors were instructed to make sure the shelf is positioned in
the center of the image, and each image should only contain
one shelf. The average resolution of shelf image is 3024 by
4032 pixels. The image collectors use different (smartphone)
cameras to capture the photos under natural in-store lighting
environments, mimicking the realistic application scenarios.
We also make sure each individual object in the collected
images is at least 80 by 80 pixels and clear enough to be
recognized by human eyes.
Before we annotate the collected data, we ﬁrst run a pre-
annotation step to reduce the human workload. We train a
object detector on our auxiliary object detection dataset that
can identify the 7 shapes we mentioned previously. We use
RetinaNet [ 23] as the base structure of the object detector.
The object detector evaluate all collected shelf images, and
propose bounding boxes for potential object of interests on
the shelf images.
Then, we sent the pre-annotated shelf images to our hu-
man annotators. The annotators will ﬁrst inspect the bound-
ing boxes on the shelf images, i.e., removing all unwanted
bounding boxes (e.g., overlapped box, occluded box and
empty boxes) as well as adding missing boxes. After all
bounding boxes have been validated, the annotator label
each bounding boxes with its meta category, SKU ID and
other attributes.
Statistics. We collect 10,385 high-resolution shelf images
in total, with, on average, 37.1 objects in each images. The
dataset contains in total 384,311 images of individual ob-
jects. Each individual object image represents a product from
the 2388 SKUs. We split the train/test set by the ratio of
0.9/0.1. The detailed statistics for different meta categories
are represented in Tab. 1.
Visualizations. To understand the data distribution of our
dataset qualitatively, we use UMAP [ 27] to visualize our
dataset. Compare to other popular visualization techniques
like t-SNE, UMAP is considered to be fast and scales better
on high dimensional data. We resize all our images to 32x32
before we send it to UMAP.
Fig. 6 depict the visualization results. We found that
most of the data are well separated when categorized by its
shape. While some data points, especially those belong to
category with relatively few images (e.g., cosmetics, liquors),
are tend to spread across the data manifold. In addition,
when analyzing data in one meta category, for example,
tobacco, we found that data with different SKU id do not
separated well with each other. This reﬂects the coarse-to-
ﬁne nature of our dataset that the products from the different
meta categories usually have distinguishable visual features,
Bottle Can Box Bag
Jar Handled Bottle Pack
Dairy Liquor Beer Cosmetics
Drinks Seasoning Tobacco
Figure 4: Sample images from our dataset with different meta-
categorizations. (top) Categorized by product types. (bottom)
Categorized by product shapes.
but products from the same meta category can look very
similar even they are different products.
Auxiliary detection dataset. A full retail product recog-
nition pipeline usually consists of two seperate parts—an
object detector for locating the potential objects of interests
and a classiﬁer to recognize the object from the detected
SKU ID: 746
Size: 250mLType: Drinks
Brand: Starbucks
Flavor: Latte
Shape: CanSKU ID: 748
Size: 250mLType: Drinks
Brand: Starbucks
Flavor: MatchaShape: Can
SKU ID: 450
Size: 1.8LType: Cosmetics
Brand: OMO
Flavor: N/AShape: Handled Bottle
SKU ID: 1049
Size: 1LType: Dairy
Brand: Yili
Flavor: OriginalShape: Box
Figure 5: Sample data with attributes information.
4
Meta Category SKUs Train Imgs. Test Imgs. Total Imgs. Imgs./SKUBy ProductDairies 323 78,288 8,867 87,155 269.8
Liquors 167 16,753 1,939 18,692 111.9
Beers 237 39,786 4,540 44,326 187.0
Cosmetics 256 7,393 932 8,325 32.5
Non-Alcohol Drinks 333 29,241 3,405 32,646 98.0
Seasonings 560 137,082 15,479 152,561 272.4
Tobacco 512 36,311 4,295 40,606 79.3By ShapeBottle 852 164,939 18,327 183,266 215.1
Can 267 44,461 4,940 49,401 185.0
Box 411 27,347 3,039 30,386 73.9
Bag 198 15,350 1,705 17,055 86.1
Jar 246 13,913 2,657 16,570 67.35
Handled Bottle 251 54,895 6,099 60,994 243.0
Pack 163 23,949 2,690 26,639 163.4
Total 2,388 344,854 39,457 384,311 160.9
Table 1: Statistics of our dataset.
seasoning
(a) (b)diary beer tobacco
drink liquor cosmeticsSKU2011 SKU2012 SKU2013 SKU2014
SKU2015 SKU2016 SKU2017 other
Figure 6: Visualization of data distribution via UMAP. (a)Visualization for all data categorized by its meta category. (b)Visualization for
category tobacco. Here we only colorize SKUs with id from 2011 to 2017, while leaving all other points black.
objects of interests. To encourage the development of a
full pipeline, we also provide an auxiliary object detection
dataset that contains human-annotated bounding boxes of
product in shelf images. Since the object detection task has
been addressed by many of the previous dataset [ 12,43,10],
we can further combine the data from previous work with our
auxiliary detection dataset to build a larger scale detection
dataset. Even without combining our data with other dataset,
our pre-annotation practice indicate our auxiliary detection
dataset is sufﬁcient large for training a robust object detector.
For the detailed statistics, we include 95,800 bounding
boxes from 1400 shelf images, labeled with 7 different
shapes described in Fig. 4. We want to emphasize that theobject detection task for retail shelf has been speciﬁcally
targeted by many other dataset, thus we do not target our
dataset as an object detection dataset. Here we provide the
detection dataset for completeness.
4. Benchmarking our dataset
In this section, we provide our evaluations to benchmark-
ing our RP2K dataset. We demonstrate our results on the
classiﬁcation task. Yet, our dataset is not limited only to the
classiﬁcation task. We will provide a further discussion on
other potential research problem of our dataset in Sec. 5.
5
4.1. Evaluation of different classiﬁcation algorithm
We evaluate a number different classiﬁcation methods on
our dataset, including two standard classiﬁcation networks
(ResNet-34 [ 15] and InceptionV3 [ 40]), as well as two
state-of-the-art ﬁne-grained classiﬁcation methods (class-
balanced loss [ 6] and API-Net [ 47]). For both class-balanced
loss (CBL) and API-Net we use ResNet-34 as its backbone
network, and we use the default parameters and settings of
their original implementation. All experiments were done in
Pytorch [30] framework and trained by Stochastic Gradient
Descent (SGD) on a single NVIDIA Tesla V100 GPU. We
set the learning rate to 0.1 and the momentum to 0.9, while
leaving other parameters to Pytorch default value. After
every 50 epochs the learning rate will decay to 10% of its
original value, and we train each network 150 epochs. For
each model, it generate one prediction from 2388 classes,
and report both top-1 accuracies and top-5 accuracies among
7 product type meta categories. The results are summarized
in Tab. 2.
Results. Although CBL and API-Net reportedly achieves
the state-of-the-art results on multiple ﬁne-grained classiﬁ-
cation dataset like CUB-200-2011 and Stanford Cars, they
did not surpass even a simple ResNet-34 network on RP2K
dataset in terms of the recognition accuracy. In addition, as
we mentioned previously, a classiﬁcation system with 95%
accuracy may considered to be very good from the computer
vision perspective, but not enough for industrial level retail
application. This indicate that there are still some room for
improving the recognition accuracy.
Besides, we found that although ResNet-34 achieves an
overall accuracy 95%, there are some categories like liquors
and cosmetics have much lower accuracy than average. This
is because 1) the number of training data of those categories
are relatively smaller than others; 2) the appearance of prod-
ucts in those categories usually look very similar.
4.2. Evaluation of different training scheme
Since images in our dataset are captured under diverse
view angles and lighting conditions, using data augmentation
during training presumably would increase the classiﬁcation
performance. In addition, we are also interested in evaluating
the performance between using a (ImageNet) pre-trained
model and training from scratch.
Thus, we further evaluate the ResNet-34 using 4 different
training schemes: training from scratch, pre-training, train-
ing from scratch with data augmentation, and pre-training
with data augmentation. For all training scheme we use the
SGD optimizer.
To mimic the real scenario environment, we use the fol-
lowing data augmentation scheme in our training:
•Adding a constant border with a width randomly chosen
between 0 and 30px.• Cropping randomly by up to 10px.
•Applying a randomly-parameterized perspective trans-
formation.
•Darkening/Brightening the image ramdomly by up to
20%.
The results of evaluating 4 training protocols are shown in
Tab. 3.
Besides reporting classiﬁcation accuracy for each meta
category, we also plot the relationship between the number
of instance image and classiﬁcation accuracy for each indi-
vidual instance. To analyze this relationship, we ﬁrst sort
our entire 2388 product by its instance count, and then group
each 10 nearby products. Then we calculate the average
top-1 classiﬁcation accuracy for each group. As shown in
Fig. 7, the prediction accuracy is decreasing with the number
of instances available in the dataset, and the variance of the
accuracy increased. This suggest that future research should
focus on improving the prediction accuracy for the ""long
tail"" part, since total accuracy could be high due to instances
with abundant training data usually have high prediction per-
formance while contribute to the statistics more. In contrast,
the long tail part may still have low accuracies.
4000
3000
2000
1000
0Instance Count
1
Product Category IDs500 1000 2000100%
80%
60%
40%Top-1 Accuracy
1500
Figure 7: Long tail problem in ﬁne-grained recognition. With the
decreased number of available images, the recognition accuracy
tends to decrease.
4.3. Auxiliary Detection Dataset
Here we provide the evaluation on the auxiliary detec-
tion dataset. We split the 95,800 bounding boxes to 80,000
training boxes and 15,800 test boxes. We use the standard
RetinaNet [ 23] to evaluate the detection performance. We
report the Average Precision (AP) for each shape, as well as
mean Average Precision (mAP) for the entire test set, with
different IoU threshold, the results are summarized in Tab. 4.
5. Other potential research problems
Besides the classiﬁcation and detection tasks, there are
many other potential research problems that can be explored
on our dataset. We list a few of them in this section.
Adversarial attacks and defenses. Adversarial attacks re-
fer to adding deliberately crafted, imperceptible noise to
6
Meta Category # imagesResNet-34 InceptionV3 CBL API-Net
Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5
Dairies 87,155 93.68% 98.45% 93.38% 98.31% 83.90% 94.26% 54.30% 80.80%
Liquors 18,692 76.99% 96.44% 79.57% 97.31% 61.88% 77.51% 96.47% 99.33%
Beers 44,326 96.76% 99.16% 95.19% 98.98% 90.52% 95.85% 97.79% 99.60%
Cosmetics 8,325 87.87% 93.71% 86.58% 96.56% 50.75% 65.23% 93.88% 99.03%
Non-Alcohol Drinks 32,464 94.86% 98.88% 93.65% 98.53% 80.52% 90.57% 96.91% 99.38%
Seasonings 152,561 97.96% 99.56% 97.28% 99.53% 93.44% 97.64% 98.71% 99.86%
Tobacco 40,606 91.89% 98.06% 91.73% 97.88% 78.41% 85.44% 93.15% 98.76%
All 384,311 95.18% 99.01% 94.69% 98.96% 87.66% 93.79% 95.34% 98.65%
Table 2: Classiﬁcation results for different categories. # images indicates the number of total images available in the corresponding
meta-category. The simple ResNet-34 network achieves a very similar performance as API-Net, which is the state-of-the-art method on
many ﬁne-grained classiﬁcation task.
Training Method Top-1 Acc. Top-5 Acc.
Scratch 95.18% 99.01%
Pre-train 95.54% 99.04%
Scratch + Augmentation 90.41% 94.74%
Pre-train + Augmentation 90.89% 95.01%
Table 3: Evaluation of different training protocols. We found
that adding data augmentation does not necessarily increase the
recognition performance. We hypothesize it is due to our dataset
already including lighting variance and camera distortion, so that
further augmenting data does not make our dataset closer to the
real image.
natural images, aiming to mislead the network’s decision
entirely [ 13]. Adversarial attacks pose serious threats to
numerous machine learning applications, from autonomous
driving to face recognition authorization.
To launch adversarial attacks, the adversary needs to solve
the optimization problem:
max
δxL(f(x+δx),y),
wherefis the targeted neural network, xis the input data,
δxis the applied perturbation, yis the ground truth label for
xandLis the surrogate loss function for training f.
To solve this non-convex optimization problem, one sim-
ple yet effective way is to use Projected Gradient Descent
(PGD) [26] via the following iteration:
δxn+1= Πδx∈∆ϵδxn+αsign∇δxL(f(x+δxn),y),
where,αis the step size, ∆ϵis the allowed perturbation
range and Πis a projection operator that projects δxto its
constraint space ∆ϵ. Here we consider the ℓ∞threat model,
i.e.,δx∈∆ϵif and only if ||δx||∞<ϵ.
To defend such adversarial examples, the most recog-
nized way is through adversarial training [ 26]: training onthe adversarial examples generated at each training epoch.
This method has been shown to be effective on many popular
datasets. Motivated by this, a series of adversarial attacks
and defenses have been proposed, which advance the under-
standing of adversarial examples [4, 37, 45, 41].
However, most of the existing defense methods are evalu-
ated on standard benchmarks such as MNIST, CIFAR-10 or
ImageNet. In those datasets, the visual difference between
images from different classes are often large. In contrast, due
to the ﬁne-grained characteristics, two images from different
classes in our dataset could hold very similar visual features.
Besides, the total number of categories in our dataset (2388)
is also considerably higher than traditional benchmarks like
CIFAR-10 (10) or ImageNet (1000). These two factors could
yield a much more challenging problem in the adversarial
defense task.
To better understand the property of our dataset under
the adversarial learning perspective, we run experiments to
evaluate the accuracy of models (with or without adversarial
training) under adversarial attacks. Here we use standard
training protocol [ 26] for adversarial training and PGD to
generate adversarial examples. We evaluate it under ℓ∞
threat model with perturbation size ϵ= 4/255and8/255.
As shown in Tab. 5, adversarial training not only drasti-
cally decreases the accuracy on standard test images, but also
shows little effect on defending adversarial examples. Given
the failure of the most popular defense method, we believe
our dataset could also serve as an alternate benchmark for
evaluating adversarial defense and attack algorithms.
Generative models on structured images. Image synthe-
sis has achieved remarkable progress in recent years with the
emergence of various generative models [ 32,1,46,29,18,
20]. The state-of-the-art generative models are capable of
generating realistic high-resolution images of many distinct
objects and scenes.
Despite the success in generating natural images, generat-
ing images with structured layouts remains challenging, as
7
Shape AP(0.5) AP(0.55) AP(0.6) AP(0.65) AP(0.7) AP(0.75) AP(0.8)
Box 0.3485 0.3475 0.344 0.3372 0.3195 0.3003 0.2670
Can 0.6886 0.6868 0.6837 0.6811 0.6766 0.6650 0.6314
Bottle 0.7525 0.7509 0.7487 0.7448 0.7365 0.7137 0.6545
Jar 0.2620 0.2559 0.235 0.2302 0.2191 0.1921 0.1748
Handled Bottle 0.4919 0.4896 0.4658 0.4505 0.4014 0.3804 0.3059
Bag 0.3449 0.338 0.3273 0.3151 0.2827 0.2518 0.1901
Pack 0.4643 0.4634 0.4634 0.4634 0.4634 0.4494 0.3789
mAP for All 0.6186 0.6164 0.6121 0.6069 0.5962 0.5739 0.3994
Table 4: Performance of object detection. AP( x) indicates AP with IoU threshold= x.
Model Clean Acc. Adv. Acc. ( ϵ= 4/255) Adv. Acc. ( ϵ= 8/255)
Regular 95.18% 0.00% 0.00%
Adv. Train ( ϵ= 4/255) 40.14% 12.35% 0.00%
Adv. Train ( ϵ= 8/255) 20.90% 11.74% 5.26%
Table 5: Accuracies on RP2K under Adversarial attacks. We use 3 different training scheme to train a ResNet-34 base model: regular
training, adversarial training with ϵ= 4/255and adversarial training with ϵ= 8/255. And we evaluate their clean accuracy, adversarial
accuracy with ϵ= 4/255and adversarial accuracy with ϵ= 8/255. When evaluating on standard test images, models with adversarial
training shows drastically decreased accuracy compare to regular trained model. When evaluating on adversarial test images, regular
model can always be fooled by adversarial attacks, while adversarial trained models only show little effectiveness for defending adversarial
examples.
pointed out by many recent work [ 31,16,39]. Our original
shelf image dataset could be a practical dataset for evaluating
generative models on structured image synthesis. The bound-
ing box combined with SKU labels in our dataset provides
the ground truth of semantic layout information. Once robust
generative models are developed, it could also facilitate to
pave a way for generating more data for the tasks of shelf
objects detection and recognition.
Few-shot learning. The ability to learn from a few ex-
amples remains a challenge for modern machine learning
systems. This problem has received signiﬁcant attention
from the machine learning community [ 9,33,38,7,36].
According to Fig. 7, the long-tail effect of our character-
istics provides more than 100 classes with the number of
instance images less than 30. Thus, our dataset could also
serve the purpose of few-shot learning algorithm evalua-
tion. Moreover, the large number of categories reside in our
dataset enables a broader range of choices to evaluate the
algorithm. Besides the research value of few-shot learning in
our dataset, few-shot learning is also important in the retail
industry since some products may only have few placed in a
retail store.
6. Discussion: applications in retail
A robust retail object recognition system is desired in
a myriad of applications in AI-powered retail store. It is
foreseeable that our dataset can be served as a starting point
for any retail applications using shelf images. Future appli-
cations based on retail object recognition system include butnot limit to:
Auditing product placement. Auditing shelf manage-
ment using object recognition system plays an essential role
in understanding the shelf conditions. On the one hand, cos-
tumers may mistakenly place products they don’t want to
buy on the wrong shelf. Identifying the wrong placement re-
quires a lot of human resources in traditional retail store, but
it can be immediately detected by computer system. On the
other hand, analysis show that customer make important buy-
ing decisions based on the placement of store shelves [ 22],
and retailers invest a lot to create ideal planograms as their
selling strategy. It is crucial to make sure that the products
are placed in their desired way.
Detecting empty shelf. Empty shelf is an indicator of
whether a product is out-of-stock. In many cases, the re-
tailers have more available stocks in their warehouse even
their store shelves show out-of-stock. Before the retailer
reﬁll the stocks, it even takes a certain amount of time for the
retailer to realize the product is out-of-stock. With a robust
recognition deployed in the store, the store can quickly send
notiﬁcation of which product is out-of-stock and tracking
the selling activities in real-time.
Image-based product retrieval. Not only for the retailer,
the costumer could also beneﬁt from a well-established
in-store object recognition system. Imagine you want to
search information for a particular product in-store, instead
of Googling the product name on web, you can take a photo
of the product and the information will immediately prompt
8
out. Moreover, based on the image, your location infor-
mation could also be retrieved, rendering a image-based
navigation system. This could increase the accessibility of
the retail store.
7. Conclusion
We introduce a new ﬁne-grained retail recognition dataset,
RP2K, which contains a large number of retail product im-
ages from 2388 different products.
Our dataset is inspired by the task of retail product recog-
nition on store shelves, which has tremendous applications
on AI-powered retail industry—image-based product re-
trieval, empty shelf detection, and sales activity tracking,
to name a few. As a ﬁne-grained classiﬁcation dataset, our
dataset has the so far largest amount of categories, while
maintaining a sufﬁcient amount of images. In addition to the
pre-deﬁned meta-categories, we also provide rich attributes
information that allows the user to adjust the ﬁne-grained
level for their evaluations.
To bridge the gap between research and real-life applica-
tions, our data are all collected in natural retail store envi-
ronments. Our experiments show that, on our dataset, even
the state-of-the-art ﬁne-grained classiﬁcation method can-
not achieve signiﬁcantly better results than a simple ResNet
baseline, indicating that there still exist large room for im-
provement.
Besides object recognition, our dataset could also be used
for other important computer vision tasks such as adversarial
learning and generative model. We believe our dataset could
further progress the revolution that is already occurring in
the retail industry, and lead to a step toward building a fully
autonomous retail store.
References
[1]Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasser-
stein generative adversarial networks. In International Con-
ference on Machine Learning , pages 214–223, 2017. 7
[2]Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L
Alexander, David W Jacobs, and Peter N Belhumeur. Bird-
snap: Large-scale ﬁne-grained visual categorization of birds.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 2011–2018, 2014. 2
[3]J Burianek, A Ahmadyfard, and J Kittler. Soil-47, the sur-
rey object image library, centre for vision, speach and sig-
nal processing, univerisity of surrey. J. Kittler.[Electronic
resource].–Mode of access http://www. ee. surrey. ac.
uk/CVSSP/demos/colour/soil47 , 2002. 2
[4]Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certiﬁed
adversarial robustness via randomized smoothing. In Interna-
tional Conference on Machine Learning , pages 1310–1320,
2019. 7
[5]Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke,
Stefan Roth, and Bernt Schiele. The cityscapes dataset forsemantic urban scene understanding. In Proceedings of the
IEEE conference on computer vision and pattern recognition ,
pages 3213–3223, 2016. 3
[6]Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge
Belongie. Class-balanced loss based on effective number of
samples. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 9268–9277, 2019. 6
[7]Yan Duan, Marcin Andrychowicz, Bradly Stadie, Ope-
nAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter
Abbeel, and Wojciech Zaremba. One-shot imitation learning.
InAdvances in neural information processing systems , pages
1087–1098, 2017. 8
[8]Mark Everingham, SM Ali Eslami, Luc Van Gool, Christo-
pher KI Williams, John Winn, and Andrew Zisserman. The
pascal visual object classes challenge: A retrospective. In-
ternational journal of computer vision , 111(1):98–136, 2015.
3
[9]Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-
agnostic meta-learning for fast adaptation of deep networks.
InProceedings of the 34th International Conference on Ma-
chine Learning-Volume 70 , pages 1126–1135. JMLR. org,
2017. 8
[10] Patrick Follmann, Tobias Bottger, Philipp Hartinger, Rebecca
Konig, and Markus Ulrich. Mvtec d2s: densely segmented
supermarket dataset. In Proceedings of the European Confer-
ence on Computer Vision (ECCV) , pages 569–585, 2018. 2,
3, 5
[11] Marian George and Christian Floerkemeier. Recognizing
products: A per-exemplar multi-label image classiﬁcation ap-
proach. In European Conference on Computer Vision , pages
440–455. Springer, 2014. 2
[12] Eran Goldman, Roei Herzig, Aviv Eisenschtat, Jacob Gold-
berger, and Tal Hassner. Precise detection in densely packed
scenes. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 5227–5236, 2019. 1,
2, 3, 5
[13] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples. arXiv
preprint arXiv:1412.6572 , 2014. 7
[14] Yu Hao, Yanwei Fu, and Yu-Gang Jiang. Take goods from
shelves: A dataset for class-incremental object detection. In
Proceedings of the 2019 on International Conference on Mul-
timedia Retrieval , pages 271–278, 2019. 2
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 6
[16] Tobias Hinz, Stefan Heinrich, and Stefan Wermter. Gen-
erating multiple objects at spatially distinct locations. In
International Conference on Learning Representations , 2019.
8
[17] Leonid Karlinsky, Joseph Shtok, Yochay Tzur, and Asaf
Tzadok. Fine-grained recognition of thousands of object
categories with single-example training. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recog-
nition , pages 4113–4122, 2017. 2
[18] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks. In
9
Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 4401–4410, 2019. 7
[19] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao,
and Li Fei-Fei. Novel dataset for ﬁne-grained image catego-
rization. In First Workshop on Fine-Grained Visual Catego-
rization, IEEE Conference on Computer Vision and Pattern
Recognition , Colorado Springs, CO, June 2011. 2
[20] Dmytro Kotovenko, Artsiom Sanakoyeu, Sabine Lang, and
Bjorn Ommer. Content and style disentanglement for artis-
tic style transfer. In Proceedings of the IEEE International
Conference on Computer Vision , pages 4422–4431, 2019. 7
[21] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3d object representations for ﬁne-grained categorization. In
4th International IEEE Workshop on 3D Representation and
Recognition (3dRR-13) , Sydney, Australia, 2013. 2
[22] Sharmistha Law and Kathryn A Braun-LaTour. Product place-
ments: How to measure their impact. The psychology of en-
tertainment media: Blurring the lines between entertainment
and persuasion , pages 63–78, 2004. 8
[23] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Dollár. Focal loss for dense object detection. In Pro-
ceedings of the IEEE international conference on computer
vision , pages 2980–2988, 2017. 4, 6
[24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
European conference on computer vision , pages 740–755.
Springer, 2014. 3
[25] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou
Tang. Deepfashion: Powering robust clothes recognition and
retrieval with rich annotations. In Proceedings of IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR) ,
2016. 2
[26] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,
Dimitris Tsipras, and Adrian Vladu. Towards deep learn-
ing models resistant to adversarial attacks. In International
Conference on Learning Representations , 2018. 7
[27] L. McInnes, J. Healy, and J. Melville. UMAP: Uniform Mani-
fold Approximation and Projection for Dimension Reduction.
ArXiv e-prints , Feb. 2018. 4
[28] Michele Merler, Carolina Galleguillos, and Serge Belongie.
Recognizing groceries in situ using in vitro training data.
In2007 IEEE Conference on Computer Vision and Pattern
Recognition , pages 1–8. IEEE, 2007. 2
[29] Mustafa Mustafa, Deborah Bard, Wahid Bhimji, Zarija Luki ´c,
Rami Al-Rfou, and Jan M Kratochvil. Cosmogan: creating
high-ﬁdelity weak lensing convergence maps using genera-
tive adversarial networks. Computational Astrophysics and
Cosmology , 6(1):1–13, 2019. 7
[30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An
imperative style, high-performance deep learning library. In
Advances in neural information processing systems , pages
8026–8037, 2019. 6
[31] Mengshi Qi, Yunhong Wang, Jie Qin, and Annan Li. Ke-gan:
Knowledge embedded generative adversarial networks forsemi-supervised scene parsing. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition ,
pages 5237–5246, 2019. 8
[32] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-
vised representation learning with deep convolutional genera-
tive adversarial networks. arXiv preprint arXiv:1511.06434 ,
2015. 7
[33] Mengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell,
Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and
Richard S Zemel. Meta-learning for semi-supervised few-
shot classiﬁcation. arXiv preprint arXiv:1803.00676 , 2018.
8
[34] Anderson Rocha, Daniel C Hauagge, Jacques Wainer, and
Siome Goldenstein. Automatic fruit and vegetable classiﬁca-
tion from images. Computers and Electronics in Agriculture ,
70(1):96–104, 2010. 2
[35] Bikash Santra and Dipti Prasad Mukherjee. A comprehensive
survey on computer vision based approaches for automatic
identiﬁcation of products in retail store. Image and Vision
Computing , 86:45–63, 2019. 2
[36] Victor Garcia Satorras and Joan Bruna Estrach. Few-shot
learning with graph neural networks. In International Confer-
ence on Learning Representations , 2018. 8
[37] Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng
Xu, John Dickerson, Christoph Studer, Larry S Davis, Gavin
Taylor, and Tom Goldstein. Adversarial training for free! In
Advances in Neural Information Processing Systems , pages
3353–3364, 2019. 7
[38] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical
networks for few-shot learning. In I. Guyon, U. V . Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R.
Garnett, editors, Advances in Neural Information Processing
Systems 30 , pages 4077–4087. Curran Associates, Inc., 2017.
8
[39] Wei Sun and Tianfu Wu. Image synthesis from reconﬁgurable
layout and style. In The IEEE International Conference on
Computer Vision (ICCV) , October 2019. 8
[40] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception ar-
chitecture for computer vision. In Proceedings of the IEEE
conference on computer vision and pattern recognition , pages
2818–2826, 2016. 6
[41] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Alek-
sander Madry. On adaptive attacks to adversarial example
defenses. arXiv preprint arXiv:2002.08347 , 2020. 7
[42] Gül Varol and Rıdvan Salih Kuzu. Toward retail product
recognition on grocery shelves. In Sixth International Con-
ference on Graphic and Image Processing (ICGIP 2014) ,
volume 9443, page 944309. International Society for Optics
and Photonics, 2015. 2
[43] Xiu-Shen Wei, Quan Cui, Lei Yang, Peng Wang, and Lingqiao
Liu. Rpc: A large-scale retail product checkout dataset. arXiv
preprint arXiv:1901.07249 , 2019. 2, 5
[44] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Be-
longie, and P. Perona. Caltech-UCSD Birds 200. Technical
Report CNS-TR-2010-001, California Institute of Technol-
ogy, 2010. 2
10
[45] Chang Xiao and Changxi Zheng. One man’s trash is another
man’s treasure: Resisting adversarial examples by adversarial
examples. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , June 2020.
7
[46] Chang Xiao, Peilin Zhong, and Changxi Zheng. Bourgan:
Generative networks with metric embeddings. In Advances
in Neural Information Processing Systems , pages 2269–2280,
2018. 7
[47] Peiqin Zhuang, Yali Wang, and Yu Qiao. Learning attentive
pairwise interaction for ﬁne-grained classiﬁcation. In AAAI ,
pages 13130–13137, 2020. 6
11"
2404.13292,D:\Database\arxiv\papers\2404.13292.pdf,"While subword tokenization is commonly used in NLP, its impact on downstream tasks is often difficult to assess directly. What specific type of NLP task is particularly well-suited for evaluating the effects of tokenization on individual words, and why?","Out-of-context tasks that rely on the meaning of individual input words, such as the Word-in-Context (WiC) task, are well-suited for evaluating tokenization effects because they directly assess the model's ability to understand and represent individual words, rather than relying on contextual cues.","in downstream NLP tasks, e.g., machine transla-
tion (Gowda and May, 2020; He et al., 2020; Park
et al., 2020; Zouhar et al., 2023), question answer-
ing (Sun et al., 2023; Singh and Strouse, 2024),
or text classification (Bostrom and Durrett, 2020;
Jabbar, 2023; Schmidt et al., 2024; Goldman et al.,
2024). However, the impact of tokenization on
NLP tasks is often indirect and thus not easy to
measure robustly: even if a word is incorrectly split,
a successful prediction may still be made by the
language model based on other tokens in context.
Out-of-context tasks that depend on individual in-
put words provide a more direct evaluation, so tasks
with both context and individual input word was
desirable for the evaluation. The only such mea-
sure we found is the Word-in-Context (WiC) task
(Pilehvar and Camacho-Collados, 2019). This task
takes three elements in input: one word and two
contextual sentences, and the expected prediction
label is whether the word has the same meaning
in both contexts. However, 80% of all words of
interest in the WiC test splits are vocabulary words
by the commonly used language models and the
remaining OOV words are limited to one to two
hundred test instances.
Our contribution with respect to extrinsic evalu-
ation is a new evaluation method that we call the
Out-of-Vocabulary Generalization Challenge . It
consists of three downstream evaluation tasks, as
presented in Section 4.
3 umLabeller: UniMorph Labeller
UMLABELLER2is the inspection tool for charac-
terizing the subword-level composition of words,
based on morphological information retrieved
from UniMorph (McCarthy et al., 2020a; Bat-
suren et al., 2022b). Let tokenizer Tsegment a
given word winto a sequence of subwords s=
(s1, ..., s n)| ∀i si∈ V where Vis a vocabulary
list of subwords and nis the length of the subword
sequence. Given wands,umLabeller assigns a
label based on how sis morphologically composed
and aligned with UniMorph morpheme segmenta-
tions. The output label has four categories: vocab ,
alien ,morph , orn/a:
•vocabulary subword : the given word wis a sub-
word in the vocabulary as w∈ V;
•alien composition : the given subword sequence
sis an alien subword composition if we find at
2https://github.com/unimorph/umLabellerleast two subwords siandsjinsthat are not
meaningful with respect to the meaning of w;
•morphological composition : the subword se-
quence sis morphological if it is neither a vocab-
ulary nor an alien subword composition;
•n/a: UniMorph has no information on the word.
3.1 Morphological Resources
Below we describe how the umLabeller algorithm
exploits morphological resources for its predictions.
The main intuition behind using morphological re-
sources is that morphological segmentation pro-
vides information about the compositional meaning
of words (Vylomova, 2018).
UniMorph. One of the state-of-the-art morpho-
logical databases in both NLP and CL is UniMorph
(Sylak-Glassman et al., 2015; Kirov et al., 2016,
2018; McCarthy et al., 2020b), a morphosyntac-
tic feature annotation of inflectional morphology
in hundreds of languages, and the most recent
version (Batsuren et al., 2022b) extended to mor-
phological segmentation of inflectional and deriva-
tional morphology. For instance, the English word
motivated is analyzed by UniMorph as follows:
{motivated | infl | motivate |-ed|#V;PST }
{motivate | deri | motive |-ate |#N→V}
from which the final segmentation _motive ate
edis obtained.
Subword tokenization methods, however, do not
aim at full morphological segmentation but rather
at obtaining a minimal number of subwords. For
this reason, from the morphemes provided by Uni-
Morph we produce all possible subword combi-
nations. We first compute all possible morpheme
n-grams for a given word. For the above example:
(_motive, ate, ed ), (_motive, ate ),
(ate, ed ), (_motive ), (ate), (ed)
Then, we merge every bigram into a unigram
by retrieving and inferring the target morpheme
from UniMorph (taking morphological composi-
tion rules into account). During this process, if we
have a new unigram (e.g., ate + ed →ated ), we
add it to the unigram list and generate all possible
new n-grams (e.g., ( _motive ,ated )) with this uni-
gram. This process continues until there is no new
unigram. Finally, for all these n-grams (except for
unigrams), we create a morphological merge list
by retrieving the target word or morpheme from
UniMorph. For the current example, we create the
following merge list:
(_motive, ate, ed )→_motivated ,
(_motivate, ed )→_motivated ,
(_motive, ated )→_motivated ,
(_motive, ate )→_motivate ,
(ate, ed )→ated
Compound morphology. Our second resource is
the SIGMORPHON-2022 Shared Task on morpho-
logical segmentation that provides compound seg-
mentation data with inflectional and derivational af-
fixes, derived from UniMorph 4.0 and MorphyNet
(Batsuren et al., 2021). For instance, the English
word copywriters is segmented as follows:
{_copywriters |_copy write er s }
This piece of compound data complements the
UniMorph segmentation and is fully compatible
with our approach of generating the morpholog-
ical merge list, as this resource linked with Uni-
Morph 4.0.
3.2 Algorithm
The umLabeller algorithm described below relies
on the morphological merge list computed in the
previous section. Given a word wand its subword
sequence s, it implements the following classifica-
tion rules:
umLabeller (w, s) =

vocab w∈ V
n/a w /∈UNIMORPH
alien µ(w, s)< n−1
morph µ(w, s)⩾n−1
where µis the number of subword tokens in sthat
can be found in the morphological merge list (and
thus are deemed to be morphologically correct).
Allowing for a tolerance of one missing element
(hence µ < n −1and not µ < n ) is motivated by
the possibility of root words not included in Uni-
Morph, and by possible phonetic or orthographic
alterations due to composition. For example, for
the word theorizing , the sequence _theor iz ing
is classified by umLabeller as morphological be-
cause izing is included in the merge list computed
in the previous section, and theor is the only token
missing from it. While we cannot formally prove
theµrule with respect to English morphology, we
have strong empirical evidence for it: through all
of our experiments, including our evaluations, we
have not found a single counterexample, i.e., a
morphologically correct composition that would
contain two or more missing merge list entries.
The full algorithm is explained as follows:
0 20000 40000 60000 80000 100000
#BPE vocabulary size050000100000150000200000
vocab
alien
morphFigure 2: umLabeller category distribution across BPE
vocabulary sizes
•The algorithm first checks whether the vocabu-
laryVcontains the word w: if yes, it returns the
label vocab .
•It then verifies whether UniMorph contains any
information about the word w. If no, it returns
the label n/a.
•The algorithm estimates µ(w, s), the number
of morphological subwords in the subword se-
quence sby aligning all possible morphological
merges of length n, retrieved from UniMorph, as
follows:
µ(w, s) = argmax
m∈UM(w,n)nX
i=1|si=mi| (1)
where UM(w,n) returns a list of morphological
merges for the word wand its length equal to
n, and m= (m1, ..., m n)is a sequence of mor-
phological morphemes retrieved from UniMorph,
morphological merges of mis the word w. For
instance, given the word motivated and the sub-
word sequence length n= 2,UM(w, n )is a list
with two morphological merges as [( _motivate,
ed), (_motive, ated )].
•if the number of morphological subwords,
µ(w, s), is greater than or equal to n-1,wis con-
sidered a morphological composition. Otherwise,
it is labelled as alien .
We provide a minimal Python implementation
in Appendix A.1.
3.3 BPE Statistics on umLabeller
In order to better understand state-of-the-art tok-
enizers, we trained 100 BPE tokenizers3with in-
creasing vocabulary sizes (from 1k to 100k with
1k increment). As the training corpus, we chose
the Book Corpus (Zhu et al., 2015) (4.5 GB of
3We used the HuggingFace implementations from
https://github.com/huggingface/tokenizers .
Table 3: WaD dataset examples. The original word represents the corresponding definition in English WordNet.
input output
split word definition label original word
trainclerking the activity of recording business transactions true -
ammo alternatively placed in genus Martynia false martynia arenaria
devenforced forced or compelled or put in force true -
snowline a fishing line managed principally by hand false handline
testovum the female reproductive cell; the female gamete true -
mouther a wind from the south false souther
Table 4: WaM dataset examples.
input output
# word morphology label
1 leaderboard derivation true
2 overpressing compound false
3 coteaches inflection true
4 sharemarkets derivation false
5 untold inflection true
Table 5: WaW dataset examples.
input output
# word a word b label
1 visitor traveler true
2 shopper earless false
3 photocopy mosaic true
4 bleed medicine true
5 poorer proxy false
text data), which is used to pre-train BERT and
RoBERTa. Then, we selected the 200,000 most fre-
quent words based on the Book Corpus, applied the
BPE tokenizers and labelled the resulting subword
compositions using umLabeller. When selecting
a vocabulary size, one wants to minimize the size
while maximizing the morphological plausibility of
word segmentations. These label distributions sug-
gest that the optimal combination might be to use
40,000 to 50,000 tokens. When further increasing
the vocabulary size, the number of words labeled
asmorph no longer increases.
4 OOV Generalization Challenge
We present the OOV Generalization Challenge4,
aimed at evaluating the generalization abilities of
models when faced with OOV words, and consist-
ing of three subtasks: Word and Definition (WaD),
Word and Morphology (WaM), and Word and Word
(WaW). More details in relation to the state-of-
the-art generalization in NLP are available in Ap-
pendix A.2.
4Datasets are released at the SIGMORPHON Shared
Task 2024 on Subword Tokenization. https://github.com/
sigmorphon/2024TokenST4.1 Subtask 1: Word and Definition (WaD)
Description. Classify whether a given word and
a given definition match semantically. WaD exam-
ples can be seen in Table 3.
Dataset development. We converted English
Wordnet (Miller et al., 1990), a freely avail-
able lexico-semantic resource, into 207K word–
definition pairs, annotating them with a ‘true’ label.
Then, we shuffled words and definitions between
the instances and labeled them as ‘false’. To gener-
ate out-of-distribution (OOD) examples for devel-
opment and test splits, we sampled words that were
unseen during training, and for negative examples,
we intentionally chose words that were lexically
similar to the original words of given definitions,
as illustrated by examples in Table 3. We sampled
10,500 instances for training, 1,500 instances for
development, and 3,000 instances for testing.
4.2 Subtask 2: Word and Morphology (WaM)
Description. Classify whether a given word con-
tains inflection, derivation, or compounding. The
examples are shown in Table 4.
Dataset development. The data for this subtask
has been automatically converted from the SIG-
MORPHON Shared Task 2022 on Morpheme Seg-
mentation (Batsuren et al., 2022a). This shared task
provided three morphological labels (inflection,
derivation, compound) for 596K English Words.
We first retrieved the most frequent 5000 subwords
from the GPT-2 Tokenizer, which are shared across
all splits (train, validation, test) and can be part of
any subword composition in any split. The main
evaluation setup is a compositional generalization
(Hupkes et al., 2020) that every word in validation
and test splits contained at least one unseen sub-
word during training. These unseen subwords are
outside of the most frequent 5000 subwords. The
train, validation, and test splits contain 5400, 900,
and 1800 instances, respectively. All splits contain
balanced distributions of instances across the three
morphological categories and output labels.
Table 6: WaD test results by umLabeller categories. All language models are their corresponding large models.
Tokenizer coverage represents the test split’s coverage on the corresponding umLabeller category. All experiments
trained on the best hyperparam sweep according to the validation results, averaged over five seeds.
Tokenizer Test distribution Accuracy
Model type size vocab morph alien vocab morph alien total
ALBERT ULM 30k 27.0 44.6 28.5 93.3±0.3 71.4 ±2.7 67.2 ±2.9 76.2±1.9
BERT BPE 30k 26.9 37.9 35.2 95.3±0.4 77.0 ±1.4 72.4 ±1.8 80.4±1.0
RoBERTa BPE 50k 33.3 34.7 31.9 95.6±0.8 72.4 ±1.0 66.8 ±3.1 77.9±0.9
DeBERTa ULM 128k 45.3 40.4 14.3 95.9±0.4 75.5 ±2.8 68.4 ±2.5 83.8±1.5
Table 7: WaM test results by umLabeller categories. All language models are their corresponding large models.
Tokenizer coverage represents the test split’s coverage on the corresponding umLabeller category. All experiments
trained on the best hyperparam sweep according to the validation results, averaged over five seeds.
Test distribution Accuracy
Model vocab morph alien vocab morph alien infl. deri. comp. total
ALBERT 25.1 40.7 34.1 86.1±1.2 84.1 ±2.1 81.2 ±1.2 91.8±1.0 77.5 ±2.0 81.7 ±2.0 83.6±1.2
BERT 26.9 27.7 45.2 88.5±1.0 86.2 ±0.8 82.3 ±0.5 89.7±0.9 80.8 ±0.5 84.8 ±0.5 85.0±0.4
RoBERTa 33.3 33.7 32.9 91.5±0.8 85.6 ±0.3 78.4 ±0.7 92.2±0.6 79.6 ±0.6 83.8 ±0.8 85.2±0.5
DeBERTa 36.3 42.1 21.6 91.8±0.6 87.5 ±1.1 84.8 ±1.0 95.4±0.4 82.6 ±1.7 87.3 ±0.9 88.4±0.8
4.3 Subtask 3: Word and Word (WaW)
Description. Classify whether two given words
are semantically related. Examples are shown in
Table 5.
Dataset development. For this subtask, we col-
lected word pairs that are semantically related to
each other from the English Wordnet. We retrieved
17.7M word pairs, including siblings (13M), hyper-
nyms (4.3M), synonyms (304K), antonyms (20K),
meronyms (71K), and substances (5.3K). Overall,
we generated 5,389 instances for training, 582 in-
stances for development, and 1,133 instances for
test split. The positive instances were randomly
sampled from the main pool of 17.7 million word
pairs, and for the negative instances, we first cre-
ated a list of unique words, then we generated neg-
ative samples by sampling two distinct words from
the list for each instance and verified that each
generated negative instance was absent from the
word pair pool. To evaluate generalization abilities,
words in the development and test splits are unseen
on the training split.
5 Manual validation
We randomly sampled 300 words of UniMorph and
its GPT-3 Tokenizer subword tokenization, half of
them as morphological composition on umLabeller
and the other half as alien. First, we instructed two
computational linguists (recruited from this paper’s
authors) about definitions of morphological com-
position and alien composition; then, they were
asked to validate the correctness of umLabellergiven the subword compositions of the GPT-3 Tok-
enizer. The inter-agreement score of two validators
was 99.3% (Cohen’s kappa is 0.829) and then in
the conflicting examples, they were asked to dis-
cuss and reach a common agreement. The final
accuracy of the sample was 98.0%, and in 6 out of
300 samples, the UniMorph segmentations were
incomplete, e.g., reform instead of re- form .
6 Experiments and Results
In this section, we present the empirical results of
our OOV Generalization Challenge 1.0 benchmark
on four publicly available models.
6.1 Experimental setup
We train the base and large variants of four models
(BERT (Devlin et al., 2018), RoBERTa (Liu et al.,
2019), ALBERT-v2 (Lan et al., 2019), DeBERTa-
v3 (He et al., 2021)), learning rates (1e-5, 2e-5,
3e-5), batch sizes (8, 16, 32) and epochs (3, 10)
and select the best-performing model based on the
validation set of the individual split. Finally, we
train five seeds of the best-performing model. The
main reason we selected these models is that they
have different tokenizers, that vary in terms of the
vocabulary size (30k, 50k and 128k) and the tok-
enization scheme (BPE, ULM).
6.2 Task performances
To understand how alien and morphological sub-
word compositions impact the compositional gen-
eralizations of language models, we used umLa-
beller to divide the test splits of three subtasks into
Table 8: WaW label coverage of test split by umLabeller categories.
Model vocab&vocab morph&morph alien&alien morph&alien vocab&alien vocab&morph total
ALBERT 14.0 8.0 20.9 14.2 23.5 19.2 99.9
BERT 13.1 4.3 28.7 9.0 26.2 18.6 99.9
RoBERTa 12.1 3.8 32.1 8.5 25.5 17.9 99.9
DeBERTa 35.3 7.1 10.4 16.3 18.6 12.1 99.9
Table 9: WaW test accuracies by umLabeller categories. All language models are their corresponding base models.
All experiments trained on the best hyperparam sweep according to the validation results, averaged over five seeds.
Model vocab&vocab morph&morph alien&alien morph&alien vocab&alien vocab&morph total
ALBERT 68.8±1.6 85.7 ±2.2 73.1 ±3.5 78.5 ±1.4 73.2 ±1.0 77.6 ±1.2 75.2±0.9
BERT 73.2±2.2 89.8 ±1.6 77.0 ±1.5 79.9 ±1.7 75.6 ±1.2 77.5 ±1.6 77.3±0.7
RoBERTa 79.0±1.6 91.6 ±1.1 75.4 ±1.7 80.3 ±0.8 78.8 ±1.2 82.3 ±2.1 78.8±0.8
DeBERTa 80.7±0.5 91.4 ±1.0 85.3 ±1.4 83.9 ±0.9 77.1 ±1.1 85.0 ±1.0 82.3±0.5
Average 75.4±4.4 89.6 ±2.0 77.7 ±3.8 80.7 ±1.6 76.1 ±1.8 80.6 ±3.0 -
Model
Test datasetModel
Predictions
umLabelerEvaluation
Statistics
UniMorph
Figure 3: UniMorph Labeller extends model evaluation
to provide insights about the role of subword tokeniza-
tion in terms of vocabulary words, alien composition,
and morphological composition. Blue diagram is the
common NLP pipeline.
smaller groups, each of which represents one of
umLabeller categories. This evaluation pipeline is
illustrated in Figure 3.
Subtask 1: WaD results. To inspect the lan-
guage models in the WaD subtask, we divide
its test split into three subgroups, each of which
corresponds to the umLabeller categories of vo-
cab, morph, and alien. Given the test dataset
D={(xi, yi)}N
i=1and the tokenizer T, each in-
stance xihas two elements: word wiand defini-
tiondi. Then the subgroup label is computed as
gi=umLabeller (wi,T.tokenize (wi)).
In Table 6, the WaD performances of language
models are provided across all three umLabeller
categories, along with the total accuracy. In sub-
task 1, if a given word wbelongs to the vocabulary
of its tokenizer, then we have very high average
accuracy (as minimum 93.2%) across all language
models. Overall, the vocab group in the test split
has 95.0% and 20.9% absolute performance differ-
ence over morph and alien compositions, respec-
tively. Thus, these results show that language mod-
els’ generalization abilities are significantly betterwith morphological composition (by 5.4% abso-
lute accuracy) than alien composition. Since the
vocab performances of all language models are sig-
nificantly higher than those of other groups, the
vocab coverage contributes heavily to the total ac-
curacy. Therefore, it is not surprising to see the
best-performing model, DeBERTa, in total accu-
racy.
Subtask 2: WaM results. As WaD subtask, we
used the exact same settings to divide the WaM
test split into three umLabeller subgroups as each
instance xihas two elements: word wiand mor-
phology mi. Each morphology miis one of three
morphological types: inflection, derivation, and
compound. The language model performances of
the WaM subtask are described in Table 7, show-
ing all three umLabeller categories and all three
morphologies. All four models performed bet-
ter on the group with morphological composition
over alien composition by large margins with ab-
solute improvements 2.7% (DeBERTa) →7.2%
(RoBERTa).
Subtask 3: WaW results. Given the WaW test
dataset D={(xi, yi)}N
i=1and the tokenizer T,
each instance xihas two words: word aiand
word bithen two subgroup labels are produced
asga
i=umLabeller (wi,T.tokenize (ai))andgb
i=
umLabeller (bi,T.tokenize (bi)). Depending on val-
ues of ga
iandgb
i, we divide the test dataset into six
groups of umLabeller label pairs. The label pair list
can be seen from the header row in Table 9. The
subgroup coverage distributions of four tokenizers
are shown in Table 8. Table 9 reports the test accu-
racies of four models across the six subgroups of
the test split. Surprisingly, each model performed
Table 10: WiC (word-in-context) development results by umLabeller categories. All language models are their
corresponding base models. Tokenizer coverage represents the test split’s coverage on the corresponding umLabeller
category. All experiments trained on the best hyperparameter sweep according to the validation results, averaged
over five seeds.
Tokenizer Dev distribution Accuracy
Model type size vocab morph alien vocab morph alien total
ALBERT ULM 30k 80.3 6.1 10.8 72.3±0.8 68.2 ±1.6 62.3 ±2.9 70.7±0.7
BERT BPE 30k 77.9 4.1 15.2 71.6±0.6 74.6 ±2.8 59.3 ±0.9 69.5±0.4
RoBERTa BPE 50k 80.7 3.8 12.7 67.6±1.8 63.3 ±7.3 62.4 ±2.7 67.1±2.1
best in a group with two morphologically com-
posed words, unlike groups with vocabulary words
in the previous two subtasks. This result shows
that OOV words are semantically more informative
than vocabulary words, only in this subtask.
Comparison to WiC. We compare the results
of our three generalization tasks to the state-of-
the-art Word-in-Context (WiC) task mentioned in
Section 2. WiC takes one word and two contex-
tual sentences in input, and the goal of the task is
to predict whether the word has the same mean-
ing in both contexts. Table 10 shows the perfor-
mances of three language models across three um-
Labeller categories of subword compositions of the
given words in its development split. The WiC
corpus demonstrates a considerable performance
decrease for all three models on alien composition
(10% for ALBERT, 5.3% for BERT, and 5.2% for
RoBERTa).
7 Conclusion
In this paper, we first formulated notions of mor-
phological and alien subword compositions. Thus,
we argued that alien subword composition fun-
damentally differs from morphological composi-
tion in supporting the semantic compositionality
of word meanings. To this end, we first presented
an open-source, large-scale, high-quality inspec-
tion tool, umLabeller, to classify a given subword
composition as alien or morphological. Then, we
introduced the OOV Generalization Challenge 1.0
benchmark composed of three downstream sub-
tasks. The empirical results in the experiments
show that morphological composition supports bet-
ter the semantic compositionality of OOV word
meanings than alien composition. Overall, our
findings suggest that the generalization capabili-
ties of language models can be further improved
if language models use morphological subword to-
kenization. Also, the text characterization tools
for language models become foundational in NLPevaluation (Simig et al., 2022; Chen et al., 2024)
and umLabeller can play a crucial role in inspect-
ing the impact of subword tokenization in language
models.
Limitations
Our work is limited to addressing tokenization in
English. The correctness and coverage of umLa-
beller directly depend on UniMorph English mor-
pheme segmentation data, which is freely available
on GitHub. We are publicly welcoming everyone,
interested in fixing the mistakes and increasing the
coverage through GitHub pull requests.
Acknowledgments
O.U. and Y .P. were supported by the Israel Science
Foundation (grant No. 1166/23).
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
Khuyagbaatar Batsuren, Gábor Bella, Aryaman Arora,
Viktor Martinovic, Kyle Gorman, Zden ˇek Žabokrt-
ský, Amarsanaa Ganbold, Šárka Dohnalová, Magda
Ševˇcíková, Kate ˇrina Pelegrinová, Fausto Giunchiglia,
Ryan Cotterell, and Ekaterina Vylomova. 2022a. The
SIGMORPHON 2022 shared task on morpheme seg-
mentation. In Proceedings of the 19th SIGMOR-
PHON Workshop on Computational Research in Pho-
netics, Phonology, and Morphology , pages 103–116,
Seattle, Washington. Association for Computational
Linguistics.
Khuyagbaatar Batsuren, Gábor Bella, and Fausto
Giunchiglia. 2021. Morphynet: a large multilingual
database of derivational and inflectional morphology.
InProceedings of the 18th sigmorphon workshop on
computational research in phonetics, phonology, and
morphology , pages 39–48.
Khuyagbaatar Batsuren, Omer Goldman, Salam Khal-
ifa, Nizar Habash, Witold Kiera ´s, Gábor Bella,
Brian Leonard, Garrett Nicolai, Kyle Gorman, Yusti-
nus Ghanggo Ate, Maria Ryskina, Sabrina Mielke,
Elena Budianskaya, Charbel El-Khaissi, Tiago Pi-
mentel, Michael Gasser, William Abbott Lane,
Mohit Raj, Matt Coler, Jaime Rafael Montoya
Samame, Delio Siticonatzi Camaiteri, Esaú Zu-
maeta Rojas, Didier López Francis, Arturo Once-
vay, Juan López Bautista, Gema Celeste Silva Vil-
legas, Lucas Torroba Hennigen, Adam Ek, David
Guriel, Peter Dirix, Jean-Philippe Bernardy, An-
drey Scherbakov, Aziyana Bayyr-ool, Antonios
Anastasopoulos, Roberto Zariquiey, Karina Sheifer,
Sofya Ganieva, Hilaria Cruz, Ritván Karahó ˇga,
Stella Markantonatou, George Pavlidis, Matvey Plu-
garyov, Elena Klyachko, Ali Salehi, Candy An-
gulo, Jatayu Baxi, Andrew Krizhanovsky, Natalia
Krizhanovskaya, Elizabeth Salesky, Clara Vania, Sar-
dana Ivanova, Jennifer White, Rowan Hall Maud-
slay, Josef Valvoda, Ran Zmigrod, Paula Czarnowska,
Irene Nikkarinen, Aelita Salchak, Brijesh Bhatt,
Christopher Straughn, Zoey Liu, Jonathan North
Washington, Yuval Pinter, Duygu Ataman, Marcin
Wolinski, Totok Suhardijanto, Anna Yablonskaya,
Niklas Stoehr, Hossep Dolatian, Zahroh Nuriah,
Shyam Ratan, Francis M. Tyers, Edoardo M.
Ponti, Grant Aiton, Aryaman Arora, Richard J.
Hatcher, Ritesh Kumar, Jeremiah Young, Daria
Rodionova, Anastasia Yemelina, Taras Andrushko,
Igor Marchenko, Polina Mashkovtseva, Alexandra
Serova, Emily Prud’hommeaux, Maria Nepomni-
ashchaya, Fausto Giunchiglia, Eleanor Chodroff,
Mans Hulden, Miikka Silfverberg, Arya D. Mc-
Carthy, David Yarowsky, Ryan Cotterell, Reut Tsar-
faty, and Ekaterina Vylomova. 2022b. UniMorph
4.0: Universal Morphology. In Proceedings of the
Thirteenth Language Resources and Evaluation Con-
ference , pages 840–855, Marseille, France. European
Language Resources Association.
Lisa Beinborn and Yuval Pinter. 2023. Analyzing cogni-
tive plausibility of subword tokenization. In Proceed-
ings of the 2023 Conference on Empirical Methods
in Natural Language Processing , pages 4478–4486.
Prabin Bhandari and Abhishek Paudel. 2024. Using
contextual information for sentence-level morpheme
segmentation. arXiv preprint arXiv:2403.15436 .
Kaj Bostrom and Greg Durrett. 2020. Byte pair encod-
ing is suboptimal for language model pretraining. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2020 , pages 4617–4624, Online.
Association for Computational Linguistics.
Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
InProceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing , pages
632–642, Lisbon, Portugal. Association for Compu-
tational Linguistics.
Hao Chen, Bhiksha Raj, Xing Xie, and Jindong Wang.
2024. On catastrophic inheritance of large founda-
tion models. arXiv preprint arXiv:2402.01909 .Ryan Cotterell, Tim Vieira, and Hinrich Schütze. 2016.
A joint model of orthography and morphological seg-
mentation. Association for Computational Linguis-
tics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Leander Girrbach. 2022. SIGMORPHON 2022 shared
task on morpheme segmentation submission descrip-
tion: Sequence labelling for word-level morpheme
segmentation. In Proceedings of the 19th SIGMOR-
PHON Workshop on Computational Research in Pho-
netics, Phonology, and Morphology , pages 124–130,
Seattle, Washington. Association for Computational
Linguistics.
Omer Goldman, Avi Caciularu, Matan Eyal, Kris Cao,
Idan Szpektor, and Reut Tsarfaty. 2024. Unpacking
tokenization: Evaluating text compression and its
correlation with model performance. arXiv preprint
arXiv:2403.06265 .
Edward Gow-Smith, Harish Tayyar Madabushi, Car-
olina Scarton, and Aline Villavicencio. 2022. Improv-
ing tokenisation by alternative treatment of spaces.
InProceedings of the 2022 Conference on Empiri-
cal Methods in Natural Language Processing , pages
11430–11443.
Thamme Gowda and Jonathan May. 2020. Finding the
optimal vocabulary size for neural machine transla-
tion. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2020 , pages 3955–3964.
Martin Haspelmath. 2020. The morph as a minimal
linguistic form. Morphology , 30(2):117–134.
Pengcheng He, Jianfeng Gao, and Weizhu Chen.
2021. DeBERTaV3: Improving DeBERTa using
ELECTRA-style pre-training with gradient-
disentangled embedding sharing. CoRR ,
abs/2111.09543.
Xuanli He, Gholamreza Haffari, and Mohammad
Norouzi. 2020. Dynamic programming encoding
for subword segmentation in neural machine transla-
tion. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
3042–3051.
Valentin Hofmann, Janet Pierrehumbert, and Hinrich
Schütze. 2021. Superbizarre is not superb: Deriva-
tional morphology improves bert’s interpretation of
complex words. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers) , pages 3594–3608.
Valentin Hofmann, Hinrich Schütze, and Janet Pierre-
humbert. 2022. An embarrassingly simple method
to mitigate undesirable properties of pretrained lan-
guage model tokenizers. In Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers) , pages 385–
393.
Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia
Bruni. 2020. Compositionality decomposed: how do
neural networks generalise? Journal of Artificial
Intelligence Research , 67:757–795.
Dieuwke Hupkes, Mario Giulianelli, Verna Dankers,
Mikel Artetxe, Yanai Elazar, Tiago Pimentel, Chris-
tos Christodoulopoulos, Karim Lasri, Naomi Saphra,
Arabella Sinclair, Dennis Ulmer, Florian Schottmann,
Khuyagbaatar Batsuren, Kaiser Sun, Koustuv Sinha,
Leila Khalatbari, Maria Ryskina, Rita Frieske, Ryan
Cotterell, and Zhijing Jin. 2023. A taxonomy and
review of generalization research in nlp. Nature Ma-
chine Intelligence , 5(10):1161–1174.
Haris Jabbar. 2023. Morphpiece: Moving away from
statistical language representation. arXiv preprint
arXiv:2307.07262 .
Cassandra L Jacobs and Yuval Pinter. 2022. Lost in
space marking. arXiv preprint arXiv:2208.01561 .
Katharina Kann, Ryan Cotterell, and Hinrich Schütze.
2016. Neural morphological analysis: Encoding-
decoding canonical segments. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing , pages 961–967, Austin, Texas.
Association for Computational Linguistics.
Christo Kirov, Ryan Cotterell, John Sylak-Glassman,
Géraldine Walther, Ekaterina Vylomova, Patrick Xia,
Manaal Faruqui, Sabrina J Mielke, Arya D McCarthy,
Sandra Kübler, et al. 2018. Unimorph 2.0: Universal
morphology. In Proceedings of the Eleventh Inter-
national Conference on Language Resources and
Evaluation (LREC 2018) .
Christo Kirov, John Sylak-Glassman, Roger Que, and
David Yarowsky. 2016. Very-large scale pars-
ing and normalization of wiktionary morphological
paradigms. In Proceedings of the Tenth International
Conference on Language Resources and Evaluation
(LREC’16) , pages 3121–3126.
Taku Kudo. 2018. Subword regularization: Improving
neural network translation models with multiple sub-
word candidates. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) . Association for
Computational Linguistics.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut. 2019. ALBERT: A lite BERT for self-
supervised learning of language representations.
CoRR , abs/1909.11942.
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
RoBERTa: A robustly optimized BERT pretraining
approach. CoRR , abs/1907.11692.
Arya D. McCarthy, Christo Kirov, Matteo Grella,
Amrit Nidhi, Patrick Xia, Kyle Gorman, Ekate-
rina Vylomova, Sabrina J. Mielke, Garrett Nico-
lai, Miikka Silfverberg, Timofey Arkhangelskiy, Na-
taly Krizhanovsky, Andrew Krizhanovsky, Elena
Klyachko, Alexey Sorokin, John Mansfield, Valts
Ernštreits, Yuval Pinter, Cassandra L. Jacobs, Ryan
Cotterell, Mans Hulden, and David Yarowsky. 2020a.
UniMorph 3.0: Universal Morphology. In Proceed-
ings of the Twelfth Language Resources and Evalua-
tion Conference , pages 3922–3931, Marseille, France.
European Language Resources Association.
Arya D McCarthy, Christo Kirov, Matteo Grella, Am-
rit Nidhi, Patrick Xia, Kyle Gorman, Ekaterina Vy-
lomova, Sabrina J Mielke, Garrett Nicolai, Miikka
Silfverberg, et al. 2020b. Unimorph 3.0: Universal
morphology. In Proceedings of the Twelfth Language
Resources and Evaluation Conference , pages 3922–
3931.
Sabrina J Mielke, Zaid Alyafeai, Elizabeth Salesky,
Colin Raffel, Manan Dey, Matthias Gallé, Arun Raja,
Chenglei Si, Wilson Y Lee, Benoît Sagot, et al. 2021.
Between words and characters: a brief history of
open-vocabulary modeling and tokenization in nlp.
arXiv preprint arXiv:2112.10508 .
George A Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J Miller. 1990.
Introduction to wordnet: An on-line lexical database.
International journal of lexicography , 3(4):235–244.
Benjamin Minixhofer, Jonas Pfeiffer, and Ivan Vuli ´c.
2023. Compoundpiece: Evaluating and improving
decompounding performance of language models.
arXiv preprint arXiv:2305.14214 .
Sathvik Nair and Philip Resnik. 2023. Words, sub-
words, and morphemes: What really matters in the
surprisal-reading time relationship? arXiv preprint
arXiv:2310.17774 .
Kyubyong Park, Joohong Lee, Seongbo Jang, and Da-
woon Jung. 2020. An empirical study of tokenization
strategies for various korean nlp tasks. In Proceed-
ings of the 1st Conference of the Asia-Pacific Chapter
of the Association for Computational Linguistics and
the 10th International Joint Conference on Natural
Language Processing , pages 133–142.
Ben Peters and Andre F. T. Martins. 2022. Beyond char-
acters: Subword-level morpheme segmentation. In
Proceedings of the 19th SIGMORPHON Workshop
on Computational Research in Phonetics, Phonology,
and Morphology , pages 131–138, Seattle, Washing-
ton. Association for Computational Linguistics."
2304.05839,D:\Database\arxiv\papers\2304.05839.pdf,"In the context of machine learning, what are the potential drawbacks of using deep neural networks for tasks where interpretability is crucial, and how do decision trees address these drawbacks?","Deep neural networks often excel in performance but are considered black boxes, making it difficult to understand their decision-making process. Decision trees, on the other hand, provide a transparent view of the learned model, highlighting the features used for classification, thus enhancing interpretability and trust in the model's decisions.","Compromis Interprétabilité-Performance Optimale des
Arbres de Classiﬁcation avec de l’Apprentissage par
Renforcement Boîte Noire
Résumé : L’interprétabilité des modèles d’IA permet à l’utilisateur d’eﬀectuer des contrôles de
sécurité aﬁn d’instaurer la conﬁance dans ces modèles. En particulier, les arbres de décision four-
nissent une vue d’ensemble du modèle appris et soulignent clairement le rôle des caractéristiques
essentielles à la classiﬁcation de ces données. Cependant, l’interprétabilité est entravée si l’arbre
de décision est trop grand. Pour apprendre des arbres compacts, un cadre d’apprentissage par
renforcement a été récemment proposé pour explorer l’espace des arbres de décision. Une tâche
de classiﬁcation supervisée donnée est modélisée comme un problème de décision de Markov
auquel on ajoute des actions qui récoltent des informations sur les caractéristiques d’une donnée,
ce qui équivaut à la construction d’un arbre de décision. En pénalisant ces actions de manière
appropriée, l’agent RL apprend à faire un compromis optimal entre la taille et les performances
de l’arbre appris. Cependant, un agent doit résoudre un problème de décision de Markov par-
tiellement observable. La principale contribution de cet article est de prouver qu’il suﬃt de
résoudre un problème entièrement observable pour apprendre un arbre de décision optimisant
le compromis interprétabilité-performance. Ainsi, n’importe quel algorithme de planiﬁcation ou
d’apprentissage par renforcement peut être utilisé. Nous démontrons l’eﬃcacité de cette ap-
proche sur un ensemble de tâches de classiﬁcation supervisée et nous la comparons à d’autres
méthodes d’optimisation de l’interprétabilité et de la performance.
Mots-clés : Apprentissage par Renforcement, Apprentissage Supervisé, Interprétabilité
Classiﬁcation Trees with RL 3
1 Introduction
The last decade or so has seen a surge in the performance of machine learning models, whether
in supervised learning [15] or RL [17]. These achievements rely on deep neural models that are
often described as black-box [18, 13, 2], trading interpretability for performance. In many real
world tasks, predictive models can hide undesirable biases (see e.g. Sec. 2 in [13] for a list of
such occurrences) hindering trustworthiness towards AIs. Gaining trust is one of the primary
goals of interpretability (see Sec. 2.4 of [2] for a literature review) along with informativeness
requests, i.e. the ability for a model to provide information on why a given decision was taken.
The computational complexity of such informativeness requests can be measured objectively, and
[5] showed that multi-layer neural networks cannot answer these requests in polynomial time,
whereas several of those are in polynomial time for linear models and DT.
In contrast to deep neural models, DTs provide a global look at the learned model and
transparently reveal which features of the input are used in taking a particular decision. This
is referred to as global [13] or model-based [18] interpretability, as opposed to post-hoc inter-
pretability [18, 2]. Even though DTs are globally intepretable, they have also been used in prior
work for post-hoc interpretability of deep neural models, e.g. in image classiﬁcation [33] or RL
[6]. The latter work provides another motivation for DTs, as their simpler nature allowed to
make a stability analysis of the resulting controllers and provided theoretical guarantees of their
eﬃcacy. In the 54 papers reviewed in [13], over 25%use DTs as the interpretable model and over
50%the more general class of decision rules.
DTs are a common interpretable model and it is thus important to improve their associated
learning algorithms. However, interpretability of DTs is hindered if the tree grows too large.
The quantiﬁcation of what is too large might vary greatly depending on the desired type of
simulability, that is whether we want individual paths from root to leaf to be short or the total
size of the tree to be small [16, p. 13]. In both cases, an algorithmic mechanism to control these
tree metrics and to manage the inevitable trade-oﬀ between interpretability and performance is
necessary. OneofthemainchallengesforlearningDTsisthatitisadiscreteoptimizationproblem
that cannot, a priori, be solved via gradient descent. Algorithms such as CART [10] build a DT
by greedily maximizing the information gain—a performance related criteria. Interpretability
can then be controlled by ﬁxing a maximal tree depth, or by using post-processing pruning
algorithms [9, 22]. Unfortunately, this two-step process provides no guaranty that the resulting
DT is achieving an optimal interpretability-performance trade-oﬀ.
An alternative way to learn DTs, that inherently takes into account the interpretability-
performance trade-oﬀ, is the recently proposed framework of Iterative Bounding Markov Decision
Processes (IBMDPs) [31]. An IBMDP extends a base MDP state space with feature bounds that
encode the current knowledge about the input, and the action space with information gathering
actionsthat reﬁne the feature bounds by performing the same test a DT would do: comparing
a feature value to a threshold. The reward function is also augmented with a penalty to take
the cost of information gathering action into account. The IBMDP reward function encodes an
interpretability-performance trade-oﬀ: an agent learns when to add decision nodes or when to
make a prediction.
In this work we study the IBMDP setting when the base MDPs encode supervised classi-
ﬁcation tasks. By doing so, we are able to analyse the optimality of policies learned with RL
with respect to the IBDMP reward function. Thus our work studies RL frameworks to learn
DTs that trade-oﬀ between interpretability (depth of the DT) and performance (accuracy of the
DT). After a literature review and the introduction of our notations in Section 2 and 3, our work
continues as follows:
Section 4 : we present a simple toy task to benchmark RL algorithms solving IBMDPs and
RR n °9503
4 H. Kohler et al.
analyse causes of failure of existing RL algorithms solving IBMDPs.
Section 5 : we present a new RL framework with optimality guarantees w.r.t the IBDMP ob-
jective.
Section 6 : we apply this framework to UCI [11] supervised classiﬁcation datasets.
All proofs are provided in Appendix. All learned DTs and the code to reproduce all experi-
ments can be found on an anonymous github1.
2 Decision Trees for Supervised Learning
2.1 Greedy Approaches
Early research on the induction of DTs focused on the ID3 algorithm, which uses a greedy
approach to select the best attribute at each node based on information gain [24]. This approach
was later extended by the C4.5 algorithm, which introduced techniques such as pruning and
handling missing data [25].
However, these methods may lead to large trees that a human cannot interpret. An other
very well-known DT induction algorithm is CART [10] which performs equivalently to C4.5 and
has the same troubles with the induction of large trees.
2.2 Optimal Decision Trees
The algorithms discussed earlier are greedy heuristics and may produce poorly performing trees
[14]. As a result, there is an increasing interest in developing algorithms that can train DTs to
achieve optimal accuracy.
Training optimal DTs for classiﬁcation can done with dynamic programming [19] or with
Mixed-Integer Linear Programming based formulations [7, 32]. However there is no guarantee
that the optimal DT will not grow very large.
To provide regularization and encourage interpretability, the size of the tree is typically lim-
ited, and then a solver is used to ﬁnd the DT that maximizes accuracy within the predetermined
size constraints. This cannot be considered as optimizing an interpretability-performance trade-
oﬀ as the size of the resulting DT is given by the user and not learned.
2.3 Online Learning
Another approach to learn DTs for classiﬁcation is to model the classiﬁcation task as an MDP.
When doing so, states correspond to training samples, and actions build the DT (either add a
decision node or a leaf node by making a prediction). The reward function of the MDP encodes
a trade-oﬀ between the depth of the tree and the performance of the tree. Indeed, there is a
penalty for querying information about a training sample feature, and rewards for predictions.
The proposed algorithms of [8] and [31] are RL agents solving Partially Observable MDPs
(POMDPs) [28, chapter 3]. In [12], an other RL agent is proposed, this time acting in a fully
observable MDP and is a special case of Iterative Bounding MDPs [31] where training samples
have categorical features.
None of these works study the optimality of their proposed method with respect to the
interpretability-performance trade-oﬀ.
1https://github.com/KohlerHECTOR/Interpretability-Performance-oﬃcial-implem
Inria
Classiﬁcation Trees with RL 5
3 Preliminaries
3.1 Supervised Classiﬁcation Tasks
In this work, we aim to learn DTs for supervised classiﬁcation tasks. We consider classiﬁcation
tasks made of a set of training examples X={x1,...,xN}(a dataset), and a set of labels
Y∈{C1,...,CK}N(one of K classes for each training example). Each of the Ntraining example
xi∈ Xhasdfeaturesxi1,...,xid. The goal of the task is to ﬁnd a classiﬁer g:Rd→ Y,
g:xi↦→ˆyi. Classiﬁers are computed by algorithms optimizing a loss function of ˆyi=g(xi)
and the true label yi. In this paper we present algorithms returning classiﬁers that are DTs
optimizing a function of both the performance and the interpretability deﬁned in the following
sections.
3.2 Markov Decision Problems
We consider an inﬁnite horizon MDP [23] deﬁned by the tuple ⟨S,A,R,T,γ⟩, whereSis the
state space,Ais the discrete action space, R:S×A↦→ [Rmin,Rmax]⊂Ris the reward function,
Tis the transition function, and γ < 1is the discount factor. The agent interacts with the
environment according to its policy π. At timet, the agent takes action at∼π(.|st), at∈A,
after which it observes the reward rtand the next state st+1with probability T(st,at,st+1). Let
Qπ(s,a) =Eπ[∑
t≥0γtR(st,at)|s0=s,a0=a]be the Q-function, Vπ(s) =Eπ[Q(s,a)]be the
value function, Aπ(s,a) =Qπ(s,a)−Vπ(s)be the advantage function, and J(π) =E[V(s0)]be
the policy return for some initial state distribution. In this work we study RL algorithms that
ﬁnd a policy π∗that maximizes J.
3.3 Classiﬁcation Markov Decision Problems
Any supervised classiﬁcation task can be cast into a classiﬁcation MDP
⟨X,{C1,...,CK},R,T,γ⟩. If the dataset to be classiﬁed is X⊊ RN×dthen the state space of the
MDP isX, that is the set of training examples. If the set of labels is Y∈{C1,...,CK}N, then
the action space is {C1,...,CK}. The transition function is stochastic, we simply transit to a new
state (draw a new data point to classify) whatever the action is: T(xi,Ch,xj) =1
N. The reward
function depends on the current state and action: R(xi,Ch) = 1ifyi=Chin the supervised
classiﬁcation task; R(xi,Ch) =−1otherwise. A policy π:xi↦→Chis a classiﬁer, and a policy
πthat maximizes the expected discounted cumulative reward, also maximizes the classiﬁcation
accuracy.
3.4 Iterative Bounding Markov Decision Problems
3.4.1 Deﬁnition
Following [31], we introduce the notion of an Iterative Bounding MDP (IBMDP). IBMDPs
are MDPs. Let us consider a Classiﬁcation MDP ⟨X,{C1,...,CK},R,T,γ⟩. We assumeX=
[0,1]N×d. An Iterative Bounding MDP ⟨S′,A′,R′,T′,ζ,p,γ⟩is deﬁned on top of it with the
following properties.
State spaceS′=X× Ω, with Ω⊊[0,1]2d. A state s∈ S′has two parts. A training
samplexi= (xi1,...,xid)∈X, and feature bounds o= (L1,...,Ld,U1,...,Ud)∈Ω.(Lk,Uk). For
each feature of the training sample xik,(Lk,Uk)represents the current known range of its value.
RR n °9503
6 H. Kohler et al.
Initially, (Lk,Uk) = (0,1)for allk, which are iteratively reﬁned by taking Information-Gathering
Actions (IGAs) deﬁned below.
Action spaceA′={C1,...,CK}∪AI. An agent in an IBMDP can either take a base action
a∈{C1,...,CK}, or an IGA inAI={1,...,d}×{1
p+1,...,p
p+1}, with parameter p∈N.
Transition function. Ifa∈{C1,...,CK}, a new training sample is drawn at random from
the state spaceX, while feature bounds are reset to (0,1). Ifa∈AI, the base state is left
unchanged, but the feature bounds are reﬁned. Given a training sample xiwith feature bounds
o= (L1,...,Ld,U1,...,Ud)The information gathering action a= (k,v)will compare xikto
v′=v×(Uk−Lk) +Lk, and will set the lower bound Uktov′ifxik>v′, otherwise Lkis set to
v′.
Reward function. The reward for a base action in {C1,...,CK}is deﬁned by the base classiﬁ-
cation MDP reward function R. For an IGA inAIthe reward is a ﬁxed value ζ∈(−inf,Rmax)≡
ζ <1(the maximum value of the base reward function). We impose ζ <1, as otherwise a policy
never taking any base action would always be optimal, though this restriction is not enough to
prevent this degenerate case for RL algorithms.
Objective function. Solving an IBDMP is ﬁnding a polciy π∗∈Π :S′→A′such that
π∗= argmax
π∈ΠJ(π)whereJ()is the expectation of cumulative discounted rewards given by R′
(the MDP objective function of Section 3.2).
3.4.2 Learning a DT using Partially Observable RL
As stated in [31], a RL algorithm for an IBMDP should return a policy depending on fea-
ture bounds only in order to be able to extract a DT. So an agent learns a DT optimizing an
interpretability-performance trade-oﬀ encoded by an IBMDP reward function by ﬁnding a policy
π∗∈ΠDT: Ω→A′such thatπ∗= argmax
π∈ΠDTJ(π). We illustrate how an agent learning such a
policy is equivalent to learning a DT in Figure 1. To learn a policy depending on feature bounds
only, [31] proposes CUSTARD, a partially observable RL algorithm learning a policy depending
only on the feature bounds of the IBMDP state and value functions depending on the full IBMDP
state. We connect CUSTARD to the class of asymmetric RL algorithms ﬁrst studied empirically
in [21] and more recently theoretically in [3, 4].
Asymmetric Q-Learning. In asymmetric Q-Learning methods, like the DQN [17] version of
CUSTARD [31], an oracle state-action function depending on the full state of the IBMDP is
learned with TD-learning [30]. This oracle Q-function is used as target for the TD-learning of
an other state-action value function, this time, that depends only on feature bounds.
Asymmetric actor-critic. In asymmetric actor-critic methods, like the PPO [27] version of
CUSTARD [31], a value function depending on the full state of the IBMDP is learned and a
policy depending only on feature bounds is learned. Note that the policy gradient theorem [29]
still holds in the asymmetric setting. We show in the next section that CUSTARD fails to learn
DTs for simple tasks.
Inria
Classiﬁcation Trees with RL 7
xi= (0.7,0.23)o= (0,0,1,1)
C1C2
(a) Initialisation of the IBMPD.
xi= (0.7,0.23)o= (0,0,1,0.5)
C1C2
(b) We take IGA (xi2,0.5)and re-
ceive reward ζ.
xi= (0.7,0.23)
o= (0,0,1,0.25)
C1C2(c) We take IGA (xi2,0.25)and re-
ceive reward ζ.
xj= (0.2,0.9)
C1C2o= (0,0,1,1)(d) We take base action C1and re-
ceive reward 1 as x∈C1.
xi2≤0.5
(e) Taking an IGA in the IBMDP
adds a decision node to a DT.
 xi2≤0.5  
xi2≤0.25 ...(f) Taking an IGA in the IBMDP
adds a decision node to a DT.
 xi2≤0.5  
xi2≤0.25 ...
C1 ...(g) Taking a base action add a de-
cision node to the DT.
Figure 1: Example trajectory of an IBMDP ⟨p= 1⟩. The state space is divided in two; green
states are training samples with label C1, magenta states are training samples with label C2.
(1a): the IBMDP is initialised: the base state xiis drawn at random from the base MDP and the
feature bounds oare set to (0,0,1,1). (1b): the agent takes the IGA (xi2,0.5); the observation
part is updated to o= (0,0,1,0.5)becausexi2= 0.23≤0.5. Another IGA is taken in (1c). (1d):
the agent takes a base action, so a new base state xjis drawn from the base transition function
and the feature bounds are reset: o= (0,0,1,1).
RR n °9503
8 H. Kohler et al.
(a)
−1.0−0.5 0.0 0.5 1.0
ζ−1.00−0.75−0.50−0.250.000.250.500.751.00Cumulative discounted IBMDP rewardsRepeata∈{C1,C2}
Best DT of depth 1
Best DT of depth 2
Best unbalanced tree
Repeata∈AI
Best DT of depth 2 is optimal (b)
Figure 2: A depth 2 binary DT that is optimal w.r.t to the IBMDP objective when ζ= 0.5(2a)
and graphs of the IBMDP reward of diﬀerent DTs as a function of ζ(2b).
4 Partially Observable RL for simple Classiﬁcation Tasks
4.1 A Binary Classiﬁcation Benchmark
In this subsection we address the question: can CUSTARD [31] ﬁnd the optimal policy in ΠDT
with respect to the IBMDP objective of Section 3.4.2. To that end, we design toy experiments
that are amenable to an analysis thanks to their very small size.
The base tasks are binary supervised classiﬁcation tasks with 16 diﬀerent data points and
two numerical features in [0,1]. Each data can be perfectly classiﬁed using a depth-2 balanced
binary tree (see for example Figure 2a). We generate 5 such classiﬁcation tasks (hence, we will
benchmark CUSTARD to retrieve 5 diﬀerent DTs). Choosing ζ= 0.5,γ= 0.99andp= 1,
induces 5 IBMDPs for which balanced binary DTs of depth 2 are optimal (see Figure 2b).
4.2 CUSTARD to retrieve IBMDP-optimal DTs
To benchmark CUSTARD, we use stable-baselines3 [26] implementations of PPO and DQN
and modify them following the deﬁnitions of asymmetric Q-learning and asymmetric actor-critic
(see Section 3.4.2). The actor network in PPO is modiﬁed to only take feature bounds as inputs
while the critic network uses the full state. An additional Q-function depending on the full
IBMDP state is learned and used as the target network. We use 5 independent runs for each of
the 5 diﬀerent IBDMPs and normalize returns on each IBMDP so that results can be aggregated.
Fig. 3a shows that none of the agents were able to consistently retrieve the best DT despite the
extreme simplicity of the task.
4.3 Deriving an exact version of CUSTARD
To better understand how theoretically sound asymmetric actor-critic algorithms [3] like CUS-
TARD PPO fail to retrieve optimal DTs for simple supervised classiﬁcation tasks, we start from
an exact version of CUSTARD where Qπand the policy gradient are computed exactly, which is
possible in the tabular setup presented next. Note that there do not exist theoretical guarantees
for CUSTARD DQN [4, Section 4.4.2] equivalent to the one for CUSTARD PPO which is why
we focus on the latter.
Inria
Classiﬁcation Trees with RL 9
We introduce a variant of an IBMDP that enforces a maximum depth of the resulting DTs—
and ensures that the DT extraction algorithm always terminates. Let this maximum depth be
M+1.Mis the maximum number of consecutive time-steps during which a policy can select an
IGA. We implement this by forcing the policy to take a base action each time it has performed M
consecutive IGAs. Interestingly, if p+ 1is prime (where pis the parameter controlling splitting
thresholds in IBMDPs), the state space already provides such information to the policy:
Proposition 1 For an IBMDP, if p+ 1is prime then there is a mapping Ω↦→Nthat maps any
feature bound to the number of consecutive IGAs taken since the last base action.
In other words, the number of consecutive IGAs since the last base action is directly encoded
in the feature bounds (please see Appendix for proofs of this and all future statements). Thus
we can benchmark, on the same IBMDP, algorithms that enforce a maximum tree depth and
algorithms such as CUSTARD [31] that do not.
Having ﬁxed a maximum tree depth M+ 1, the number of unique feature bounds, i.e. the
cardinality of the observation space |Ω|, becomes ﬁnite and is at most (2pd)M. Herepd=|AI|
is the number of IGAs available at any time (if available at all) and the factor of 2stems from
the two possible state transitions following an IGA. Since the state-action space of an IBMDP
becomes ﬁnite, and its transition and reward functions are known, one can compute the policy
gradient exactly. This will let us investigate whether the sub-optimal performance of CUSTARD
is due to approximation errors—e.g. introduced by the learned value function—or if it is a
limitation of the gradient descent approach in itself.
Because Ωis ﬁnite, we can additionally implement policy gradient on tabular policies which
would eliminate any representation error of the policy. With a slight abuse of notation, we let
in this case θ(o,a)be the logit of observation-action pair (o,a), i.e.π(a|o)∝exp(θ(o,a)). By a
straightforward application of the chain rule on Lemma C.1 of [1] we obtain:
Proposition 2 Letθ∈RΩ×A′be the logits of a tabular reactive policy of the IBMDP, then:
∂J(πθ)
∂θ(o,a)=∑
s∈S′1O(s)=opπθ(s)
1−γπθ(a|o)Aπθ(s,a). (1)
Here 1O(s)=o= 1if the feature bound part of siso, 0 otherwise.
4.4 Ablation study
Starting from the exact CUSTARD algorithm deﬁned above, we perform an ablation study to
get to a CUSTARD algorithm similar to [31]. Algorithms are tested on the same IBMDPs as in
Section 4.1. The main features ablated from the exact CUSTARD are:
Using an approximated ˆQπ-function instead of a Qπ-table updated exactly. In that case,
ˆQπis a neural network similar to the one in CUSTARD PPO.
Using neural network for the policy πinstead of a table. In that case, the policy network
is similar to the one in CUSTARD PPO.
The results of the ablation are presented on Figure 3b. The exact CUSTARD algorithm
consistently ﬁnds the optimal policy. This is important as it means that the partially observable
frameworkofCUSTARDisnotthereasonforthepoorresults,atleastwhenimplementedexactly.
In practice however, we ﬁnd that both the approximation errors of the neural Q-function and the
policy representation error hinder performance. Indeed, when the policy is encoded by a neural
network in place of a table, the aggregated cumulative IBMDP reward converges to sub-optimal
values after just a few iterations. When the Qπfunction is encoded by a neural network, some
instances of the associated CUSTARD algorithm converged to the optimal policy—reﬂected by
the higher standard deviations on Fig. 3b, but many did not.
RR n °9503"
2312.03911,D:\Database\arxiv\papers\2312.03911.pdf,"While the paper highlights the benefits of gradient-guided nested sampling for high-dimensional problems, how does the use of gradients in this method compare to other gradient-based sampling techniques, such as Hamiltonian Monte Carlo (HMC), in terms of their strengths and limitations?","Gradient-guided nested sampling leverages gradients to guide the selection of live points, offering advantages in high-dimensional settings compared to traditional nested sampling methods. However, unlike HMC, which directly uses gradients to propose new samples, this method primarily uses gradients for reflection within the slice sampling framework, potentially leading to different exploration patterns and convergence properties.","Preprint.
Dynamic nested sampling (Higson et al., 2019; Speagle, 2020) is a variant of nested sampling which
proposes eliminating and replacing multiple points at each iteration. It was initially implemented in
thedyPolyChord2anddynesty3packages, but now is common to many implementations (Ash-
ton et al., 2022). It has two main use-cases; increasing the number of posterior samples generated
by nested sampling, and implementing parallelization schemes.
2.3 H AMILTONIAN SLICE SAMPLING
HSS was first introduced in the context of slice sampling (Neal, 2003), as a variant of Hamiltonian
Monte Carlo. As in slice sampling, the algorithm initially selects an initial point from the current
set of live points and a direction. An initial momentum variable𝑝ini, which is a𝑑-dimensional array
(where𝑑is the dimension of the space), is also defined, typically by randomly sampling a unit
vector. The algorithm then proceeds by simulating the trajectory of a particle located at the initial
point with the chosen initial velocity integrated with some time step Δ𝑡, such that at each step the
position of the particle is updated according to 𝑥′=𝑥+𝑝Δ𝑡. When the particle goes beyond the
slice, it is reflected back into the slice. This reflection is performed by updating the momentum from
𝑝to𝑝′using the equation
𝑝′=𝑝−2(𝑝·𝑛)𝑛, 𝑛 :=∇L(𝜃)/∥∇L(𝜃)∥, (5)
where𝑛is the unit vector in the direction of the likelihood gradient and thus the normal vector to an
iso-likelihood surface. Note that, because we are only using the direction of the gradient, one can
equivalently use the gradient of the log-likelihood, i.e. the score, which is more efficient to compute.
We summarize the algorithm in appendix B.
As highlighted in Neal (2003), eq. (5) is only exact when the point where the reflection of the
trajectory of the particle takes place is exactly on the boundary L(𝜃)=L∗. In practice, we can
either use a small tolerance 𝜖to define a neighborhood around the slice and reflect a trajectory
whenever the particle is within this neighborhood, or reflect a trajectory whenever the particle lands
at a point outside the boundary. The latter method has a theoretical risk of a particle getting “stuck”
behind the boundary (in which case the trajectory would be rejected, and a new initial momentum
would be chosen).
HSS (or GMC) has been used for nested sampling before (Betancourt, 2011; Feroz & Skilling, 2013;
Speagle, 2020). However, the dynesty implementation and defaults of HSS lacks the efficiency
and reliability of other sampling methods. In addition, in these public implementations, a score has
to be manually provided since the package is not compatible with modern differentiable program-
ming frameworks.
3 C ONTRIBUTIONS
In this section, we outline the key combination of ingredients in GGNS we use to significantly im-
prove its performance in high-dimensional settings in comparison with existing publicly available
tools.
In brief: we introduce trimming & adaptive step size techniques to remove the hyperparameter
tuning difficulties that have beset previous implementations, bring in the current state-of-the-art in
parallelization and cluster recognition, and implement in differentiable programming which removes
the requirement of providing a score function. With these innovations we find that one only needs
∼ O( 1)bounces to have decorrelated the chain from the start point, allowing sublinear 𝑓sampler
scaling. Finally, for maximum posterior scaling, the fact that gradients guide the path means one
no longer requires 𝑛live∼O(𝑑), giving an in-principle linear scaling with dimensionality for the
purposes of posterior estimation.
This linear scaling has a theoretical basis. For methods such as slice sampling, taking 𝑛steps in
a𝑑dimensional space leads to sampling an 𝑛-dimensional subspace. Therefore, we need to reach
O(𝑑)steps to explore the full space. For Hamiltonian slice sampling, on the other hand, every
time we use the gradient for a reflection, we get information about the full 𝑑dimensional space.
Therefore, each step is exploring the full volume, leading to the requirement of O(1)reflections.
2https://dypolychord.readthedocs.io/en/latest/
3https://dynesty.readthedocs.io/en/stable/
4
Preprint.
This is a similar argument to the better scaling with the dimensionality of Hamiltonian Monte Carlo
methods, compared to methods such as random walk Metropolis-Hastings.
In detail, our contributions are the following. We include a complete algorithm in Appendix H
ablation studies showing the importance of various components in Appendix G.
Adaptive Time Step Control We add an adaptive time-step control mechanism in the HSS algo-
rithm. In HSS, particles move in straight lines and eventually reflect off the hard likelihood boundary.
To ensure the trajectories between reflections strike a balance between efficiency and accuracy, we
introduce the concept of a variable time step, denoted as d𝑡. This time step is adjusted dynamically
during the course of the algorithm. By monitoring the number of reflections, we increase or decrease
d𝑡to optimize the computational efficiency while maintaining trajectory integrity. This approach,
inspired by Neal’s work in (Neal, 2003), enables us to employ larger time steps, thereby reducing
the number of reflections without compromising trajectory quality.
Trajectory Preservation In our second enhancement, we introduce a novel approach to preserv-
ing and utilizing trajectory4information during the HSS updates. Specifically, we store all points
along the trajectory after a designated number of reflections, where minref<maxref. This
archive of trajectory points allows us to efficiently select a new live point by uniformly sampling
the stored trajectories in a fully parallel manner. We also perturb trajectories by adding some noise
delta p, to achieve faster decorrelation of the samples.
Pruning Mechanism To further enhance efficiency, we introduce a “pruning” mechanism during
the HSS process. Points that have remained outside the slice for an extended duration are identi-
fied and removed from consideration. These pruned points are then reset to their initial positions,
and new momenta are randomly assigned. This mechanism significantly improves the computa-
tional efficiency of the proposed method, as we do not waste computational resources evaluating the
likelihood of points that have drifted far away from the slice.
Parallel Evolution of Live Points As in Burkoff et al. (2012); Henderson & Goggans (2014);
Martiniani et al. (2014); Handley et al. (2015b), we implement a dynamic approach to live point
management, whereby half of the live points are “killed” at each iteration and replaced with new
points. The new set of live points evolves with our HSS algorithm entirely in parallel, given that the
HSS algorithm boils down to simulating simple dynamics for all the live points. This parallelism
dramatically accelerates the algorithm’s execution.
Mode Collapse Mitigation To address the issue of mode collapse, we incorporate a clustering
recognition and evolution algorithm as developed and implemented in PolyChord (Handley et al.,
2015b). During the execution of the nested sampling process, we identify clusters of points and
keep track of the volume of each cluster. Then, we spawn points proportionally to this volume. This
addition helps maintain diversity among live points, preventing them from converging prematurely
to a single mode.
Robust Termination Criterion Our final contribution involves the introduction of an alternative
termination criterion, which we find to more robust. Unlike previous implementations of nested
sampling that rely on the remaining prior volume X, we utilize the property that the quantity 𝑋L(𝜃)
follows a characteristic trajectory—initially increasing, reaching a peak, and then decreasing. We
terminate the algorithm when 𝑋L(𝜃)has decreased by a predetermined fraction from its maximum
value. This termination is further explained in appendix I. This criterion proves to be more resilient
to variations in hyperparameters, including the number of live points.
Differentiable Programming Whilst nested sampling algorithms written in differential program-
ming languages exist in jax (Albert, 2020) and torch (Paszke et al., 2019; Anau Montel et al.,
2023), these do not make use of gradients in guiding the choice of a new live point, Therefore, their
choice of using a differentiable programming language is motivated mainly by the advantages of
GPU interoperability. To our knowledge, ours is the first algorithm utilizing the gradients derived
by differentiable programming to guide the choice of a new live point. Furthermore, this adaptation
of nested sampling to hardware intended for modern machine learning workflows, featuring massive
parallelization on GPUs; is particularly important in data processing settings that combine nested
sampling with deep learning, such as when the prior or likelihood models are given by deep neu-
4The term trajectory here refers to the states of the chain, not the intermediate states of a Hamiltonian
trajectory, as in Nishimura & Dunson (2020).
5
Preprint.
Figure 1: Comparison of likelihood evaluations (left panel) and error in the estimation of logZ
for different dimensionalities between this work (blue), and other nested sampling algorithms
(PolyChord in green and dynesty in orange; showing more efficient log-log linear scaling while
achieving a higher-fidelity estimate. All comparisons are done for a Gaussian likelihood with a di-
agonal covariance matrix. The error bars show the standard deviation over 10 runs. Error bars
forPolyChord anddynesty are also present, but barely visible. Note that the last point for
dynesty is not shown, as it is too large to fit in the plot.
ral networks. We show an example of this when we combine nested sampling and generative flow
networks in Section 5.
We summarize the hyperparameters in appendix C and provide an ablation study in appendix G.
4 E XPERIMENTS
4.1 C OMPARISON WITH OTHER NESTED SAMPLING METHODS
We compare the performance of gradient-guided nested sampling with two popular nested sampling
algorithms, already introduced in Section 2: PolyChord anddynesty . We use the same likeli-
hood function for all algorithms, which is a Gaussian likelihood with a diagonal covariance matrix,
and therefore hasDKL(P|Π)∝𝑑.
ForPolyChord , since𝑓sampler∝𝑛repeats=5𝑑, from eq. (2) we therefore expect 𝑛like∝𝑛live×5𝑑2.
Fordynesty , its default mode swaps between a region sampler with 𝑛like∝𝑛live×𝑒𝑑/𝑑0𝑑in
low dimensions to a slice sampler with 𝑛repeats=𝑑, giving𝑛like∝𝑛live×𝑑2. For GGNS , since
𝑓sampler∼maxref∼O( 1), we instead expect 𝑛like∝𝑛live×𝑑.
For demonstrating the various competing effects discussed in Sections 2.1, 2.2 and 3, we set
𝑛𝑙𝑖𝑣𝑒=200, independent of dimensionality. Note that constant 𝑛livemode is not usually recom-
mended for these samplers, since as discussed in Section 2.1 we need a minimum number of live
points to tune the live point generation hyperparameters. Since GGNS uses gradients to guide the
choice of live points, it is not restricted in this way.
The results are shown in Figure 1. We observe the scaling expected from the discussion above.
At constant𝑛live,PolyChord has quadratic scaling with dimensionality, providing good evidence
estimates until the dimensionality becomes similar to the 𝑛live=200.dynesty is most efficient but
exponentially scaling in low dimensions, and swaps to quadratic scaling in higher dimensions when
it moves over to slice sampling, at a lower constant than PolyChord due to its default 𝑛repeats=𝑑
in comparison with 5𝑑. Note however that this factor of 5default efficiency is traded off against
poor evidence estimates, even in low dimensions, once it is in slice sampling mode.
GGNS , as predicted, has by far the best (linear) scaling and performs evidence estimation accurately
even as the dimensionality approaches the number of live points since its live point generation is
guided by gradients rather than the other live points. Note, however, that as expected from eq. (3)
the error increases with the square root of the dimensionality at fixed 𝑛live.
6
Preprint.
Table 1: Log-evidence function estimation
bias (mean and standard deviation over 10
runs). The first rows are from our method,
while the rest are from Zhang & Chen
(2022); Lahlou et al. (2023). Note that the
last three methods are using importance-
weighted bound 𝐵RW. In bold font, we show
the estimates that are unbiased at the one
standard deviation level.Method Gaussian mixture Funnel
HMC −1.876±0.527−0.835±0.257
SMC −0.362±0.293−0.216±0.157
On-policy PIS-NN −1.192±0.482−0.018±0.020
Off-policy GFlowNet TB −0.003±0.011−0.026±0.020
On-policy GFlowNet TB −1.301±0.434−0.012±0.108
Ours 0.029±0.132−0.051±0.353
Figure 2: First row: The true image and noise that
we aim to reconstruct. Second row: The mean from
out gradient-guided nested sampling and the standard
deviation. We see how the GGNS posterior matches
the expected one.
TruthMean
1.0
0.5
0.00.51.0
Standard Deviation
0.000.050.100.150.200.25
Nested Sampling
1.0
0.5
0.00.51.0
0.000.050.100.150.200.25
Residual
0.00.10.20.3
4.2 C ALCULATION OF EVIDENCE
The calculation of the Bayesian evidence is a good way to evaluate the performance of inference
algorithms. In this section, we confirm the performance of gradient-guided nested sampling with
other methods to sample from a target density. We compare with the following methods: Hamilto-
nian Monte Carlo (HMC, MacKay, 2003; Hoffman et al., 2014), Sequential Monte Carlo (SMC,
Halton, 1962; Gordon et al., 1993; Chopin, 2002; Del Moral et al., 2006), Path Integral Sampler (PIS,
Zhang & Chen, 2022) and generative flow networks (GFlowNet Bengio et al., 2021; 2023; Lahlou
et al., 2023). For SMC, the settings follow the code release of Arbel et al. (2021)5. For PIS, we
compare with the on-policy version alone, as it obtains better results than the off-policy version. For
GFlowNet, we compare with the off and on policy versions, as they perform differently for different
tasks, the former being better for multimodal distributions as it is better at exploration, and the lat-
ter requiring less samples to converge. We focus on GFlowNets trained with the trajectory balance
loss (Malkin et al., 2022).
We compare these methods with GGNS in two tasks, already introduced in (Hoffman et al., 2014;
2019; Zhang & Chen, 2022; Lahlou et al., 2023): The first one is the funnel distribution , which
is a 10D distribution with a funnel shape. The second one is a Gaussian mixture in 2-dimension,
which consists of a mixture of 9 mode-separated Gaussians.
As our benchmark, we use the accuracy of the estimate in the log-evidence, or log-partition func-
tion. We report the mean and standard deviation of the estimation bias over 10 independent runs
in Section 4.2. We observe that gradient-guided nested sampling obtains unbiased estimates in both
tasks, something that does not happen for any of the other methods studied in this work. While
our standard deviation is higher than that of other methods, these can be reduced by adjusting the
hyperparameters of our method. However, eq. (3) shows that the nested sampling log-evidence error
can only be reduced sublinearly by increasing the number of live points 𝑛live, which increases the
computational cost. We cannot therefore expect substantial improvements in GGNS log-evidence
error bars without innovations in the nested sampling algorithm itself.
4.3 I MAGE GENERATION
We also tested the performance of GGNS on a high-dimensional problem, sampling the posterior
distribution over image pixels. To do this, we chose the problem of inferring the pixel values of
background galaxies in strong gravitational lensing systems (e.g., Adam et al., 2022). We assumed a
correlated (and non-zero mean) normal prior distribution for the background source. A sample from
5https://github.com/deepmind/annealed_flow_transport.git .
7
Preprint.
Figure 3: The error in the estimate of the nor-
malization of the reward function for the torus
task, as a function of the dimensionality of the
problem. We see that GGNS obtains consistent
estimates of the normalization, even in high di-
mensional settings.
FAB
Gradient-Guided Nested Sampling
Figure 4: Contour lines for the target distribution, and samples, for the first four dimensions of the
32-dimensional “Many Wells” problem. On the left, we show results for Flow Annealed Importance
Sampling Boostrap (FAB) with a replay buffer, and on the right for GGNS . Unlike FAB, GGNS
recovers all modes.
the prior was generated (representing the background galaxy) and was distorted by a the potential
of a foreground lens. Gaussian noise was then added to produce a noisy simulated data. Given the
data, the posterior of a model (a pixelated image of the undistorted background source) could be
calculated by adding the likelihood and the prior terms. Furthermore since the model is perfectly
linear (and known) and the noise and the prior are Gaussian, the posterior is a high-dimensional
Gaussian posterior that could be calculated analytically, allowing us to compare the samples drawn
withGGNS with the analytic solution.
Figure 2 shows a comparison between the true image, and its noise, and the one recovered by GGNS .
We see that we can recover both the correct image, and the noise distribution. We emphasize that
this is a uni-modal problem and that the experiment’s goal is to demonstrate the capability of GGNS
to sample in high dimensions (in this case, 256), such as images, and to test the agreement between
the samples and a baseline analytic solution.
4.4 S YNTHETIC MOLECULE TASK
Finally, we test GGNS on task, inspired by molecular conformations6. First, we build a reward
function on an 𝑛dimensional torus, which extends the reward function introduced in Lahlou et al.
(2023) to higher dimensional spaces. We define the reward function as:
6To apply this to real molecular configurations, we need a fully differentiable chemical simulator. We leave
this for future work
8
Preprint.
𝑅𝑛(x,𝛼,𝛽,𝑐)=©­
«𝑛∑︁
𝑖evensin(𝛼𝑥𝑖)+𝑛∑︁
𝑗oddcos(𝛽𝑥𝑗)+𝑐ª®
¬3
, 𝑥𝑖∈[0,2𝜋). (6)
This reward function models the multimodality we expect in molecular conformations, but has the
advantage of having a normalization that can be calculated analytically, as detailed in appendix F.
This means that we can assess the accuracy of GGNS by comparing the estimated normalization with
the true value in high dimensional settings. The results are shown in Section 4.4, where we see that
GGNS obtains consistent estimates of the normalization, even in high dimensional settings.
We also compare our method to Flow Annealed Importance Sampling Boostrap (FAB, Midgley
et al., 2022), with a replay buffer. This method has achieved state of the art results in sampling
tasks, and was already applied to the Boltzmann distribution of the alanine dipeptide molecule. We
useGGNS in two synthetic tasks first introduced in Midgley et al. (2022): A mixture of 40 Gaussians
in two dimensions, and the 32-dimensional “Many Well” problem7. The Many Well problem is a
particularly challenging one, due to the high its high dimensionality. We show results for the first
four dimensions in Figure 4. We see how GGNS does an even better job than FAB at recovering
all existing modes. We show visual comparison for the torus reward eq. (6) and the mixture of 40
Gaussians in appendix D.
5 C OMBINATION WITH GENERATIVE FLOW NETWORKS
We show how the samples obtained from the proposed nested sampling procedure can augment
amortized sampling algorithms, such as the generative flow networks considered in Section 4.2.
In Lahlou et al. (2023), it was shown that Euler-Maruyama integration of a stochastic differential
equation (SDE) can be viewed as the generative process of a generative flow network. The drift
and diffusion terms of the SDE can be trained as the GFlowNet’s forward policy to sample from a
target distribution given by an unnormalized density. In particular, GFlowNet objectives can be used
to learn the reverse to a Brownian bridge between a target distribution and a point, amounting to
approximating the reverse to particular kind of diffusion process. The trajectory balance objective –
which directly optimizes for agreement of the forward and reverse path measures – is equivalent in
expected gradient to the objective of the path integral sampler (Zhang & Chen, 2022) when trained
using on-policy forward exploration, but can also be trained using off-policy trajectories to accelerate
mode discovery, which was found to be beneficial in Lahlou et al. (2023); Malkin et al. (2023).
Extending the setup of Zhang & Chen (2022); Lahlou et al. (2023), we consider the problem of
sampling from a mixture of 25 well-separated Gaussians (see Figure 5), with horizontal and vertical
spacing of 5 between the component means and each component having variance 0.3. The learned
sampler integrates the SDE 𝑑x𝑡=𝝁(x,𝑡)𝑑𝑡+5𝑑w𝑡, where 𝝁is the output of a neural network (a
small MLP) taking xand𝑡as input, with initial condition x0=0up to time𝑡=1. The reward for x1is
the density of the target distribution. The neural network architecture and training hyperparameters
are the same as in Lahlou et al. (2023).
We generate a dataset Dof 2715 approximate samples from the target distribution first using GGNS ,
and then we use bootstrapping to generate equally weighted samples, using the bootstrapping algo-
rithm in Handley (2019). We consider five algorithms for training the SDE drift 𝝁:
(1)On-policy TB: We train 𝝁by optimizing the trajectory balance objective on trajectories ob-
tained by integration of the SDE being trained (equivalent to the path integral sampler objective
and to minimization of the KL divergence between forward and reverse path measures).
(2)Exploratory TB: We optimize the trajectory balance objective on trajectories obtained from a
noised version of the SDE, which adds Gaussian noise with standard deviation 𝜎to the drift
term at each step. Consistent with Lahlou et al. (2023), we linearly reduce 𝜎from 0.1to 0 over
the first 2500 training iterations. Such exploration is expected to aid in discovery of modes.
(3)Backward TB: We optimize the trajectory balance objective on trajectories sampled from the
reverse (diffusion) process begun at samples from D.
(4)Backward MLE: We sample trajectories from the reverse process begun at samples from D
and train 𝝁so as to maximize the log-likelihood of these trajectories under the forward process.
This objective amounts to training a diffusion model or score-based generative model (Song &
7We use the publicly available implementations of these reward functions at this URL.
9
Preprint.
Ermon, 2019; Ho et al., 2020) on D, as the optimal 𝝁is the score of the target distribution
convolved with a Gaussian and appropriately scaled.
(5)Forward + backward TB: We optimize the trajectory balance objective both on trajectories
obtained by integrating the SDE forward from samples from Dand on reverse trajectories begun
at samples fromD. This method resembles the training policy used by Zhang et al. (2022).
Forward on-policy TB
log Z (VLB): 1.540
Forward exploratory TB
log Z (VLB): 1.261
Backward TB
log Z (VLB): 3.914
Backward MLE
log Z (VLB): 0.340
Forward + backward TB
log Z (VLB): 0.225
Ground truth
log Z = 0
1000 2000 3000 4000 5000
Iteration0.00.20.40.60.81.0Z (variational lower bound)
Forward on-policy TB
Forward exploratory TB
Backward TB
Backward MLE
Forward + backward TB
Figure 5: Above: KDE plots of samples from
trained stochastic control models (ground truth
distribution at lower right). Mixing forward
sampling with noising trajectories initialized at
nested sampling outputs results in all modes being
modelled accurately. Below: Variational lower
bound on the partition function for the five sam-
plers. The theoretical maximum – achieved by the
Schr ¨odinger bridge between the Dirac distribution
at the origin and the target distribution – is 𝑍=1.KDE plots of samples from the trained mod-
els, as well as training metric plots, are shown
in Figure 5. Training with on-policy TB alone
(1) results in mode collapse, a typical effect of
training with a reverse KL objective (Malkin
et al., 2023). We see that while noise introduced
in forward exploration (2) helps mode discov-
ery, it is insufficient for all modes to be found.
Training using trajectory balance on backward
trajectories (3) results in spurious modes, as
the model is unlikely to see states that are far
from those seen along reverse trajectories from
Dduring training. Maximum (4) discovers all
modes of the distribution, as they are repre-
sented inD, but closer inspection reveals that
they are not modeled as accurately; this effect is
more pronounced when the dataset Dis small.
The best sampling performance is reached by
models that perform a mix of forward explo-
ration and reverse trajectories from the dataset
samples.
It is important to note that with well-tuned ex-
ploratory policies – as in (2) – it is possible
to coax the model into discovering all of the
modes and modeling them with high fidelity.
However, the model is highly sensitive to the
exploration parameters: if the exploration rate
is too high or not reduced slowly enough, the
model is slow to converge and blurs of ‘fat-
tens’ the modes, while an exploration rate that
is too low results in mode collapse. On the other
hand, mixing forward exploration with back-
ward trajectories from the approximate samples
allows the sampler to model all of the modes
accurately without such tuning. Notably, we
found that the forward trajectories in (5) can
be sampled either on-policy or from a tempered
policy, with little difference in performance.
6 D ISCUSSION AND CONCLUSIONS
We have introduced a new nested sampling algorithm based on Hamiltonian Slice Sampling.
Gradient-guided nested sampling improves on previous nested sampling algorithms by removing
the linear dependence of the number of live points on dimensionality. It also makes use of the power
of differentiable programming frameworks and parallelization for significant speed improvements.
We have shown that the proposed method scales much better with dimensionality than other nested
sampling algorithms, thanks to the use of gradient information. This better scaling allows us to
apply nested sampling in high-dimensional problems that were too computationally expensive for
previous methods. We have also shown that GGNS can be combined with generative flow networks
to obtain large numbers of samples from complex posterior distributions. Applications of GGNS to
difficult real-world inference problems, both on its own and in combination with amortized sampling
methods, are left for future work.
10
Preprint.
REPRODUCIBILITY STATEMENT
An implementation of GGNS inPyTorch (Paszke et al., 2019), along with notebooks to repro-
duce the results from the experiments, is available at https://github.com/Pablo-Lemos/
GGNS .
ACKNOWLEDGMENTS
P.L. would like to thank the Simons Foundation for their generous support. This research was
made possible by a generous donation by Eric and Wendy Schmidt with the recommendation of the
Schmidt Futures Foundation. The work is in part supported by computational resources provided
by Calcul Quebec and the Digital Research Alliance of Canada. Y .H. and L.P. acknowledge sup-
port from the National Sciences and Engineering Council of Canada grant RGPIN-2020-05102, the
Fonds de recherche du Qu ´ebec grant 2022-NC-301305 and 300397, and the Canada Research Chairs
Program. Y .B. and N.M. acknowledge funding from CIFAR, Genentech, Samsung, and IBM.
REFERENCES
Alexandre Adam, Adam Coogan, Nikolay Malkin, Ronan Legin, Laurence Perreault-Levasseur,
Yashar Hezaveh, and Yoshua Bengio. Posterior samples of source galaxies in strong gravitational
lenses with score-based priors. arXiv preprint arXiv:2211.03812 , 2022.
Joshua G. Albert. JAXNS: a high-performance nested sampling package based on JAX. arXiv
e-prints , art. arXiv:2012.15286, December 2020. doi: 10.48550/arXiv.2012.15286.
Noemi Anau Montel, James Alvey, and Christoph Weniger. Scalable inference with Autoregressive
Neural Ratio Estimation. arXiv e-prints , art. arXiv:2308.08597, August 2023. doi: 10.48550/
arXiv.2308.08597.
Michael Arbel, Alexander G. D. G. Matthews, and Arnaud Doucet. Annealed flow transport monte
carlo. International Conference on Machine Learning (ICML) , 2021.
Greg Ashton, Noam Bernstein, Johannes Buchner, Xi Chen, G ´abor Cs ´anyi, Andrew Fowlie, Farhan
Feroz, Matthew Griffiths, Will Handley, Michael Habeck, et al. Nested sampling for physical
scientists. Nature Reviews Methods Primers , 2(1):39, 2022.
Robert J. N. Baldock, Noam Bernstein, K. Michael Salerno, L ´ıvia B. P ´artay, and G ´abor Cs ´anyi.
Constant-pressure nested sampling with atomistic dynamics. Physical Review E , 96(4):043311,
October 2017. doi: 10.1103/PhysRevE.96.043311.
Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow
network based generative models for non-iterative diverse candidate generation. Advances in
Neural Information Processing Systems , 34:27381–27394, 2021.
Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio.
GFlowNet foundations. Journal of Machine Learning Research , 24:1–76, 2023.
Julian Besag. Comments on “representations of knowledge in complex systems” by u. grenander
and mi miller. J. Roy. Statist. Soc. Ser. B , 56(591-592):4, 1994.
Michael Betancourt. Nested sampling with constrained Hamiltonian Monte Carlo. In AIP Confer-
ence Proceedings , volume 1305, pp. 165–172. American Institute of Physics, 2011.
Benjamin Bloem-Reddy and John Cunningham. Slice sampling on hamiltonian trajectories. In
Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd Interna-
tional Conference on Machine Learning , volume 48 of Proceedings of Machine Learning Re-
search , pp. 3050–3058, New York, New York, USA, 20–22 Jun 2016. PMLR. URL https:
//proceedings.mlr.press/v48/bloem-reddy16.html .
Brendon J. Brewer and Daniel Foreman-Mackey. DNest4: Diffusive Nested Sampling in C++ and
Python. arXiv e-prints , art. arXiv:1606.03757, June 2016. doi: 10.48550/arXiv.1606.03757.
Johannes Buchner. UltraNest - a robust, general purpose Bayesian inference engine. The Journal of
Open Source Software , 6(60):3001, April 2021. doi: 10.21105/joss.03001.
11"
2306.16425,D:\Database\arxiv\papers\2306.16425.pdf,"In the context of cross-domain recommendation, what are the potential drawbacks of using a single embedding representation for samples from different domains, especially when the domains have significant differences?","A single embedding representation may not fully capture the cross-domain meaning when domains have large differences, as each domain possesses unique knowledge. This can lead to the loss of domain-specific information and hinder the effectiveness of knowledge transfer.","KDD ’23, August 6–10, 2023, Long Beach, CA, USA Wei Zhang, Pengye Zhang, Bo Zhang, Xingxing Wang, & Dong Wang
the loss of the mixed tower is calculated as follows:
𝐿𝑜𝑠𝑠 𝑚𝑖𝑥𝑒𝑑 =𝐿𝑜𝑠𝑠 𝑠𝑟𝑐+𝐿𝑜𝑠𝑠 𝑡𝑔𝑡
=𝑁𝑠∑︁
𝑥𝑖∈𝑋𝑠p𝑠
𝑖𝐿(𝑦𝑠
𝑖,𝑓(𝑥𝑖,Θ𝑚𝑖𝑥𝑒𝑑))
+𝑁𝑡∑︁
𝑥𝑖∈𝑋𝑡𝐿(𝑦𝑡
𝑖,𝑓(𝑥𝑖,Θ𝑚𝑖𝑥𝑒𝑑))(3)
where𝑥𝑖is the sample from the source or the target domain, 𝑓is
short for the neural network operation, 𝐿denotes the loss function,
and we use cross-entropy in this paper, and Θ𝑚𝑖𝑥𝑒𝑑 is the trainable
parameters in the mixed network. p𝑠
𝑖is the output of another com-
ponent indicating the weight of each source sample, and we will
discuss it in the next subsection. In this way, we can obtain a mixed
tower trained jointly by the source domain and the target domain.
2.3.3 Pure Tower. The pure tower only reads the target samples,
which are exactly the same as the target samples in the mixed tower.
and the loss in the pure target domain is formulated as follows:
𝐿𝑜𝑠𝑠 𝑝𝑢𝑟𝑒=𝑁𝑡∑︁
𝑥𝑖∈𝑋𝑡𝐿(𝑦𝑡
𝑖,𝑓(𝑥𝑖,Θ𝑝𝑢𝑟𝑒)) (4)
where𝑥𝑖∈𝑋𝑡is the sample only from the target domain, and
Θ𝑝𝑢𝑟𝑒 is the parameters in the pure tower. 𝐿𝑝𝑢𝑟𝑒 guides the back-
propagation of the pure network during training, which is the same
as the traditional single-domain training process. In this way, we
can obtain a pure tower trained only by the target domain.
2.3.4 Impact of Source Domain. In SCN, the two towers (mixed
and pure) have the same shape of network structure, learning rate,
etc. The only difference is that the mixed tower additionally reads
the samples of the source domain, and these source domain samples
affect the network parameters in the mixed tower through the gra-
dient of backpropagation. Therefore, we only need to use the same
set of target domain samples to calculate the loss on the two towers,
and the difference between the two losses is the impact result of
the source domain. Fortunately, during the training process, the
loss on the same target domain samples has already been calculated
as𝐿𝑜𝑠𝑠 𝑝𝑢𝑟𝑒 and𝐿𝑜𝑠𝑠 𝑡𝑔𝑡, so the information gain can be defined as
follows:
𝑟=𝐿𝑜𝑠𝑠 𝑔𝑎𝑖𝑛=𝐿𝑜𝑠𝑠 𝑝𝑢𝑟𝑒−𝐿𝑜𝑠𝑠 𝑡𝑔𝑡
=𝑁𝑡∑︁
𝑥𝑖∈𝑋𝑡[𝐿(𝑦𝑡
𝑖,𝑓(𝑥𝑖,Θ𝑝𝑢𝑟𝑒))−𝐿(𝑦𝑡
𝑖,𝑓(𝑥𝑖,Θ𝑚𝑖𝑥𝑒𝑑))](5)
where𝑁𝑡is the number of target samples, Θ𝑝𝑢𝑟𝑒 is the training
parameter of the pure tower in SCN. The term 𝑟represents how
much the loss reduces after the parameters are updated by addi-
tional source samples. When the information brought by the source
domain samples is positive, the mixed tower in the SCN can predict
more accurately. In particular, when predicting the same target
domain samples, 𝐿𝑜𝑠𝑠 𝑡𝑔𝑡will be smaller than 𝐿𝑜𝑠𝑠 𝑝𝑢𝑟𝑒, and𝑟>0
at this time. Otherwise, the noisy information will result in 𝑟<=0,
meaning that there is a negative transfer in the cross-domain train-
ing.2.3.5 Parameter Synchronization. To reduce the offset caused by
the two-tower learning route, we perform parameter synchroniza-
tion every𝑘steps, where 𝑘=1000 in this paper, that is, synchronize
the parameters of the mixed network to the pure network, so that
𝐿𝑜𝑠𝑠 𝑚𝑖𝑥𝑒𝑑/𝑠𝑟𝑐/𝑡𝑔𝑡/𝑝𝑢𝑟𝑒 will not introduce too much noise due to
network learning offset.
2.4 Information Flow Network
Main Description : Receive the reward and gradient updates from
the SCN, and transmit source domain sample weights and repre-
sentations. Since not all information from source domain samples
is useful, so weighted transmission is necessary.
The SCN is capable of assessing negative transfer and detecting
it during cross-domain training. As mentioned before, negative
transfer in existing methods is often caused by misusing source do-
main samples, some of which are only partially useful for the target
domain. For instance, a user’s interests in movies and books may
overlap, but not completely match due to differences in presentation
format.
The main function of the information flow network(IFN) in fig-
ure 3 has three aspects: 1) Evaluate the potential profit coefficient
of a single sample in the source domain for the target domain.2)
Consistent alignment of the evaluation goal with the goal of maxi-
mizing the effect of the target domain model. 3) Effective transfer of
the information gained in the source domain to the target domain.
Note that IFN mainly predicts the benefit of a single source do-
main sample to the target domain, while SCN accurately evaluates
the total benefit of the source domain based on the difference in
the prediction accuracy of the two towers for the target domain.
Conceptually, IFN is responsible for prediction and SCN is respon-
sible for evaluation. In section 2.4.2, the role of SCN in helping IFN
training will be explained in detail. In the following section 2.4.1,
we will first discuss how to effectively transfer the information
obtained in the source domain to the target domain.
Figure 3: An illustration of the IFN component.
2.4.1 Semantic-Align Network. In this section, we will discuss the
challenge of addressing the discrepancy in sample representation
caused by the dissimilarities of features in different domains. In
cross-domain transfer learning, it is crucial to effectively transfer
information from the source domain to the target domain, however,
A Collaborative Transfer Learning Framework for Cross-domain Recommendation KDD ’23, August 6–10, 2023, Long Beach, CA, USA
the mismatch of features can create a significant difference in sam-
ple representation. The number of features in various domains can
be vastly different, leading to the discrepancy between the source
and target domains in the SCN. Hence, a single mixed network
cannot be utilized for samples from both domains.
Despite the challenges, transfer learning has proven to be a pow-
erful tool in the fields of computer vision (CV) and natural language
processing (NLP). The success of transfer learning is attributed to
its ability to preserve cross-domain information through semantic
tokens between different domains. In the field of CV, a token can
be represented as pixels composing a point, line, or surface, while
in NLP, a token can be represented as a word. When facing a new
scenario, even though the problem form may change, these basic
tokens remain highly consistent with the source task, allowing for
knowledge transfer and application.
We consider users, items, and contexts as basic semantic tokens,
meaning that users interact with items in specific contexts. This
user-item-context relationship can be applied to any recommenda-
tion scenario. To handle this, we split the source domain sample
vector representation v𝑠
𝑖into three separate tokens: v𝑠𝑢𝑠𝑒𝑟,v𝑠
𝑖𝑡𝑒𝑚,
andv𝑠
𝑐𝑜𝑛𝑡𝑒𝑥𝑡. Each token comprises features related to it. For ex-
ample, the user token representation can be simplified as follows:
v𝑠
𝑢𝑠𝑒𝑟=[e𝑢𝑖𝑑||e𝑚𝑎𝑙𝑒||e𝑖𝑜𝑠] (6)
where e𝑢𝑖𝑑,e𝑚𝑎𝑙𝑒 ,e𝑖𝑜𝑠are the embeddings of user-id, gender, and
device type respectively.
In order to achieve the alignment of the source domain to the
target domain, we design a special compression network. The input
shape of this network is equivalent to the source domain semantic
token, and the output shape is equivalent to the corresponding
target domain semantic token:
v𝑠′
𝑢𝑠𝑒𝑟=𝑀𝐿𝑃(v𝑠
𝑢𝑠𝑒𝑟)∈R𝑑𝑖𝑚𝑡
𝑢𝑠𝑒𝑟 (7)
where𝑑𝑖𝑚𝑡𝑢𝑠𝑒𝑟 is the dimension of embedded user features in the
target domain, and 𝑀𝐿𝑃 is a set of DNN layers with activation
functions. The transformed source sample representations have
the same dimensionality as the target domain and are aligned at
the semantic token granularity. In this way, the subsequent source
domain can use the same network as the target domain for later
processing.
But it has to be mentioned that if in an ideal multi-domain envi-
ronment, the source domain and the target domain use exactly the
same features, then this part of the SAN structure can be ignored,
that is, the source domain can be directly used without additional
semantic token mapping. The original embedding result of the
domain is enough.
2.4.2 Selector Network. The selector network is mainly responsible
for evaluating the information gain of the source domain samples
for the target domain. Regarding the network design, this paper uses
a multi-layer MLP network, with the last layer using the sigmoid
activation function:
p𝑠
𝑖=1
1+𝑒𝑥𝑝(−𝑊𝑚ℎ𝑚−1+𝑏𝑚)(8)
where𝑚is the number of the DNN layers, ℎ𝑚−1is the output of
the last DNN layer, and 𝑊𝑚and𝑏𝑚are the params to be trained.In the previous SAN structure, the source domain sample v𝑠
𝑖
has been obtained and fed into the dual-tower to obtain the final
information gain evaluation.
Through the weight p𝑠
𝑖of IFN, the loss on each source sample is
dynamically adjusted. The network parameters in SCN are thus up-
dated through gradient from both the target domain and weighted
source domain, so the information from the source domain can be
""partially adapted "" to the target.
The selector network itself has no explicit labels, i.e., there is no
label information to indicate whether a source sample is suitable
for the target or not. Recall that in the SCN structure, it is already
possible to evaluate the gain increase 𝑟of a batch of source domain
samples for the target domain, but this gain 𝑟is a scalar value,
which cannot correspond to each source domain sample one-to-
one, and thus cannot use the traditional stochastic gradient descent
to get updated. But fortunately, there are already mature solutions
to such problems in the field of reinforcement learning, which can
be updated through reinforcement learning algorithms.
The term𝑟𝑘can be regarded as the reward, where 𝑘is the batch
indicator. The accumulated reward can be defined as 𝑟𝑎𝑐𝑐𝑢=𝑟𝑘+
𝛾∗𝑟𝑘+1+...+𝛾𝑛−𝑘∗𝑟𝑘, where𝑟𝑛is the reward of the last batch
and𝛾is the weight factor and is set to 0.80 uniformly.
Finally, we adopt REINFORCE[ 25] algorithm to update the IFN
component, and the parameters are updated as follows:
Θ𝑖𝑓 𝑛←Θ𝑖𝑓 𝑛+𝛼1
𝑁𝑁𝑠∑︁
𝑖=1▽Θ𝑖𝑓 𝑛𝑙𝑜𝑔(p𝑠
𝑖)𝑟𝑎𝑐𝑐𝑢(9)
where Θ𝑖𝑓 𝑛is the parameter of the IFN, p𝑠
𝑖is the output weight
for the𝑖th source sample, 𝑟𝑎𝑐𝑐𝑢is the accumulated reward, and 𝛼
controls how much influence the source samples should have on the
target domain. Like this, the 𝑟𝑎𝑐𝑐𝑢is regarded as the indirect loss
part. In order to make the gradient update of the selector more stable,
we accumulate the reward 𝑟𝑎𝑐𝑐𝑢and perform the gradient return
of the IFN every 1000 steps, so as to avoid excessive fluctuation of
the batch effect and stabilize the parameter update process.
With the symmetric design of SCN, we are able to evaluate the
information gain of source domain samples. When the information
gain brought by the sample is positive, the mixed tower in SCN can
predict the target more accurately, so that 𝐿𝑜𝑠𝑠 𝑡𝑔𝑡decreases, and the
reward is positive, that is, the reward encourages IFN to increase
this weight on such source samples. Similarly, when the sample
has a negative impact on the target domain, 𝐿𝑜𝑠𝑠 𝑡𝑔𝑡>𝐿𝑜𝑠𝑠 𝑝𝑢𝑟𝑒,
the reward is negative, and the selection strategy of IFN will be
corrected when the gradient is updated.
2.5 Representation Enhancement Network
Main Description : Use contrastive learning strategies to distin-
guish the similarity and differences between samples from different
domains, so as to reduce the harmful impact of domain-specific
knowledge in the source domain on the target domain.
In the paradigm of transfer learning, we have found through
practice that a single embedding representation cannot fully cover
cross-domain meaning when the differences between domains are
large. Because each domain has its own unique knowledge, even for
the same item in different domains. For example, in the search and
feed scenarios, the search domain has query information, while the
KDD ’23, August 6–10, 2023, Long Beach, CA, USA Wei Zhang, Pengye Zhang, Bo Zhang, Xingxing Wang, & Dong Wang
feed domain does not. Therefore, we attempt to retain some domain-
specific knowledge during the transfer process to accommodate
situations where the source and target domains have significant
differences. Specifically, in our scenario, our approach is mainly
based on the following two ideas:
Maximizing the similarity between sequence embedding.
For the same user, although there are differences in the behavior
sequences in different domains, the user’s interest remains stable
and is expressed through different heterogeneous sequence rep-
resentations in different domains. Therefore, we believe that the
user representations between different domains should be kept as
consistent as possible. User representations mainly consist of se-
quence features, so we believe that it is necessary to maximize the
similarity between sequence embeddings.
Minimizing the similarity between ID embedding. Based
on maximizing user similarity, we also want to retain the unique
information of each domain, expecting to provide differentiated
information from the source domain to the target domain. ID Em-
bedding is a microscopic representation of a domain, so we try to
control the differentiated representation of the same ID between
different domains, thereby retaining more additional information.
Figure 4: An illustration of the REN component.
As illustrated in Figure 4, we proposed the Representation En-
hancement Network (REN) to address the above ideas. These two
ideas act as constraints, somewhat similar to minimax game design,
can be formalized as follows:
𝑚𝑖𝑛
𝑠𝑖𝑚(v𝑠
𝑖𝑑,v𝑡
𝑖𝑑)
𝑚𝑎𝑥h
𝑠𝑖𝑚(v𝑠
𝑠𝑒𝑞,v𝑡
𝑠𝑒𝑞)i (10)
where v𝑠
𝑖𝑑is the id embedding after source domain embedding
lookup, and v𝑡
𝑖𝑑is the target one. v𝑠𝑠𝑒𝑞is for the sequence embedding.
In particular, for the sequence embedding, we use the average
pooling for the user behavior id sequence. 𝑠𝑖𝑚is the similarity
metric, where we use the normalized cosine similarity for simplicity
as is shown:
𝑠𝑖𝑚(v𝑖,v𝑗)=v𝑖·v𝑗
||v𝑖||2·||v𝑗||2(11)
where||·|| 2is the length in Euclidean space for the vector. The
term evaluates how similar are the two vectors. The loss formula
can be expressed as follows through an equivalent transformation
while optimizing both max and min at the same time:
𝐿𝑜𝑠𝑠 𝑟𝑒𝑛=𝑠𝑖𝑚(v𝑠
𝑖𝑑,v𝑡
𝑖𝑑)−𝑠𝑖𝑚(v𝑠
𝑠𝑒𝑞,v𝑡
𝑠𝑒𝑞) (12)The auxiliary loss of REN is added to the SCN and acts as an
assistant to help the mixed tower better recognize the domain-
specific and invariant features. The main idea of the dual embedding
design is to maintain the differences between domains, and also, the
separation of embedding also avoids possible conflicts of different
encoding among domains.
3 EXPERIMENTS
In this section, extensive offline and online experiments are per-
formed on both the large-scale recommender system in Meituan
and public benchmark datasets to answer the following research
questions:
RQ1 Does our proposed method outperform the baseline meth-
ods?
RQ2 How does each part of our CCTL model work?
RQ3 How does the model perform when deployed online?
Before presenting the evaluation results, we first introduce the
experimental setup, including datasets, baselines, metrics, and pa-
rameter settings.
3.1 Experimental Setup
3.1.1 Datasets. We adopt public datasets and industrial datasets
to comprehensively compare CCTL models and baseline models.
The statistics of the datasets are shown in Table 1.
Amazon dataset : This dataset has been widely used to evaluate
the performance of collaborative filtering approaches. We use the
two largest categories, Books and Movies&TV, to form the cross-
domain datasets. We convert the ratings of 4-5 as positive samples.
The dataset we used contains 979,151 user-book ratings and 432,286
user-movie ratings. There are 61,437 shared users, 835,005 books,
and 368,953 movies. We aim to improve the movie-watching domain
recommendation(as the target domain led from knowledge). The
statistics are summarized in the table, and hence we hope to improve
the target domain performance by transferring knowledge from
source domains.
Taobao dataset : This dataset was first released by Alibaba-
Taobao and is widely used as a common benchmark in CTR predic-
tion tasks. It is a user behavior log for Taobao mobile application,
including click, purchase, add-to-cart, and favorite behaviors. This
dataset involves 987,994 users and 4,162,024 items. We rank accord-
ing to the number of samples under each category, divide the top
70% categories as the source domain, and the remaining categories
as the target domain.
Industrial dataset : This dataset is an industrial dataset col-
lected by Meituan App which is one of the top-tier mobile Apps
in our country. It is much larger than the Amazon and Taobao
public datasets. In the source domain, the data contains the sample
information of the user on the list page, involving 230M users and
1.1M items, where 𝑀is short for 106. The target domain mainly
includes 183M users and 0.7M items. There is an intersection be-
tween the two domains on users and items, which are 67.83% and
70.26% respectively.
3.1.2 Baselines .We compare both single-domain and cross-domain
methods. Existing cross-domain methods are mainly proposed
A Collaborative Transfer Learning Framework for Cross-domain Recommendation KDD ’23, August 6–10, 2023, Long Beach, CA, USA
Table 1: Statistics of public and industrial datasets.
DatasetsSource Domain Target Domain
Users Items Samples Users Items Samples
Amazon 61,437 835,005 979,151 61,437 368,953 432,286
Taobao 690,006 69,386,796 70,000,525 296,716 29,436,098 30,150,807
Meituan 230M 1.1M 1200M 183M 0.7M 250M
for cross-domain recommendation and we extend them for cross-
domain CTR prediction when necessary (e.g., to include attribute
features rather than only IDs and to change the loss function).
A. Single-Domain Methods.
•LR. Logistic Regression[ 20]. It is a generalized linear model.
•DNN. Deep Neural Network[ 1]. It contains an embedding
layer, a fully connected layer(FC), and an output layer.
•DeepFM. DeepFM model[ 6]. It combines factorization ma-
chine(FM, the wide part) and DNN (the deep part), thus
modeling multi-order feature interactions.
•DIN. Deep Interest Network model[ 35]. It models dynamic
user interest based on historical behavior sequences.
•AFM. Attentional Factorization Machine[ 26]. It learns the
weight of feature interactions through an attention design.
B. Cross-Domain Methods.
•Finetune. Finetune Network[ 31]. A typical transfer learning
method in the field of CTR prediction trains on the source
domain and finetunes the networks in the target domain.
•DANN. Domain-Adversarial Neural Network[ 5]. It maps the
source and the target domain to the same space through the
adversarial network and a gradient flip design, so that the
domain-invariant information can be learned.
•CoNet. Collaborative cross Network[ 9]. The cross-connection
units are added on MLP++ to enable the knowledge to trans-
fer among domains.
•MiNet. Mixed Interest Network[ 17]. It jointly models long
and short-term interest in the source domain with a short-
term interest in the target domain.
•STAR. An industrial framework using star topology design[ 22],
which models different domains of CTR as multi-task learn-
ing.
•CCTL. CCTL is the proposed approach in this paper.
3.1.3 Parameter Settings .We set the dimension of the embed-
ding vectors for each feature as embedding dim = 8. We set the
number of fully connected layers in neural network-based mod-
els as L=4, where the industrial dataset is [1024, 512, 128, 1] and
the public dataset is [256, 128, 32, 1]. For the industrial dataset, the
batch size is set to 8192, while set to 128 for the Amazon and Taobao
datasets. All the methods are implemented in Tensorflow and we
use Adam[ 4] as the optimization algorithm uniformly. Each method
has been run 3 times and the reported are the average results.
3.1.4 Metrics .We evaluate the CTR prediction performance with
two widely used metrics. The first one is the area under the ROC
curve (AUC) which reflects the pairwise ranking performance be-
tween click and non-click samples. The other metric is log loss,
which is to measure the overall likelihood of the test data andhas been widely used for classification tasks. At the same time,
We deploy models online in real industrial systems. Our online
evaluation metric is the real CTR( 𝐶𝑇𝑅 =#𝑐𝑙𝑖𝑐𝑘
#𝑝𝑎𝑔𝑒𝑠) which is de-
fined as the number of clicks over the number of impressions and
GMV(𝐺𝑀𝑉 =1000∗#𝑝𝑎𝑦 𝑎𝑚𝑜𝑢𝑛𝑡
#𝑝𝑎𝑔𝑒𝑠) which represents the revenue
amount per thousand impressions, as an evaluation metric in our
A/B tests.
3.2 RQ1: Does our CCTL model outperform the
baseline model?
We evaluate the performance of CCTL and baseline models on three
datasets. The performances of single-domain and multi-domain are
compared simultaneously. The single-domain model is introduced
for two purposes: 1) to explain the limitations of the current single-
domain, and show the non-optimal fintune problem; 2) to make an
intuitive comparison with the multi-domain model. From Table 2,
we have the following observations:
In single-domain, the more complex the model, the better the
effect, but its marginal returns show diminishing status. At the
same time, we noticed that as the computational complexity of the
model increases, the benefit of AUC increases to a marginally di-
minishing state, especially when both DIN and AFM adopt variants
based on the attention structure, and the performance improvement
is limited. Our analysis may be based on the current model com-
plexity, which is gradually approaching the amount of information
contained in the sample, resulting in limited effect improvement.
For cross-domain methods, the overall effect is better than single-
domain ones, and the benefits of cross-domain information flow
are better than the increase in the complexity of a single model.
However, the finetune model may not be better than the traditional
single model (for example, on the Meituan dataset, its effect basi-
cally slightly worse than DIN and AFM), we think the reason is the
non-optimal solution finetune problem mentioned above. CoNet
introduces cross-connection units to enable dual knowledge trans-
fer across domains. DANN maps the source and target domains to
the same space through an adversarial network and gradient flip
design so that the samples in the source domain similar to the tar-
get can be integrated. However, these two methods also introduce
higher complexity and random noise, because all samples are used
indiscriminately. The MiNet method starts from the perspective of
information flow from the source domain to the target domain, tries
to split information from a long-term and short-term perspective,
and transfers interest information from the source domain through
a specific network structure. But the domain-specific info is not
considered, and it lacks the ability to filter and constrain the infor-
mation on the source domain. The STAR method adopts separate
MLPs to tackle multiple domains, while we see the negative trans-
fer in Meituan and Taobao datasets, which is probably caused by
KDD ’23, August 6–10, 2023, Long Beach, CA, USA Wei Zhang, Pengye Zhang, Bo Zhang, Xingxing Wang, & Dong Wang
Table 2: Experimental results on different methods.
DomainsMeituan Amazon Taobao
Model AUC Loss AUC Loss AUC Loss
Single DomainLR 0.6805 0.5589 0.7421 0.4677 0.7907 0.3894
DNN 0.6876 0.5537 0.7456 0.4448 0.7953 0.3741
DeepFM 0.6893 0.5497 0.7473 0.4432 0.7962 0.3724
DIN 0.6932 0.5468 0.7480 0.4454 0.8095 0.3692
AFM 0.6923 0.5457 0.7460 0.4492 0.7990 0.3854
Cross DomainFinetune 0.6921 0.5452 0.7564 0.4381 0.8280 0.3585
DANN 0.6946 0.5404 0.7610 0.4331 0.8420 0.3601
CoNet 0.6942 0.5416 0.7621 0.4332 0.8418 0.3666
MiNet 0.6953 0.5377 0.7665 0.4305 0.8447 0.3640
STAR 0.6945 0.5391 0.7672 0.4298 0.8440 0.3643
CCTL 0.7001 0.5288 0.7790 0.4230 0.8521 0.3578
domain shifts. In contrast, our proposed CCTL use an information
flow network to provide filtered and beneficial samples to the target
domain. Apart from the alignment for semantic tokens, we further
use a representation enhancement network to maintain the domain
differences. Therefore, the performance in the target domain can
be improved compared with baselines.
3.3 RQ2: How does each part of our CCTL
model work?
A.Effect of the Selector Network .We evaluate the role of the
Selector Network in IFN in two ways on the Meituan dataset. Firstly,
we track the output sample weight P of the selector during the train-
ing process, then plot the distribution histogram in tensorboard.
The horizontal axis is the selector output probability, the vertical
axis is the number of samples, and the right side is marked as the
steps of model training. As can be seen from Figure 5(a), the dis-
tribution changes when training, and finally most of the samples
form a similar Gaussian distribution with an average of P=0.656.
And some of the source domain samples are not selected (P=0 on
the left). This shows that the probability of the selector network is
gradually learned as the model is trained.
(a) Selector 𝑃’s trend in training.
(b) Sensitivity for selector weight.
Figure 5: The effect evaluation of the Selector Network.
Secondly, we adjust the weight parameter 𝛼in eq (9)of the
selector network, where 𝛼∈[0,1]. By adjusting 𝛼the importance
of the selector network can be changed. When 𝛼is 0, it degenerates
into an ordinary multi-domain sample task. As can be seen from
Figure 5(b), when the size of 𝛼is changed from small to large, the
effect test-AUC in the target domain shows a growing trend, andas𝛼continues to approach 1, the effect growth slows down. This
shows the selector has a positive influence on the model.
B.Effect of different modules. In the proposed CCTL framework
of this paper, IFN and REN can be viewed as auxiliary tasks to assist
the main module SCN in training the target domain. Therefore, in
order to verify their impact, we conducted ablation experiments on
the IFN and REN modules respectively. The results of the ablation
experiments are shown in table 3.
Table 3: The ablation study for the components in CCTL.
Ablations AUC LogLoss
CCTL w/o IFN 0.8385 0.3647
CCTL w/o REN 0.8493 0.3602
Full CCTL 0.8521 0.3578
It can be seen from table 3 that the effect of removing IFN is
greater than that of releasing REN. Removing IFN means using
all source domain samples for training. In this case, the model’s
performance is only slightly better than that of the Finetune, which
indicates that using source domain samples indiscriminately is not
a wise choice. When the REN module is removed, the represen-
tations of different domains may be close to each other, resulting
in a lack of discrimination between different domains. Through
the above ablation experiments, the necessity of IFN and REN has
been demonstrated, and it is shown that IFN contributes more ben-
efits, indicating that the selection and weighting of source domain
samples are very important.
3.4 RQ3: How does the model perform when
deployed online?
We deployed CCTL in Meituan App, where the ad serving system
architecture is shown in Figure 6. It is worth noting that although we
involved multiple network structures during offline training when
serving online, we only need to use the pure tower in SCN(note:
the parameters in the mixed tower have been synchronized to the
pure tower during training), i.e., just export the network of target
domain in the online model service. In this way, we ensure that its
model complexity is equivalent to that of the online model, without
adding additional calculations to the online service.
A Collaborative Transfer Learning Framework for Cross-domain Recommendation KDD ’23, August 6–10, 2023, Long Beach, CA, USA
Figure 6: Architecture of the online deployment with CCTL.
We conducted online experiments in an A/B test framework over
three months during Sep.2022 - Dec. 2022, where the base serving
model is a variant of DIN according to our business characteris-
tics. Our online evaluation metric is the real CTR( 𝐶𝑇𝑅=#𝑐𝑙𝑖𝑐𝑘
#𝑝𝑎𝑔𝑒𝑠)
which is defined as the number of clicks over the number of impres-
sions and GMV( 𝐺𝑀𝑉 =1000∗#𝑝𝑎𝑦 𝑎𝑚𝑜𝑢𝑛𝑡
#𝑝𝑎𝑔𝑒𝑠) which represents the
revenue amount per thousand impressions. A larger online CTR
indicates the enhanced effectiveness of a CTR prediction model.
The online A/B test shows that CCTL leads to an increase of online
CTR of 4.37% and GMV of 5.43% compared with the base model.
This result demonstrates the effectiveness of CCTL in practical CTR
prediction tasks. After the A/B test, CCTL serves the main ad traffic
in Meituan App.
4 RELATED WORKS
Our paper belongs to the field of cross-domain CTR prediction and
has close ties to single-domain CTR prediction. In this section, we
provide a brief overview.
Cross-Domain CTR Prediction: Cross-domain learning refers
to a technique that enhances the performance of a target domain
by incorporating knowledge from source domains. This is accom-
plished through various algorithms, including transfer learning[ 16],
multi-task learning[ 33], and multi-view learning[ 30]. These meth-
ods aim to transfer knowledge in a deep manner, allowing multi-
ple base networks to benefit from each other, and provide more
flexible representation transfer options. Cross-domain learning is
useful for various applications, including computer vision, nat-
ural language processing, and recommendation systems, where
data and tasks across domains are related. In the CTR prediction
field, STAR[22] tries to mix multiple sources of data for training a
unified model. CoNet[ 9] aims to make cross-domain information
flow more efficiently by enabling dual knowledge transfer through
cross-mapping connections between hidden layers in two base net-
works. MiNet[ 17] further divides multiple sources into long-term
and short-term interests and directly models cross-domain interests.
Under the paradigm of union training, the DASL[ 14] model includes
both the dual embedding and dual attention components to jointly
capture cross-domain information. Meanwhile, AFT[ 7] learns fea-
ture translations between different domains within a generative
adversarial network framework.
Data sparsity[ 15] is also a classic problem that Cross-Domain
Recommendation seeks to solve. This may be due to the sparsity
of the target domain data itself, or to local sparsity caused by the
long-tail effect and cold-start effect. Commonly used methods forsuch problems involve using data from the source domain to ad-
dress the sparsity of target domain data. For example, ProtoCF[ 21]
efficiently transfers knowledge from arbitrary base recommenders
by extracting meta-knowledge to construct discriminative proto-
types for items with very few interactions. PTUPCDR[ 38] proposes
that a meta-network fed with users’ characteristic embeddings
is learned to generate personalized bridge functions for achiev-
ing personalized transfer of user preferences. TLCIOM[ 11] uses
domain-invariant components shared between dense and sparse
domains to guide neural collaborative filtering to achieve infor-
mation flow from one dense domain to multiple sparse domains.
CCDR[ 28] and SSCDR[ 10] address the problem of the cold-start
domain lacking sufficient user behaviors through techniques such
as contrastive learning and semi-supervised learning.
All these approaches tried different ways of separating domain
info, or integrating the source samples through different model
architectures, while few of them focus on the information weight
of the source sample, which is the key point this paper addresses.
Single-Domain CTR Prediction: CTR prediction models have
shifted from traditional shallow approaches to modern deep ap-
proaches. These deep models commonly use embedding and MLP.
Wide&Deep[ 1] and DeepFM[ 6] improve expression power by com-
bining low- and high-order features. User behavior is transformed
into low-dimensional vectors through embedding techniques. DIN[ 35]
uses attention to capture the diversity of user interest in historical
behavior. DIEN[ 34] adds an auxiliary loss and combines attention
with GRU to model evolving user interest. MIND[ 13] and DMIN[ 27]
argue for multiple representations to capture complex patterns in
user and item data, using capsule networks and dynamic routing.
The Transformer is used for feature aggregation inspired by the suc-
cess of self-attention in sequence-to-sequence learning. MIMN[ 18]
employs a memory-based architecture to aggregate features and
addresses long-term user interest modeling. SIM[19] extracts user
interest using cascaded search units, improving scalability and
accuracy in modeling lifelong behavior data. The framework of
this paper focuses on the information flow design, so these single-
domain techniques can be merged with ours easily in the SCN
design, which may further boost the model performance.
5 CONCLUSION
In this paper, we propose a collaborative cross-domain transfer
learning framework, which enables the reuse of beneficial source do-
main samples. In the framework, the information flow network(IFN)
can evaluate the importance of the source domain samples, in which
the SAN helps align meaningful semantic tokens among domains.
The symmetric companion network(SCN) is designed with a sym-
metric structure to approximately fit the information gain of the
samples in the source domain to the target domain. The representa-
tion enhancement network(REN) maintains domain-specific char-
acteristics through a contrastive learning mechanism. In this way,
only beneficial information in the source domain is transferred to
the target, which reduces the seesaw effect and the negative transfer.
Then we validate our method on both public and real-world indus-
trial datasets by performance comparison and ablation study, which
proved its effectiveness. Finally, we deployed the CCTL model on
Meituan App. The CCTL model has brought significant business
improvement and served as the mainstream traffic.
KDD ’23, August 6–10, 2023, Long Beach, CA, USA Wei Zhang, Pengye Zhang, Bo Zhang, Xingxing Wang, & Dong Wang
REFERENCES
[1]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .
2016. Wide & deep learning for recommender systems. In Proceedings of the 1st
workshop on deep learning for recommender systems . 7–10.
[2]Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks
for youtube recommendations. In Proceedings of the 10th ACM conference on
recommender systems . 191–198.
[3]Michael Crawshaw. 2020. Multi-task learning with deep neural networks: A
survey. arXiv preprint arXiv:2009.09796 (2020).
[4]John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods
for online learning and stochastic optimization. Journal of machine learning
research 12, 7 (2011).
[5]Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. 2016.
Domain-adversarial training of neural networks. The journal of machine learning
research 17, 1 (2016), 2096–2030.
[6]Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: a factorization-machine based neural network for CTR prediction. arXiv
preprint arXiv:1703.04247 (2017).
[7]Xiaobo Hao, Yudan Liu, Ruobing Xie, Kaikai Ge, Linyao Tang, Xu Zhang, and Leyu
Lin. 2021. Adversarial feature translation for multi-domain recommendation. In
Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data
Mining . 2964–2973.
[8]Tianxing He, Jun Liu, Kyunghyun Cho, Myle Ott, Bing Liu, James Glass, and
Fuchun Peng. 2021. Analyzing the forgetting problem in pretrain-finetuning of
open-domain dialogue response models. In Proceedings of the 16th Conference
of the European Chapter of the Association for Computational Linguistics: Main
Volume . 1121–1133.
[9]Guangneng Hu, Yu Zhang, and Qiang Yang. 2018. Conet: Collaborative cross
networks for cross-domain recommendation. In Proceedings of the 27th ACM
international conference on information and knowledge management . 667–676.
[10] SeongKu Kang, Junyoung Hwang, Dongha Lee, and Hwanjo Yu. 2019. Semi-
supervised learning for cross-domain recommendation to cold-start users. In
Proceedings of the 28th ACM International Conference on Information and Knowl-
edge Management . 1563–1572.
[11] Adit Krishnan, Mahashweta Das, Mangesh Bendre, Hao Yang, and Hari Sundaram.
2020. Transfer learning via contextual invariants for one-to-many cross-domain
recommendation. In Proceedings of the 43rd International ACM SIGIR Conference
on Research and Development in Information Retrieval . 1081–1090.
[12] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang.
2022. Fine-tuning can distort pretrained features and underperform out-of-
distribution. arXiv preprint arXiv:2202.10054 (2022).
[13] Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Huan Zhao, Pipei Huang,
Guoliang Kang, Qiwei Chen, Wei Li, and Dik Lun Lee. 2019. Multi-interest
network with dynamic routing for recommendation at Tmall. In Proceedings of
the 28th ACM international conference on information and knowledge management .
2615–2623.
[14] Pan Li, Zhichao Jiang, Maofei Que, Yao Hu, and Alexander Tuzhilin. 2021. Dual
Attentive Sequential Learning for Cross-Domain Click-Through Rate Prediction.
InProceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &
Data Mining . 3172–3180.
[15] Tong Man, Huawei Shen, Xiaolong Jin, and Xueqi Cheng. 2017. Cross-domain
recommendation: An embedding and mapping approach.. In IJCAI , Vol. 17. 2464–
2470.
[16] Shuteng Niu, Yongxin Liu, Jian Wang, and Houbing Song. 2020. A decade survey
of transfer learning (2010–2020). IEEE Transactions on Artificial Intelligence 1, 2
(2020), 151–166.
[17] Wentao Ouyang, Xiuwu Zhang, Lei Zhao, Jinmei Luo, Yu Zhang, Heng Zou,
Zhaojie Liu, and Yanlong Du. 2020. Minet: Mixed interest network for cross-
domain click-through rate prediction. In Proceedings of the 29th ACM international
conference on information & knowledge management . 2669–2676.
[18] Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Practice
on long sequential user behavior modeling for click-through rate prediction.
InProceedings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining . 2671–2679.
[19] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang
Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelongsequential behavior data for click-through rate prediction. In Proceedings of the
29th ACM International Conference on Information & Knowledge Management .
2685–2692.
[20] Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting
clicks: estimating the click-through rate for new ads. In Proceedings of the 16th
international conference on World Wide Web . 521–530.
[21] Aravind Sankar, Junting Wang, Adit Krishnan, and Hari Sundaram. 2021. Protocf:
Prototypical collaborative filtering for few-shot recommendation. In Proceedings
of the 15th ACM Conference on Recommender Systems . 166–175.
[22] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang
Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, et al .2021. One model to
serve all: Star topology adaptive recommender for multi-domain ctr prediction. In
Proceedings of the 30th ACM International Conference on Information & Knowledge
Management . 4104–4113.
[23] Huinan Sun, Guangliang Yu, Pengye Zhang, Bo Zhang, Xingxing Wang, and
Dong Wang. 2022. Graph Based Long-Term And Short-Term Interest Model
for Click-Through Rate Prediction. In Proceedings of the 31st ACM International
Conference on Information & Knowledge Management . 1818–1826.
[24] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network
for ad click predictions. In Proceedings of the ADKDD’17 . 1–7.
[25] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for
connectionist reinforcement learning. Machine learning 8, 3 (1992), 229–256.
[26] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.
2017. Attentional factorization machines: Learning the weight of feature interac-
tions via attention networks. arXiv preprint arXiv:1708.04617 (2017).
[27] Zhibo Xiao, Luwei Yang, Wen Jiang, Yi Wei, Yi Hu, and Hao Wang. 2020. Deep
multi-interest network for click-through rate prediction. In Proceedings of the
29th ACM International Conference on Information & Knowledge Management .
2265–2268.
[28] Ruobing Xie, Qi Liu, Liangdong Wang, Shukai Liu, Bo Zhang, and Leyu Lin.
2022. Contrastive cross-domain recommendation in matching. In Proceedings
of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining .
4226–4236.
[29] Yufeng Xie, Mingchu Li, Kun Lu, Syed Bilal Hussain Shah, and Xiao Zheng.
2022. Multi-task Learning Model based on Multiple Characteristics and Multiple
Interests for CTR prediction. In 2022 IEEE Conference on Dependable and Secure
Computing (DSC) . IEEE, 1–7.
[30] Xiaoqiang Yan, Shizhe Hu, Yiqiao Mao, Yangdong Ye, and Hui Yu. 2021. Deep
multi-view learning methods: A review. Neurocomputing 448 (2021), 106–129.
[31] Xiangli Yang, Qing Liu, Rong Su, Ruiming Tang, Zhirong Liu, Xiuqiang He, and
Jianxi Yang. 2022. Click-through rate prediction using transfer learning with
fine-tuned parameters. Information Sciences 612 (2022), 188–200.
[32] Xuanhua Yang, Xiaoyu Peng, Penghui Wei, Shaoguo Liu, Liang Wang, and Bo
Zheng. 2022. AdaSparse: Learning Adaptively Sparse Structures for Multi-Domain
Click-Through Rate Prediction. In Proceedings of the 31st ACM International
Conference on Information & Knowledge Management . 4635–4639.
[33] Yu Zhang and Qiang Yang. 2021. A survey on multi-task learning. IEEE Transac-
tions on Knowledge and Data Engineering (2021).
[34] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang
Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate
prediction. In Proceedings of the AAAI conference on artificial intelligence , Vol. 33.
5941–5948.
[35] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui
Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through
rate prediction. In Proceedings of the 24th ACM SIGKDD international conference
on knowledge discovery & data mining . 1059–1068.
[36] Feng Zhu, Chaochao Chen, Yan Wang, Guanfeng Liu, and Xiaolin Zheng. 2019.
Dtcdr: A framework for dual-target cross-domain recommendation. In Proceed-
ings of the 28th ACM International Conference on Information and Knowledge
Management . 1533–1542.
[37] Feng Zhu, Yan Wang, Chaochao Chen, Jun Zhou, Longfei Li, and Guanfeng Liu.
2021. Cross-domain recommendation: challenges, progress, and prospects. arXiv
preprint arXiv:2103.01696 (2021).
[38] Yongchun Zhu, Zhenwei Tang, Yudan Liu, Fuzhen Zhuang, Ruobing Xie, Xu
Zhang, Leyu Lin, and Qing He. 2022. Personalized transfer of user preferences for
cross-domain recommendation. In Proceedings of the Fifteenth ACM International
Conference on Web Search and Data Mining . 1507–1515."
2103.04909,D:\Database\arxiv\papers\2103.04909.pdf,"The paper describes a model-based reinforcement learning approach for autonomous racing.  What are the key challenges in applying this approach to real-world scenarios, and how does the paper address these challenges?","The paper addresses the sim2real gap, a major challenge in applying reinforcement learning to real-world scenarios, by demonstrating that the learned latent state-space model is robust enough to transfer the policy from simulation to a real-world test environment.  The paper also addresses the issue of bang-bang control, which is often undesirable in real-world robotics applications, by regularizing the actor's loss with action penalties to discourage sharp changes in control commands.","Fig. 4: Top-view of the race tracks and relative Progress
Maps adopted for reward design. Austria (length = 7945𝑚),
Columbia( 612𝑚),Treitlstrasse( 5165𝑚)areusedfortraining,
Barcelona ( 201𝑚) is used only for evaluation. Red circles
indicate diﬃcult parts of the track. The red circles show where
all model-free agents fail to improve their performance.
Reward shaping. The reward design is critical for learning a
policy. Sparse rewards over long episodes makes the learning
problem challenging. Starting from a simple approach that
rewards agents only when ﬁnished the race, we gradually
reﬁne the reward to provide a high-density learning signal.
The reward signal we propose for this task is deﬁned as
𝑐j𝑝𝑡 𝑝𝑡 1j=𝑐Δ𝑝𝑡 (8)
where𝑝𝑡denotes the progress that has been made on the
track at time 𝑡and𝑐is a constant scalar. When colliding
with the wall or other objects, the agent receives a penalty
and the episode terminates (see also Figure 2). The progress
is computed by a distance transform applied to the gridmaps
for each track. This yields a normalized progress estimate
for each pixel on the map at a resolution of 5cm per pixel.
Therefore, the progress value can be easily queried from these
maps given the current pose of the car. In Figure 4 (bottom)
we show the precomputed progress maps for each of our
tracks. The progress value ranges from 0 (light) to 1 (dark),
where a progress of 1 corresponds to one full lap.
V. E/x.pc/p.pc/e.pc/r.pc/i.pc/m.pc/e.pc/n.pc/t.pc/s.pc
Here, we describe our experiments and evaluate the per-
formance of model-free versus model-based RL algorithms.
A. Experimental Setup
For our experiments, we train the agents in simulation
before we transfer them to real cars. In the following, we
provide an overview of the simulation and real-world setup
as well as the training process.
Simulation environment. To simulate a car model, we use
the open-source physics engine PyBullet [ 47]. The car model
is a rigid body system based on the URDF model in [ 48].
Our training environment can simulate a broad set of sensory
inputs, such as LiDAR sensors, RGB cameras, and odometry.
The model is actuated by applying force to the steering and
acceleration joints.
Training pipeline. During training in simulation, we place
the agent randomly on the track at the beginning of an
episode. Each training episode has a maximum length of 2000
timesteps, resulting in 20 seconds of real-time experience.
Agents learn to directly maximize the progress covered inTABLE I: Track characteristics.
Track Min. Width Length Min. Radius
Austria 1.86m 79.45m 2.78m
Columbia 3.53m 61.20m 7.68m
Treitlstrasse 0.89m 51.65m 3.55m
Barcelona 1.86m 201.00m 2.98m
a small, predeﬁned time interval. Observations and actions
are normalized. To account for latency experienced during
testing and to increase the eﬀectiveness of actions, we repeat
each action multiple times. We regularly evaluate the agent by
placing it on a ﬁxed starting position for each track and let it
run for at most 4000timesteps (i.e., 40 seconds) and average
the maximum progress reached over ﬁve consecutive trials.
Dreamer is trained for 2 million timesteps. The model-free
baselines are trained for 8 million timesteps.
Hardware setup. The hardware platform is the F1TENTH
race series [ 49]. It consists of an oﬀ-the-shelf model race
car chassis, with a Traxxas Velineon 3351R brushless DC
electric motor, which is driven by a VESC 6 MkIV electronic
speed controller (ESC). The laser range measurements are
produced by a Hokuyo UST-10LX LiDAR sensor. On-board
computation and control tasks are performed on an NVIDIA
Jetson TX2. The on-board system runs Ubuntu 18.04 as the
base operating system and hosts the core services for the
Robot Operating System (ROS) stack [ 50]. To run Dreamer
agents on our hardware, we use Docker. A ROS interface
node is used to translate observation and action messages.
The motor force commands are processed by integration to
get the desired speed values. We added an adaptive low pass
ﬁlter for the steering commands to protect the servo from
high frequent steering operations.
Track diﬃculty. To compare the diﬃculty of the tracks, we
classiﬁed them according to several characteristics (as also
discussed in [ 51] and [52]). In particular, we measure the
minimal track width, the track length (shortest path), and its
minimum curve radius. For curves, we measure the radius of
the largest circle that tangentially touches the outside of the
curve and also touches the inside of the curve. In Table I we
summarize the characteristics of the tracks.
B. Experimental Evaluation
In this section, we present the baseline algorithms and
discuss their performance in various racing scenarios. We
conduct three diﬀerent experiments: I) Evaluation of the
learning curves of Dreamer and model-free algorithms in
simulation, II) Evaluation of the generalization ability by
testing the trained models in simulation, and III) Evaluation
of the transferability on our real testing platform.
Model-free baselines. We compare the performance of the
Dreamer agent against the following advanced model-free
baselines: D4PG, an enhanced version of DDPG [ 7], MPO, a
stable oﬀ-policy algorithm [ 9], SAC, an oﬀ-policy actor-critic
algorithm with less sensitivity to hyperparameters [ 18], PPO,
an on-policy algorithm [ 8], and PPO-LSTM, the recurrent
0 2 4 6
1e60.00.51.0Max Progress
8 8 8
AUSTRIA
0 2 4 6
1e6012
COLUMBIA
0 2 4 6
1e6012
TREITLSTRASSE
D4PG
MPOLSTM-PPO
PPOSAC DREAMER
0 1 2
1e60.00.51.0Max Progress
AUSTRIA
0 1 2
1e6012
COLUMBIA
0 1 2
1e6012
TREITLSTRASSE
DREAMER+DISTANCE
DREAMER+OCCUPANCYD4PG
MPOPPO
SACLSTM-PPO
AUT COL TRT BRC0.00.20.40.60.81.0Max Progress
TRAINED ON AUSTRIA
AUT COL TRT BRC0.00.20.40.60.81.0Max Progress
TRAINED ON TREITLSTRASSE
AUT COL TRT BRC020406080Lap Time (sec)
AUT COL TRT BRC020406080Lap Time (sec)
DREAMER+DISTANCE DREAMER+OCCUPANCY MPOFig. 5:Left:Learning curves of model-free methods ( top row) over 8𝑀steps and Dreamers ( bottom row ) over 2𝑀steps.
The dashed lines report the maximum performance obtained by the other algorithms as baselines. Performance averages over
5 runs.Right:Maximum progress and lap time of trained models over diﬀerent tracks in simulation. The bars show the
result averaged over 10 episodes on each track. The delimiters show the minimum and maximum achieved. For Lap-Time
results, we consider the best episode that ﬁnished one full lap.
version of PPO using long short-term memory [ 53]. PPO-
LSTM is chosen as a baseline since policies built by recurrent
networks demonstrate remarkable performance in learning to
control [54], [55], [56], [57]. We tuned the hyperparameters
for each baseline algorithm with Optuna [58].
Learning performance and sample eﬃciency. In Figure 5
(left) we show the learning curves of the model-free and
model-based algorithms in three diﬀerent tracks. Except for
the simple track Columbia, all advanced model-free methods
fail to perform one lap in more complex maps successfully.
On the other hand, Dreamer eﬃciently learns to complete the
tasks regardless of the degrees of complexity of the tracks.
As shown in Figure 5, the performance of the Dreamer
agents equipped with distance and occupancy reconstruction
models are comparable in Austria and Columbia. However,
the experiments on Treitlstrasse show better performance
with occupancy-map reconstruction, with which the agent
can almost complete two full laps over the evaluation-time
window. This experiment suggests that the occupancy-map
reconstruction speeds up the training process. However, it
biases the learning process on the track on which it was
trained. This aﬀects the generalization capabilities of the
policy, as illustrated in the following experiment.
Performance on unseen tracks. In this experiment, we
evaluate the cross-track generalizability of the learned policies
and demonstrate the domain-adaptation skills of Dreamer. We
trained polices on a single, ﬁxed track, i.e., Austria (AUT) and
Treitlstrasse (TRT) and reported their evaluation performance
on other unseen tracks: Columbia (COL) and Barcelona
(BRC). We compare Dreamer to the best-performing model-
free policy, MPO, and to a Follow-the-gap (FTG) agent,
which is a LiDAR-based reactive method that was successfully
applied in past F1TENTH competitions [59].
Considering the learning methods shown in Figure 5 (right,
top row), all the algorithms can complete the simple track,
COL. However, only Dreamer can generalize the racingtask and transfer the learned skills to other complex tracks.
Generally, policies trained in AUT achieve better performance
as they can complete at least one lap in each other track.
Conversely, even though TRT contains challenging turns,
the trained agents perform poorly on unseen complex tracks
(AUT). The reason might be that most turns in TRT have the
same direction, and consequently, the trained policy cannot
generalize to altered scenarios.
Moreover, we observed that the Dreamer-Occupancy agent
shows lower generalization skills compared to Dreamer-
Distance. The learned policy results in a more aggressive strat-
egy, presenting a lower lap-time as shown in Figure 5 (right,
bottom row). However, the car often gets dangerously close to
the track walls. This results in unsafe behavior that increases
the probability of collisions. Comparing the performance of
Dreamer with the adaptive FTG, we observe similar lap-times
on all the tracks. However, the predictable behavior of FTG
results in a more stable controller. This is not surprising
considering that FTG is a programmed algorithm, and its
parameters have been carefully tuned to drive on the most
challenging track, AUT. In conclusion, reconstructing the
occupancy area allows the agent overﬁtto the track it was
trained on. This speeds up learning, but hurts robustness to
domain changes and thus worsens generalization.
Max ProgressImagination Horizon210050200.40.81.41.0DreamerDistanceDreamerOccupancy1 lap
Fig. 6: Dreamer’s performance vs. imagination horizon. Batch
length = 50, and action repeat = 4. n=5.
0 50 100 150 200 250
Steps1
01Steering
1= 0.0, 2= 0.0
1= 0.5, 2= 5.0
1= 5.0, 2= 5.0
Collision
1
0 1
SteeringNorm. Count
1
0 1
Steering
1
0 1
SteeringFig. 7: Impact of action regularization on TRT. The ﬁrst row reports the steering command’s distributions under various
regularization weights. The second row reports the continuous command over a single simulation snapshot. n=5
Inﬂuence of imagination horizon. Figure 6 shows that
as we increase the imagination horizon for Dreamer, it
performs better. While this increase helps generate long-term
trajectories in latent space as training data, we observe that
a further increase leads to a drop in performance in some
of the experiments. We suspect that this drop is caused by
compounding model errors for longer horizons.
Action Regularization. Additionally, we observed that the
driving behavior of the car resembles an on-oﬀ control law,
resulting in unnecessarily curvy trajectories. While this type
of behavior is expected for acceleration control [ 60], it is
counterproductive to perform unnecessary steering commands.
We experimented with diﬀerent regularization techniques
for continuous control to minimize the steering eﬀort and
enforce temporal smoothness, similar to the approaches in
[61] and [62]. The resulting policy showed smoother steering
commands (see Figure 7) but did not show superior perfor-
mance compared to the one trained without regularization.
The objective for the regularized action model is deﬁned as
E𝑝𝜃𝑞𝜙""𝐻∑︁
𝑡=0𝛾𝑡¹𝑉𝜆¹𝑠𝑡º 𝜇1jj𝑎𝑡jj2
2 𝜇2jj𝑎𝑡 𝑎𝑡 1jj2
2º#

where𝑉𝜆¹𝑠𝑡ºdenotes the original value estimation term, 𝜇1
and𝜇2are the control eﬀort and the temporal smoothing
coeﬃcients, respectively.
Sim2Real transfer. In this experiment, we evaluate Dreamer
policies with respect to their sim2real transferability. We
test trained dreamers deployed on the car in a physical test
track. A video /one.supdemonstrating the driving performance of the
Dreamer agent is supplied with the submission. It presents the
Dreamer agent completing a full lap in TRT in the forward
direction. Then, to evaluate its generalization capabilities,
we tested the agent in reversed direction. We observe that
even if the agent was not trained in the reversed direction
and not directly on the TRT track, the agent can complete
a successful lap. Finally, we placed two obstacles after the
most challenging turn and ran the agent in this conﬁguration.
The Dreamer agents can complete the lap and securely avoid
obstacles.
/one.supThe video can be viewed at: https://youtu.be/8ofWVLArZJQVI. C/o.pc/n.pc/c.pc/l.pc/u.pc/s.pc/i.pc/o.pc/n.pc/s.pc
We show that Dreamer, a model-based deep RL algorithm,
outperforms several other model-free RL algorithms in
simulation. Furthermore, we empirically demonstrate that
Dreamer is able to successfully transfer the policy that
it learned in simulation to a real-world test environment
without the use of explicit domain randomization techniques.
Ultimately, we show how observation models and model
horizonaﬀectgeneralizationanddomainadaptationoflearned
policies and that model-based agents can enable robust
autonomy in real-world settings.
Why Dreamer? Dreamer is a comparably sample-eﬃcient,
high-performance deep RL algorithm. Its learned state space
model can not only be used for policy learning, but also for
trajectory planning approaches [5].
How does the sim2real gap inﬂuence Dreamer’s perfor-
mance?The discrepancy between simulation and reality is
and remains one of the main challenges in RL. However,
Dreamer’slatentstate-spacemodelisrobustenoughtotransfer
the learned policy to the real world. This result is key for the
deployment of RL algorithms in the real world.
What are the limitations of this approach? We observed
that our learned policies often resemble bang-bang control
[62], which is often not desirable in real-world robotics
applications. To avoid this, the objective function of the
learning problem has to be carefully designed. Our experi-
ments mitigated its emergence by regularizing the original
actor’s loss with action penalties to discourage sharp changes
between extreme values. This method relaxed the bang-bang
behavior observed in the obtained policies. Furthermore, the
lack of structure in the latent space of the world model makes
it hard to interpret and is subject to ongoing research. All
code, data and appendix are available at: https://github.
com/CPS-TUWien/racing_dreamer .
A/c.pc/k.pc/n.pc/o.pc/w.pc/l.pc/e.pc/d.pc/g.pc/m.pc/e.pc/n.pc/t.pc/s.pc
L.B. was supported by the Doctoral College Resilient
Embedded Systems. M.L. was supported in part by the ERC-
2020-AdG 101020093 and the Austrian Science Fund (FWF)
under grant Z211-N23 (Wittgenstein Award). R.H. and D.R.
were supported by The Boeing Company and the Oﬃce of
Naval Research (ONR) Grant N00014-18-1-2830. R.G. was
partially supported by the Horizon-2020 ECSEL Project grant
No. 783163 (iDev40) and A.B. by FFG Project ADEX.
R/e.pc/f.pc/e.pc/r.pc/e.pc/n.pc/c.pc/e.pc/s.pc
[1]R. S. Sutton and A. G. Barto, RL: An introduction . MIT press, 2018.
[2]D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, “Dream to control:
Learning behaviors by latent imagination,” arXiv:1912.01603 , 2019.
[3]D. Ha and J. Schmidhuber, “Recurrent world models facilitate
policy evolution,” in Advances in Neural Information Processing
Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, Eds., vol. 31. Curran Associates,
Inc., 2018. [Online]. Available: https://proceedings.neurips.cc/paper/
2018/ﬁle/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf
[4]D. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba, “Mastering atari
with discrete world models,” in International Conference on Learning
Representations , 2020.
[5]D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and
J. Davidson, “Learning latent dynamics for planning from pixels,” in
Int. Conf. on Machine Learning . PMLR, 2019, pp. 2555–2565.
[6]T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Oﬀ-
policy maximum entropy deep reinforcement learning with a stochastic
actor,” in Int. Conf. on Machine Learning , 2018, pp. 1861–1870.
[7]G. Barth-Maron, M. W. Hoﬀman, D. Budden, W. Dabney, D. Horgan,
D.TB,A.Muldal,N.Heess,andT.Lillicrap,“DistributedDistributional
Deterministic Policy Gradients,” in International Conference on
Learning Representations , 2018.
[8]J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
“Proximal policy optimization algorithms,” 2017.
[9]A. Abdolmaleki, J. T. Springenberg, Y. Tassa, R. Munos, N. Heess,
and M. Riedmiller, “Maximum a posteriori policy optimisation,” in
International Conference on Learning Representations , 2018. [Online].
Available: https://openreview.net/forum?id=S1ANxQW0b
[10]R. Hasani, M. Lechner, A. Amini, D. Rus, and R. Grosu, “A natural
lottery ticket winner: Reinforcement learning with ordinary neural
circuits,” in International Conference on Machine Learning . PMLR,
2020, pp. 4082–4093.
[11]Y. Yu, “Towards sample eﬃcient reinforcement learning,” in Proceed-
ings of the 27th International Joint Conference on Artiﬁcial Intelligence ,
ser. ĲCAI’18. AAAI Press, 2018, p. 5739–5743.
[12]V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,
D. Wierstra, and M. Riedmiller, “Playing atari with deep reinforcement
learning,” arXiv preprint arXiv:1312.5602 , 2013.
[13]H. Zhu, J. Yu, A. Gupta, D. Shah, K. Hartikainen, A. Singh, V. Kumar,
and S. Levine, “The ingredients of real world robotic reinforcement
learning,” in International Conference on Learning Representations ,
2020. [Online]. Available: https://openreview.net/forum?id=rJe2syrtvS
[14]W. Zhao, J. P. Queralta, and T. Westerlund, “Sim-to-Real Transfer
in Deep RL for Robotics: a Survey,” in IEEE Symposium Series on
Computational Intelligence (SSCI) , 2020, pp. 737–744.
[15]X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel,
“Sim-to-real transfer of robotic control with dynamics randomization,”
2018 IEEE International Conference on Robotics and Automation
(ICRA), May 2018. [Online]. Available: http://dx.doi.org/10.1109/
ICRA.2018.8460528
[16]J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez,
and V. Vanhoucke, “Sim-to-Real: Learning Agile Locomotion For
Quadruped Robots,” in Proceedings of Robotics: Science and Systems ,
Pittsburgh, Pennsylvania, June 2018.
[17]M. Kaspar, J. D. Muñoz Osorio, and J. Bock, “Sim2Real Transfer for
Reinforcement Learning without Dynamics Randomization,” in 2020
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), 2020, pp. 4383–4388.
[18]T.Haarnoja,S.Ha,A.Zhou,J.Tan,G.Tucker,andS.Levine,“Learning
to walk via deep reinforcement learning,” in Proceedings of Robotics:
Science and Systems , FreiburgimBreisgau, Germany, June 2019.
[19]A. Singh, L. Yang, C. Finn, and S. Levine, “End-To-End Robotic
Reinforcement Learning without Reward Engineering,” in Proceedings
of Robotics: Science and Systems , 2019.
[20]R. S. Sutton, “Dyna, an integrated architecture for learning, planning,
and reacting,” SIGART Bull. , vol. 2, no. 4, p. 160–163, Jul. 1991.
[Online]. Available: https://doi.org/10.1145/122344.122377
[21]M. P. Deisenroth and C. E. Rasmussen, “PILCO: A Model-Based
and Data-Eﬃcient Approach to Policy Search,” in Proceedings of the
28th International Conference on Machine Learning , ser. ICML’11.
Madison, WI, USA: Omnipress, 2011, p. 465–472.
[22]W. Schwarting, T. Seyde, I. Gilitschenski, L. Liebenwein, R. Sander,
S. Karaman, and D. Rus, “Deep latent competition: Learning torace using visual control policies in latent space,” arXiv preprint
arXiv:2102.09812 , 2021.
[23]J. Kabzan, M. de la Iglesia Valls, V. Reĳgwart, H. F. C. Hendrikx,
C. Ehmke, M. Prajapat, A. Bühler, N. Gosala, M. Gupta, R. Sivanesan,
A. Dhall, E. Chisari, N. Karnchanachari, S. Brits, M. Dangel, I. Sa,
R. Dubé, A. Gawel, M. Pfeiﬀer, A. Liniger, J. Lygeros, and R. Siegwart,
“Amz driverless: The full autonomous racing system,” 2019.
[24]J. Kabzan, L. Hewing, A. Liniger, and M. N. Zeilinger, “Learning-
based model predictive control for autonomous racing,” IEEE Robotics
and Automation Letters , vol. 4, no. 4, pp. 3363 – 3370, 2019-10.
[25]L. Andresen, A. Brandemuehl, A. Honger, B. Kuan, N. Vödisch,
H. Blum, V. Reĳgwart, L. Bernreiter, L. Schaupp, J. J. Chung, M. Burki,
M. R. Oswald, R. Siegwart, and A. Gawel, “Accurate Mapping and
Planning for Autonomous Racing,” in 2020 IEEE/RSJ Int. Conference
on Intelligent Robots and Systems (IROS) , 2020, pp. 4743–4749.
[26]E. Velenis and P. Tsiotras, “Minimum Time vs Maximum Exit Velocity
Path Optimization During Cornering,” in Proceedings of the IEEE
International Symposium on Industrial Electronics, 2005. ISIE 2005. ,
vol. 1, 2005, pp. 355–360.
[27]A. Rucco, G. Notarstefano, and J. Hauser, “An Eﬃcient Minimum-
Time Trajectory Generation Strategy for Two-Track Car Vehicles,” IEEE
Trans on Control Systems Tech , vol. 23, no. 4, pp. 1505–1519, 2015.
[28]J. P. Timings and D. J. Cole, “Minimum maneuver time calculation
using convex optimization,” Journal of Dynamic Systems, Measurement,
and Control , vol. 135, no. 3, 2013.
[29]J. L. Vázquez, M. Brühlmeier, A. Liniger, A. Rupenyan, and J. Lygeros,
“Optimization-based hierarchical motion planning for autonomous
racing,” in 2020 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS) , 2020, pp. 2397–2403.
[30]M. Jaritz, R. de Charette, M. Toromanoﬀ, E. Perot, and F. Nashashibi,
“End-to-end race driving with deep reinforcement learning,” in 2018
IEEE International Conference on Robotics and Automation (ICRA) ,
2018, pp. 2070–2075.
[31]M. Riedmiller, M. Montemerlo, and H. Dahlkamp, “Learning to drive
a real car in 20 minutes,” in 2007 Frontiers in the Convergence of
Bioscience and Information Technologies , 2007, pp. 645–650.
[32]A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J. Allen, V. Lam,
A. Bewley, and A. Shah, “Learning to drive in a day,” in 2019
International Conference on Robotics and Automation (ICRA) , 2019,
pp. 8248–8254.
[33]F. Fuchs, Y. Song, E. Kaufmann, D. Scaramuzza, and P. Duerr,
“Super-Human Performance in Gran Turismo Sport Using Deep
Reinforcement Learning,” arXiv preprint arXiv:2008.07971 , 2020.
[Online]. Available: https://arxiv.org/abs/2008.07971
[34]G. Bellegarda and K. Byl, “An Online Training Method for Augmenting
MPC with Deep Reinforcement Learning,” in 2020 IEEE/RSJ Int. Conf.
on Intelligent Robots and Systems (IROS) , 2020, pp. 5453–5459.
[35]G. Williams, N. Wagener, B. Goldfain, P. Drews, J. M. Rehg, B. Boots,
and E. A. Theodorou, “Information theoretic mpc for model-based
reinforcement learning,” in 2017 IEEE International Conference on
Robotics and Automation (ICRA) , 2017, pp. 1714–1721.
[36]S. Schaal, “Is imitation learning the route to humanoid robots?” Trends
in cognitive sciences , vol. 3, no. 6, pp. 233–242, 1999.
[37]S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning
and structured prediction to no-regret online learning,” in Proceedings
of the fourteenth international conference on artiﬁcial intelligence and
statistics. JMLR Workshop and Conference Proceedings, 2011, pp.
627–635.
[38]C. Vorbach, R. Hasani, A. Amini, M. Lechner, and D. Rus, “Causal
navigation by continuous-time neural networks,” Advances in Neural
Information Processing Systems , vol. 34, 2021.
[39]M. Lechner, R. Hasani, D. Rus, and R. Grosu, “Gershgorin loss
stabilizes the recurrent neural network compartment of an end-to-
end robot learning scheme,” in 2020 IEEE International Conference
on Robotics and Automation (ICRA) . IEEE, 2020, pp. 5446–5452.
[40]A. Y. Ng, S. J. Russell et al., “Algorithms for inverse reinforcement
learning.” in Icml, vol. 1, 2000, p. 2.
[41]Y.-H.Wu,N.Charoenphakdee,H.Bao,V.Tangkaratt,andM.Sugiyama,
“Imitation learning from imperfect demonstration,” in International
Conference on Machine Learning . PMLR, 2019, pp. 6818–6827.
[42]M. Sun and X. Ma, “Adversarial imitation learning from incomplete
demonstrations,” arXiv preprint arXiv:1905.12310 , 2019.
[43]M. Lechner, R. Hasani, R. Grosu, D. Rus, and T. A. Henzinger,
“Adversarial training is not ready for robot learning,” in 2021 IEEE
International Conference on Robotics and Automation (ICRA) . IEEE,
2021, pp. 4140–4147.
[44]M. Lechner, R. Hasani, A. Amini, T. A. Henzinger, D. Rus, and
R. Grosu, “Neural circuit policies enabling auditable autonomy,” Nature
Machine Intelligence , vol. 2, no. 10, pp. 642–652, 2020.
[45]M. Lechner, R. Hasani, M. Zimmer, T. A. Henzinger, and R. Grosu,
“Designing worm-inspired neural networks for interpretable robotic
control,” in 2019 International Conference on Robotics and Automation
(ICRA). IEEE, 2019, pp. 87–94.
[46]A. Amini, I. Gilitschenski, J. Phillips, J. Moseyko, R. Banerjee,
S. Karaman, and D. Rus, “Learning robust control policies for end-to-
end autonomous driving from data-driven simulation,” IEEE Robotics
and Automation Letters , vol. 5, no. 2, pp. 1143–1150, 2020.
[47]E. Coumans and Y. Bai, “Pybullet, a python module for physics
simulation for games, robotics and machine learning,” http://pybullet.
org, 2016–2019.
[48]V. S. Babu and M. Behl, “f1tenth. dev-an open-source ros based f1/10
autonomous racing simulator,” in 16th Int. Conf. on Automation Science
and Engineering (CASE) . IEEE, 2020, pp. 1614–1620.
[49]M. O’Kelly, H. Zheng, A. Jain, J. Auckley, K. Luong, and R. Mang-
haram, “Tunercar: A superoptimization toolchain for autonomous
racing,” in 2020 IEEE International Conference on Robotics and
Automation (ICRA) . IEEE, 2020, pp. 5356–5362.
[50]Stanford Artiﬁcial Intelligence Laboratory et al., “Robotic operating
system.” [Online]. Available: https://www.ros.org
[51]F. Braghin, F. Cheli, S. Melzi, and E. Sabbioni, “Race driver
model,” Computers & Structures , vol. 86, no. 13, pp. 1503–
1516, 2008, structural Optimization. [Online]. Available: https:
//www.sciencedirect.com/science/article/pii/S0045794908000163
[52]F. Lamiraux and J. . Lammond, “Smooth motion planning for car-like
vehicles,” IEEE Transactions on Robotics and Automation , vol. 17,
no. 4, pp. 498–501, 2001.
[53]S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
computation , vol. 9, no. 8, pp. 1735–1780, 1997.
[54]R. Hasani, M. Lechner, A. Amini, D. Rus, and R. Grosu, “Liquid
time-constant networks,” in Proceedings of the AAAI Conference on
Artiﬁcial Intelligence , vol. 35, no. 9, 2021, pp. 7657–7666.
[55]R. Hasani, A. Amini, M. Lechner, F. Naser, R. Grosu, and D. Rus,
“Response characterization for auditing cell dynamics in long short-term
memory networks,” in 2019 International Joint Conference on Neural
Networks (ĲCNN) . IEEE, 2019, pp. 1–8.
[56]M. Lechner and R. Hasani, “Learning long-term dependencies in
irregularly-sampled time series,” arXiv preprint arXiv:2006.04418 ,
2020.
[57]R. Hasani, M. Lechner, A. Amini, L. Liebenwein, M. Tschaikowski,
G. Teschl, and D. Rus, “Closed-form continuous-depth models,” arXiv
preprint arXiv:2106.13898 , 2021.
[58]T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama, “Optuna: A
next-generation hyperparameter optimization framework,” 2019.
[59]V. Sezer and M. Gokasan, “A novel obstacle avoidance algorithm:
“follow the gap method”,” Robotics and Autonomous Systems ,
vol. 60, no. 9, pp. 1123–1134, 2012. [Online]. Available: https:
//www.sciencedirect.com/science/article/pii/S0921889012000838
[60]D. Liberzon, Calculus of Variations and Optimal Control Theory: A
Concise Introduction . USA: Princeton University Press, 2011.
[61]S. Mysore, B. Mabsout, R. Mancuso, and K. Saenko, “Regularizing
action policies for smooth control with reinforcement learning,” 2021.
[62]T. Seyde, I. Gilitschenski, W. Schwarting, B. Stellato, M. Riedmiller,
M. Wulfmeier, and D. Rus, “Is bang-bang control all you need?
solving continuous control with bernoulli policies,” Advances in Neural
Information Processing Systems , vol. 34, 2021."
2310.15585,D:\Database\arxiv\papers\2310.15585.pdf,"How does the use of teacher forcing in the proposed model relate to the concept of curriculum learning, and what are the potential benefits of this approach in the context of training neural module networks?","Teacher forcing acts as a form of curriculum learning, guiding the model through progressively more complex reasoning tasks by gradually transitioning from fully guided inputs to relying more on the model's own predictions. This approach helps the model learn and adapt to the task's complexity, leading to improved generalization and faster convergence during training.","and promotes a more human-like reasoning process. This highlights the impor-
tance of curriculum learning in improving the training dynamics and enhancing
the model’s ability to reason and generalize effectively.
Interestingly, [10] demonstrated that leveraging the programs generated from
questions as additional supervision for the LXMERT integrated model led to a
reduction in sample complexity and improved performance on the GQA-OOD
(Out Of Distribution) dataset [9].
Building upon this, our work aims to capitalize on both the transparency
offered by NMN architectures and the high-quality transformer-encoded rep-
resentations by implementing a composable NMN that integrates multimodal
vision and language features.
Teacher forcing. Teacher forcing (TF) [17] is a widely used technique in
sequence prediction or generation tasks, especially in RNNs with an encoder-
decoderarchitecture.Itinvolvestrainingthemodelusingthetrueoutputasnovel
input, which helps improve prediction accuracy. However, during inference, the
model relies on its own predictions without access to ground-truth information,
leading to a discrepancy known as exposure bias.
Scheduled sampling (SS) is a notable approach to mitigating the train-test
discrepancy in sequence generation tasks [2]. It introduces randomness during
training by choosing between using ground truth tokens or the model’s predic-
tions at each time step. This technique, initially developed for RNN architec-
tures, has also been adapted for transformer networks [14], aiding to align the
model’s performance during training and inference.
NMNs, on the other hand, are trained using only the output of a module
as input for the next module, which has drawbacks. Errors made by an inter-
mediate module can propagate to subsequent modules, leading to cumulative
bad predictions. This effect is particularly prominent during the early stages of
training when the model’s predictions are close to random.
NMNs can leverage the TF strategy to enhance their training process. Ini-
tially, training begins with a fully guided schema, where the true previous out-
puts are used as input. As training progresses, the model gradually transitions
to a less guided scheme, relying more on the generated outputs from previous
steps as input. This gradual reduction in guidance and increased reliance on
the model’s own predictions, named decaying TF, helps NMNs better learn and
adapt to the complexity of the task. With decaying TF, modules can conform
to their expected behavior for their respective sub-tasks.
3 Cross-modal neural module network
Our model takes an image, question, and program triplet as input and predicts
an answer. We extract aligned language and vision features for the image and
question using a cross-modal transformer. The program, represented as a se-
quence of modules, is used to build an NMN, which is then executed on the
image to answer the question (refer to Fig. 1). In the next subsections we detail
the feature extraction process and describe the program executor.
Fig.1: The proposed modular VQA framework. Plain arrows represent the out-
put flow, while dotted arrows represent the Multi-Task loss backward flow.
Cross-modal features. Compositional visual reasoning involves the abil-
ity to make logical and geometric inferences on complex scenes by leveraging
both visual and textual information. This requires accurate representations of
objects and questions. To address this, we employ LXMERT [15], a pretrained
transformer model specifically designed for multi-modal tasks. LXMERT has
demonstrated impressive performance across various tasks and serves as our fea-
ture extractor. In our approach, we discard the answer classification component
and freeze the model’s weights. To extract cross-modal representations, we pro-
cess the image Ithrough the object-relationship encoder and the question Q
through the language encoder. Then, the Cross-Modality Encoder aligns these
representations and produces object bounding box features vjfor each object bj
inI, as well as word embeddings txtifor each word wiofQ.
Neural modules. Our NMN approach tackles complex reasoning tasks by
decomposing them into simpler sub-tasks, inspired by human reasoning skills
like object detection, attribute identification, object relation recognition, and
objectcomparison.Wedevelopedalibraryofmodulestailoredtoaddressspecific
sub-tasks. These modules are designed to be intuitive and interpretable, using
simple building blocks like dot products and MLPs. They are categorized into
three groups: attention, boolean, and answer modules. For instance, the Select
attention module focuses on detecting object bounding boxes by applying an
attention vector to the available bounding boxes within an image. On the other
hand, boolean modules like AndorOrmake logical inferences, while answer
modulessuchas QueryName provideprobabilitydistributionsoverthevocabulary
of possible answers. To get a glimpse of the variety within our module library you
can refer to Table 1, which showcases an example from each module category.
Modular network instantiation. A program in our framework consists of
a sequence of neural modules (Table 1). These modules are instantiated within
a larger Neural Module Network (NMN) following the program sequence. Each
Table1:Samplemoduledefinitions. S:softmax, σ:sigmoid, r:RELU, Wi:weight
matrix, a: attention vector ( 36×1),V: visual features ( 768×36),t: text features
(768×1),⊙: Hadamard product.
Name Dependencies Output Definition
Select −attention x=r(W t ),Y=r(WV ),o=S(W(YTx))
RelateSub [a]attentionx=r(W t ),Y=r(WV ),z=S(W(YTx))
o=S(W(x⊙y⊙z))
VerifyAttr [a]boolean x=r(W t ),y=r(W(V a),o=σ(W(x⊙y))
And [b1,b2]boolean o=b1×b2
ChooseAttr [a]answer x=r(W t ),y=r(W(V a),o=S(W(x⊙y))
QueryName [a]answer y=r(W(V a)),o=S(W y )
module has dependencies, denoted as dm, which allow it to access information
from the previous modules, and arguments, denoted as txtm, which condition
its behavior. For example, the FilterAttribute module, which relies on the
output of the Selectmodule, aims to shift attention to the selected objects by
considering the attribute that corresponds to the provided text argument. To
handle module dependencies, the program executor employs a memory buffer to
store the outputs, further used as inputs for subsequent modules. This approach
also enables the computation of multi-task losses (see Sec. 4) by comparing the
outputs produced by the modules with the expected ground-truth outputs.
4 Teacher guidance for neural module networks
To achieve explainable reasoning, we use teacher forcing (TF) to guide the mod-
ules by providing them with ground-truth inputs. We also employ a multi-task
(MT) loss to provide feedback and correct their behaviors towards the expected
intermediate outputs. This process is illustrated in Fig. 2.
Given a program p, a question qand image I, the modular network executes
ponIwith the textual arguments txtmencoded in q, producing an answer a.
This can be represented as a=p(I, q), where p=m1◦m2◦...◦mndenotes the
sequential execution of nmodules within the program. Each module mtinputs
the output of the previous module mt−1and performs a specific computation or
reasoning step to contribute to the final answer. The NMN is trained by mini-
mizing the cross entropy loss LCEover the set of (p, q, I, a )examples. In fact,
when a module is provided with its golden input and expected output, it is in-
dependently optimized to perform its specific sub-task. However, when modules
are jointly trained in a sequential manner, they learn to adapt their behaviors to
work together and engage in explicit reasoning without taking shortcuts. This
collaborative approach enables the modules to develop a deeper understanding
of the task and enables them to perform complex reasoning operations.
From a back-propagation perspective, during the early stages of training,
the gradients are computed based on the losses of individual modules when
processing correct inputs. As a result, the backward gradient flow of the MT
Fig.2: The teacher guidance for the program execution process related to the
question‘Onwhatistheanimaltotherightofthelaptopsleeping?’.Plainarrows
represent input guidance and dotted arrows represent the output feedback.
loss is interrupted at the first ground truth input. However, in the case of col-
laborative module interactions without TF, the full back-propagation can be
computed. The intermediate outputs are preserved in continuous form through-
out the program execution, enabling the flow of backward gradients between
modules. Errors and updates can be propagated through the entire network, fa-
cilitating effective learning and enhancing the overall performance of the NMN.
We give details about our guidance mechanism in the following subsections.
Input guidance. The modules receive input guidance through decaying
teacher forcing (TF). As shown in Fig. 2, at each reasoning step tthe executor
randomly decides whether to use the predicted output ˆot−1or the ground-truth
output o∗
t−1from the previous module mt−1as its input. This decision is made
by flipping acoin,where o∗
t−1is chosen with a probabilityof ϵeandˆot−1is chosen
with a probability of 1−ϵe. The coin-flipping process for input selection occurs
at each reasoning step, allowing the model to train on various sub-programs.
The probability ϵeof selecting o∗
t−1depends on the epoch number e. As training
progresses and the epoch number increases, ϵedecreases, giving more preference
to the module’s predictions over the ground-truth intermediate outputs.
Output feedback. We employ a multi-task (MT) loss approach to provide
feedback to the modules based on their outputs. The loss consists of a weighted
sumL=αLatt+βLbool+γLanswerof individual losses for the attention modules,
boolean modules and answer modules, with α,βandγscaling factors. Each
module is assigned its own average loss, considering its frequency of appearance,
to prevent overemphasis on frequent modules at the expense of infrequent ones.
For Boolean modules, we rely on the provided answer to infer the module’s
output and generate the intermediate Boolean outputs. However, for attention
modules,itisnecessarytoestablishcorrespondencesbetweentheboundingboxes
in the image graph and those obtained from Faster-RCNN, which brings us to
the issue of the ground-truth intermediate outputs, detailed in the following.
Soft matching and hard matching. Two mapping techniques, namely
hardmatchingandsoftmatching,areemployedtoaligntheground-truthbound-
ing boxes with those obtained from the feature extractor. In the hard matching
approach, a ground-truth bounding box bgis matched with the bounding box
o∗
ifrom the feature extractor that has the highest Intersection over Union (IoU)
factor. On the other hand, the soft mapping matches bgwith all o∗
ithat have
an IoU value above a threshold, resembling a multi-label classification task. The
choice of the matching technique directly affects the representation of the atten-
tion intermediate output vectors. Hard mapping produces one-hot-like vectors,
while soft mapping multi-label vectors, with one(s) for positive matching and
zeros for negative matching boxes. It is important to acknowledge that not all
modules have ground-truth outputs that can be extracted.
5 Protocol design
Dataset & metrics. The GQA balanced dataset [8] consists of over 1 million
compositional questions and 113,000 real-world images. The questions are rep-
resented by functional programs that capture the reasoning steps involved in
answering them. To ensure consistent evaluation, the dataset authors suggest
using the testdev split instead of the valsplit when utilizing object-based fea-
tures due to potential overlap in training images. In line with the latter and
following LXMERT, our model is trained on the combined train+val set. For
testing, we evaluate the model’s performance on the testdev-all split from
the unbalanced set. This allows us to gather additional examples and gain a
comprehensive understanding of the NMN’s behavior. To simplify the module
structure in the GQA dataset, we consolidate specific modules into more general
ones based on similar operations. For example, modules like ChooseHealthier
and ChooseOlder are combined into ChooseAttribute module, with an argu-
ment txtmspecifying the attribute to select. This reduces the number of modules
from 124 to 32. Our experiments directly utilize the pre-processed GQA dataset
programs, with a specific focus on evaluating the teacher forcing training on the
Program Executor module. While our system employs a transformer model as a
generator to convert the question into its corresponding program, this task is rel-
atively straightforward compared to the training of the executor. As in previous
studies [11,4], we achieve nearly perfect translation results on testdev-all .
We assess the performance of our approach by measuring answer accuracy.
Additionally, we conduct a qualitative evaluation of the intermediate outputs,
visualized through plotted images in Sec. 6.
Evaluated methods. As presented in the previous sections, we propose two
contributions to improve neural module networks for VQA. First, we use teacher
guidance during training, leading to better generalization. Second, we leverage
cross-modal language and vision features to capture complex relationships be-
tween text and images, resulting in more accurate and interpretable results.
We use the following notations to describe the various experimental setups:
-LXV: Employ the cross-modal representations from the LXMERT model [15].
-TF: Apply decaying teacher forcing to guide the inputs of the modules.
-MT: Apply multi-task losses to guide the expected outputs of the modules.
-Soft: Use the soft matching technique described in Sec. 4.
-Hard: Employ the hard matching technique described in Sec. 4.
-BertV: Use unimodal contextual language and vision representions, where
contextual text embeddings are extracted by the BERT model [5] and Faster-
RCNN bounding boxes features are provided by the GQA dataset [8].
-FasttextV : Employ unimodal non-contextual fastText embeddings [3] along
with Faster-RCNN bounding boxes features.
6 Results analysis
In our evaluation, we begin by comparing the different teacher-guided training
strategies. We also compare the impact of the addition of the multi-task losses
and its correlation with the decaying TF along with the soft and hard matching
techniques. Later, we compare the usage of multi-modal representations against
uni-modal representations.
Analysis of the teacher guided training. We aim to enable modular
reasoning for visual question answering on the GQA dataset. We evaluate the
effectivenessofourapproachbymeasuringtheansweraccuracyofseveralmodels
(described in Sec. 5), and report the results in Table. 2. Overall, our findings
demonstrate that using a combination of input guidance (denoted as TF) and
output guidance ( MT) achieves the highest accuracy, with a score of 63.2%.
Whencomparing LXV-TF (decayingteacherforcing)with LXV-MT (multi-
task loss), we observe that the multi-task loss alone achieves higher accuracy
than using decaying teacher forcing alone. This can be attributed to the fact
that when using TF alone, the final loss Lis solely determined by the answer
modules loss Lanswerand during early training stages, the application of TF lim-
its the backpropagation process, preventing it from reaching the first modules of
the programs. As a result, the impact of Lansweron initial modules is limited.
Interestingly, the combination of multi-task loss and decaying teacher forcing
exhibits complementary effects, leveraging the strengths of both techniques to
enhance training dynamics and overall performance.
To assess the effectiveness of the decaying teacher forcing guidance, we com-
pareLXV-MT against LXV-TF-MT . The TFguidance has led to accuracy
Table 2: Performance of various training methods on the testdev-all set.
Model accuracy
LXV-TF-hard 0.548
LXV-MT-hard 0.598
LXV-TF-MT-hard 0.630
LXV-TF-soft 0.536
LXV-MT-soft 0.563
LXV-TF-MT-soft 0.632
improvements in both softandhardmatching settings for NMN. This tech-
nique can be viewed as a form of curriculum learning, where the model trains
on programs of increasing length and complexity. During training, we observed
a faster increase in accuracy for the models using TFcompared to those with-
out TF, as the answer modules receive ground truth inputs in the early stages.
As training progresses, the training performance continues to improve until it
reaches a peak, after which it slightly degrades due to the reduced use of TF
and the modules adjusting to collaborative functioning. Nonetheless, as training
continues, the testing performance surpasses that of the models without TF.
When combining the MTloss with LXV-TF , modules are optimized based
on their intermediate outputs losses and they can benefit from the additional
guidance provided by the back-propagation of LattandLbool. We reach the best
performances outlined by LXV-TF-MT-soft andLXV-TF-MT-hard . The
increase in accuracy ranges from +8.2%in the hardmatching setting to +9.6%
in the softmatching setting.
Unimodal vscross-modal representations. We measure the impact of
different input representations on the performance (see Table. 3). For unimodal
embeddings we encode the question with fastText word embeddings or BERT
language model, and the image with Faster-RCNN features. For cross-modal
representations, we encode the question and the image with LXMERT. The
experiments are conducted using our best training strategies from the previous
section, i.e. we employ the TF guidance and the MT loss for all the experiments.
When comparing fastText andBERT, empirical observations indicate that
BERTtends to achieve better performance when utilizing hard matching, which
involves a focused and selective attention mechanism. Conversely, fastText
demonstratesimprovedperformancewiththesoftmatchingmechanism,enabling
a multi-label approach. The choice between these matching mechanisms relies on
the inputs of the models and the training strategy, as each model may demon-
strate superior performance in different scenarios.
Cross-modal aligned features provided by LXMERT (denoted as LXV) have
shown a significant increase in accuracy, with a +12.1%improvement when us-
ing soft matching and a +12.4%improvement when using hard matching. This
validates our intuition that leveraging cross-modal features pretrained on diverse
tasks and large datasets can greatly benefit NMNs. By incorporating these fea-
tures,themodularreasoningprocessisperformedwithabetterunderstandingof
Table 3: Language and vision representations results on testdev-all .
Model accuracy
FasttextV-TF-MT-hard 0.495
BertV-TF-MT-hard 0.506
LXV-TF-MT-hard 0.630
BertV-TF-MT-soft 0.485
FasttextV-TF-MT-soft 0.511
LXV-TF-MT-soft 0.632
word embeddings and bounding box features, leading to enhanced performance
and more accurate predictions.
Qualitative analysis of the modular approach. In Fig. 3, we illustrate
the reasoning process for three different questions. We highlight the bounding
boxes with the highest attention values from the attention output vector. For
boolean modules, we display the output probability and finally the predicted
answer. Taking “Question 2” as an example, the first step successfully selects
the skateboard as the object of focus. In the second step, the attention shifts to
white objects. Since the skateboard is not white, the attention is then redirected
to the white building. The “exist” module assesses if there is an object with a
high attention value and produces a probability score based on which the answer
is predicted. These examples demonstrate the explainability of our approach and
the ability to trace the model’s decision-making process.
Fig.3: Visualization of the reasoning process.
7 Conclusion
We have presented a neural module framework trained using a teacher guid-
ance strategy, which has demonstrated several key contributions. First, our ap-
proach enhances generalization and promotes a transparent reasoning process,
as evidenced by the experimental results on the GQA dataset. Additionally, the
utilization of cross-modal language and vision features allows to capture intri-
cate relationships between text and images, leading to improved accuracy. By
harnessing our proposed approach, the neural modules acquire the capability to
learntheirreasoningsub-tasksbothindependentlyandinanend-to-endmanner."
1906.05108,D:\Database\arxiv\papers\1906.05108.pdf,"Given the context of a system that aims to protect user privacy during matrix factorization, what specific privacy concern arises when users send their gradients to the server in plaintext, and how does the proposed system address this concern?","Sending gradients in plaintext allows the server to infer the user's ratings, violating privacy. The proposed system addresses this by using homomorphic encryption, which allows the server to perform computations on encrypted data without decrypting it, thus preventing the server from accessing the user's private information.","Plug equation (16) into (17):
Gt
jk
ut
ik+D∑
m=1ut
imvt
jm=Gt+1
jk
ut
ik+ 21
ut
ik∑N
n=1vt
nkGt
nk+
D∑
m=1(ut
im+ 21
ut
imN∑
n=1vt
nmGt
nm)vt+1
jm (18)
which is,
Gt
jk
ut
ik−Gt+1
jk
ut
ik+ 21
ut
ik∑N
n=1vt
nkGt
nk
=D∑
m=1[(ut
im+ 21
ut
imN∑
n=1(vt
nmGt
nm))vt+1
jm−ut
imvt
jm]
(19)
Letαk= 2∑N−1
n=0vt
nkGt
nk,
Gt
jk
ut
ik−Gt+1
jk
ut
ik+αk
ut
ik=D∑
m=1[(ut
im+αm
ut
im)vt+1
jm−ut
imvt
jm]
=D∑
m=1[(vt+1
jm−vt
jm)ut
im+αmvt+1
jm
ut
im]
(20)
From equation (13), we can have:
ut
im=Gt
jm
Gt
jkut
ik (21)
Plug equation (21) into (20):
Gt
jk
ut
ik−Gt+1
jk
ut
ik+αk
ut
ik
=D∑
m=1[(vt+1
jm−vt
jm)Gt
jm
Gt
jkut
ik+αmvt+1
jm
Gt
jm
Gt
jkut
ik]
=ut
ik
Gt
jkD∑
m=1[(vt+1
jm−vt
jm)Gt
jm] +Gt
jk
ut
ikD∑
m=1[αmvt+1
jm
Gt
jm]
(22)
Denoteβjandγjas follow:


βj=∑D
m=1[(vt+1
jm−vt
jm)Gt
jm]
γj=∑D
m=1[αmvt+1
jm
Gt
jm](23)
We will have:
Gt
jk
ut
ik−Gt+1
jk
ut
ik+αk
ut
ik=ut
ik
Gt
jkβj+Gt
jk
ut
ikγj (24)
Since we know there must be one real scalar of ut
ikthat
satisﬁes equation (24). We can use some iterative methods to
compute a numeric solution of (24), e.g., Newton’s method.After getting ut
i, we can use equation (10) to compute ri,
which can be written as:
rij=Gt
jk
ut
ik+D∑
m=1ut
imvt
jm (25)
In summary, knowing the gradients of a user uploaded in
two continuous steps, we can infer this user’s rating informa-
tion. Thus, we propose a secure matrix factorization frame-
work based on homomorphic encryption, which will be elab-
orated in the next section.
5 FedMF: Federated Matrix Factorization
To overcome this information leakage problem, we propose
to encode the gradients such that server cannot inverse the en-
coding process. Then, the encoded data leaks no information.
Meanwhile, the server should still be able to perform updates
using the encoded gradients. One way to achieve such a goal
is using homomorphic encryption.
Figure 1 shows a framework of our method, called FedMF
(Federated Matrix Factorization). Two types of participants
are involved in this framework, the server and the users. As
previously illustrated in Sec. 2.1, we assume that the server
is honest-but-curious, the users are honest, and the privacy of
the users is protected against the server.
Key Generation: As the typical functions involved in ho-
momorphic encryption (Sec. 2.2), we ﬁrst generate the public
keyandsecret key . The key generation process is carried out
on one of the users. The public key is known to all the partic-
ipants including the server. And the secret key is only shared
between users and needs to be protected against the server.
After the keys are generated, different TLS/SSL secure chan-
nels will be established for sending the public key andsecret
keyto the corresponding participants.
Parameter Initialization: Before starting the matrix fac-
torization process, some parameters need to be initialized.
The item proﬁle matrix is initialized at the server side while
the user proﬁle matrix is initialized by each user locally.
Matrix Factorization: Major steps include,
1. The server encrypts item proﬁle Vusing public key , get-
ting the ciphertext CV. From now on, the latest CVis
prepared for all users’ download.
2. Each user downloads the latest CVfrom the server, and
decrypts it using secret key , getting the plaintext of V.
Vis used to perform local update and compute the gra-
dientG. ThenGis encrypted using public key , getting
ciphertextCV. Then a TLS/SSL secure channel is built,
CVis sent back to the server via this secure channel.
3. After receiving a user’s encrypted gradient, the server
updates the item proﬁle using this ciphertext : Ct+1
V=
Ct
V−CG. Afterwards, the latest CVis prepared for
users’ downloading.
4. Step 2 and 3 are iteratively executed until convergence.
Security against Server: As shown in Fig. 1, only cipher-
text is sent to the server in FedMF. So no bit of informa-
tion will be leaked to the server as long as our homomor-
phic encryption system ensures ciphertext indistinguishabil-
ity against chosen plaintext attacks [Goldreich, 2009 ].
Figure 1: Overview of FedMF
No Accuracy Decline: We also claim that FedMF is accu-
racy equivalent to the user-level distributed matrix factoriza-
tion. This is because the parameter updating process is same
as the distributed matrix factorization (Sec. 3) if the homo-
morphic encryption part is removed.
6 Prototype and Evaluation
In this section, we choose Paillier encryption method [Pail-
lier, 1999 ]to instantiate a prototype of our system and use a
real movie rating dataset to evaluate the prototype.
6.1 Prototype Implementation
We use Paillier encryption [Paillier, 1999 ]to build a proto-
type of FedMF. Paillier encryption is a probabilistic encryp-
tion schema based on composite residuosity problem [Jager,
2012 ]. Given publick key and encrypted plaintext, paillier en-
cryption has the following homomorphic property operations:
op1.E(m1)·E(m2) (moden2) =E(m1+m2(moden ))
op2.E(m1)·gm2(moden2) =E(m1+m2(moden ))
op3.E(m1)m2(moden2) =E(m1m2(moden ))
Typically, paillier encryption requires the plaintext to be
positive integer. But in our system, the data are all in the
form of ﬂoating point numbers and some of them might be
negative. Thus we need to extend the encryption method to
support our system.
Float . In brief, a base exponent was multiplied to the dec-
imal, the integer part of the multiplication result Iand the
base exponent ewas treated as the integer representation of
the ﬂoating point number, i.e. (I,e). In the encryption pro-
cess, onlyIis encrypted, the ciphertext will be (CI,e). Then
the ciphertext with the same ecan directly conduct operation
op1to get the encrypted summation of plaintext, ciphertext
with different eneeded to recalibrate such that eis the same.
In practice, we use the same base exponent such that no in-
formation will leak from e.
Negative number. Amax number parameter is set to han-
dle the negative numbers. The max number can be set tohalf ofninpk, which means we assume all of our data is
smaller than max number , such an assumption is easy to sat-
isfy sincenis usually set to a very large prime number. Then
we perform mode non all the plaintext, all the positive num-
ber have no changes and all the negative numbers become
positive numbers greater than max number . In the decryption
process, if the decrypted plaintext is greater than max num-
ber, we minusnto get the correct negative plaintext.
FullText or PartText. Usually the rating or feedback com-
prises a sparse matrix [Koren et al. , 2009 ]which means the
amount of feedback from a user could be very limited. There-
fore, two different settings are implemented our system. Both
of them follow the overall steps of FedMF, but are slightly
different at the user uploading process. In one setting called
FullText , users upload gradients for all the items; the gradient
is set to 0 if a user does not rate an item. In the other setting
called PartText , users only upload the gradients of the rated
items. They both have advantages and disadvantages, Part-
Text leaks information about which items the user has rated
but has higher computation efﬁciency, FullText leaks no in-
formation but needs more computation time.
We utilize an open source python package, python-paillier3
to accomplish the encryption part in our prototype system.
6.2 Evaluation
Dataset: To test the feasibility of our system, we use a
real movie rating dataset [Harper and Konstan, 2016 ]from
MovieLens which contains 100K rating information made by
610 users on 9724 movies. This dataset is also used in other
homomorphic-encrypted MF works such as [Nikolaenko et
al., 2013 ]and[Kim et al. , 2016 ].
Parameters: In Paillier encryption, we set the length of
public key to 1024. The bandwidth of communication is set
to1 Gb/s . In the matrix factorization process, we set the di-
mension of user and item proﬁle to 100.
3https://github.com/n1analytics/python-paillier
#Item #Rating PartText FullText
40 8307 34.39 90.94
50 9807 44.05 113.34
60 11214 46.34 141.52
80 13817 52.91 182.27
160 22282 92.81 374.85
320 34172 140.51 725.72
640 49706 178.24 1479.40
1280 67558 264.10 2919.91
2560 83616 334.79 5786.01
Table 1: Time consumption of each iteration (seconds).
Figure 2: User-Server time consumption ratio of FullText
Environment: All the test experiments are performed on
a server with 5.0GHz 6-core CPU and 32GB RAM, where
the operation system is Windows and the program language
is Python. We used a module called gmpy4to accelerate the
homomorphic encryption part in Python such that it is as fast
as C++ implementation.
Performance : Since neither distributed computing or ho-
momorphic encryption mechanisms will affect the computa-
tion values, FedMF will output the same user and item pro-
ﬁles as the original MF algorithm. Hence, the major objective
of the experiments is testing the computation time of FedMF.
Fixing the number of users to 610, Table 1 shows the time
consumption of each iteration of PartText andFullText (one
iteration means all of the 610 users’ uploaded gradients are
used to update the item proﬁles once). For both PartText and
FullText , the time consumption is quite good when there are
not too many items, and the time efﬁciency decreases when
more items are given. Roughly, the time consumed for each
iteration linearly increases with the number of items. Com-
pared with Fulltext ,PartText is more efﬁcient but it leaks
some information. Particularly, PartText is nearly 20 times
faster than the Fulltext solution.
Fig. 2 and 3 show the ratio of the user and server updating
time when the number of items changes. The communica-
tion time is dismissed from the ﬁgures because it is too small
compared with the user and server updating time. For exam-
ple,∼80MB of gradient data need to be sent to server when
the item number is 2560 and it will cost only 1.25 seconds.
From these ﬁgures, we can ﬁnd out that ∼95% of time in
one iteration is spent on server updates, which means if we
4gmpy is a c-coded Python extension module that supports
multiple-precision arithmetic, https://github.com/aleaxit/gmpy
Figure 3: User-Server time consumption ratio of PartText
increase the computing power of the server or improve the
homomorphic encryption method such that the complexity of
computation on ciphertext is lowered, the time efﬁciency of
the whole system will improve signiﬁcantly. This would be
our future work.
7 Conclusion and Future Work
In this paper, we propose a novel secure matrix factoriza-
tion framework in federated machine learning, called FedMF .
More speciﬁcally, we ﬁrst prove that a distributed matrix fac-
torization system where users send gradients to the server in
forms of plaintext will leak users’ rating information. Then,
we design a homomorphic encryption based secure matrix
factorization framework. We have proved that our system is
secure against an honest-but-curious server, and the accuracy
is same as the matrix factorization on users’ raw data.
Experiments on real-world data show that FedMF’s time
efﬁciency is acceptable when the number of items is small.
Also note that our system’s time consumption linearly in-
creases with the number of items. To make FedMF more
practical in reality, we still face several challenges:
More efﬁcient homomorphic encryption. As we have dis-
cussed before, about 95% of our system’s time consumption
is spent on server updates, where the computation is all per-
formed on the ciphertext. If we can improve the homomor-
phic encryption’s efﬁciency when conducting operations on
ciphertext, our system’s performance will increase.
Between FullText and PartText. Our experiments have
shown that PartText is much more efﬁcient than FullText , but
PartText reveals the set of items rated by a user. This infor-
mation, without the exact rating scores, may still leak users’
sensitive information [Yang et al. , 2016 ]. Perhaps we can
ask users to upload more gradients than only the rated items,
but not all the items, so as to increase efﬁciency compared to
FullText , while not leaking the exactly rated item set.
More secure deﬁnitions. Currently, we use a typical hor-
izontal federated learning secure deﬁnition, which assumes
honest participants and an honest-but-curious server. Next,
we can explore more challenging secure deﬁnitions, such as
how to build a secure system where the server is honest-but-
curious, and some participants are malicious and the mali-
cious participants may collude with the server.
References
[Acar et al. , 2018 ]Abbas Acar, Hidayet Aksu, A Selcuk
Uluagac, and Mauro Conti. A survey on homomorphic
encryption schemes: Theory and implementation. ACM
Computing Surveys (CSUR) , 51(4):79, 2018.
[Ammad-ud-din et al. , 2019 ]Muhammad Ammad-ud-din,
Elena Ivannikova, Suleiman A. Khan, Were Oyomno,
Qiang Fu, Kuan Eeik Tan, and Adrian Flanagan. Federated
collaborative ﬁltering for privacy-preserving personalized
recommendation system. CoRR , abs/1901.09888, 2019.
[Berlioz et al. , 2015 ]Arnaud Berlioz, Arik Friedman, Mo-
hamed Ali Kaafar, Roksana Boreli, and Shlomo
Berkovsky. Applying differential privacy to matrix fac-
torization. In Proceedings of the 9th ACM Conference on
Recommender Systems , pages 107–114. ACM, 2015.
[Dwork, 2011 ]Cynthia Dwork. Differential privacy. Ency-
clopedia of Cryptography and Security , pages 338–340,
2011.
[Goldreich, 2009 ]Oded Goldreich. Foundations of cryptog-
raphy: volume 2, basic applications . Cambridge univer-
sity press, 2009.
[Harper and Konstan, 2016 ]F Maxwell Harper and
Joseph A Konstan. The movielens datasets: History
and context. Acm transactions on interactive intelligent
systems (tiis) , 5(4):19, 2016.
[Jager, 2012 ]Tibor Jager. The generic composite residuosity
problem. In Black-Box Models of Computation in Cryptol-
ogy, pages 49–56. Springer, 2012.
[Kim et al. , 2016 ]Sungwook Kim, Jinsu Kim, Dongyoung
Koo, Yuna Kim, Hyunsoo Yoon, and Junbum Shin. Efﬁ-
cient privacy-preserving matrix factorization via fully ho-
momorphic encryption. In Proceedings of the 11th ACM
on Asia Conference on Computer and Communications
Security , pages 617–628. ACM, 2016.
[Kone ˇcn`yet al. , 2016 ]Jakub Kone ˇcn`y, H Brendan McMa-
han, Felix X Yu, Peter Richt ´arik, Ananda Theertha Suresh,
and Dave Bacon. Federated learning: Strategies for
improving communication efﬁciency. arXiv preprint
arXiv:1610.05492 , 2016.
[Koren et al. , 2009 ]Yehuda Koren, Robert Bell, and Chris
V olinsky. Matrix factorization techniques for recom-
mender systems. Computer , (8):30–37, 2009.
[Kosinski et al. , 2013 ]Michal Kosinski, David Stillwell, and
Thore Graepel. Private traits and attributes are predictable
from digital records of human behavior. Proceedings of the
National Academy of Sciences , 110(15):5802–5805, 2013.
[Nikolaenko et al. , 2013 ]Valeria Nikolaenko, Stratis Ioanni-
dis, Udi Weinsberg, Marc Joye, Nina Taft, and Dan Boneh.
Privacy-preserving matrix factorization. In Proceedings of
the 2013 ACM SIGSAC conference on Computer & com-
munications security , pages 801–812. ACM, 2013.
[Paillier, 1999 ]Pascal Paillier. Public-key cryptosystems
based on composite degree residuosity classes. In Interna-
tional Conference on the Theory and Applications of Cryp-
tographic Techniques , pages 223–238. Springer, 1999.[Wang et al. , 2016 ]Leye Wang, Daqing Zhang, Yasha
Wang, Chao Chen, Xiao Han, and Abdallah M’hamed.
Sparse mobile crowdsensing: challenges and opportuni-
ties. IEEE Communications Magazine , 54(7):161–167,
2016.
[Yang et al. , 2016 ]Dingqi Yang, Daqing Zhang, Bingqing
Qu, and Philippe Cudr ´e-Mauroux. Privcheck: privacy-
preserving check-in data publishing for personalized lo-
cation based services. In Proceedings of the 2016 ACM
International Joint Conference on Pervasive and Ubiqui-
tous Computing , pages 545–556. ACM, 2016.
[Yang et al. , 2019 ]Qiang Yang, Yang Liu, Tianjian Chen,
and Yongxin Tong. Federated machine learning: Concept
and applications. ACM Transactions on Intelligent Systems
and Technology (TIST) , 10(2):12, 2019."
1803.08544,D:\Database\arxiv\papers\1803.08544.pdf,"What are the key challenges in analyzing microscopic images of blood samples, and how do these challenges influence the choice of image processing techniques?","The presence of overlapping cells and the need to distinguish between different types of white blood cells pose significant challenges in analyzing microscopic blood images. These challenges necessitate the use of image processing techniques that can effectively segment and classify cells, while also accounting for variations in cell morphology and potential noise in the images.","Image preprocessing operations are performed to sup press the undesired distortions 
present in the image and enhance image features rel evant for further analysis. The 
noise is removed from the image using various filte ring techniques depending upon 
the type of noise. We have utilized Wiener filter w hich adequately reduced the blurri- 
ness without reducing the image sharpness. Further histogram equalization technique 
is applied to enhance the contrast in the image. Fi gure 3 depicts histogram of the im- 
age for performing adaptive histogram equalization.  Gray-scale transformation 
Brightness thresholding is chosen to modify brightn ess and threshold used is 192. 
This gray-scale transformation results in a binary image as shown in Figure 4. [14-
16]. 
The component mainly analyzed in the dataset is leu cocyte other than that every 
other component needs to be eliminated from the dat aset. Further In the dataset being 
examined it is possible that certain percentage of leucocytes is present on the edges of 
the image. In the image cleaning process, the leuco cytes which are at the edge in the 
sample image under study and other irrelevant eleme nts present in the image are re- 
moved in order to reduce errors in the later stages  of the identification process. There 
are two cleaning operations which are required  
• Removal of Noise or Cleaning of Image 
• Removal of abnormal components 
The first one is done with help of filtering algori thms especially wiener and median 
filter. The component having a small area is usuall y the component located on the 
edges. The component with large values of area must  be cells that are overlapping 
leucocytes. Therefore area and the convex area both  need to be calculated for the 
removal of the unwanted components . Figure 5 depicts the morphologically cleaned 
image which is obtained by removing the leucocytes on the edges and irregular com- 
ponents.  
2.2  Image Segmentation.  
Segmentation process partitions an image into disti nct regions  on the basis of features 
of interest. Segmentation in the present work invol ves segregation of white blood 
cells. Five components of white blood cells include : Neutrophil, Basophil, Eosinophil, 
Lymphocyte and Monocyte. ALL symptoms are associate d only with the lymphocytes 
since morphological components of normal and malign ant lymphocytes are signifi- 
cantly different ; so other four components of whit e blood cells namely neutrophil, 
basophil, eosinophil and myelocytes, are neglected during segmentation process.   
  Figure 6, Figure 7 and Figure 8 depicts results o f the k – means clustering with 
segregation of different nucleus and cytoplasm. [17 -19].The performance of segmen- 
tation approaches such as k – means [20], texture b ased segmentation [21] and color 
based segmentation [22] have been compared. The bri ef description of the perfor- 
mance measures used is: 
• Probability Random Index (PRI): It is a nonparametr ic evaluation of the goodness 
of segmentation. It is obtained by summing the numb er of pixel pairs with same 
label and number with different label in both S (te st samples) and G (ground reali- 
ty) and then dividing it by total number of pixel p airs. For a given a set of ground 
truth segmentations G k, PRI is evaluated using: 
PRI Stest, Gk			1
/2     1 	  1 	  
∀	,&	1 
Where c ij  is an event that describes a pixel pair (i, j) hav ing same or different label 
in the test image S test  
 
• Variance of Information (VOI): Variation of Informa tion gives the measure of 
distance between two clusters.  It gives partition of pixels with different cluster s. 
Clustering with clusters is represented by a random  variable X, X = {1… k} such 
that 	||
,  	 ∈ "", #$%	$ 		 ∑"" 	is the variation of information between two 
clusters X and Y. Thus VOI(X, Y) is represented usi ng  
VOI(X, Y) = H(X) = H(Y) – 2I(X, Y) (2) 
Where H(X) is entropy of X and I(X, Y) is mutual in formation between X and Y.  
 
• Global Consistency Error (GCE): Local refinement er ror is calculated using equa- 
tion (3), where s i and g j contain pixel, p k, so that s ∈ S, g ∈	 G,	 where	 S	   segment is 
obtained after segmentation by the algorithm being evaluated and G denotes refer- 
ence segment. The value obtained from (3) is used t o evaluate global consistency 
errors using (4), where n denotes set of difference  operation. R(x, y) represents the 
set of pixels corresponding to region x that includ es pixel y. GCE quantify the 
amount of error in segmentation. Table 1 depicts th e comparative values of seg- 
mentation algorithms on the basis of VOI, PRI and G CE for the dataset evaluated. 
[23-25] 
+,-,g,/0	ǀ2,s,/0/2,g,/0ǀ
ǀ2,s,/0ǀ3 
GCE 7, 8	1
nmin <	∑ES, G.  , ES, G.  >4 
Table 1.  Qualitative Analysis of Performance Parameters 
 PRI  GCE  VOI  
Color k -means  0.932  O.00 79  0.089  
k-means  0.94 2 0.009 1 0.092  
Texture based  0.94 1 0.013 2 0.015  
 
 
Fig. 2.  Microscopic Image of blood Sam- 
ples 
 
 
Fig. 3.  Histogram for Adaptive Threshold- 
ing  
Fig. 4.  Binary Image 
 
Fig. 5.  Morphologically Cleaned Image 
3 Identification and Classification 
Identification involves extraction of color, Geomet ric, Textural and statistical fea- 
tures. Final step i.e. the labeling of sample as ma lignant or benign is achieved through 
image classification process. Image classification analyzes various image features to 
arrange data into categories. Classification algori thms typically employ two process- 
es: training and testing. In the training process, relevant properties of typical image 
features are isolated and a unique description of e ach classification category is creat- 
ed.  
Two categories of classification algorithms namely supervised and unsupervised 
are generally used. In supervised classification, s tatistical processes are employed to 
extract class descriptors. Classification used in t he present work relies on clustering 
algorithms to automatically segment the training da ta into various prototype classes. 
During the testing phase, features of sample datase t are compared with the previously 
calculated standard values. Depending upon the valu es of the input image finally clas- 

sification is achieved with the help of Nearest Nei ghbor (kNN) and Naïve Bayes 
Classifier, comparison of which is also presented. The work was carried out on the 
dataset of 60 samples . 
3.1  Identification of grouped leucocytes.  
Microscopic images of blood samples usually contain  cells which are overlapping, 
this complicates the analysis and identification pr ocess. Segregation of Region of 
Interest (ROI) is achieved through k-Means clusteri ng. In order to segregate leuco- 
cytes roundness has been used as a measure. Roundne ss checks whether the shape of 
the object is circular or not by excluding the loca l irregularities. Roundness can be 
gained by dividing the area of a circle to the area  of an object by using the convex 
perimeter. 
2@A$%$B-- 	4	""	C	""	#DB#	
	@$EBF_BD HBIBD J5 
The value of roundness is 1 if the object is circul ar and the value of roundness is 
less than 1 for the non-circular objects. Roundness  as a measure is less sensitive to 
irregular boundaries because it excludes the local irregularities. Threshold value cho- 
sen is 0.80 to distinguish between the single leuco cyte and clusters of overlapping 
leucocytes. The components which are having the rou ndness value more than the 
value of threshold are considered as the individual  leucocyte while the components 
which are having value less than the threshold are considered as grouped leucocytes. 
Figure 9 represents roundness metrics obtained for various leucocytes. The individual 
leucocytes are sent next for the further study and the grouped leucocytes are sent to 
the separation process. [26-28] 
 
 
Fig. 6.  Cluster 1: k-means Clustering 
 
  
Fig. 7.  Cluster 2: k-means Clustering 

 
Fig. 8. Cluster 3: k-means Clustering   
Fig. 9.  Metrics Close to One Indicates 
Roundness.  
Solidity is used to find out the density of a compo nent. It is obtained as: 
7@L % IM	 	#DB# 
@$EBF	#DB# 6 
If the value is 1 then it can be identified as a so lid object. If the value is less than 1 
then it is a component having irregular boundaries.  The threshold value for solidity 
which is used for identifying the abnormal componen ts is obtained from the image 
which is having individual leucocytes. An optimum t hreshold value of 0.80 can effi- 
ciently be used to find out abnormal components fro m the image this is depicted in 
Figure 10. The components which are having the soli dity value less than the threshold 
are removed. [29, 30]  
 
 
Fig. 10.  Measure of Solidity  
Fig. 11.  Standard Deviation of identified 
leucocytes 

 3.2  Nucleus and cytoplasm selection  
The leucocytes segmented can now be used to extract  the nucleus and cytoplasm. This 
is achieved by cropping the image with the bounding  box. This step separates out 
each leucocyte. The borders of images obtained in t he above step have to be cleaned 
in order to proceed further. Next step involves cro pping out the outer portion of  the 
leucocyte to segregate cytoplasm. This process segr egates cytoplasm. From the close 
examination it can be concluded that white blood ce lls nuclei are more in contrast on 
the green component of the RGB color space. So, we can get nucleus by using the 
threshold. [31]  
 
 
Fig. 12.  Segmented leucocytes with Corresponding Diameter a nd Area 
3.3  Feature Extraction 
Feature extraction is the process of converting the  image into data so that we can 
check these values with the standard values and fin ally identify infected samples. 
Figure 12 depicts individual segmentation of the ly mphocytes with its area and di- 
ameter. Features required to train model parameters  include are:  
• Color Features – It includes mean color values of t he grey images acquired.  
• Geometric Features – It includes perimeter, radius,  area, rectangularity, compact- 
ness, convexity, concavity, symmetry, elongation, e ccentricity, solidity etc.  
• Texture Features – It Includes entropy, energy, hom ogeneity, correlation as ob- 
tained.  

• Statistical Features – It includes mean, variance a nd standard deviation. The values 
are computed are shown in Figure11.  
+L@$O#I @$	 	 1 	H#P@D	#F - 
H $@D	#F -7 
+B$ID AIM	 	RH#P@D	#F - J H $@D	#F - J
H#P@D	#F - 8 
2BI#$OAL#D IM	 		#DB# 
H#P@D	#F -	""	H $@D	#F - 9 
U@$EBF IM	 		BD HBIBD_@$EBF 
BD HBIBD 10  
U@H#I$B-- 	4	""	C	""	#DB#	
	BD HBIBD J11  
Elongation indicates the object elongation towards particular axis. Rectangularity 
depicts how well the bounding box is filled. Eccent ricity is the ratio of the major axis 
length and the foci of the ellipse. Convexity shows  the relative amount of difference 
of object from its convex object. Compactness is th e ratio of the area of an object and 
area of circle having same perimeter. Figure 13 dep icts the geometric characteristics 
of the infected cell and Figure 14 depicts texture features of the infected cell. [32, 33] 
 
 
Fig. 13.  Geometric Features of Infected Cells.  -1 -0.5 00.5 1
Cell 1Cell 3Cell 5Cell 7Cell 9 Elongation 
Eccentricity 
Rectangularity 
Compactness 
 
Fig. 14.  Texture Features of Infected Cells.  
3.4  Image Classification 
 Proposed algorithm is tested with Nearest Neighbor  (kNN) and Naïve Bayes Classi- 
fier on the dataset of 60 pretested samples, the ac curacy achieved is 92.8%. 
The process involved following steps  
1.  Finalization of feature set  
2.  Selection of appropriate Algorithm 
3.  Mapping and Training of model parameters 
Features such as Elongation, Eccentricity, Rectangu larity, Convexity, Compactness, 
entropy, energy, homogeneity, correlation and stand ard deviation are used to train 
model parameters to identify infected cells. Confus ion Matrix has been utilized to 
compute performance of classifiers. Performance par ameters included accuracy, sen- 
sitivity and specificity shown in Figure 15. 
 
  
Fig. 15.     Performance Analysis of classifiers 00.2 0.4 0.6 0.8 11.2 1.4 
Cell 
1Cell 
3Cell 
5Cell 
7Cell 
9Cell 
11 Cell 
13 Entropy 
Correlation 
Energy 
Homogeneity 
00.2 0.4 0.6 0.8 1
Accuracy Sensitivity Specificity Naives Bayes 
Classifier 
Nearest 
Neighbour(Knn) 
Classifier "
2206.09867,D:\Database\arxiv\papers\2206.09867.pdf,"While the paper focuses on using WiFi signals for human activity recognition, how might the proposed method be adapted or extended to address the challenges of recognizing activities in more complex, real-world environments with varying levels of noise and interference?","The paper's method could be adapted to handle complex environments by incorporating robust noise filtering techniques and incorporating contextual information, such as environmental factors and prior knowledge about the user's typical activities.","Table 3 . Classiﬁcation accuracy on the dataset HHI.
Methods approaching departinghand
shakinghigh ﬁve huggingkicking
leftlegkicking
right legpointing
lefthandpointing
right handpunching
lefthandpunching
right handpushing OA
GoogleNet [17] 0.93 0.93 0.79 0.76 0.64 0.54 0.50 0.78 0.77 0.59 0.59 0.68 0.71
ResNet-18 [18] 0.92 0.90 0.85 0.79 0.77 0.68 0.60 0.82 0.80 0.60 0.65 0.76 0.76
Squeeze-Net [19] 0.95 0.93 0.83 0.76 0.70 0.66 0.62 0.78 0.79 0.60 0.72 0.74 0.76
E2EDLF [20] 0.96 0.92 0.89 0.84 0.86 0.78 0.82 0.85 0.90 0.73 0.80 0.86 0.85
SVM 0.99 0.96 0.90 0.83 0.82 0.73 0.79 0.69 0.62 0.74 0.77 0.74 0.78
2DWNN 0.93 0.89 0.93 0.88 0.67 0.99 0.99 0.89 0.94 0.99 0.62 0.82 0.88
STWNN 0.99 0.99 0.93 0.96 0.85 0.84 0.83 0.89 0.92 0.76 0.75 0.87 0.90
Based on the skeletons, the trained STWNN can further gen-
erate skeletons in non-line-of-sight (NLOS) scenes. Based
on the skeletons, these results by SVM obtained an accuracy
of 94 %, which is slightly lower than STWNN-based WiFi
only. The reason behind this is mainly due to the distortions
in predicted skeletons in NLOS scenes.
Table 2 represents classiﬁcation accuracy on the WAR
dataset. STWNN outperforms all the others with over 93%
accuracy. STWNN achieves the best except for the action
run. Regarding action run, STWNN has relatively 1% lower
accuracy than 2DWNN but still an absolute improvement of
16% than LSTM [13] in the original paper. Even in more
complex actions of lie-down and fall, these improvements by
STWNN compared with LSTM can also achieve 1%and5%,
respectively.
These classiﬁcation results on the HHI dataset are shown
in Table 3. The OA of STWNN ranks the ﬁrst ( 90%). Es-
pecially, most of these results of 2DWNN and STWNN are
better than all the other models, except for these results of
hugging and punching-with-left-hand, but still are compara-
ble with E2EDLF, which show that 2DWNN and STWNN are
both very strong baselines.
These classiﬁcation results on the CSLOS dataset are
shown in Table 3. The classiﬁcation accuracy of STWNN
ranks ﬁrst compared to all other two methods in two LOS
scenes E1 and E2. As for E1, STWNN achieves the best
results on all actions, with 2 %higher than SVM [23]. Con-
cerning E2, the performance of STWNN is better except
for no movement and walking which still are comparable
with those of SVM [23]. In other words, STWNN has good
robustness in comparison to the other two models.
Table 4 . Classiﬁcation accuracy on the dataset CSLOS
Scenes Methodsno
movefalling walkingsitting
standingturningpicking
upAverage
E1SVM [23] 0.98 0.86 1.00 0.91 0.90 0.92 0.94
2DWNN 0.89 0.80 0.73 0.86 0.67 0.94 0.81
STWNN 0.96 0.96 0.93 0.99 0.92 0.99 0.96
E2SVM [23] 0.95 0.82 0.99 0.82 0.81 0.82 0.89
2DWNN 0.84 0.78 0.75 0.83 0.69 0.84 0.79
STWNN 0.93 0.94 0.94 0.95 0.90 0.88 0.92
Qualitative Results In this section, we show the effective-
ness of CSI data on WV AR. Besides, we demonstrate the spa-
tiotemporal scheme and the attention module of STWNN are
meaningful at the feature level.
Skeleton visualization is further to show the effectiveness
of WV AR as mentioned in section 4.1. As seen in Fig. 4(a)-(h), our STWNN yields robust skeletons close to Alphapose.
Particularly, these partially covered actions such as kick are
also well-estimated. This demonstrates that our CSI data on
WV AR has a good efﬁciency in these two scenarios.
In Fig. 5, we show gSOM [24] projections of the features
of the WAR dataset by 2DWNN and 3DWNN before and after
using the attention module, respectively. Features extracted
from the same action tend to be near each other, and vice
versa. Compared Fig. 5(a) with Fig. 5(c), we can ﬁnd the fea-
tures from the same action of 3DWNN are more compact than
that of 2DWNN in terms of lie-down, walk, and stand-up.
It proves that 3DWNN has better potential than 2DWNN in
exploring the effective spatiotemporal features. Perceptually,
comparing Fig. 5(c) with (d), features from the same category
after using the attention module are more clustered than that
before, such as run and pick-up. It further indicates that the
attention module improves the efﬁciency of the features to a
certain extent.
Fig. 5 . The features projections of the dataset WAR by
2DWNN and STWNN before and after using the attention
module.
4. CONCLUSION
In this paper, an end-to-end spatiotemporal WiFi-based neu-
ral network STWNN was proposed to enhance the perfor-
mance of privacy-preserving WiFi-based HAR. Its strength
lay in the ability for the effective exploitation of the multi-
scale spatiotemporal features and explicit maintenance of
self-attention features. Moreover, we collected synchronous
video and WiFi datasets WV AR to enable STWNN to see
the skeleton in complex visual conditions like partial and
full occlusions scenarios. In addition, we have compared the
results of our proposed STWNN with the results of SVM,
2DWNN, and state-of-the-art competitors. The experiments
on four benchmark datasets WV AR, WAR, HHI, and CSLOS
showed that STWNN compares favorably against competitive
baselines.
5. REFERENCES
[1] Mariko Isogawa, Ye Yuan, Matthew O’Toole, and
Kris M Kitani, “Optical non-line-of-sight physics-based
3D human pose estimation,” in CVPR , 2020, pp. 7013–
7022.
[2] Yiyue Luo, Yunzhu Li, Michael Foshey, Wan Shou,
Pratyusha Sharma, Tomas Palacios, Antonio Torralba,
and Wojciech Matusik, “Intelligent Carpet: Inferring
3D human pose from tactile signals,” in CVPR , 2021,
pp. 11255–11265.
[3] Bang Wu, Zixiang Ma, Stefan Poslad, and Yidong Li,
“WiFi ﬁngerprint based, indoor, location-driven activi-
ties of daily living recognition,” in BESC . IEEE, 2018,
pp. 148–151.
[4] Bo Tan, Qingchao Chen, Kevin Chetty, Karl Wood-
bridge, Wenda Li, and Robert Piechocki, “Exploiting
WiFi channel state information for residential healthcare
informatics,” IEEE Commun. Mag. , vol. 56, no. 5, pp.
130–137, 2018.
[5] Fei Wang, Jinsong Han, Shiyuan Zhang, Xu He, and
Dong Huang, “CSI-net: Uniﬁed human body charac-
terization and action recognition,” arXiv:1810.03064 ,
2018.
[6] Tianhong Li, Lijie Fan, Mingmin Zhao, Yingcheng Liu,
and Dina Katabi, “Making the invisible visible: Ac-
tion recognition through walls and occlusions,” in ICCV ,
2019, pp. 872–881.
[7] Djamila Romaissa Beddiar, Brahim Nini, Mohammad
Sabokrou, and Abdenour Hadid, “Vision-based human
activity recognition: a survey,” Multimed. Tools. Appl. ,
vol. 79, no. 41, pp. 30509–30555, 2020.
[8] Chenning Li, Zhichao Cao, and Yunhao Liu, “Deep AI
enabled ubiquitous wireless sensing: A survey,” CSUR ,
vol. 54, no. 2, pp. 1–35, 2021.
[9] Shiwei Fang, Sirajum Munir, and Shahriar Nirjon, “Per-
son tracking and identiﬁcation using cameras and Wi-
Fi channel state information (CSI) from smartphones:
dataset,” in SenSys , 2020, pp. 26–30.
[10] Rami Alazrai, Ali Awad, Alsaify Baha’A, Mohammad
Hababeh, and Mohammad I Daoud, “A dataset for
Wi-Fi-based human-to-human interaction recognition,”
Data Brief , vol. 31, pp. 105668, 2020.
[11] Hao Wang, Daqing Zhang, Yasha Wang, Junyi Ma, Yux-
iang Wang, and Shengjie Li, “RT-Fall: A real-time and
contactless fall detection system with commodity WiFi
devices,” IEEE Trans. Mobile Comput. , vol. 16, no. 2,
pp. 511–526, 2016.[12] Alsaify Baha’A, Mahmoud M Almazari, Rami Alazrai,
and Mohammad I Daoud, “A dataset for Wi-Fi-based
human activity recognition in line-of-sight and non-line-
of-sight indoor environments,” Data in Brief , vol. 33,
pp. 106534, 2020.
[13] Siamak Youseﬁ, Hirokazu Narui, Sankalp Dayal, Ste-
fano Ermon, and Shahrokh Valaee, “A survey on behav-
ior recognition using WiFi channel state information,”
IEEE Commun. Mag. , vol. 55, no. 10, pp. 98–104, 2017.
[14] Koby Crammer and Yoram Singer, “On the algorith-
mic implementation of multiclass kernel-based vector
machines,” J. Mach. Learn Res. , vol. 2, no. Dec, pp.
265–292, 2001.
[15] Tin Kam Ho, “Random decision forests,” in ICDAR .
IEEE, 1995, vol. 1, pp. 278–282.
[16] Sean R Eddy, “What is a hidden Markov model?,” Nat.
Biotechnol. , vol. 22, no. 10, pp. 1315–1316, 2004.
[17] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-
manet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,
Vincent Vanhoucke, and Andrew Rabinovich, “Going
deeper with convolutions,” in CVPR , 2015, pp. 1–9.
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun, “Deep residual learning for image recognition,” in
CVPR , 2016, pp. 770–778.
[19] Forrest N Iandola, Song Han, Matthew W Moskewicz,
Khalid Ashraf, William J Dally, and Kurt Keutzer,
“SqueezeNet: AlexNet-level accuracy with 50x fewer
parameters and¡ 0.5 mb model size,” arXiv:1602.07360 ,
2016.
[20] Rami Alazrai, Mohammad Hababeh, Alsaify Baha’A,
Mostafa Z Ali, and Mohammad I Daoud, “An end-to-
end deep learning framework for recognizing human-to-
human interactions using Wi-Fi signals,” IEEE Access ,
vol. 8, pp. 197695–197710, 2020.
[21] Mingmin Zhao, Tianhong Li, Mohammad Abu Al-
sheikh, Yonglong Tian, Hang Zhao, Antonio Torralba,
and Dina Katabi, “Through-wall human pose estima-
tion using radio signals,” in CVPR , 2018.
[22] Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu
Lu, “RMPE: Regional multi-person pose estimation,”
inICCV , 2017, pp. 2334–2343.
[23] Alsaify Baha’A, Mahmoud M Almazari, Rami Alazrai,
and Mohammad I Daoud, “Exploiting Wi-Fi signals for
human activity recognition,” in ICICS . IEEE, 2021, pp.
245–250.
[24] Nejc Ilc and Andrej Dobnikar, “Generation of a clus-
tering ensemble based on a gravitational self-organising
map,” Neurocomputing , vol. 96, pp. 47–56, 2012."
1810.09957,D:\Database\arxiv\papers\1810.09957.pdf,"While the paper highlights the benefits of using NSML for machine learning tasks, it also mentions some limitations.  What are the potential challenges associated with managing datasets within the NSML platform, and how might these challenges impact the scalability and efficiency of machine learning projects?","The paper notes that NSML lacks version control for datasets, making it difficult to manage revisions and potentially hindering the efficient use of multiple datasets for multi-task or transfer learning. This could limit the platform's scalability for complex projects requiring diverse data sources.","NSML: Meet the MLaaS platform with a real-world case study
Figure 2. Dataset view in NSML web application
Additionally, users can organize teams for collaboration.
They can share their sessions, results, models, and program
codes with team members. The users can set the private
dataset within the team if needed so that only team members
with a permission are able to access the dataset. Figure 2
shows an example view of dataset list, and a user can check
the meta-data of the registered dataset like the size and the
latest access time.
3.4 User Interface
3.4.1 Command line interface
Basically, NSML supports a series of commands through the
command line interface (CLI). CLI does not require a spe-
ciﬁc operating system or environment, thus it can improve
user experiences with platform independence. Figure 3a
and 3b show the processes of runandinfer commands, re-
spectively. Existing code can be launched on NSML with
a few additional lines. As shown in Table 1, all NSML
commands belong to the following four categories. Some
commands are only available to a system administrator.
Account Management : While NSML provides only au-
thorization like login for normal users, the adminis-
trators can manage the users’ properties including the
permission level and the credit through this category.
(a)
(b)
Figure 3. Example of client tool command executing sequence
The credit is used to regulate the monopolized usage of
the cluster by a speciﬁc user. It is consumed when the
user runs sessions according to the credit policy. If the
credit is exhausted, the existing sessions may be safely
stopped and the user cannot launch any more sessions.
Session Control : The user can operate own code in a unit
of a session with the resources in NSML. The request
for system resources is estimated by the NSML sched-
uler, and can be rejected when the available resources
are insufﬁcient to meet the requirements. The session
has saved all the information a user used including
codes, logs, and meta-information, so that the user can
reproduce or revise the owned sessions from the previ-
ous setting ( e.g.hyperparameter).
Data Analysis : These commands are used to collect the
progress and results from a session. Using the col-
lected data, the user can visualize learning curves, ob-
tain trained models, and compare the results to other
sessions. We described the details on visualization in
Section 3.4.2.
NSML Service : NSML supports a number of additional
services, for instance, allowing users to easily machine-
learning work with remote resources. This is discussed
in detail later.
NSML: Meet the MLaaS platform with a real-world case study
Table 1. User Client Tool
Purpose Commands
Account Manage credit, login, logout
Session Control backup, command, diff, download, fork, getid, logs, ps, resume, rm, run, stop
Data Analysis eventlen, events, exec, memo, model, plot, pull, sh, submit
NSML Service automl, dataset, gpumornitor, gpustat, infer, status
Figure 4. Example of NSML analysis view: three sessions with different hyperparameters shown on NSML scalar plot. Top: The
comparison part of each session. Bottom: Each plot of event data which users reported. The orange colored session is highlighted by the
mouse on the ‘test/accuracy’ plot.
3.4.2 Web Interface and Visualization
To improve user experience and system usability, we imple-
mented web application of NSML. The web app helps users
to more intuitively and easily perform the operations for ses-
sions: stopping, forking, logging, and deleting, which could
also be performed in the NSML CLI environment. Users
can compare and analyze multiple sessions simultaneously
through NSML web app. We also introduce powerful tools
for visualizing the progress of users’ sessions. Users can
represent the results of their sessions as graphs by Visdom6
and TensorBoard (Girija, 2016). Especially, our scalar plot
has some advantages in comparing multiple sessions with
different setup, as shown in Figure 4.
As represented at the top-right side of Figure 4, the compar-
ison panel is provided for comparing the arguments used in
multiple sessions easily. The visualization window is com-
posed of ‘common arguments’ and ‘exclusive arguments’
part to represent the arguments of each session. For the ‘ex-
clusive arguments’, a matrix-based diagram represents the
different arguments in a scalable way. Through the diagram,
users can easily recognize the overview of the differences
between each session’s argument such as whether the argu-
6https://github.com/facebookresearch/
visdomments exist in the session or not. Additionally, users can se-
lect and analyze the arguments in which they are interested,
from the right side of the diagram. Also, by highlighting in
each line graph when mouse interacted on the row of the
visualization, the user recognizes which session is indicated
currently.
Meanwhile, if multiple users work on the platform, web
interface also provides administration features such as man-
aging GPU usage of the total platform and checking resource
usage of the speciﬁc session. Web interface also visualizes
ranks among test results of the sessions so that users can
easily compare their session performances and arguments.
This feature will be useful for competitions.
3.4.3 Serving API
NSML platform provides serving APIs for the trained mod-
els as well as developing the models. The user can launch a
session for serving the model created on the NSML using
the command included in the client tool, and also test the
inference modules to conﬁrm the action of the submitted
models.
Furthermore, NSML is capable of serving through a REST-
ful API in order to improve accessibility and expand the
application layers of NSML with web-based services. For
NSML: Meet the MLaaS platform with a real-world case study
example, if a user wants to serve their model directly on
the Web, the user trains the model on the NSML platform,
and simply submits their own inference procedure to the
platform. At the service start time, the user starts the session
with the submitted procedure for end-users to access the
application that uses the trained models.
3.5 Hyperparameter tuning
Optimizing hyperparameter is one of the most time-
consuming step of building a machine learning model. As
the number of hyperparameter grows, the size of search
space grows exponentially, and thus ﬁnding the best hyper-
parameters gets more difﬁcult as well. As other ML plat-
forms, NSML also supports the parallelized hyperparameter
tuning. It is not only constrained to grid or random search,
but also possible to apply many state-of-the art tuning algo-
rithms such as population based training (Jaderberg et al.,
2017).
3.6 Implementations
NSML consists of three core modules; high computing
resources, server-side NSML, and a scheduler. We chose
Docker (Merkel, 2014) to set an environment for machine
learning instances. Docker container is known as stable,
light, and low-latent virtualization layer comparing to vir-
tual machine(Felter et al., 2015) or virtual environment
for python (Reitz & Schlusser, 2016). Furthermore, since
Docker supports a network connection to communicate con-
tainers in a machine from another machine, we can control
the sessions over the network. Consequently, many features
will be available for session management such as stopping
or removing a session with Docker. Containerized ML envi-
ronment can guarantee stability as well as reproducibility,
since it is able to manage the crucial system resources like
memory or processors directly.
Modern machine learning tasks require a lot of the system
resources, and it may damage or even drive the host sys-
tem into a panic by using excessive usage of the resource.
In NSML, we used a virtualization layer to manage the
resources assigned to each session, independently and ex-
clusively. If a certain session claims too much memory over
exceeding the allocated amount, it will be killed immedi-
ately by NSML without any damage on the host machine.
Even without the layer, server-side NSML contains many
external components like database, messaging queue, and
shared ﬁle system. Since many existing libraries used in
NSML support python bindings, including machine learn-
ing applications, NSML was written in python primarily.
The way of current implementation contributes sufﬁcient
ﬂexibility to NSML ﬂexible by providing more alternatives
for the components. For instance, a system administrator
may want to use any other kinds of DB schema such as SQL
Figure 5. Example of NSML leaderboard
Figure 6. Feedback from competition participants
and NoSQL for NSML, and it is replaceable depending on
how NSML manager will set their infrastructure because
almost all frameworks and systems support python API. In
addition, some part of the command-line interface layer was
implemented in Go (Donovan & Kernighan, 2015) due to
security and low latency.
We decided to design and implement our own scheduler
and resource manager instead of using 3rd party libraries
like docker swarm, since NSML scheduler asks a ﬁne-
grained management for the host machine’s resource in-
cluding GPUs. The scheduler part was developed in python
for readily integrating to the entire system, moreover, it is
running on the virtualization layer using docker for easy
deployment and maintenance.
4 E XPERIMENTS AND CASESTUDIES
4.1 Experimental Setup
Our proposed NSML has been running on our computing
cluster, which includes ncomputing nodes with hundreds
of GPUs. Each computing node has up to 256GB of system
memories and 8 GPUs. To demonstrate the reproducibility
of NSML, we conducted the training of the existing models
NSML: Meet the MLaaS platform with a real-world case study
Table 2. Training Example Summary
Dataset Epochs Batch size Initial LR Best Accuracy (%) Previous work (%)
MNIST 10 128 0.5 99.2 99.1 (LeCun et al., 1998)
CIFAR-100 300 128 0.1 76.44 76.27 (Zhong et al., 2017)
ImageNet 120 2,048 0.8 74.8 75.3 (He et al., 2016)
Table 3. User statistics in NLP competition
Questions stage 1 Questions stage 2 Movie stage 1 Movie stage 2
# of users 93 89 55 58
# of models 3,907 3,658 5,044 5,267
Average # of models per user 42.01 41.10 91.71 90.81
Max # of models per user 329 383 1,103 732
# of users less than 5 models 24 27 14 13
for image classiﬁcation datasets; MNIST (LeCun & Cortes,
2010), CIFAR-100 (Krizhevsky & Hinton, 2009), and Im-
ageNet (Russakovsky et al., 2015). All the programming
codes are written in python using PyTorch (Paszke et al.,
2017) and Torchvision (Marcel & Rodriguez, 2010) libraries.
As a environmental setup, we allocated one GPU for both
MNIST and CIFAR-100 each, and eight GPUs for a model
on ImageNet.
For MNIST example, we designed a simple convolutional
neural network which consisted of two convolutional layers
and two inner product layer. We achieved 99.22 percent of
accuracy for the validation set of MNIST example, and it
is acceptable level according to the previous work (LeCun
et al., 1998). We also trained ResNet-110 and ResNet-50
models for CIFAR-100 and ImageNet datasets using the
previous work (Zhong et al., 2017; He et al., 2016), respec-
tively. The result explains that we reproduce the previous
work using NSML successfully. The summary of the result
is shown in Table 2.
4.2 ML Competitions via NSML
We have held three machine learning competitions on our
proposed NSML. Each task is related to the followings; 1)
General natural language tasks, 2) Predicting tilted angle of
given document images, and 3) Correcting keyboard typo
on mobile devices, respectively. Except the ﬁrst competition,
the best models in last two competitions perform better than
the baseline which were on real customer services. After
the competitions were ﬁnished, the best models have been
applied to enhancing the services. The entire pipeline of
competitions are; preprocessing for data to result submission
and test, all the processes were running on NSML, and we
also supported real-time leaderboard on our website so thatall participants could immediately check their ranking.
Figure 5 presents an example of NSML leaderboard from
the ﬁrst competition. The ﬁgure shows the list of user ID,
dataset, ranking, score, and name of evaluation metric and
order. In addition, it is able to display submission history
for each user to check trend of model performance.
4.2.1 Natural Language Tasks
We suggested two problems for the natural language process-
ing competition; 1) Predicting preference score of movie
from user’s review and 2) Recognizing if two sentences are
asking the same questions or not. The former one is a kind
of regression task, thus mean squared error (Lehmann &
Casella, 1998) was chosen as an evaluation metric. For the
latter one, we used accuracy as an evaluation metric since
the latter task is binary classiﬁcation.
This competition was composed of three rounds. First two
rounds had been scheduled for two weeks via online respec-
tively, with different size of datasets. The size of dataset
used in the second round was twice as much as in the ﬁrst
round. The ﬁnal round was held for two full days in ofﬂine
with a new, but comparatively small datasets. The initial
participants were a hundred teams selected from more than
1,300 users and 700 teams. The end of each round, only
participants with higher score were survived.
During the competition, we have collected feedback and
interview from our participants. Figure 6 shows summary
of NSML-related feedback from the user. The majority of
participants have been satisﬁed with user interface of NSML.
They were satisﬁed by NSML’s usability to develop their
models and visualization through web interface. However,
they were not that satisﬁed on system robustness. According
NSML: Meet the MLaaS platform with a real-world case study
Figure 7. Monitoring on GPUs for each session. Top: History of GPU utilization of one session. Bottom: Dashboard with average GPU
utilization of each session
to the user interview, the most of dissatisfaction was caused
by lack of resources, especially GPUs, and the other was
because of system failure in networking or database.
Table 3 shows statistics of participants behavior. As shown
in the table, most of users succeeded to run at least one
session in the ﬁrst phase. Consequently, we can conjecture
that NSML does not have high entry barrier for majority of
users. Only small ratio of participants launched less than
5 models, then, NSML proved its accessibility and user
convenience for developing machine learning models.
4.2.2 Object-Rotating Angle Prediction
The objective of the second competition was to predict the
angle from the given tilted images. Accuracy has been used
for the evaluation metric with 1 degree of error range. The
competition was held as a single-game without multi-rounds
for a month. All participants were selected from our orga-
nization internally, without considering their background.
Only few of the participants had a knowledge of machinelearning or computer vision, and even, some of them were
not educated in the regular course of computer science. The
ﬁrst column of Table 4 represents behavior of Angle predic-
tion competition. Only slightly above a half of participants
used our framework very rarely (less than 5 training and
submissions), which means other half of them used NSML
frequently for training.
4.2.3 Keyboard-Typo Correction
For the latest competition, the examination problem was
to correct an invalid keystroke from the given sequence
of keyboard inputs on a mobile device. The model which
participants should design decides whether the given key-
board input is correct or not, and if not, corrects it. F1 mea-
sure (Chinchor, 1992) has been used for the evaluation met-
ric. This competition also consisted of only a single round,
and was held for two and a half months. Similar to the com-
petition for angle prediction, all participants were selected
from the our company internally without considering their
background.
NSML: Meet the MLaaS platform with a real-world case study
Table 4. User statistics in Angle prediction and Keyboard correction competitions
Angle Prediction Keyboard Correction
Average # of models per user 78.87 93.18
Max # of models per user 546 1,075
Ratio of users less than 5 models 53.3% 50.8%
Average # of submissions per user 63.57 84.94
Max # of submissions per user 553 2,008
Ratio of users less than 5 submissions 56.7% 56.6%
0%10%20%30%40%50%60%70%80%90%
(a)(b)GPU usageGPU usage over 80%
Figure 8. Monitoring ratio of running GPUs. Blue bars present
ratio of all running GPUs, and orange bars show ratio of GPUs
with over 80% of utilization. (a) Before supporting visualization
of GPU utilization, (b) After supporting GPU utilization
The second column of Table 4 represents information for
Keyboard correction competition. The numbers of the com-
petition was analogous to the previous one, and at least over
a half of participants still continued to develop their model
using our platform after the middle of competition. It im-
plies that NSML shows its convenience for improving the
existing ML works as well as the lower entry barrier for
starter.
5 D ISCUSSION
5.1 Resource Utilization
Currently, hundreds of users perform their ML work on
NSML which is running on a cluster with massive comput-
ing resources. In the last several months, utilization of our
resources, the ratio of running GPUs to the total GPUs is
over 70% on average, and especially, is achieved full uti-
lization ( ≈100% ) at the peak. Even though the number of
entire GPUs keeps increasing in the last a few months, aver-
age of GPU utilization along time has not been decreased.
Consequently, usability of NSML is remarkable, especiallyon dealing with many experiments demanding massive com-
puting resources.
Meanwhile, even though the users want to optimize their
code in terms of resource efﬁciency, they cannot achieve
without knowing the GPU usage of their model. For easy
proﬁling and monitoring, we also introduced the monitoring
tools which collects and visualizes the utilization of a single
GPU automatically via Kibana (Chhajed, 2015). Figure 7
shows both history of utilization and occupied memory of
one GPU and dashboard for average GPU utilization on
the latest NSML sessions. The bottom side is shown on
the front web page, so the users are informed average GPU
usage of their sessions. The top image would be displayed
if the users need information in detail, and with the image,
they would be able to check which line of code harms the
resource efﬁciency.
The utilization has had a tendency to increase consistently
ever since the monitoring feature was active. With the fea-
ture, the users can ﬁgure out efﬁciency of code and try to
improve it themselves. And they actually increased the num-
ber of GPUs with over 80% of GPU utilization. As it is seen
in Figure 8, percentage of running GPUs has increased less
than 5%, while percentage of GPUs with over 80% utiliza-
tion has increased about 10% after the visualization feature.
We surmised that the users may improve their strategy for
GPU usage referring to the collected information.
5.2 Limitations and Future Work
Although we have succeeded to improve resource efﬁciency
and usability for ML research through NSML, a few limita-
tions are still left. Above all, one of the most important con-
cerns is related to managing datasets efﬁciently. Although
NSML supports several ﬁle systems including shared ﬁle
system over networking, however, the absence of version
control for datasets makes it difﬁcult to even patch a mi-
nor revision on the existing one. Moreover, as multi-task
learning and transfer learning are expected to become popu-
lar, the demand for using multiple datasets grows gradually.
However, a single task using the multiple dataset is not yet
NSML: Meet the MLaaS platform with a real-world case study
supported due to the limitation from the current design of
NSML’s session representation. We are considering these
features as a top priority, and make it available in the near
future.
The other important issue is an advanced visualization fea-
tures. Though the existing visualization tools provide abun-
dant illustrations for the scalar plotting, NSML is still unable
to express in a various media including visualization for the
complicated concepts, audio/video output, embedding layer,
feature maps, and any other complex representations in an
easy way.
In addition, we are implementing the distributed learning
feature on NSML, which will provide the parallelism over
multiple nodes. With this feature, a user can deploy a gigan-
tic dataset or model which are difﬁcult to load into a single
machine. We expect it to enhance the overall system utiliza-
tion and relief the lack of resources caused by fragmented
resource allocation.
6 C ONCLUSION
As the application of machine learning is tightly coupled
with industry and commercial services, the efﬁcient devel-
oping environments and its infrastructure become more im-
portant for large scale clusters which many companies and
organizations already owned. In this paper, we proposed
NSML as a novel platform for machine learning as a service
(MLaaS), and demonstrated its potential with some exam-
ples and use cases. NSML also provides comfortable tools
and a serverless foundation to users as well as collaborative
environments. With several competitions on NSML, espe-
cially, we convinced that NSML can be a key player for
commercialization of machine learning models.
REFERENCES
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,
J., Devin, M., Ghemawat, S., Irving, G., Isard, M., Kud-
lur, M., Levenberg, J., Monga, R., Moore, S., Murray,
D. G., Steiner, B., Tucker, P., Vasudevan, V ., Warden,
P., Wicke, M., Yu, Y ., and Zheng, X. Tensorﬂow: A
system for large-scale machine learning. In Proceed-
ings of the 12th USENIX Conference on Operating Sys-
tems Design and Implementation , OSDI’16, pp. 265–283,
Berkeley, CA, USA, 2016. USENIX Association. ISBN
978-1-931971-33-1. URL http://dl.acm.org/
citation.cfm?id=3026877.3026899 .
Armbrust, M., Fox, A., Grifﬁth, R., Joseph, A. D., Katz, R.,
Konwinski, A., Lee, G., Patterson, D., Rabkin, A., Stoica,
I., et al. A view of cloud computing. Communications of
the ACM , 53(4):50–58, 2010.
Baylor, D., Breck, E., Cheng, H.-T., Fiedel, N., Foo, C. Y .,Haque, Z., Haykal, S., Ispir, M., Jain, V ., Koc, L., et al.
Tfx: A tensorﬂow-based production-scale machine learn-
ing platform. In Proceedings of the 23rd ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining , pp. 1387–1395. ACM, 2017.
Boettiger, C. An introduction to docker for reproducible
research. SIGOPS Oper. Syst. Rev. , 49(1):71–79, Jan-
uary 2015. ISSN 0163-5980. doi: 10.1145/2723872.
2723882. URL http://doi.acm.org/10.1145/
2723872.2723882 .
Chhajed, S. Learning ELK Stack . Packt Publishing Ltd,
2015.
Chinchor, N. Muc-4 evaluation metrics. In Proceedings of
the 4th Conference on Message Understanding , MUC4
’92, pp. 22–29, Stroudsburg, PA, USA, 1992. Association
for Computational Linguistics. ISBN 1-55860-273-9.
doi: 10.3115/1072064.1072067. URL https://doi.
org/10.3115/1072064.1072067 .
Copeland, M., Soh, J., Puca, A., Manning, M., and Gol-
lob, D. Microsoft Azure: Planning, Deploying, and
Managing Your Data Center in the Cloud . Apress,
Berkely, CA, USA, 1st edition, 2015. ISBN 1484210441,
9781484210444.
Diginmotion. Amazon web services. 2011.
Donovan, A. A. and Kernighan, B. W. The Go programming
language . Addison-Wesley Professional, 2015.
Felter, W., Ferreira, A., Rajamony, R., and Rubio, J. An
updated performance comparison of virtual machines and
linux containers. In Performance Analysis of Systems and
Software (ISPASS), 2015 IEEE International Symposium
On, pp. 171–172. IEEE, 2015.
Girija, S. S. Tensorﬂow: Large-scale machine learning on
heterogeneous distributed systems. 2016.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual
learning for image recognition. In 2016 IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2016,
Las Vegas, NV , USA, June 27-30, 2016 , pp. 770–778,
2016. doi: 10.1109/CVPR.2016.90. URL https://
doi.org/10.1109/CVPR.2016.90 .
Helsley, M. Lxc: Linux container tools. IBM devloperWorks
Technical Library , 11, 2009.
Hightower, K., Burns, B., and Beda, J. Kubernetes: Up and
Running Dive into the Future of Infrastructure . O’Reilly
Media, Inc., 1st edition, 2017. ISBN 1491935677,
9781491935675.
NSML: Meet the MLaaS platform with a real-world case study
Jaderberg, M., Dalibard, V ., Osindero, S., Czarnecki, W. M.,
Donahue, J., Razavi, A., Vinyals, O., Green, T., Dunning,
I., Simonyan, K., et al. Population based training of neural
networks. arXiv preprint arXiv:1711.09846 , 2017.
Krishnan, S. T. and Gonzalez, J. U. Building Your Next Big
Thing with Google Cloud Platform: A Guide for Develop-
ers and Enterprise Architects . Apress, Berkely, CA, USA,
1st edition, 2015. ISBN 1484210050, 9781484210055.
Krizhevsky, A. and Hinton, G. Learning multiple layers
of features from tiny images. Technical report, Citeseer,
2009.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classiﬁcation with deep convolutional neural networks.
InAdvances in neural information processing systems ,
pp. 1097–1105, 2012.
LeCun, Y . and Cortes, C. MNIST handwritten digit
database. 2010. URL http://yann.lecun.com/
exdb/mnist/ .
LeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE , 86(11):2278–2324, November 1998.
Lehmann, E. L. and Casella, G. Theory of Point Estimation .
Springer-Verlag, New York, NY , USA, second edition,
1998.
Liang, D., Krishnan, R. G., Hoffman, M. D., and Jebara, T.
Variational autoencoders for collaborative ﬁltering. arXiv
preprint arXiv:1802.05814 , 2018.
Marcel, S. and Rodriguez, Y . Torchvision the machine-
vision package of torch. In Proceedings of the
18th ACM International Conference on Multimedia ,
MM ’10, pp. 1485–1488, New York, NY , USA, 2010.
ACM. ISBN 978-1-60558-933-6. doi: 10.1145/1873951.
1874254. URL http://doi.acm.org/10.1145/
1873951.1874254 .
Merkel, D. Docker: Lightweight linux containers for consis-
tent development and deployment. Linux J. , 2014(239),
March 2014. ISSN 1075-3583. URL http://dl.acm.
org/citation.cfm?id=2600239.2600241 .
Min, S., Lee, B., and Yoon, S. Deep learning in bioinformat-
ics.Brieﬁngs in bioinformatics , 18(5):851–869, 2017.
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,
DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,
A. Automatic differentiation in pytorch. In NIPS-W ,
2017.
Reitz, K. and Schlusser, T. The Hitchhiker’s Guide to
Python: Best Practices for Development . ” O’Reilly Me-
dia, Inc.”, 2016.Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,
M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale
Visual Recognition Challenge. International Journal of
Computer Vision (IJCV) , 115(3):211–252, 2015. doi:
10.1007/s11263-015-0816-y.
Xiong, W., Wu, L., Alleva, F., Droppo, J., Huang, X., and
Stolcke, A. The microsoft 2017 conversational speech
recognition system. In 2018 IEEE International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP) , pp. 5934–5938. IEEE, 2018.
Yamada, Y ., Iwamura, M., and Kise, K. Shakedrop regular-
ization. arXiv preprint arXiv:1802.02375 , 2018.
Yu, A. W., Dohan, D., Luong, M.-T., Zhao, R., Chen, K.,
Norouzi, M., and Le, Q. V . Qanet: Combining local
convolution with global self-attention for reading com-
prehension. arXiv preprint arXiv:1804.09541 , 2018.
Zhong, Z., Zheng, L., Kang, G., Li, S., and Yang, Y .
Random erasing data augmentation. arXiv preprint
arXiv:1708.04896 , 2017."
2109.06363,D:\Database\arxiv\papers\2109.06363.pdf,"While the paper explores the vulnerability of sensor fusion models to adversarial attacks, it also suggests that the use of a secondary input, like an image, can be exploited to override the primary sensor's data.  What are the implications of this finding for the design and implementation of robust sensor fusion systems in real-world applications, particularly in safety-critical domains like autonomous driving?","This finding highlights the importance of considering not only the robustness of individual sensors but also the potential vulnerabilities arising from their integration.  Robust sensor fusion systems must be designed to mitigate the risks of adversarial manipulation of secondary inputs, especially in safety-critical applications where even a single compromised sensor could lead to catastrophic consequences.","Type Disappearance Spoof
Baseline 0.94 0.89
Distorted Inputs 0.92 0.85
MaxSSN [7] 0.87 0.81
MaxSSN + LEL [7] 0.80 0.39
Adversarial Training 0.63 0.51
Table 1 . Table showing the success rate (in %) of our attacks
against various defenses.
we consider an object as ”correctly identiﬁed” if the bound-
ing box was correctly drawn according to the LIDAR sensor
1. Amongst all the potential bounding boxes, 91% were cor-
rectly identiﬁed, despite having conﬂicting image data. Fur-
thermore, only 19% of all bounding boxes detected did not
correspond to any ground truth bounding box.
This experiment strongly suggests that the model favors
LIDAR data when detecting objects, which helps explain the
difﬁculty in the spooﬁng attacks. For instance, when we com-
pare the L2 norm per pixel, we ﬁnd that the spooﬁng attack
requires much more distortion: while the disappearance at-
tack required a median per-pixel distortion of roughly 0.28 in
L2 space, the spooﬁng attack required a L2 distortion of over
1.3 in L2 space. This is line with a previous work that found
another sensor fusion model, MV3D, also favors LIDAR [2].
While this is contrary to what was found in Wang et al [10],
we believe this is because their lack of a true sensor fusion
model built from the ground up. This also suggests that the
use of image in this architecture proves to be an ”Achilles’
heel”: while most of the detection of an object is done using
the LIDAR input, it is not sufﬁcient, as the image provides a
way for adversaries to override this and attack the model.
5. EXPLORING DEFENSES
In this section we analyze some potential defenses against
adversarial examples on sensor fusion models. For each de-
fense, we test our raw-pixel disappearance attack and our
spooﬁng attack on the same vehicle samples as in the evalua-
tion for our attacks. All results can be seen in Table 1.
We ﬁrst attempt to address a belief that training on a wider
range of inputs (like fog and snow) will help mitigate adver-
sarial examples. We train an instance of A VOD on an aug-
mented version of the dataset in which we apply all distor-
tions as suggested by Hendryks and Dietterich [21] to every
training image. While training on various input distortions
may be important for safe performance of A Vs, we ﬁnd that it
does not eliminate the threat against adversarial examples.
We next analyze methodologies proposed by Kim and
Ghosh [7] that provide robustness against single-source dis-
tortion in sensor fusion models. We refer readers to the
1Note that we could have easily swapped and used the image input as
”ground truth” but that would not make any change to the ﬁnal resultpaper for details, but in short, the authors propose novel
loss functions and a new fusion layer called LEL to protect
against noisy distortion. We test the two designs proposed
that achieved the best performance under noise: using the
new loss function called MaxSSN with and without LEL. We
train both models according to the speciﬁcations shown in the
paper and achieve a similarly stated accuracy. We ﬁnd that
the models do mitigate against our disappearance attacks, but
only to a success rate of around 80%. However, the addition
of the LEL does better in defending against spooﬁng attacks.
Unfortunately, it is worth noting that there exists a trade-off
as both models suffer in a drop of AP score compared to the
original model when run on benign inputs.
A popular methodology to protect against adversarial ex-
amples is adversarial training. We utilize a preliminary tech-
nique proposed by Zhang and Wang [22]. We ﬁnd that the
AP score drops after adversarial training, but so does the suc-
cess rate of the adversarial attacks drop as well; the raw-pixel
disappearance attack drops to a 63% effectiveness while the
spooﬁng attack drops to a 51% success rate. While these re-
sults are not ideal, they do not necessarily eliminate adversar-
ial training as a viable option to provide robustness. On the
contrary, they demonstrate that a better adversarial training
algorithm may be able to provide robustness. We leave this
exploration to future work. We also believe another avenue
to explore is incorporating defenses outside the model. In a
larger system, it could be feasible to utilize the different sen-
sors, for example, to validate one another before feeding into
the ﬁnal detection model. We leave these possible defenses
for future work to explore.
6. CONCLUSION
In this paper we explore a fusion model’s security against ad-
versarial examples. We discover that the use of a secondary
input provides limited defense against a myriad of different
adversarial attacks. Though we only evaluate on one model
due to the lack of availability of open-source models, our eval-
uation on alternative fusion layers and training loss functions
suggest that other models may also be vulnerable to single
image attacks. We urge future works on sensor fusion models
to help increase robustness.
7. ACKNOWLEDGEMENTS
We thank our reviewers for their comments. We also wish to
thank Fahad Kamran, Jiachen Sun, Yulong Cao for their help
and insights. This project is partially supported by Mcity and
NSF under the grants CMMI-2038215, CNS-1930041, CCF-
1628991, CNS-1544678, CNS-1850533, CNS-1929771, and
CNS-1932464.
8. REFERENCES
[1] Yue Zhao, Hong Zhu, Ruigang Liang, Qintao Shen,
Shengzhi Zhang, and Kai Chen, “Seeing isn’t believ-
ing: Towards more robust adversarial attack against real
world object detectors,” in Proceedings of the 2019
ACM SIGSAC Conference on Computer and Communi-
cations Security , New York, NY , USA, 2019, CCS ’19,
p. 1989–2004, Association for Computing Machinery.
[2] Jiachen Sun, Yulong Cao, Qi Alfred Chen, and Z. Mor-
ley Mao, “Towards robust lidar-based perception in au-
tonomous driving: General black-box adversarial sensor
attack and countermeasures,” in 29th USENIX Security
Symposium (USENIX Security 20) . Aug. 2020, pp. 877–
894, USENIX Association.
[3] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou,
Lingxi Xie, and Alan Yuille, “Adversarial examples
for semantic segmentation and object detection,” 2017
IEEE International Conference on Computer Vision
(ICCV) , Oct 2017.
[4] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes,
Bo Li, Amir Rahmati, Florian Tram `er, Atul Prakash,
Tadayoshi Kohno, and Dawn Song, “Physical adver-
sarial examples for object detectors,” in Proceedings
of the 12th USENIX Conference on Offensive Technolo-
gies, USA, 2018, WOOT’18, p. 1, USENIX Associa-
tion.
[5] Lifeng Huang, Chengying Gao, Yuyin Zhou, Changqing
Zou, Cihang Xie, Alan Yuille, and Ning Liu, “Upc:
Learning universal physical camouﬂage attacks on ob-
ject detectors,” 2019.
[6] S. Thys, W. V . Ranst, and T. Goedem ´e, “Fooling auto-
mated surveillance cameras: Adversarial patches to at-
tack person detection,” in 2019 IEEE/CVF Conference
on Computer Vision and Pattern Recognition Workshops
(CVPRW) , 2019, pp. 49–55.
[7] Taewan Kim and Joydeep Ghosh, “On single source
robustness in deep fusion models,” in NeurIPS , 2019.
[8] Charles R. Qi, Wei Liu, Chenxia Wu, Hao Su, and
Leonidas J. Guibas, “Frustum pointnets for 3d object
detection from rgb-d data,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , June 2018.
[9] Jason Ku, Melissa Moziﬁan, Jungwook Lee, Ali
Harakeh, and Steven Lake Waslander, “Joint 3d pro-
posal generation and object detection from view aggre-
gation,” CoRR , vol. abs/1712.02294, 2017.
[10] Shaojie Wang, Tong Wu, and Yevgeniy V orobeychik,
“Towards robust sensor fusion in visual perception,”
2020.[11] Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng
Zhou, Won Park, Sara Rampazzi, Qi Alfred Chen, Kevin
Fu, and Zhuoqing Morley Mao, “Adversarial Sen-
sor Attack on LiDAR-based Perception in Autonomous
Driving,” in Proceedings of the 26th ACM Conference
on Computer and Communications Security (CCS’19) ,
London, UK, November 2019.
[12] Yuxin Wen, Jiehong Lin, Ke Chen, and Kui Jia,
“Geometry-aware generation of adversarial and cooper-
ative point clouds,” CoRR , vol. abs/1912.11171, 2019.
[13] James Tu, Mengye Ren, Sivabalan Manivasagam, Ming
Liang, Bin Yang, Richard Du, Frank Cheng, and Raquel
Urtasun, “Physically realizable adversarial examples for
lidar object detection,” in IEEE CVPR , June 2020.
[14] Nicholas Carlini and David Wagner, “Towards evalu-
ating the robustness of neural networks,” 2017 IEEE
Symposium on Security and Privacy (SP) , May 2017.
[15] Yi Huang, Adams Wai Kin Kong, and Kwok-Yan Lam,
“Adversarial signboard against object detector,” in
British Machine Vision Conference , 2019.
[16] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian
Xia, “Multi-view 3d object detection network for au-
tonomous driving,” in IEEE CVPR , 2017.
[17] Andreas Geiger, Philip Lenz, Christoph Stiller, and
Raquel Urtasun, “Vision meets robotics: The kitti
dataset,” International Journal of Robotics Research
(IJRR) , 2013.
[18] Anish Athalye, Logan Engstrom, Andrew Ilyas, and
Kevin Kwok, “Synthesizing robust adversarial exam-
ples,” 2017.
[19] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
Omar Fawzi, and P. Frossard, “Universal adversarial
perturbations,” 2017 IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pp. 86–94, 2017.
[20] Konda Reddy Mopuri, Aditya Ganeshan, and R. Babu,
“Generalizable data-free objective for crafting univer-
sal adversarial perturbations,” IEEE Transactions on
Pattern Analysis and Machine Intelligence , vol. 41, 01
2018.
[21] Dan Hendrycks and Thomas Dietterich, “Benchmarking
neural network robustness to common corruptions and
perturbations,” in International Conference on Learning
Representations , 2019.
[22] H. Zhang and J. Wang, “Towards adversarially robust
object detection,” in ICCV , 2019, pp. 421–430."
