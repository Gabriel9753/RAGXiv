{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 411, which is longer than the specified 300\n",
      "Created a chunk of size 301, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "loader = TextLoader(\"catbank.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS expects a file_name dict as key\n",
    "for document in chunks:\n",
    "    document.metadata['file_name'] = document.metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'catbank.txt', 'file_name': 'catbank.txt'}, page_content='In the heart of the bustling city of Whiskerville, there stood a peculiar establishment unlike any other - the Feline Financial Bank, a bank exclusively for cats. This extraordinary institution was founded by Sir Purrington, a visionary Siamese cat who believed that every feline deserved a place to secure their precious belongings - be it their beloved toys, treasured catnip, or the much-coveted tuna treats.'),\n",
       " Document(metadata={'source': 'catbank.txt', 'file_name': 'catbank.txt'}, page_content='The architecture of the bank was a marvel in itself. Crafted with sleek lines and comfortable lounging spots, it blended elegance with cat-friendly design. The entrance featured a grand revolving door, sized perfectly for all breeds, from the majestic Maine Coons to the dainty Munchkins.'),\n",
       " Document(metadata={'source': 'catbank.txt', 'file_name': 'catbank.txt'}, page_content='Inside, the bank was a haven of tranquility and order. The tellers, all of whom were sophisticated cats in dapper uniforms, managed transactions with a graceful efficiency. They offered services ranging from savings accounts for future vet visits to investment portfolios in promising catnip startups.'),\n",
       " Document(metadata={'source': 'catbank.txt', 'file_name': 'catbank.txt'}, page_content='The most popular feature was the high-security vault. Guarded by the bravest of feline warriors, it was said to be impenetrable. Here, cats deposited their most valued possessions, resting easy with the knowledge that their treasures were safe.'),\n",
       " Document(metadata={'source': 'catbank.txt', 'file_name': 'catbank.txt'}, page_content='Feline Financial Bank was not just a bank; it was a symbol of feline independence and sophistication, a testament to what cats could achieve when they dared to dream big.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\workspace\\arxplore2\\src\\test_sub\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "# from ragas.testset.generator import TestsetGenerator\n",
    "# from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "llm = ChatOpenAI(openai_api_base=\"http://localhost:5000/v1\", openai_api_key=\"lm-studio\")\n",
    "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/allenai-specter\", model_kwargs={\"device\": \"cpu\"})\n",
    "\n",
    "# generator = TestsetGenerator.from_langchain(\n",
    "#     generator_llm=llm,\n",
    "#     critic_llm=llm,\n",
    "#     embeddings=emb,\n",
    "# )\n",
    "\n",
    "# testset = generator.generate_with_langchain_docs(chunks, test_size=1, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25}, raise_exceptions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(chunks, emb)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\",\"question\"]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.runnables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnablePassthrough\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n\u001b[0;32m      4\u001b[0m rag_chain \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      5\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: retriever,  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: RunnablePassthrough()}\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;241m|\u001b[39m prompt\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m|\u001b[39m llm\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain.runnables'"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.output_parser import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Simulate a testset with arXiv-related questions and ground truths\n",
    "testset_data = {\n",
    "    \"question\": [\n",
    "        \"What is the main contribution of the paper on transformer models?\",\n",
    "        \"How does the paper address the vanishing gradient problem?\",\n",
    "        \"What dataset was used in the GAN paper?\",\n",
    "        \"Can you summarize the results of the paper on quantum computing?\",\n",
    "        \"What is the key finding in the latest NLP paper?\",\n",
    "        \"How does the proposed method in the reinforcement learning paper work?\",\n",
    "        \"What optimization techniques were used in the neural network paper?\",\n",
    "        \"What are the future research directions mentioned in the computer vision paper?\",\n",
    "        \"What are the applications of the proposed model in the graph networks paper?\",\n",
    "        \"How does the paper on unsupervised learning differ from traditional methods?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "        \"The paper proposes a new architecture for transformers that improves efficiency in training large models.\",\n",
    "        \"The paper addresses the vanishing gradient problem by introducing residual connections.\",\n",
    "        \"The paper used the CIFAR-10 dataset for training the GAN model.\",\n",
    "        \"The paper shows promising results in quantum error correction using a new approach.\",\n",
    "        \"The key finding is the improvement of language model performance by using novel training techniques.\",\n",
    "        \"The proposed method is a deep Q-learning algorithm with additional exploration mechanisms.\",\n",
    "        \"Adam optimizer and batch normalization were primarily used in the paper.\",\n",
    "        \"Future research directions include extending the method to handle multimodal data.\",\n",
    "        \"The proposed model has applications in social network analysis and biological systems.\",\n",
    "        \"The paper introduces self-supervised learning, which differs by not requiring labeled data.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the testset\n",
    "testset = Dataset.from_dict(testset_data)\n",
    "\n",
    "# Prepare the questions and ground truths\n",
    "questions = testset.to_pandas()[\"question\"].to_list()\n",
    "ground_truth = testset.to_pandas()[\"ground_truth\"].to_list()\n",
    "\n",
    "# Initialize an empty data dictionary for the new dataset\n",
    "data = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": ground_truth}\n",
    "\n",
    "# Simulate RAG process: querying and retrieving documents\n",
    "for query in questions:\n",
    "    # Mock RAG chain and retriever responses (replace with actual RAG chain and retriever)\n",
    "    mock_answer = \"This is a mock answer for query: \" + query  # Simulate answer from RAG\n",
    "    mock_context = [\"This is a context for the query: \" + query]  # Simulate context from retriever\n",
    "    \n",
    "    # Append the results to the data dictionary\n",
    "    data[\"question\"].append(query)\n",
    "    data[\"answer\"].append(mock_answer)\n",
    "    data[\"contexts\"].append(mock_context)\n",
    "\n",
    "# Create a dataset from the dictionary\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Output the generated dataset (for display or further evaluation)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset,\n",
    "    metrics=[\n",
    "        context_relevancy,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "df = result.to_pandas()\n",
    "\n",
    "heatmap_data = df[['context_relevancy', 'context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']]\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list('green_red', ['red', 'green'])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", linewidths=.5, cmap=cmap)\n",
    "\n",
    "plt.yticks(ticks=range(len(df['question'])), labels=df['question'], rotation=0)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add LangFuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse = Langfuse(\n",
    "  secret_key=\"sk-lf-8be80c67-4187-4e43-9d01-544195dc9f03\",\n",
    "  public_key=\"pk-lf-d7653f64-8086-4365-b05c-865ead3478a3\",\n",
    "  host=\"http://localhost:3000\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = langfuse.trace(\n",
    "    name = \"eval\",\n",
    "    user_id = \"eval_user\",\n",
    "    metadata = {\n",
    "        \"email\": \"prod@company.com\",\n",
    "    },\n",
    "    tags = [\"evaluation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.iterrows():\n",
    "    for metric_name in [\"faithfulness\", \"answer_relevancy\", \"context_recall\"]:\n",
    "        langfuse.score(\n",
    "            name=metric_name,\n",
    "            value=row[metric_name],\n",
    "            trace_id=trace.id\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
